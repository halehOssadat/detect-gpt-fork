{"name": "perturbation_10_z", "predictions": {"real": [0.0, 0.0, 0.02844345563856701, 0.0, 0.0, 0.0, 0.0, -0.02407871858485192, 0.25036286942893593, -0.27845699555517484, 0.0, 0.15607746564861041, -0.4796460212018151, 0.0, 0.0, 0.08401530783639578, 0.2530562944869558, -0.27945520397385337, 0.0, 0.05155568290068203, -0.02319023555969862, 0.0, 0.0, 0.0, 0.0, 0.607614741252289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06879104856296789, 0.7013825685867163, 0.0, 0.7151364245947098, 0.0, -0.4922945116162711, 0.3066868851553877, 0.0, -0.06826127778043665, 0.0, 0.0, -0.5089176123376787, 0.47110457446084203, 0.0, 1.1264619571309409, 0.0, 0.0, -0.12270532364098123, 0.0, 0.1484455762827501, 0.0, 0.35733843308166174, 0.48603619045781266, 0.14298121835511723, 0.25115621331020055, 0.0, 0.0, 0.0, 0.1812939818690163, 0.0, 0.0, 0.9953321623938993, 0.0, 0.3626334981518834, 0.48564042904496385, 0.035170492091754255, 0.0, 0.0, 0.05328209948206889, 0.0, 0.10913360020843953, 0.0, 0.0, 0.0, 0.0, 0.288353532622201, 0.0, 0.0, -1.006014671956803, 0.0, 0.0, 0.0, 0.0, -0.029668868102010453, 0.2092065646270567, 0.0, 0.03653715774542994, 0.44868497732511453, 0.0, 0.45381329892369004, 0.0, 0.0, 0.6939902191803882, 0.0, -0.4443563659899724, 0.0, 0.0, 0.4980025101464767, 0.0, 0.0, 0.0, 0.0, -0.6695376604902938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6364862119607505, 0.0, 0.0, -0.5353512695175766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24288170890097957, 0.0, 0.007479971049892975, 0.0, 0.0, 0.0, 0.0, 0.0, -0.41041039108939714, 0.0, 0.0, 0.0, 0.012019549390870885, 0.0, 0.3832222599798222, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3193162732374901, 0.8062991424012474, 0.0, 0.0, 0.0, 0.6160704054437257, 0.0, 0.15109802827520377, 0.0, 0.0, -0.16556842063904487, 0.0, 0.0, 0.7752230727508824, 0.15429746740200698, -0.2650772849133726, 0.5614782710877301, 0.3942727677606725, 0.1920319328281568, 0.19925346157897514, -0.48148402204156504, 0.0, 0.38479379009558184, -0.820258549523362, 0.03233292947045583, 0.0, 0.5309562445964353, -0.1250348404051405, 0.0, 0.0, -0.41586191645455395, 0.47665946815875104, 0.7238381085041092, -0.42821060210919604, 0.0, 0.5036348155130413, -0.37755612987457393, 0.33251000921647, 0.0, -0.20494634804142572, 0.0, -0.8485959022168099, 0.0, 0.5962584539266776, 0.2343519734874049, 0.0, 0.0, 0.24336167433699193, 0.2708811269113493, -0.7437208057608639, 0.0, 0.0, 0.280941793516297, -0.1813921183278362, 0.0, 0.0, 0.0, -0.17976035048857916, 0.0, 0.0, 0.07130508516657, 0.0, 0.0, 0.0, 0.4697101817794167, 0.0, 0.0, 0.0, 1.6499173600900634, 0.6454295179084479, 0.8473731921527919, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9582463726308614, 0.0, 0.0, 0.0, 0.0, 0.5584409831670042, -0.4349639940325067, 0.0, 0.13760153679350223, 0.0, -0.3050004007576854, 0.0, 1.8845837440863094, 0.0, 0.0, 0.0, 0.18982170892920933, 0.535007920226995, -0.02254709663075792, -0.0026776385226868686, 0.002185113678545852, 0.0, 0.0, 0.0, 0.4552607350786814, 0.03020775726375163, 1.142637845305499, -0.3921324650962102, 0.0, -0.47224571748409483, 0.6471687992531178, 0.5916722028504956, -0.08817853824666691, 0.0, 0.0, -0.23608987005203172, 0.0, 0.0, -0.13475430885720924, 0.060806480288412325, -0.8621529206454407, 0.0, 0.1680653618065075, 0.0, 0.0, -0.34871221827780263, 0.0, 0.3077220993174166, 0.28970791706397875, 0.0, 0.28465109276409317, -0.6711702931032284, 0.0, 0.0, 0.0, 0.0, 0.23063755464390756, -0.20332532568414757, 1.3379525270539203, 0.0, -0.17487722426062996, 0.0, 0.0, 0.0, 0.5062644070041544, 0.0, 0.0, -0.4915127044939631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34189987177678427, -0.5928688126863324, 0.0, 0.0, 0.007871892021931478, 0.0, -0.1893418275218987, 0.8770794235705124, 0.44671097111152347, 0.0, 0.3368667162221346, 0.14107262851041946, -0.009758531279751546, 0.14761174979815495, 0.0, 0.0, -0.4215802799671599, 0.0, -0.3537530731490697, -0.07323616458520718, -0.6215406446292862, 0.0, -0.51235011239388, 0.16066406137607425, 0.19048517407438176, 0.0, 0.0, -0.5999169410436261, 0.0, -0.03864251610017206, 1.7250853981182945, 0.0, -0.9173941680872639, 0.0, 0.38200151227907375, 0.0, 0.0, 0.0, 0.37031698398467355, 1.642259104752719, 0.0, 0.0, 0.0, 0.0, 0.0, -0.18177594936078398, 0.0, 0.0, -0.45205478588329634, 0.0, 0.0, 0.3541447321997387, 0.0, 0.056839419899783014, 0.0, 0.0, -0.13735946586209852, 0.38675611605304827, 0.0, 0.0, 0.0, 0.0, 0.025301632054938964, 0.0, 0.0, -0.7117253483448865, 0.0, -0.6996229309117001, 0.7413777495099033, -0.11042159514955524, 0.0, 0.0, 0.0, -0.10393218232608456, -0.31759210779016095, -0.8412501178222064, 0.0, 0.0, -0.11073683701611874, 0.0, 0.0, 0.0, -0.30467418979667416, 0.0, 0.0, 0.0, 0.0, -0.6569576338704106, -0.10952699784852883, 0.0, 0.6286343150304419, 0.34976939675537083, -0.23593698855346587, 0.0, 0.9191497783720443, -0.3144914474079187, 0.0, -0.29920433234114624, 0.0, 0.0, 0.6727709599471188, 0.0, 0.0, 0.0, 0.0, 0.3265682236453809, 0.0, -0.40354636374144237, 0.0, 0.1505304667013488, 0.0, 0.0, 0.0, 1.704916398785069, -0.5456512100657834, 0.38531198429049984, 0.0, 0.41425978384000245, 0.2516490018624242, 0.020201148196293726, 0.3958886041003846, -1.2109299494942545, -0.03393898786050367, 0.8433181678177808, 0.0, 0.0, 0.0, 0.06355879834609718, 0.0, 0.0, 0.0, 0.0, 0.06021259021981395, 0.0, -0.03601414958084186, -0.6209954775498137, 0.0, -0.3246776284167357, 0.0, 0.0, 0.8550352903698155, 0.0, 0.0, 0.0, 0.14200443372171603, 0.0, 0.0, 0.0, 0.0, 0.19981787971022325, 0.0, 0.0, 0.09023898485459221, 0.0, 0.0, 0.0, 0.0, 0.5864023704771003, 0.0, 0.0, 0.14546188302449986, 0.0, 0.0, 0.0, 0.5405069061197321, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6418922623347438, 0.12677446675642956, 0.7090590905622377, 0.8730831723524366, 0.0, 0.0, 0.0, 0.9801723387880732, 0.0, 0.0, 0.0, 0.09406919128357354, 0.544369955342471, 0.6031373482147107, 0.0, 0.8712564420786959, 0.0, -0.16843901936685615, 0.0, -0.4210404042440653, 0.6662336517647741, 0.25108023118787176, 0.7854315149607037, 0.0, 0.0, 0.0, -0.4630817626267613, 0.0, -0.06877514050241786, 1.458441525336324, -0.9649533389248187, 0.0, 0.0, 0.0, 0.0, -0.3822500801102816, -0.51007088674435], "samples": [0.0, 0.0, -0.18506947050372635, 0.0, 0.0, 0.0, 0.0, -0.04176940022345547, 0.45417317443156596, 0.35675080097179074, 0.0, 0.49066086527162145, 0.925989059220677, 0.0, 0.0, 1.2083895001104372, 1.9671335228883229, -0.35117732203182106, 0.0, 0.7892793307349796, -0.20246842406445545, 0.0, 0.0, 0.0, 0.0, 0.845451913126948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21594995967807779, 1.4984160894913174, 0.0, 1.7658397477041805, 0.0, 0.3579946096615111, 0.7947406009720004, 0.0, 0.6090208105824559, 0.0, 0.0, 1.1245840262273585, 0.5241041640334111, 0.0, 1.5020857426289786, 0.0, 0.0, 0.3969267858101204, 0.0, 0.5495868292292807, 0.0, 0.1527674919272646, 0.19916504798736784, 0.21441624587203115, 0.1905541774282389, 0.0, 0.0, 0.0, 0.8798679687618061, 0.0, 0.0, -0.826252934529843, 0.0, 0.5141241610205023, 0.5865209531371047, -0.3763836392277184, 0.0, 0.0, 0.4943364260404211, 0.0, 0.23125815316933743, 0.0, 0.0, 0.0, 0.0, 0.8493953024723184, 0.0, 0.0, 0.8147532011828075, 0.0, 0.0, 0.0, 0.0, 0.8697013808717277, 0.5858273264091125, 0.0, 0.4082784813577337, 1.478772539263926, 0.0, 0.4565008936183089, 0.0, 0.0, 0.6543589019537426, 0.0, -0.19537846417581353, 0.0, 0.0, 0.5426327558633982, 0.0, 0.0, 0.0, 0.0, 0.35251979154430924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.760393531788492, 0.0, 0.0, 0.03667240332876221, 0.0, 0.0, 0.0, 0.0, 0.0, -0.12927246441978676, 0.0, -0.00017822003020160796, 0.0, 0.0, 0.0, 0.0, 0.0, 2.007338177274347, 0.0, 0.0, 0.0, 1.0428095112140907, 0.0, 0.6575808079582546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21674681716853195, 0.8660676627428058, 0.0, 0.0, 0.0, 0.6348810990389864, 0.0, 0.6103227321480433, 0.0, 0.0, -0.1800173851927292, 0.0, 0.0, 0.7107873893378482, -0.15174281582101334, 0.19001838012999997, 3.372873302291766, -0.016494625422989007, 1.1082208976391619, 0.5187011067040391, 1.0349781848621356, 0.0, 0.8937634053459181, 1.1551663495330482, -0.3739681258092256, 0.0, 0.9698098212183993, 0.7684668549610273, 0.0, 0.0, 1.5075993416020745, 1.3898568312334039, -0.4687627883421384, 1.5005577948432764, 0.0, 0.6596577226035523, 0.09655864451075086, 0.3609689990489746, 0.0, -0.7629704928211916, 0.0, 1.5584366356470216, 0.0, 0.9368690379504595, 0.32060452708563036, 0.0, 0.0, 0.42983481796259276, 1.4494732367668506, -0.7290094178635594, 0.0, 0.0, 0.27240781980862216, 0.7681790607022763, 0.0, 0.0, 0.0, 1.3711717094670854, 0.0, 0.0, 0.18731877673666353, 0.0, 0.0, 0.0, 1.2033580838710618, 0.0, 0.0, 0.0, 1.358216367069415, 1.2214066571736977, 1.0641757289253158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003139948445329115, 0.0, 0.0, 0.0, 0.0, 0.8633070630856351, 0.14100848985337622, 0.0, 0.7041682574389209, 0.0, -0.5535897389641172, 0.0, 1.6363713152963237, 0.0, 0.0, 0.0, 0.9399039216929859, 0.4155495408029722, 1.3915776768929116, 0.42806499935407094, 2.05346432533246, 0.0, 0.0, 0.0, 1.3738592474147884, 0.5908095759895259, 0.08397555966261165, -0.17940162770384716, 0.0, 0.20186748463340543, 0.9999611234472293, 0.5179617917280815, 0.0742454158372327, 0.0, 0.0, -0.33980208634013837, 0.0, 0.0, 1.0232993399212027, 0.7442154874089273, 0.3776287369674437, 0.0, 0.7867350602705899, 0.0, 0.0, 1.1181709180843045, 0.0, 0.3797054479004092, 0.4357803633116193, 0.0, -0.24431995459055852, 0.14216827904815174, 0.0, 0.0, 0.0, 0.0, 0.9947583571552917, 1.4852017740955856, 2.0369732746373077, 0.0, 1.201094917989661, 0.0, 0.0, 0.0, 0.45826456747036576, 0.0, 0.0, 0.06599935843229351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32055078206841453, 2.3320429503337206, 0.0, 0.0, 0.19010565987238953, 0.0, 1.7654204936814017, 0.9854033909499952, 1.0928637308984575, 0.0, 0.41905011274148996, 0.6656507002493766, -0.8340371476378544, 0.39093189604375683, 0.0, 0.0, 1.2382831943044188, 0.0, 1.1703814663554204, -0.012917040587684139, 0.3553802917803994, 0.0, 0.728058286093071, 0.5839022488173391, 0.50383781774621, 0.0, 0.0, 0.029054549742833165, 0.0, 0.9263652753390346, 1.2544133301940388, 0.0, -0.48236689498019436, 0.0, 0.2351479844908079, 0.0, 0.0, 0.0, 0.015595897166687387, 1.4378188594233796, 0.0, 0.0, 0.0, 0.0, 0.0, 1.135087371619299, 0.0, 0.0, -0.6272045778798698, 0.0, 0.0, 0.9304977181321047, 0.0, 0.9452423487847955, 0.0, 0.0, -0.3765809791564706, 0.4991700673896281, 0.0, 0.0, 0.0, 0.0, 1.05866804861224, 0.0, 0.0, 0.6399686830725019, 0.0, 1.472113591046534, 0.7362138145487417, 0.5043995070217226, 0.0, 0.0, 0.0, -0.08436559070434851, 0.04814379869233326, -0.014457021158599821, 0.0, 0.0, 0.42422910508233275, 0.0, 0.0, 0.0, 0.49526415597166645, 0.0, 0.0, 0.0, 0.0, 1.4752993496592375, 0.8191382069242598, 0.0, 1.4157350534241713, 0.7202668742437854, 0.40916247841468445, 0.0, -0.41397900207073834, 0.11193384722189448, 0.0, 1.0041558354891915, 0.0, 0.0, 0.6653745806532957, 0.0, 0.0, 0.0, 0.0, -0.09719143109234109, 0.0, 0.9932918574069386, 0.0, 0.6629165620961792, 0.0, 0.0, 0.0, 2.535744665656393, 0.5030240305132585, 0.07862217007515583, 0.0, 1.7329606098117278, 0.037047249359130945, 0.6678307645364164, 0.8320458567116419, 0.8972015712979708, 0.47381661195589236, 1.2717218819903444, 0.0, 0.0, 0.0, 0.9409727986577715, 0.0, 0.0, 0.0, 0.0, -0.775425863344069, 0.0, 0.14793847021239614, 1.105934617560348, 0.0, 1.165604252359278, 0.0, 0.0, -0.5231068668483928, 0.0, 0.0, 0.0, 0.9154388701415956, 0.0, 0.0, 0.0, 0.0, 0.7426005548661273, 0.0, 0.0, 0.9894190502931477, 0.0, 0.0, 0.0, 0.0, 0.825192986975365, 0.0, 0.0, -0.902116931957919, 0.0, 0.0, 0.0, 2.085205935224423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8996973212446854, 0.5355185662961658, 0.9708635178408271, 0.5498962125732545, 0.0, 0.0, 0.0, 0.729295774316392, 0.0, 0.0, 0.0, 0.2738967763448373, 0.6195557933236037, 0.37732212252008995, 0.0, 0.8463554597421769, 0.0, 0.07413937827535685, 0.0, -0.5398348469683547, 0.3170782649730125, 0.18697588769930973, 0.1696189949435317, 0.0, 0.0, 0.0, 0.3804115035358363, 0.0, 0.45247570876218834, 1.0325251480839541, 1.8283045022085787, 0.0, 0.0, 0.0, 0.0, -0.8364087907121615, 0.40613976626158826]}, "info": {"pct_words_masked": 0.3, "span_length": 2, "n_perturbations": 10, "n_samples": 500}, "raw_results": [{"original": "Correct command for starting Celery Flower (#9483)", "sampled": "Correct command for starting Celery Flower (#9483)This", "perturbed_sampled": ["Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This", "Correct command for starting Celery Flower (#9483)This"], "perturbed_original": ["Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)", "Correct command for starting Celery Flower (#9483)"], "original_ll": -6.312844753265381, "sampled_ll": -6.824895858764648, "all_perturbed_sampled_ll": [-6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648, -6.824895858764648], "all_perturbed_original_ll": [-6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381, -6.312844753265381], "perturbed_sampled_ll": -6.824895858764648, "perturbed_original_ll": -6.312844753265381, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixing mypy issues inside tests model (#20026)", "sampled": "Fixing mypy issues inside tests model (#20026)Coding", "perturbed_sampled": ["Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding", "Fixing mypy issues inside tests model (#20026)Coding"], "perturbed_original": ["Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)", "Fixing mypy issues inside tests model (#20026)"], "original_ll": -6.958688259124756, "sampled_ll": -7.136562824249268, "all_perturbed_sampled_ll": [-7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268, -7.136562824249268], "all_perturbed_original_ll": [-6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756, -6.958688259124756], "perturbed_sampled_ll": -7.136562824249268, "perturbed_original_ll": -6.958688259124756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "sampled": "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "perturbed_sampled": ["Make the pod operator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working as of version 25 and this is a separate PR, so other versions of Kubernetes will not be affected", "Make K8sPodOperator backwards compatible. * Make the KubernetesPodOperator backwards compatible. This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this change is only working on Fedora 25 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time that is needed for developers to implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a branch only, so your version of Kubernetes will not be affected", "Make K8sPodOperator backwards compatible (#12384) * Make KubernetesPodOperator backwards compatible This PR reduces the amount of time required for developers to fix this issue, so it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "Make Kubernetes backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly build the library, as it should now be backward compatible quickly. Please be aware that this is only working on Fedora 25 and newer. This is a separate PR, so your version of Fedora will not be affected", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly integrate Kubernetes pod operations with the K8s library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 13 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "Make K8sPodOperator backwards compatible (#12384) * Make the K9sPodOperator backwards compatible This PR reduces the amount of time required for developers to implement the API, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "This PR will make the KubernetesPodOperator backwards compatible (#12384) Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is not a maintenance PR, so your version of Kubernetes will not be affected", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of effort it takes for developers to properly implement the pod operator as it should now be backward compatible quickly. Please note that this PR is only working on Fedora 25 and newer. This is a bug fix, so your version of Kubernetes will not be affected", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time and resources it has taken for developers to properly implement this library, as it can be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a bug fix PR, so your version of Kubernetes will not be affected"], "perturbed_original": ["Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be able to continue using the existing classes * correct spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR will release the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spellcheck * spellcheck * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the existing classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This update reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be able to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This fix reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using their current configuration. custom classes * spellcheck * spelling * clear * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible (#12384) This feature significantly reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users are allowed to continue to use their airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to Airflow 2.0 or 2.0 of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom deployment method. spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the difficulty in upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to 1.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean non-Unfortunately files in 1.10 * clean unecessary files in 1.10 * clean incorrect files in 1.10", "Make K8sPodOperator backwards compatible with K8s * Make the KubernetesPodOperator backwards compatible with K8s * This PR significantly reduces the burden of upgrading to Airflow 2.0 for users of K8s Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10"], "original_ll": -3.1624696254730225, "sampled_ll": -3.173940658569336, "all_perturbed_sampled_ll": [-3.30611252784729, -3.0362792015075684, -3.2029454708099365, -3.0573723316192627, -3.2703936100006104, -3.0372023582458496, -3.188918113708496, -2.9296228885650635, -3.189502477645874, -3.295402765274048], "all_perturbed_original_ll": [-3.027723550796509, -3.101689577102661, -3.088731527328491, -3.0884273052215576, -3.1894264221191406, -3.055068254470825, -3.1432342529296875, -3.16821551322937, -3.777820110321045, -3.0438740253448486], "perturbed_sampled_ll": -3.1513751745224, "perturbed_original_ll": -3.1684210538864135, "perturbed_sampled_ll_std": 0.12192980282224172, "perturbed_original_ll_std": 0.2092371788089409}, {"original": "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "sampled": "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "perturbed_sampled": ["[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One"], "perturbed_original": ["[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)"], "original_ll": -5.5672454833984375, "sampled_ll": -5.9538984298706055, "all_perturbed_sampled_ll": [-5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055, -5.9538984298706055], "all_perturbed_original_ll": [-5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375, -5.5672454833984375], "perturbed_sampled_ll": -5.9538984298706055, "perturbed_original_ll": -5.5672454833984375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix spelling (#11404)", "sampled": "Fix spelling (#11404)This", "perturbed_sampled": ["Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This", "Fix spelling (#11404)This"], "perturbed_original": ["Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)", "Fix spelling (#11404)"], "original_ll": -6.915605068206787, "sampled_ll": -8.098139762878418, "all_perturbed_sampled_ll": [-8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418, -8.098139762878418], "all_perturbed_original_ll": [-6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787, -6.915605068206787], "perturbed_sampled_ll": -8.098139762878418, "perturbed_original_ll": -6.915605068206787, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "sampled": "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "perturbed_sampled": ["[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash"], "perturbed_original": ["[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)"], "original_ll": -6.001043796539307, "sampled_ll": -6.292699813842773, "all_perturbed_sampled_ll": [-6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773, -6.292699813842773], "all_perturbed_original_ll": [-6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307, -6.001043796539307], "perturbed_sampled_ll": -6.292699813842773, "perturbed_original_ll": -6.001043796539307, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5614] Enable Fernet by default (#6282)", "sampled": "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "perturbed_sampled": ["[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "[AIRFLOW-5614] Enable Fernet by default (#6282)One"], "perturbed_original": ["[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "[AIRFLOW-5614] Enable Fernet by default (#6282)"], "original_ll": -5.7969746589660645, "sampled_ll": -6.356044769287109, "all_perturbed_sampled_ll": [-6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109, -6.356044769287109], "all_perturbed_original_ll": [-5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645, -5.7969746589660645], "perturbed_sampled_ll": -6.356044769287109, "perturbed_original_ll": -5.7969746589660645, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was rised during git add.", "sampled": "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "perturbed_sampled": ["Make Cloud Build system tests setup runnable . This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Bugfix: Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Make the system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Make Cloud Build system tests setup runnable (#10692) This fix has error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Bugfix: Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Make Cloud Build system tests setup runnable (#10692) This change was made by accident. Fixed: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Make Cloud Build and Quick setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Make Cloud Build system tests setup runnable (#10692) This change fixes the bug Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. During installation it could crash", "Make Cloud Seers tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash"], "perturbed_original": ["Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): not found that was rised during git add.", "Make Cloud Build System for setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was rised during git add.", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied to use rised during git add.", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission issue. This was rised during git add.", "- Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was rised during git add.", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that will occur during git add.", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Failed because of an error that was rised during git add.", "Make Cloud Build system tests setup correctly. This change fixes error: open(quickstart.sh): Permission denied that was rised during git add.", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that occurred during git add.", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was rised by add."], "original_ll": -5.5679779052734375, "sampled_ll": -4.530129432678223, "all_perturbed_sampled_ll": [-4.831544399261475, -4.061102390289307, -4.185430526733398, -4.584073543548584, -4.061102390289307, -4.641841411590576, -4.416544437408447, -4.554994583129883, -5.238556861877441, -4.582553863525391], "all_perturbed_original_ll": [-5.682657241821289, -5.384016990661621, -5.735199451446533, -5.704613208770752, -5.542702674865723, -5.427023410797119, -5.462834358215332, -5.650634288787842, -5.455552101135254, -5.605453014373779], "perturbed_sampled_ll": -4.515774440765381, "perturbed_original_ll": -5.565068674087525, "perturbed_sampled_ll_std": 0.3436724452840149, "perturbed_original_ll_std": 0.12082167809972766}, {"original": "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it for now. This is captured in #18777", "sampled": "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining more than one adult animal at a time", "perturbed_sampled": ["#118178 (#18778) This test fails too often. Quarantining more than one adult animal at a time", "Exiting by Testing (#18778) This test fails too often. Quarantining more than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This works out too often. Quarantining more than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test does not happen often. Quarantining more than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test can vary very often. Quarantining more than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails due to a missing parameter in Quarantining more than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Previously. Possibly. No More than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This happens too often. Quarantining more than one adult animal at a time", "Quarantine - This test fails too often. Quarantining more than one adult animal at a time", "Quarantine iest_no_orphan_process_will_be_left (#18778) This code fails far too often. Quarantining more than one adult animal at a time"], "perturbed_original": ["Quarantine iest_no_orphan_process_will_be_left as this test fails too often. Quarantining it for now. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too . Ignore it for now. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too . I have removed it for now. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This process can overrun too often. Quarantining it for now. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it for now. This change was mentioned in #18777", "Quarantine iest_no_orphan_process_will_be_left . This test fails too often. Quarantining it for now. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining is recommended now. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it fixes that. This is captured in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it for now. This is likely a bug in #18777", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails . Quarantining it for now. This is captured in #18777"], "original_ll": -4.738646507263184, "sampled_ll": -4.5175275802612305, "all_perturbed_sampled_ll": [-4.7595930099487305, -4.760810375213623, -4.551549911499023, -4.577063083648682, -4.714786529541016, -4.50690221786499, -5.013444900512695, -4.509510517120361, -4.23546028137207, -4.461698055267334], "all_perturbed_original_ll": [-4.835363388061523, -4.8779826164245605, -4.746135234832764, -4.8260369300842285, -4.606930255889893, -4.947941303253174, -4.743286609649658, -4.742777347564697, -4.46783447265625, -4.948217868804932], "perturbed_sampled_ll": -4.609081888198853, "perturbed_original_ll": -4.774250602722168, "perturbed_sampled_ll_std": 0.201584578508429, "perturbed_original_ll_std": 0.14220996723753415}, {"original": "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company", "sampled": "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen", "perturbed_sampled": ["Update Thumbtack points of contact in Airflow Users list to add previously-listed person is no longer at the companyWhen", "Update Thumbtack points of contact in Airflow Users list (#9701) When a person is no longer at the companyWhen", "Update status of contact in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen", "Update person of contact in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen", "Update Thumbtack points of contact in Airflow Ltd (#9701) The previously-listed person is no longer at the companyWhen", "Update Thumbtack points of contact in Airflow Users list when previously-listed person is no longer at the companyWhen", "Update Thumbtack points to current owner or administrator in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen", "Update Thumbtack points of contact in company list (#9701) The previously-listed person is no longer at the companyWhen", "Update Thumbtack points of contact in Airflow Users list (#9701) The manager is no longer at the companyWhen", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer employed at the companyWhen"], "perturbed_original": ["Update Thumbtack points of contact in Airflow Users list (#9701) : The point of contact person is no longer at the company", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer in the listed company", "Update Thumbtack points of contact in the Thumbtack list (#9701) The previously-listed person is no longer at the company", "Update Thumbtack points of contact in Airflow Users list (#9701) The previous list editor is no longer at the company", "Update Thumbtack points of contact , Users list (#9701) The previously-listed person is no longer at the company", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer a representative of the company", "Update d the new point of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company", "Update Thumbtack points of contact For Airflow Users list (#9701) The previously-listed person is no longer at the company"], "original_ll": -5.169980049133301, "sampled_ll": -5.450675010681152, "all_perturbed_sampled_ll": [-5.936692714691162, -5.463989734649658, -5.6121506690979, -5.975283145904541, -5.390868186950684, -5.5492730140686035, -5.364322662353516, -5.069237232208252, -5.7320098876953125, -5.358303070068359], "all_perturbed_original_ll": [-4.890855312347412, -5.169980049133301, -5.113526821136475, -4.275178909301758, -5.369269847869873, -5.123790740966797, -5.169980049133301, -4.817920684814453, -5.53757905960083, -5.3023834228515625], "perturbed_sampled_ll": -5.545213031768799, "perturbed_original_ll": -5.077046489715576, "perturbed_sampled_ll_std": 0.2649973618282687, "perturbed_original_ll_std": 0.3337447465898218}, {"original": "Mark trigger-controller-dag test as xfail (#8015)", "sampled": "Mark trigger-controller-dag test as xfail (#8015)The", "perturbed_sampled": ["Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The", "Mark trigger-controller-dag test as xfail (#8015)The"], "perturbed_original": ["Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)", "Mark trigger-controller-dag test as xfail (#8015)"], "original_ll": -7.303321838378906, "sampled_ll": -7.655359745025635, "all_perturbed_sampled_ll": [-7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635, -7.655359745025635], "all_perturbed_original_ll": [-7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906, -7.303321838378906], "perturbed_sampled_ll": -7.655359745025635, "perturbed_original_ll": -7.303321838378906, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "sampled": "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "perturbed_sampled": ["Right handling of Breeze parameters (#7084) While working on improving the way we run Breeze commands, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way to run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, these Breeze parameters have been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll find them in the Breeze_Kube.conf file. Breeze parameters type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving how we run Kubernetes, we decided to try and fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief ) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters . As we continue working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll find them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been on our heads for a while. (Brief ly) If you're looking for a list of Breeze parameters, you'll find them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we work with Breeze, we decided to try to improve our handling of those nasty Breeze parameters that aren't quite right. In other words, it's been missing a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For commands, see \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Airflow at the moment, I decided to try to fix some really nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of all Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters. While working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's got to be done for a while. (Brief : If you're looking for a hint on the Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "in handling of Breeze parameters . While we're working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you can see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that we had going left and right. In other words, it's been a nightmare for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\""], "perturbed_original": ["[AIRFLOW-6491] Handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, I found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add support for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes service I found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old kind parameter is no longer valid, however it is not properly removed/saved. We used opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Kubernetes parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not checked. We use the new version to add automated tests for the following functions (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Breeze : While working on improving how we run Kubernetes tests, we found out that I need to fix handling of Breeze - we change Kubernetes version used via Kind and the old versions are no longer valid, however this old version is not properly removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of parameters - we changed the version used via Kind and the resulting parameters are no longer valid, because the version was not properly removed/saved. We use Breeze instead of Type, and I need to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the ability of Kind when we run Kubernetes tests, we had a situation that I need to change a particular set of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was supposed to be removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Breeze test version changes. While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of Breeze test versions when we change Kubernetes version used via Kind and Kind_Secret method. Those versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated testing that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of parameter changing (#7084) While working on improving the way of automating Kubernetes tests, we found out that we need to fix the issue of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Jenkins, we found out that I need to fix handling of parameters - we change Kubernetes version used via Kind le. Now old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests to handle this feature. (As from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "[AIRFLOW-6491] Improve d handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests for that feature. (Sequel from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)"], "original_ll": -4.156164169311523, "sampled_ll": -2.9110028743743896, "all_perturbed_sampled_ll": [-2.8230626583099365, -2.951509475708008, -3.0398969650268555, -2.844710111618042, -3.041106700897217, -3.1637156009674072, -3.0960724353790283, -3.142057418823242, -2.687166452407837, -3.0377845764160156], "all_perturbed_original_ll": [-4.168951034545898, -4.252627372741699, -3.9778575897216797, -4.027075290679932, -4.015662670135498, -4.200284957885742, -4.074826717376709, -4.067097187042236, -4.606200695037842, -4.480108737945557], "perturbed_sampled_ll": -2.982708239555359, "perturbed_original_ll": -4.187069225311279, "perturbed_sampled_ll_std": 0.14614037975348673, "perturbed_original_ll_std": 0.19801100608165106}, {"original": "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only used for development, not production.", "sampled": "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You can create subplanets when adding a sub-column to existing charts or sub-colors in", "perturbed_sampled": ["Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use subplot to plot your sub-data: Note You can create subplanets when adding a sub-column to existing charts or sub-colors in", "upgrade postgres subchart to 10.5.3 (#17041) We were using 10.5.1 and the latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You can create subplanets when adding a sub-column to existing charts or sub-colors in", "Chart: Update d to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these sub-data to plot your sub-data: Note You can create subplanets when adding a new chart to existing charts or sub-colors in", "Chart: Update postgres database to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You 'll want to use subplanets when adding sub-maps to existing charts or sub-colors in", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Add your data\u00b6 Next, follow the following instructions to plot your sub-data: Note You can create subplanets when adding a plot to existing charts or sub-colors in", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current version is 10.5.3. Plot your data\u00b6 Use these instructions to plot your sub-data: <unk>You can create subplanets when adding a sub-column to existing charts or sub-colors in", "Chart: Upgrade subchart to 10.5.3 (#17041) We were on 6.3.12 before the upgrade and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You can create subplanets when adding a sub-column to charts or sub-colors in", "Chart: Update d to 10.5.3 (#17041) We were on 6.3.12 yesterday, the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use the table in the below list to plot your sub-data: Note You can create subplanets when adding a sub-column to existing charts or sub-colors in", "I have updated postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version 10.5.3. Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You can create subplanets in chart, add a sub-column to existing charts or sub-colors in", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the latest version is 10.5.3.\n\nSubplotting: Plotting Sub-Data Use these instructions to plot your sub-data: Note You can create subplanets when adding a sub-column to a chart or sub-colors in"], "perturbed_original": ["Chart: Update postgres subchart to 10.5.3 (#17041) We were on 10.5.0 while the current postgres version is 10.5.3. We have dropped support for Helm 2 already so Helm users won't be affected. Secondly this postgres should only used for development, not production.", "Updated postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the latest version is 10.5.3. We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres version is being used for development, not production.", "Chart: Update postgres subchart to 10.5.3 . You were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 2 already so Helm 3 users will not be affected. Secondly this postgres subchart is used for development, not production.", "Chart: Change postgresql subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 2 already so Helm 3 users won't have much options. Secondly this postgres should only used for the production.", "Chart: Update subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. Second of all we have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this charts is only used for development, not production.", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and have the latest version of 10.5.3 in Helm. We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only be used for development, not production.", "Please upgrade Helm 3.x postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Postgres already so Helm 3 users won't be affected. Secondly , this feature should only used for development, not production.", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version in 10.5.3 is 10.5. We have dropped support for Helm 2 already so Helm 2 users won't be affected. Secondly this postgres data table should mainly be used for development, not production.", "Chart: Update Db to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 4, so Helm 3 users won't be affected. Secondly this version is only used for development, not production.", "Chart: Update postgres subchart to 10.5.5! We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 2 and Helm 3 users \u2013 please update your database as affected. Secondly this postgres should only used for development, not production."], "original_ll": -3.8848581314086914, "sampled_ll": -3.266268491744995, "all_perturbed_sampled_ll": [-3.1815452575683594, -3.2419631481170654, -3.4629814624786377, -3.3552002906799316, -3.398826837539673, -3.711291551589966, -3.358654260635376, -3.479912757873535, -3.6689188480377197, -3.303861618041992], "all_perturbed_original_ll": [-3.718674421310425, -3.717750072479248, -3.68837308883667, -4.199406623840332, -3.779963254928589, -3.7498199939727783, -3.9126949310302734, -3.744216203689575, -3.384404420852661, -3.98331618309021], "perturbed_sampled_ll": -3.4163156032562254, "perturbed_original_ll": -3.7878619194030763, "perturbed_sampled_ll_std": 0.16203983191498145, "perturbed_original_ll_std": 0.2022245733688744}, {"original": "Enable Black on Connexion API folders (#10545)", "sampled": "Enable Black on Connexion API folders (#10545)The", "perturbed_sampled": ["Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The", "Enable Black on Connexion API folders (#10545)The"], "perturbed_original": ["Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)", "Enable Black on Connexion API folders (#10545)"], "original_ll": -6.662216663360596, "sampled_ll": -7.110589981079102, "all_perturbed_sampled_ll": [-7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102, -7.110589981079102], "all_perturbed_original_ll": [-6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596, -6.662216663360596], "perturbed_sampled_ll": -7.110589981079102, "perturbed_original_ll": -6.662216663360596, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix documentation for provider's release (#14654)", "sampled": "Fix documentation for provider's release (#14654)We", "perturbed_sampled": ["Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We", "Fix documentation for provider's release (#14654)We"], "perturbed_original": ["Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)", "Fix documentation for provider's release (#14654)"], "original_ll": -6.447770595550537, "sampled_ll": -7.337686061859131, "all_perturbed_sampled_ll": [-7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131, -7.337686061859131], "all_perturbed_original_ll": [-6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537, -6.447770595550537], "perturbed_sampled_ll": -7.337686061859131, "perturbed_original_ll": -6.447770595550537, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the third party action.", "sampled": "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodule update commands to update only those we need... which doesn't require extra dependencies at all. This requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "perturbed_sampled": ["Run \"third party\" actions from submodules instead (#13514) Rather than having to mirror the new repos we can instead use git submodule update to update only those we need to and it doesn't require extra dependencies at all. This requires minimal amount of code changes (and single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in production than git repo build (#13515) Add", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can simply run git submodule update commands to update any submodule we need... which doesn't require extra dependencies at all. This requires minimal amount of code changes (a single dependency) from core to be passed down to new repo, as opposed to manually re-compiling changes like before for each repos we updated, so can be deployed very quickly in many cases. (#13515) Add", "Run \"third party\" github repos for new submodules instead (#13514) Rather than having to mirror these on all repos we could use git submodule update commands to update only those we need... which doesn't require extra dependencies at all. This requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes we made before for each new repo. In addition it can be implemented more quickly in many cases. Add", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use github update or update with we have a single dependency which doesn't require extra dependencies at compile time, requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to check any additional dependency of the repos we can instead use git submodule update commands instead, and add only those we need... which doesn't require any extra dependencies at all. This requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes we made before for each repo, so is less work once implemented correctly in many cases. (#13515) Add", "Run all github actions from submodules instead (#13514) Rather than having to mirror all the repos we work on, we should use git submodule update commands to update only those we need... which doesn't require extra dependencies at all. This requires minimal if any code changes (a single dependency) to be made, which needs to be passed down to each submodule as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "Run \"third party\" dependencies by reading them from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodule update to update only those we need... which doesn't require extra dependencies at all and requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes of before for each repo... so can be implemented faster in many cases. (#13515) Add", "Run \"third party\" github actions from submodules instead (#13514) Rather than automatically mirror all the repos we can instead use git submodule update commands to update only those submodules, which doesn't add any extra dependencies at all. This reduces the amount of extra dependencies (a single dependency) from core to be passed down to each new repository, as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "Run \"third party\" actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use inbuilt update r to update only the ones we need... which doesn't require extra dependencies at all. This requires minimal amount of code changes (a single dependency) from each release to be passed down to each new build as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "Run \"third party\" github actions from submodules instead (#13514) Instead of having to mirror all the repos we can instead use git submodule update commands to update only those submodules, which doesn't require extra dependencies of the host. This requires minimal ly extra code changes (a single dependency) from core to be passed in for each new repo, as opposed to manually re-compiling changes we made before for each repo. This can be implemented more quickly in many cases. (#13515) Add"], "perturbed_original": ["Run \"third party\" github actions from github (#13514) Rather than having to mirror all the repos we can use github actions to pull in the third party actions we want to use - with recent(ish) changes in a bunch of submodules on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the repo for each action.", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos itories I can instead use git submodules that allow for direct changes in the third party actions we want to use - so by running changes in review for submodules on GitHub we get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our code to mirror the actions code, nor do we have to maintain a fork of the third party action.", "Run third party github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in review for review and audit on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to \"pollute\" our repo with the actions ie do we have to build a fork of the github repo for each action.", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodules to run in the third party actions we want to run merged with recent(ish) changes in review /audit on GitHub github branch branches. The GitHub branches still get the same \"review/audit\" visibility for changes, but this way we don't have to mirror our repo with the actions code, nor do we have to maintain an extra git repo and or maintain a fork of the third party action.", "Run \"third party\" github actions , instead (#13514) Rather than having to mirror all the actions. github actions, we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the same support for git integration for changes. In this way we don't have either \"pollute\" the git repo with the actions code, nor do we have to maintain a fork of the third party action.", "Add Support for \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the third party actions, we can instead use git submodules to allow us to mirror only the third party actions we actually use - with recent(ish) changes in review for submodules . This means we still get the same git status updates for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the third party action.", "Run new github actions from submodules instead (#13514) Rather than having to run the new actions from scratch we can instead use git hub to pull in the third party actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to git fork of the third party action.", "Run \"third party\" github actions from submodules in your repo. Rather than having to mirror all the repos we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in the submodules , we still get the same \"review/audit\" visibility for changes, issues etc. That way we don't have to either \"pollute\" our repo with the actions code, nor do we need to maintain a fork of the third party action.", "Run \"third party\" github actions from their respective github repos (#13514) Rather than attempting to mirror all the repos we can create git submodules with the actions in the third party github actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the repo for each action.", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the actions into the repo we can instead use git submodules to pull in the third party actions we want to use - with the code in those submodules on GitHub. We still get the same \"review/audit\" visibility APIs, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the third party action."], "original_ll": -3.680201292037964, "sampled_ll": -3.4457101821899414, "all_perturbed_sampled_ll": [-3.628356695175171, -3.5590479373931885, -3.5117452144622803, -3.8675148487091064, -3.563167095184326, -3.4009835720062256, -3.5174953937530518, -3.5328869819641113, -3.7708232402801514, -3.675781726837158], "all_perturbed_original_ll": [-3.5615453720092773, -3.988992214202881, -3.786623001098633, -3.8892061710357666, -3.7358713150024414, -3.4997456073760986, -3.885793685913086, -3.4860050678253174, -3.7117691040039062, -3.4135727882385254], "perturbed_sampled_ll": -3.602780270576477, "perturbed_original_ll": -3.6959124326705934, "perturbed_sampled_ll_std": 0.12998299668457935, "perturbed_original_ll_std": 0.1870033097209392}, {"original": "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can take forever where it needs to be compiled. So for first-time users this can be a turn off. If pandas is already installed this will work fine, but if not users have an option to run `pip install apache-airflow[pandas]` closes #12500", "sampled": "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "perturbed_sampled": ["Make `pandas` an external dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API We only use `pandas` in <unk>DbApiHook.get<unk> if our `query` has the `coreData` key returned by the API <unk>pandas<unk> is needed to query DB keys <unk>pandas<unk> is needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "Make `pandas` an API dependency (#17575) We only use <unk>pandas<unk> in calls to `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in calls to <unk>query<unk> if our query has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the <unk>coreData<unk> key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "can use an optional core Data key (#17526) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "Make pandas a optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the <unk>coreData<unk> key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys db_get_field() throws an exception if the field is invalid (#17529)\n\ndiscards invalid fields if", "Make `pandas` an API package dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the <unk>coredata<unk> key returned by the API (#17526) is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "Make `pandas` a core dependency . We only use `pandas` in `DbApiHook.get` if our `query` has the same type as the data returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the same type as the data returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "Make `pandas` an optional core dependency (#17575) We only use `pandas` in <unk>query<unk> if our `query` has the `coreData` key returned by the call. We only use `pandas` in `DbApiHook.get` if our <unk>query<unk> has the `coreData` key returned by the call. db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the <unk>coreData<unk> key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys or needed to resolve DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "with an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the <unk>coreData<unk> key returned by the API (#17526)\n\nWe only use `pandas` in <unk>system.get<unk> if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined in fields if"], "perturbed_original": ["Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, and even though `pandas` now supports git packages it still hangs forever where it needs to be compiled. So for first-time users this can be a turn off. If pandas is already installed it will be no problem but if not users have an option to run `pip install apache-airflow[pandas]` closes #12500", "Make `pandas` an optional dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can take a few moments to load a package if it needs to be compiled. So for first-time users this may be a turn off. If pandas is already installed this will work fine, but all users have an option to run `pip install pandas<unk>. #12500", "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can take forever where it needs to be. So for first-time users this can be a problem. If pandas is already installed this will work or if you already have an installation in your webroot then run `pip install apache-airflow[pandas]` closes #12500", "Make `pandas` an optional core dependency (#17575) We are currently using `pandas` as a core dependency. Not all users will like this, plus while `pandas` contains many pre-compiled packages it still can take forever where it needs to be compiled. So for first-time users this can be a turn off. If pandas is already installed this will work fine, but if not users have an option to run `pip install pandas<unk>. #12500", "Make `pandas` an optional option (#17575) We only use this option for `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can take forever where it needs to be compiled. So for first-time users this is a turn off. If pandas is already installed this will happen but if not this should be possible. Adding pandas as an option to run `pip install apache-airflow[pandas]` closes #12500", "is indeed an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, and while `pandas` now supports many pre-compiled packages it still can 't be installed where it needs to be , for first-time users this can be a turn off. If pandas is already installed this will work fine, but if not users should provide an option to run `pip install apache-airflow[pandas]` closes #12500", "Make `pandas` an optional core library. We only use `pandas` in `DbApiHook.get_pandas_df`. Not many other places use it, plus while `pandas` now supports many pre-compiled packages it still can take forever where it needs to be compiled. So for first-time users this can be a bit cumbersome. If pandas is already installed this will work fine, but if not users have an option to run `pip install pandas<unk>. #12500", "Make `pandas` a core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports multiple packages it still can take forever where it needs to . So for first-time users this can be a turn off. If <unk>pandas<unk> is already installed this will work fine, but if not users would miss the option to run `pip install apache-airflow[pandas]` closes #12500", "as an optional core dependency (#17575) users can now use `pandas` in `DbApiHook.get_pandas_df`. Not all users probably want this plus while `pandas` now supports many pre-compiled packages it still can take forever where it must be compiled. So for first-time users this might be a turn off. If pandas is already installed this will work fine, but if not users have an option to run `pip install apache-airflow[pandas]` closes #12500", "Make `pandas` an Apache dependency so I can remove that dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can be used in some ways where they need to be compiled. So for first-time users this may be a turn off. If Python is already installed this will work fine, but if not users have an option to run `pip install apache-airflow[pandas]` closes #12500"], "original_ll": -3.469585657119751, "sampled_ll": -2.036691427230835, "all_perturbed_sampled_ll": [-2.549152374267578, -2.498491048812866, -2.403395652770996, -2.099757194519043, -2.5624091625213623, -2.4625048637390137, -2.056199550628662, -2.5746874809265137, -2.7338762283325195, -2.7367701530456543], "all_perturbed_original_ll": [-3.568239450454712, -3.317852020263672, -3.5906214714050293, -3.10577130317688, -3.6093785762786865, -3.6720542907714844, -3.42638897895813, -3.752962350845337, -3.6668708324432373, -3.452277421951294], "perturbed_sampled_ll": -2.4677243709564207, "perturbed_original_ll": -3.516241669654846, "perturbed_sampled_ll_std": 0.2191172783699524, "perturbed_original_ll_std": 0.18437009294586099}, {"original": "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * add test to all executors * fix test", "sampled": "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *", "perturbed_sampled": ["[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * import * nit * remove outdated import * nit * remove unused import * fix missing plugin *", "[AIRFLOW-4565] instrument celery executor * remove unused import * nit * nit instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * Remove import plugin *", "[AIRFLOW-4565] instrument celery * remove unused import * instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * instrument celery * missing plugin *", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery -d * remove unused import * nit * remove unused import * fix missing plugin *", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * fix missing plugin * remove unused import * fix missing plugin *", "* instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import when detecting missing plugin *", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *"], "perturbed_original": ["[AIRFLOW-4565] instrument celery executor (#5321) : instrument celery executor * remove unused import * nit * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * remove test/test test * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * restore to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * add test to import * fix test * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused test nit * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * add test for celery executor * remove unused import * nit * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery * remove unused import * nit * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor (#5213) * remove unused import * nit * add test to all executors * fix test", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * add test to all test executors * fix test"], "original_ll": -4.977297782897949, "sampled_ll": -4.835750579833984, "all_perturbed_sampled_ll": [-4.900543212890625, -4.546375274658203, -4.892939567565918, -4.788941860198975, -4.628445148468018, -5.129321098327637, -4.104909420013428, -4.52321195602417, -5.019603729248047, -4.835750579833984], "all_perturbed_original_ll": [-5.045376300811768, -4.513159275054932, -4.977297782897949, -5.242428302764893, -4.936855792999268, -4.895683288574219, -4.758317470550537, -5.188751220703125, -4.65473747253418, -4.961190223693848], "perturbed_sampled_ll": -4.737004184722901, "perturbed_original_ll": -4.917379713058471, "perturbed_sampled_ll_std": 0.2811867080133835, "perturbed_original_ll_std": 0.21441028468048853}, {"original": "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "sampled": "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "perturbed_sampled": ["Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As"], "perturbed_original": ["Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)"], "original_ll": -6.876770496368408, "sampled_ll": -7.274985313415527, "all_perturbed_sampled_ll": [-7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527, -7.274985313415527], "all_perturbed_original_ll": [-6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408, -6.876770496368408], "perturbed_sampled_ll": -7.274985313415527, "perturbed_original_ll": -6.876770496368408, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result on custom XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` as it handled this directly.", "sampled": "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as while iterating over a property list or a list of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "perturbed_sampled": ["Make `XCom.get_one` return full, not abbreviated values . If you used this class method in certain cases (such as while iterating through a property list or a chain of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "should return full, not abbreviated values (#18274) If you used this method directly (such as while iterating over a property list or a list of nested objects), it was possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the XCom.return() method, for retrieving", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as while iterating over a property list or a list of nested objects), it is possible to have it return an abbreviated value (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method in an unsafe manner (such as while iterating over a property list or a list of nested objects), it was possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "Make `XCom.get_one` return full, not zero values (#18274) If you used this class method directly (such as while iterating over a list of selected objects or the list of nested objects), it is possible to have `XCom.get_one` return no value (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "properly to return full, not abbreviated values . Whenever you call this class method for the first time (such as while iterating over a property list or a list of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "Make `XCom.get_one` return full, negative or zero values (#18274) If you used this method directly (e.g., while iterating over a property list or a list of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or one that are less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you call the initialization of this class method directly (such as while iterating over a property list or a list of nested property values), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the last two bits of the integer). Fix the correct code for retrieving", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as while iterating over a property list or a list of values), it is possible to have `XCom.get_one` return abbreviated results (such as zero or a number less than the number of properties). So the correct way is by retrieving", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as while navigating a property list or a list of nested objects), it is possible to have `XCom.get_one` return abbreviated values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving"], "perturbed_original": ["to return full, not abbreviated values (#18274) If you used this operator directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` process. This would most likely give this sort of result on custom XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` as it handled this directly.", "Make `XCom.get_one` return full, not abbreviated values (#18274) If using this class method directly (such as as a custom operator link) then the value would _always_ be subject ed to `orm_deserialize_value` which would likely give the wrong result on custom XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` which handled this directly.", "Make `XCom.get_one` always return true values not abbreviated values (#18274) If you used this method directly (such as in a custom operator link) then the value would _always_ be subject to conversion, which would likely give you a poor result on custom XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` as it handled this directly.", "Make `XCom.get_one` be responsible for long names and not abbreviated values (#18274) If we used this class method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result on custom operators. This wasn't a problem when using `ti.xcom_pull` as it handled this directly.", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as a web link) then the value would _always_ be sent to the `orm_deserialize_value` which would likely give the wrong result , when used on XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` as it handled this directly.", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class as a standard return type (such as in a custom backend) then the value would _always_ be subject to the `orm_deserialize_value` which would give the wrong result on custom XCom backends. ... This is not a problem for anyone using `ti.xcom_pull` as it handled this directly.", "Make `XCom.get_one` return full, not abbreviated values (#18274) . If one used that method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result on custom XCom backends. This wasn't a problem for the `ti.xcom_pull` as it used that directly.", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would cause the wrong result on certain backends. This would be a problem for anyone using this method, as it handled this directly.", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you tried calling the class method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result on custom operator link code. This wasn't a bug for anyone using `ti.xcom_pull` as it handled this directly.", "Make `XCom.get_one` return full, i.e. serialized values (#18274) If you used this class method directly (such as in a custom operator link) then it would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result to most XCom backends. This wasn't a problem for me using `ti.xcom_pull` as it handled this directly."], "original_ll": -4.204904556274414, "sampled_ll": -3.348747968673706, "all_perturbed_sampled_ll": [-3.3378772735595703, -3.7180304527282715, -3.510768175125122, -3.348653554916382, -3.4058985710144043, -3.6105732917785645, -3.258544921875, -3.485975503921509, -3.3948121070861816, -3.4371094703674316], "all_perturbed_original_ll": [-4.278319835662842, -4.40738582611084, -4.281774520874023, -4.164431095123291, -4.263716220855713, -4.119911193847656, -4.24691104888916, -4.081881046295166, -4.268614292144775, -3.9947469234466553], "perturbed_sampled_ll": -3.450824332237244, "perturbed_original_ll": -4.210769200325013, "perturbed_sampled_ll_std": 0.1293285654249731, "perturbed_original_ll_std": 0.11375359069331456}, {"original": "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1", "sampled": "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "perturbed_sampled": ["Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to simplify building on instances of more types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB dependency on the PKDB and replace it with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support for several different file types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We may need to enable support of more types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix but only have to remove the PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support for composite types", "Upgrade to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "Upgrade FAB to 3.1.1 (#11884) We 've decided to remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the code fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to remove the need for fixing actions of more types"], "perturbed_original": ["Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is not in the latest release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release the old for now on 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite s-dat since it is merged and release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can now apply the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is not added to this release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it has been removed and release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove FAB 3.0.1 Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is no longer needed for the Airflow release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with this patch (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release with 3.1.1"], "original_ll": -3.8137173652648926, "sampled_ll": -3.6194043159484863, "all_perturbed_sampled_ll": [-3.8550736904144287, -3.2850756645202637, -3.580502510070801, -3.657784938812256, -3.4812209606170654, -3.6165435314178467, -3.6194088459014893, -3.5885539054870605, -3.51715350151062, -3.7062814235687256], "all_perturbed_original_ll": [-3.6958839893341064, -4.068507671356201, -4.524504661560059, -3.7520298957824707, -3.6839795112609863, -3.78837513923645, -3.635934352874756, -3.5561137199401855, -3.4335665702819824, -3.9302356243133545], "perturbed_sampled_ll": -3.5907598972320556, "perturbed_original_ll": -3.8069131135940553, "perturbed_sampled_ll_std": 0.14147598001410755, "perturbed_original_ll_std": 0.29341020074251206}, {"original": "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "sampled": "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "perturbed_sampled": ["[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For"], "perturbed_original": ["[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)"], "original_ll": -5.319941997528076, "sampled_ll": -5.743818759918213, "all_perturbed_sampled_ll": [-5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213, -5.743818759918213], "all_perturbed_original_ll": [-5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076, -5.319941997528076], "perturbed_sampled_ll": -5.743818759918213, "perturbed_original_ll": -5.319941997528076, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-7041] make bowler dependency local (#7691)", "sampled": "[AIRFLOW-7041] make bowler dependency local (#7691)How", "perturbed_sampled": ["[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "[AIRFLOW-7041] make bowler dependency local (#7691)How"], "perturbed_original": ["[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "[AIRFLOW-7041] make bowler dependency local (#7691)"], "original_ll": -6.978909492492676, "sampled_ll": -7.5300517082214355, "all_perturbed_sampled_ll": [-7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355, -7.5300517082214355], "all_perturbed_original_ll": [-6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676, -6.978909492492676], "perturbed_sampled_ll": -7.5300517082214355, "perturbed_original_ll": -6.978909492492676, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix Python Docstring parameters (#12513)", "sampled": "Fix Python Docstring parameters (#12513)The", "perturbed_sampled": ["Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The", "Fix Python Docstring parameters (#12513)The"], "perturbed_original": ["Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)", "Fix Python Docstring parameters (#12513)"], "original_ll": -6.606939315795898, "sampled_ll": -7.411397457122803, "all_perturbed_sampled_ll": [-7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803, -7.411397457122803], "all_perturbed_original_ll": [-6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898, -6.606939315795898], "perturbed_sampled_ll": -7.411397457122803, "perturbed_original_ll": -6.606939315795898, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "sampled": "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "perturbed_sampled": ["Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The"], "perturbed_original": ["Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)"], "original_ll": -6.3373517990112305, "sampled_ll": -6.62135648727417, "all_perturbed_sampled_ll": [-6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417, -6.62135648727417], "all_perturbed_original_ll": [-6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305, -6.3373517990112305], "perturbed_sampled_ll": -6.62135648727417, "perturbed_original_ll": -6.3373517990112305, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not start due to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "sampled": "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "perturbed_sampled": ["[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-665] Display a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix issue that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-666] Fix exceptions. * [AIRFLOW-666] Add a debug message when webserver returns an error when webserver fails to build. * [AIRFLOW-666] Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6625] Improve webserver command with script checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking * Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an exception or when the webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * ** Upgrade webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking. * [AIRFLOW-665] Update webserver to latest build. * [AIRFLOW-665] Add webserver to latest build. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when there is an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver configuration files for build. * [AIRFLOW-665] Update websystem file to a new build name. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-662] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Detect a crash that may", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 .0 * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with exception when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may"], "perturbed_original": ["[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not start due to existing state. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not work after connecting to webserver interface. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver daemon it can complain that it will not start due to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not start due to existing pidfile. This patch improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When Apache webserver is started as daemon it can happend that it will not start due to existing pidfile. This PR improves whole webserver command with pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking * When running webserver as daemon it happend that it will not write to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that daemon will not start due to existing pid file. PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not start due to existing pidfile. This PR improves whole command setup and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) RELEASE BLOG. Improve webserver command with pidfile checking When running webserver as daemon it can happend that it doesn\u2019t start due to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can be that it will not start due to existing pidfile. This PR improves whole webserver command and adds pidfile checker. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking"], "original_ll": -3.0690603256225586, "sampled_ll": -2.5040814876556396, "all_perturbed_sampled_ll": [-2.5770187377929688, -2.486332416534424, -2.893216848373413, -2.8193447589874268, -2.7595419883728027, -2.372016191482544, -2.70444917678833, -2.6046829223632812, -2.568054676055908, -2.5388731956481934], "all_perturbed_original_ll": [-3.167984962463379, -3.130680561065674, -3.0914573669433594, -3.013465642929077, -3.0611491203308105, -3.1965510845184326, -3.1905789375305176, -3.2059597969055176, -3.785993814468384, -3.0994198322296143], "perturbed_sampled_ll": -2.632353091239929, "perturbed_original_ll": -3.1943241119384767, "perturbed_sampled_ll_std": 0.15171957339344144, "perturbed_original_ll_std": 0.20615659530864974}, {"original": "UX Enhancement: Add button to clear search query from DAG search (#11583)", "sampled": "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "perturbed_sampled": ["UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "UX Enhancement: Add button to clear search query from DAG search (#11583)I"], "perturbed_original": ["UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "UX Enhancement: Add button to clear search query from DAG search (#11583)"], "original_ll": -5.474490642547607, "sampled_ll": -6.062567710876465, "all_perturbed_sampled_ll": [-6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465, -6.062567710876465], "all_perturbed_original_ll": [-5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607, -5.474490642547607], "perturbed_sampled_ll": -6.062567710876465, "perturbed_original_ll": -5.474490642547607, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "sampled": "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "perturbed_sampled": ["[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get"], "perturbed_original": ["[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)"], "original_ll": -5.80667781829834, "sampled_ll": -6.214869022369385, "all_perturbed_sampled_ll": [-6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385, -6.214869022369385], "all_perturbed_original_ll": [-5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834, -5.80667781829834], "perturbed_sampled_ll": -6.214869022369385, "perturbed_original_ll": -5.80667781829834, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "sampled": "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "perturbed_sampled": ["[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint"], "perturbed_original": ["[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)"], "original_ll": -5.340404510498047, "sampled_ll": -5.649989128112793, "all_perturbed_sampled_ll": [-5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793, -5.649989128112793], "all_perturbed_original_ll": [-5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047, -5.340404510498047], "perturbed_sampled_ll": -5.649989128112793, "perturbed_original_ll": -5.340404510498047, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6635] Speed up static checks (#7256)", "sampled": "[AIRFLOW-6635] Speed up static checks (#7256)By", "perturbed_sampled": ["[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6635] Speed up static checks (#7256)By"], "perturbed_original": ["[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6635] Speed up static checks (#7256)"], "original_ll": -5.906435489654541, "sampled_ll": -6.376589775085449, "all_perturbed_sampled_ll": [-6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449, -6.376589775085449], "all_perturbed_original_ll": [-5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541, -5.906435489654541], "perturbed_sampled_ll": -6.376589775085449, "perturbed_original_ll": -5.906435489654541, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "sampled": "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "perturbed_sampled": ["[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The"], "perturbed_original": ["[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)"], "original_ll": -5.909849166870117, "sampled_ll": -6.192701816558838, "all_perturbed_sampled_ll": [-6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838, -6.192701816558838], "all_perturbed_original_ll": [-5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117, -5.909849166870117], "perturbed_sampled_ll": -6.192701816558838, "perturbed_original_ll": -5.909849166870117, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "sampled": "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "perturbed_sampled": ["[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The"], "perturbed_original": ["[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)"], "original_ll": -5.948135852813721, "sampled_ll": -6.146110534667969, "all_perturbed_sampled_ll": [-6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969, -6.146110534667969], "all_perturbed_original_ll": [-5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721, -5.948135852813721], "perturbed_sampled_ll": -6.146110534667969, "perturbed_original_ll": -5.948135852813721, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move out sendgrid emailer from airflow.contrib (#9355)", "sampled": "Move out sendgrid emailer from airflow.contrib (#9355)So", "perturbed_sampled": ["Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Move out sendgrid emailer from airflow.contrib (#9355)So"], "perturbed_original": ["Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Move out sendgrid emailer from airflow.contrib (#9355)"], "original_ll": -7.032699108123779, "sampled_ll": -7.654195308685303, "all_perturbed_sampled_ll": [-7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303, -7.654195308685303], "all_perturbed_original_ll": [-7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779, -7.032699108123779], "perturbed_sampled_ll": -7.654195308685303, "perturbed_original_ll": -7.032699108123779, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "sampled": "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "perturbed_sampled": ["Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314"], "perturbed_original": ["Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704"], "original_ll": -4.304908275604248, "sampled_ll": -3.866276741027832, "all_perturbed_sampled_ll": [-3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832, -3.866276741027832], "all_perturbed_original_ll": [-4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248, -4.304908275604248], "perturbed_sampled_ll": -3.866276741027832, "perturbed_original_ll": -4.304908275604248, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Pin pandas-gbq to <0.15.0 (#15114)", "sampled": "Pin pandas-gbq to <0.15.0 (#15114)Description:", "perturbed_sampled": ["Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "Pin pandas-gbq to <0.15.0 (#15114)Description:"], "perturbed_original": ["Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)", "Pin pandas-gbq to <0.15.0 (#15114)"], "original_ll": -5.486528396606445, "sampled_ll": -5.660397052764893, "all_perturbed_sampled_ll": [-5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893, -5.660397052764893], "all_perturbed_original_ll": [-5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445, -5.486528396606445], "perturbed_sampled_ll": -5.660397052764893, "perturbed_original_ll": -5.486528396606445, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "sampled": "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "perturbed_sampled": ["[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, the feature is likely to be introduced at least in Firefox 51.\n\nSee also the", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 . However, a similar approach is likely to be introduced sometime in Firefox 51.\n\nSee also the", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, the feature is likely to be introduced sometime in the future. See also the", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is expected to be implemented in Firefox 51.\n\nSee also the", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. A similar feature is likely to be introduced sometime in Firefox 43. See also the", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks and licence hooks. The feature proposed by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "I went back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 . However, a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and further development, but a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "[AIRFLOW-5404] Switch back to Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and 48 although a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "[AIRFLOW-5404] Switch back to using pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the"], "perturbed_original": ["[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy pre-commit hook implemented by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (currently ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence s library implemented by Jarek Potiuk was just merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to fuzzy licence matching in pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to fuzzy licence using Lucas-C pre-commit-hooks The fuzzy licence spec by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to the pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged back in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to using fuzzy licence matching. The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ), so we can switch back to it.", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was no longer sufficient for the current set of libraries. He merged it in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching idea from Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ), so we can switch back to it.", "switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged as a new bug in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching problem in Jarek 's suggestion has been accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it."], "original_ll": -4.546263217926025, "sampled_ll": -4.400304794311523, "all_perturbed_sampled_ll": [-4.42287015914917, -4.63132905960083, -4.498336315155029, -4.367741584777832, -4.656753063201904, -4.107122898101807, -4.2470831871032715, -4.630137920379639, -4.587060928344727, -4.249606132507324], "all_perturbed_original_ll": [-4.174354553222656, -4.538599014282227, -4.612839221954346, -4.408997535705566, -4.645228385925293, -4.923218250274658, -4.482120513916016, -4.854561805725098, -4.41837215423584, -4.546496391296387], "perturbed_sampled_ll": -4.439804124832153, "perturbed_original_ll": -4.560478782653808, "perturbed_sampled_ll_std": 0.18290964526926692, "perturbed_original_ll_std": 0.2066484669843436}, {"original": "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "sampled": "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their PgBouncer instance. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "perturbed_sampled": ["Chart: Allow setting an existing secret for PgBouncer config (#15296) : Previously, if a user wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their PgBouncer instance. This is no longer required. The secret is now enabled by default.\n\nBug : Add a test class for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer user, they would need to specify an existing secret for their PgBouncer instance. This is no longer necessary and is enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting an existing secret in config (#15296) Previously, when a user wanted to supply the username and password of a private PgBouncer user, they had to have set an existing secret for their PgBouncer instance. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The old bug fixes for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if an admin wanted to supply the username / password of a private PgBouncer user, they had to specify an existing secret for their user. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class now supports the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting user secret for PgBouncer config (#15296) Previously, when a user wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their user which was always specified in their config. This is no longer necessary and is now allowed by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Add an existing secret for PgBouncer config (#15296) Previously, if an administrator wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their PgBouncer instance. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class no longer shows an (un)formatted HTML-formatted-HTML attribute on", "Fixed setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer configuration, they had to specify an existing secret for their PgBouncer instance. This bug was no longer a problem and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting an existing secret in PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer user, they had to define the existing secret for their user. This is no longer necessary as this settings is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer user, they were required to specify an existing secret for the instance. This is no longer necessary and is now enabled by default.\n\nBug #15296: Add a test class for the (un)formatted HTML-formatted-HTML attribute on", "Chart: Allow setting an existing secret for the PgBouncer config . In the past, if a user wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their PgBouncer instance. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test was failing on the (un)formatted document on"], "perturbed_original": ["Chart: Allow setting an existing secret for a secret (#15296) : If a user wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the <unk>pgbouncer.ini.<unk>. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply a secret and password to the chart for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) with Chart, instead of supplying secrets directly.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the secret and password for their `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying user credentials directly.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password to the `users.txt` secret for use with PgBouncer, they had to be set directly in the `values.yaml` file. This allowed users to create this secret out of existing values (by editing the `pgbouncer.ini`) and avoid supplying secrets directly.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to set the username and password to a secret for use by pgbouncer, they had to be set directly in the `values.yaml` file of the configuration. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password set in the `users.txt` secret for use by pgbouncer, they had to set directly in the users.txt. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "Chart: Allow suppling existing secret for PgBouncer config (#15296) Previously, if you wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, then you needed to be set directly in the secret. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "Chart: Create an existing secret for PgBouncer config (#15296) ! Before, if a user wanted to supply the username and password to the `users.txt` file for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and specify their passwords into their secrets directly.", "Chart: Allow setting the username, Password to secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password to the secret for config for the pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if you wanted to supply the username and password to the `users.txt` secret for use with PgBouncer, they had to be set directly in the `values.yaml` file. This change allows you to create this secret out of band (with the appropriate secret set) and avoid supplying secrets directly."], "original_ll": -3.481090545654297, "sampled_ll": -2.8832123279571533, "all_perturbed_sampled_ll": [-3.018812656402588, -2.8902924060821533, -3.212548017501831, -3.1693203449249268, -3.1450111865997314, -2.9627459049224854, -3.0012364387512207, -3.2055742740631104, -2.9817264080047607, -2.937530279159546], "all_perturbed_original_ll": [-3.768296480178833, -3.663977861404419, -3.5426807403564453, -3.399305582046509, -3.3992726802825928, -3.5218794345855713, -3.8318445682525635, -3.5564229488372803, -3.5522024631500244, -3.518444061279297], "perturbed_sampled_ll": -3.052479791641235, "perturbed_original_ll": -3.5754326820373534, "perturbed_sampled_ll_std": 0.11296425930766987, "perturbed_original_ll_std": 0.13450881246329754}, {"original": "Added Viscovery to the list of companies using Apache Airflow (#18683)", "sampled": "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "perturbed_sampled": ["Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Added Viscovery to the list of companies using Apache Airflow (#18683)The"], "perturbed_original": ["Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Added Viscovery to the list of companies using Apache Airflow (#18683)"], "original_ll": -5.624001979827881, "sampled_ll": -6.033973693847656, "all_perturbed_sampled_ll": [-6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656, -6.033973693847656], "all_perturbed_original_ll": [-5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881, -5.624001979827881], "perturbed_sampled_ll": -6.033973693847656, "perturbed_original_ll": -5.624001979827881, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: They both operate on system time, and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Also the `utcnow()` style I have replaced will be much", "sampled": "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format", "perturbed_sampled": ["Don't use time.time() or timezone.utcnow() for duration <unk>time.time()<unk>, `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object output as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'options'.format) to change the size of the Time datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format method to set how long of time to wait before parsing a given date\n\nAdd option `'format", ".format. Also use time.time() or timezone.utcnow() for a temporary date (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format (default is 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option <unk>'wait' to Time.format, indicating how many seconds of time to wait before continuing parsing the date\n\nAdd option `'format", "Don't use time.time() or timezone.utcnow() for duration mode: `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` argument in Time 's options\n\nFixed bug: fix bug that returned all data object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the Time datetime object to a longer Time datetime object\n\nAdded option <unk>'wait_for' to Time.format, indicating how many seconds of time to wait before continuing for given date\n\nAdd option `'format", "by time.time() or timezone.utcnow() for duration calculations <unk>time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method that read datetime object (thanks @dana ) Added option `'type-argument' , allowing the user to specify which type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to spend continuing parsing a given date\n\nAdd option `'format", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed a bug that made non-time object type arguments appear as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of datetime object to a larger datetime object\n\nAdded option <unk>'wait' to Time.format, displaying how many seconds of time to wait before continuing parsing for a given date\n\nAdd option `'format", "Don't use time.time() or time.time() methods in duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all replaced with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method on Time object (thanks @dana) Added option `'type-argument' to Time.format, allowing me to specify the type arguments to be passed to the format method (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to set the size of the datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing on the given date\n\nAdd option `'format", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined in Time.format or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default to the number of seconds on Time object (thanks @DrewYea )\n\nAdded option <unk>'time' to Time.format, allowing the user to specify the type arguments to be formatted (thanks @dana )\n\nAdded option `'size' to Time.format (and option 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option <unk>'wait' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format", "Don't use time.time() or time.length() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made a few types default as argument to Time.format method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @ClintM ) Added option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to take into account the Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of wait time to wait before parsing a given date\n\nAdd option `'format", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` using the time.time() 's options\n\nFixed a bug that made non-time object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify which type of time object arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to fit into the Time datetime object Add option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format", "uses time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - <unk>, 'time end', and <unk>time=<unk> are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option <unk>'type' to Time.format, allowing the user to specify the type arguments to be output in Time.format output (thanks @DrewYea )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the output to a standard datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format"], "perturbed_original": ["Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal error: it calculates both a duration and system time, and that can go wrong. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem uncommon among long running processes it is more common than we might think. Since most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Also the `utcnow()` style I have replaced will be much", "Don't use time.time() or timezone.utcnow() for duration s. Using `time.time() - start`, or `timezone.utcnow() - start_dttm` is fine in 99% of cases, but it has one fatal flaw: They both operate on system time, and this is time to go backwards. While this might be surprising, it can happen usually due to clocks being adjusted. And while it might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is simply never safe to be safe : to be wrong. Also the `utcnow()` style I have replaced will be much", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start_dtxm<unk> and `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: They both operate on system time, and that can go backwards. While this might be surprising, it can happen , usually due to clocks being adjusted. And while this might also be an inconvenience for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Also the `utcnow()` style I have replaced is very much", "Don't use time.time() or timezone.utcnow() for Clock calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work for 99% of time. However, it has one fatal flaw: They both operate on a different clock, and that can go from this might be some time. Occasionally such thing can happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Also the `utcnow()` you should have replaced will be much", "Don't use time.time() or timezone.utcnow() for duration calculations . <unk>time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but has one huge flaw. They both operate on timestamps and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it might seem rare, for long running processes it is more common than we expect. Most of these durations are harmless to get wrong (just being logs) but it's better to be safe than sorry. Also the `utcnow()` style I have replaced will be much", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: They both operate on system time, and that can go wrong. While the nature of this failure might not be surprising, it can happen -- usually due to clocks being adjusted. And while is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Since the `utcnow()` functions have replaced the time.time() function pretty much", "Don't use the timezone.utcnow() for restart (#12353) ! While <unk>timezone.utcnow() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of the cases it has one fatal flaw: They both operate on the old time zone, and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe . Also the `utcnow()` style I have mentioned here would be much", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: both operate on system time, and that can go backwards. While this might be surprising, it can happen -- due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are quite easy to get wrong (just being logs) so it's better to be safe than sorry. Also using a style call that has been correctly replaced will be much", "Don't use time.time() or timezone.utcnow() ! This is also a related bug in duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has a fatal flaw: They both operate on system time, and that means that these are backwards. While this might be surprising, it does happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe and be careful. Also the `utcnow()` style I just mentioned will be much", "Don't use PHP timezone.utcnow() for duration calculations (#12353) `time.time() - duration<unk> or `timezone.utcnow() - start_dttm` will work fine in 99% of cases. However, it has one fatal flaw: the timing methods operate on system time, and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong , but since the errors are very common (more likely in memory and logs) it is better to be safe than sorry. Also the `utcnow()` style I have replaced will be much"], "original_ll": -3.4374382495880127, "sampled_ll": -2.517864465713501, "all_perturbed_sampled_ll": [-2.671351671218872, -2.8421554565429688, -2.8689215183258057, -2.8456850051879883, -2.6976664066314697, -2.4991989135742188, -2.8215575218200684, -2.6201069355010986, -2.6197829246520996, -2.843557834625244], "all_perturbed_original_ll": [-3.39247989654541, -3.445739269256592, -3.4940950870513916, -3.743377447128296, -3.5021140575408936, -3.3618927001953125, -3.86909556388855, -3.455078601837158, -3.537623167037964, -3.6928958892822266], "perturbed_sampled_ll": -2.732998418807983, "perturbed_original_ll": -3.5494391679763795, "perturbed_sampled_ll_std": 0.12183096080727832, "perturbed_original_ll_std": 0.1566147584383514}, {"original": "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "sampled": "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "perturbed_sampled": ["[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In"], "perturbed_original": ["[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)"], "original_ll": -6.389296531677246, "sampled_ll": -6.7546000480651855, "all_perturbed_sampled_ll": [-6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855, -6.7546000480651855], "all_perturbed_original_ll": [-6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246, -6.389296531677246], "perturbed_sampled_ll": -6.7546000480651855, "perturbed_original_ll": -6.389296531677246, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "sampled": "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "perturbed_sampled": ["Convert properties with query to real methodsAs * Convert properties with query to real methodsAs", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "Convert properties with query to real methods As (#7900) * Convert properties with query to real methodsAs", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "Convert properties with query to real methodsAs. * Convert properties with query to real methodsAs", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "Convert properties with query to real methods As. Convert properties with query to real methodsAs", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs"], "perturbed_original": ["Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods. * (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods. * Convert properties with query to real methods", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Convert properties with query to real methods #7900 (#7900) * Convert properties with query to real methods"], "original_ll": -4.225117206573486, "sampled_ll": -4.787213325500488, "all_perturbed_sampled_ll": [-5.020752906799316, -4.787213325500488, -4.8489909172058105, -4.787213325500488, -4.707037925720215, -4.787213325500488, -4.787213325500488, -4.787213325500488, -4.8520121574401855, -4.787213325500488], "all_perturbed_original_ll": [-4.225117206573486, -4.225117206573486, -4.225117206573486, -4.225117206573486, -3.9561495780944824, -4.225117206573486, -4.002171993255615, -4.225117206573486, -4.225117206573486, -4.229057312011719], "perturbed_sampled_ll": -4.815207386016846, "perturbed_original_ll": -4.176319932937622, "perturbed_sampled_ll_std": 0.07819687716199569, "perturbed_original_ll_std": 0.09912211589695852}, {"original": "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse the order of links to reduce mouse distance for most popular", "sampled": "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "perturbed_sampled": ["Fix overflow of DAGs table with hide/reveal of \"links\" (#17810) * Conserve horizontal space by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "Fix oversized rows in DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of links (#11689) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal navbar width by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large table with lots of numbers) (#10752) *", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of \"links\" (#11836) * Show oversized width of table (e.g. high columns or lots of numbers) (#10752) *", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of columns) (#10752) *", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of numbers (#11836) * Fix overflow of table (e.g. large rows full of numbers) (#10752) *", "Fix increase width of DAGs table with hide/reveal of \"links\" (#11866) * Optimize table space by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "Fix oversized or underflow of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by putting space between columns of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "Fix oversized width of DAGs table with hide/reveal of links (#11866) * Conserve horizontal space by adding hide/reveal of \"links\" (#11886) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "Fix oversized width of DAGs table with hide/reveal of \"links\" : * Conserve horizontal space by adding <unk>n<unk>n-less \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *"], "perturbed_original": ["Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal \"links\" option on DAGs table * Organize order of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal \"links\" (#11866) * Conserve horizontal space by adding hide/reveal \"links\" in DAGs table * Update the order of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Restrict horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse the view of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve mouse position of most popular features as well by adding hide/reveal \"links\" in DAGs table * Reverse behavior of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve screen room by adding hide/reveal \"links\" in DAGs table * Rotation of the order of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by hiding/revealing \"links\" in DAGs table * Reverse the order of links to reduce mouse movement in the most popular", "Fix oversized width for \"Cache\" table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse the order of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Get rid of extra horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse style of links to reduce mouse distance for most popular", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) : Increased width of horizontal space by adding hide/reveal \"links\" in DAGs table (#11865): Reverse the order of links to reduce mouse distance for most popular", "* Reduce the width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve mouse on long views by adding hide/reveal \"links\" in DAGs table * Reverse the order of links to reduce mouse distance for most popular"], "original_ll": -4.3492431640625, "sampled_ll": -3.6270110607147217, "all_perturbed_sampled_ll": [-3.713127374649048, -3.805830955505371, -3.642979145050049, -3.805938243865967, -3.5762383937835693, -3.8958797454833984, -3.4944233894348145, -3.9772188663482666, -3.830289840698242, -4.362819194793701], "all_perturbed_original_ll": [-4.396472454071045, -4.289270877838135, -4.546956539154053, -4.533599376678467, -4.465357303619385, -4.073208332061768, -4.665144920349121, -4.390213489532471, -4.373913288116455, -4.249467372894287], "perturbed_sampled_ll": -3.8104745149612427, "perturbed_original_ll": -4.398360395431519, "perturbed_sampled_ll_std": 0.23084696317532746, "perturbed_original_ll_std": 0.1601543259475634}, {"original": "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "sampled": "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "perturbed_sampled": ["[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We"], "perturbed_original": ["[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)"], "original_ll": -5.826155662536621, "sampled_ll": -6.287606716156006, "all_perturbed_sampled_ll": [-6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006, -6.287606716156006], "all_perturbed_original_ll": [-5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621, -5.826155662536621], "perturbed_sampled_ll": -6.287606716156006, "perturbed_original_ll": -5.826155662536621, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "sampled": "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "perturbed_sampled": ["AwsGlueJobOperator: * add new flags to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add function for Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix error during update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* use Run Job API to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue file job\n\n* fix failure to update Glue object #16794\n\n* fix error", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue job build output * fix error"], "perturbed_original": ["AwsGlueJobOperator: add run_job_kwargs to Glue job run * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job run * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run. * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_kwargs to glue job run * add run_kwargs to hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add / to apply hook and operator tests", "AwsGlueJobOperator: add run_job_kwargs to Glue job hook and operator tests * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "Add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests"], "original_ll": -4.089051723480225, "sampled_ll": -3.424016237258911, "all_perturbed_sampled_ll": [-3.9704346656799316, -3.296816110610962, -3.340743064880371, -3.9061381816864014, -3.5098726749420166, -3.424016237258911, -3.512601613998413, -3.340743064880371, -3.6323773860931396, -3.656595230102539], "all_perturbed_original_ll": [-3.9627110958099365, -3.8936209678649902, -3.9627110958099365, -4.089051723480225, -4.089051723480225, -4.060330867767334, -4.236091613769531, -4.553621292114258, -3.9610910415649414, -3.956216812133789], "perturbed_sampled_ll": -3.559033823013306, "perturbed_original_ll": -4.076449823379517, "perturbed_sampled_ll_std": 0.2216961775497728, "perturbed_original_ll_std": 0.18461271910616758}, {"original": "Make Smart Sensors DB Migration idempotent (#13892)", "sampled": "Make Smart Sensors DB Migration idempotent (#13892)With", "perturbed_sampled": ["Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With", "Make Smart Sensors DB Migration idempotent (#13892)With"], "perturbed_original": ["Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)", "Make Smart Sensors DB Migration idempotent (#13892)"], "original_ll": -6.335483551025391, "sampled_ll": -6.85119104385376, "all_perturbed_sampled_ll": [-6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376, -6.85119104385376], "all_perturbed_original_ll": [-6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391, -6.335483551025391], "perturbed_sampled_ll": -6.85119104385376, "perturbed_original_ll": -6.335483551025391, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5702] Fix common docstring issues (#6372)", "sampled": "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "perturbed_sampled": ["[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-5702] Fix common docstring issues (#6372)The"], "perturbed_original": ["[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-5702] Fix common docstring issues (#6372)"], "original_ll": -5.809638023376465, "sampled_ll": -6.241412162780762, "all_perturbed_sampled_ll": [-6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762, -6.241412162780762], "all_perturbed_original_ll": [-5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465, -5.809638023376465], "perturbed_sampled_ll": -6.241412162780762, "perturbed_original_ll": -5.809638023376465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "sampled": "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "perturbed_sampled": ["[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding DHCP and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #539 - Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding MQTT and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #731 - Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding MTP and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #571 - Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Fix EAP operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 - Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators , fix BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #509 - Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and Google Documents. Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 - Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #512 - Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 - Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #587 - Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery , etc. (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #700 - Fix EAP #727 - Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix"], "perturbed_original": ["* Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to renamed GCP modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR updated regarding AIP-21 (renaming GCP operators and hooks): * corrected test modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) * Added the following changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings * added additional contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP This Update contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP operators * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "* Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains a few improvements from AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains the following for the first part of AIP-21 (renaming GCS operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * adde deprecation to the GCP modules * adde deprecation to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR : * regarding AIP-21 (renaming GCP modules on these query hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators and hooks * Add support for BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * updated test.cc * modified tests * updated UPDATING.md", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and Hooks in AIP-21 This PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP operators * added adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md"], "original_ll": -4.861384868621826, "sampled_ll": -2.3684208393096924, "all_perturbed_sampled_ll": [-2.2877120971679688, -2.4770729541778564, -2.4799249172210693, -2.4701526165008545, -2.6447694301605225, -2.5173683166503906, -2.764734983444214, -2.7577500343322754, -2.3774256706237793, -2.8323352336883545], "all_perturbed_original_ll": [-4.938051700592041, -5.09507417678833, -4.628307342529297, -4.721357822418213, -4.900869846343994, -4.708433628082275, -4.499581336975098, -5.008437633514404, -4.5684685707092285, -4.5094194412231445], "perturbed_sampled_ll": -2.5609246253967286, "perturbed_original_ll": -4.757800149917602, "perturbed_sampled_ll_std": 0.17117777026660122, "perturbed_original_ll_std": 0.2035392688188062}, {"original": "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "sampled": "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "perturbed_sampled": ["DbApiHook: Don't return in get_pandas_df (#9730) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Update dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df () (#8970) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9634) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Handle pthread in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Set type for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Correct database namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs () (#9730) * Proc: Add pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Handle pthread s in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() on hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs for DbApi hooks (#9730) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Implement dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in it using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df () (#9569) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return false when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "DbApiHook: Support kwargs in get_pandas_df _insert() (#9586) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#9256) * DbApiHook: Don't check"], "perturbed_original": ["DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in * PrestoHook: Support kwargs in * * HiveServer2Hook: Support kwargs in get_pandas_df", "Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * BigQuerySolutionHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * BigQueryHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * DataHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "* BqLogHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "DbApiHook: Support kwargs in get_pandas_df (#9730) * BigQueryHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df"], "original_ll": -1.856416940689087, "sampled_ll": -2.613673448562622, "all_perturbed_sampled_ll": [-2.550078868865967, -2.6464695930480957, -2.754627227783203, -2.651331663131714, -2.93609356880188, -2.697516441345215, -2.496678113937378, -2.719914436340332, -2.567626953125, -2.739948272705078], "all_perturbed_original_ll": [-2.0779783725738525, -2.2908382415771484, -1.8772614002227783, -1.8299074172973633, -1.856416940689087, -1.856416940689087, -1.856416940689087, -1.8394564390182495, -1.8969486951828003, -1.8441760540008545], "perturbed_sampled_ll": -2.676028513908386, "perturbed_original_ll": -1.9225817441940307, "perturbed_sampled_ll_std": 0.1189745657922096, "perturbed_original_ll_std": 0.1404461070679826}, {"original": "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "sampled": "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "perturbed_sampled": ["Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579"], "perturbed_original": ["Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579"], "original_ll": -4.961422443389893, "sampled_ll": -4.961422443389893, "all_perturbed_sampled_ll": [-4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893], "all_perturbed_original_ll": [-4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893, -4.961422443389893], "perturbed_sampled_ll": -4.961422443389893, "perturbed_original_ll": -4.961422443389893, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if an SlaMiss already exists in DB while inserting slas. If an SLA for a task is missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record into the SlaMiss table again, this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "sampled": "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of `OK` (that means that there was a proper removal of corruption). This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with a Windows machine when run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "perturbed_sampled": ["Fix IntegrityError in file system operations. The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of `OK` (that means that the process had a proper removal of corruption). This fix breaks backwards compat with DagEdit; see previous issue for details. Features Fixed file manager issue with a Windows machine when run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries in user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas routine will only be called once if any files have a corruption . The return value is `OK` (that means that the editor can perform a proper removal of corruption). This fix breaks compatibility with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with missing files on old machine when run in a branch of the application. (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once only if any of the extracted files have a corruption status of `OK` (that means that there was a proper removal of the file) This fix needs compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file system files that ended up with a corruption status of <unk>OK<unk> when opened in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of `OK` (that means that there has been proper removal of corruption). This must be carried out backwards with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with a new local machine when run in a new directory (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT data is stored in an unordered sequence so it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` callback. The DagFileProcessor.manage_slas callback will be only called once if any files have a corruption status of `OK` (that means that there was a proper removal of corruption). This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with a Windows PC run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in a special sequence that can be used to read, write,", "Fix IntegrityError in Slas editor The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of `OK` (that means that there was a proper removal ). This fix breaks backwards compat with current DagEdit versions, see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with the Windows machine when running DagEdit in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence where it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of IntegrityError; corruption status means that there was a proper removal . This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug ) Fix file manager issue with a Windows machine when run in Safe mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user accounts (#18981) DSDT is usually stored in an file so it can be used to read, write,", "Fix IntegrityError s (#19553) The DagFileProcessor.manage_slas callback should be called only once files and data files have a corruption status of `OK` (that means that there was a proper removal of corruption). This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue on Windows machine when run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files . This is usually stored in an unordered sequence so it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once the files have a corruption status of `OK` (that means that there was a fixed amount of corruption). This fix is not compat with DagEdit; try calling it for Dagger, though. Features (# 87829) Fixed file manager issue with a Windows machine when run in safe mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of \"integrity\". This means that there was an error involving removal . This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. Users Fixed file manager issue with a Windows machine when run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually a system file format that contains an active file, so it can be used to read, write,"], "perturbed_original": ["Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The IntegrityError occurs in this method due to not insert an Sla record which already exists in DB while running the task. If an SLA for a task is missed and we follow-up with another task on checking SLA again, this task comes up again if there is a recent run of the task and we try to insert the record into the SlaMiss table again, this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if an SlaMiss already in DB while inserting slas. If an SLA for a task is successfully complete and recorded, on doing the SLA again, this task comes up but the task record has been closed, there's no recent run of the task and we try to insert the old SLA into the SlaMiss and this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not insert data if an SlaMiss already exists in DB while running the task. If an SLA for a file is missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record in the SlaMiss table again, this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung and Kaxil Naik <kaxilnaik@apache.org>", "on sla mismatcher in `DagFileProcessor.manage_slas` (#19553) Task does not consider if an SLA exists in DB while inserting slas. If an SLA for a file was missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record into the SlaMiss table . This results in an error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix es Invalid `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not insert if an SlaMiss already exists in DB while inserting slas. If an SLA for a particular process is already missed and recorded, on checking SLA again, this task comes up . If there's no recent run of the task and we try to insert the record into the SlaMiss table again, this results in Integrity error. This PR fixes the problem by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chen Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix IntegrityError . (#19553) The DagFileProcessor.manage_slas does not consider if an SLA record already exists in DB while inserting slas. If an SLA record for a task is missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record into the DB again, this results in Integrity error. This PR fixes this issue, by avoiding insert if the record already exists . Author: Uranus Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if SlaMiss already exists in DB while inserting record If an SLA for a task is missed and recorded, we need the SLA again, this task comes up again. If there's no recent run of the task and we try to insert the record in SlaMiss table again, this results in Integrity error. This PR fixes that by avoiding insert if the SlaMiss already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix IntegrityError in SLA process The DagFileProcessor.manage_slas does not consider if an SlaMiss already exists in DB while inserting slas. If an SLA for a task is not recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we have to insert the record into the DB again, this results in Integrity error. This PR fixes that by avoiding insert if an SlaMiss already exists. Author: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if an SlaMiss ing event exists in DB while inserting slas. If an SLA for a task is missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record into sla table again, this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: aanu Arun <kaxilnaik@apache.org>", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider insertion of existing record into the SlaMiss already existing in the DB while inserting slas. If an SLA for a task is missed and recorded, on checking SLA again, the error comes up again if there's no recent run of the task and we try to insert the record into the SlaMiss DB, this results in Integrity error. This PR fixes that by avoiding insert ing slas where the record already exists Co-authored-by: Uranus Naik <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>"], "original_ll": -3.6974518299102783, "sampled_ll": -3.0906713008880615, "all_perturbed_sampled_ll": [-3.9990906715393066, -3.273472547531128, -3.27181339263916, -3.204223155975342, -3.2987680435180664, -3.680945873260498, -3.3130218982696533, -3.7625160217285156, -3.7259910106658936, -3.3337345123291016], "all_perturbed_original_ll": [-3.812821865081787, -3.7353599071502686, -3.864699125289917, -4.013596057891846, -3.858278512954712, -3.947326183319092, -3.6432995796203613, -4.032623767852783, -4.10432243347168, -3.656736373901367], "perturbed_sampled_ll": -3.4863577127456664, "perturbed_original_ll": -3.8669063806533814, "perturbed_sampled_ll_std": 0.26342465055627723, "perturbed_original_ll_std": 0.15043077990374207}, {"original": "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "sampled": "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "perturbed_sampled": ["[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For"], "perturbed_original": ["[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)"], "original_ll": -6.993696212768555, "sampled_ll": -7.434621810913086, "all_perturbed_sampled_ll": [-7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086, -7.434621810913086], "all_perturbed_original_ll": [-6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555, -6.993696212768555], "perturbed_sampled_ll": -7.434621810913086, "perturbed_original_ll": -6.993696212768555, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improvements for transfer operators references (#12482)", "sampled": "Improvements for transfer operators references (#12482)The", "perturbed_sampled": ["Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The", "Improvements for transfer operators references (#12482)The"], "perturbed_original": ["Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)", "Improvements for transfer operators references (#12482)"], "original_ll": -6.506933212280273, "sampled_ll": -7.1466569900512695, "all_perturbed_sampled_ll": [-7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695, -7.1466569900512695], "all_perturbed_original_ll": [-6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273, -6.506933212280273], "perturbed_sampled_ll": -7.1466569900512695, "perturbed_original_ll": -6.506933212280273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "sampled": "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "perturbed_sampled": ["Time Display better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "Handle DST better : Improve Instance tool tips (#8104) We fix zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "Handle DST better in Task Instance s (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in Task Instances. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "Handle DST better in Task Instance s. (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback . Run Cortana now", "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback in search results: See Cortana now", "Display Zone \"name\" better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which led to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "Handle DST in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience . The Feedback section of Cortana is full screen right now", "Handle DST better in Task Instance tool tips (#8104) We displayed the Tool Tips largely based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve d the task experience (#8104) Cortana now", "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana 's language recognition. (#8104) Cortana now", "Handle DST better in Task Instance tool tips (#8104) We displayed the Task Instance tooltip with a change based on the current time, which could lead to a delay of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now"], "perturbed_original": ["Handle it in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to confusion when the date we displayed was not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance tool tips automatically displayed the zone \"name\" based on the current time. This could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "it better in Task Instance tool tips (#8104) We displayed the zone based on the current time, which could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance ! (#8104) We displayed a \"name\" based on the current time, which could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance tool . We displayed the zone \"name\" based on the current time, which could lead to errors when the date to be displayed was not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance tool tips (#8104) We displayed the date based on the current time, which could cause confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "Handle DST better , by using Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to confusion when the date to be given is not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance tool . We displayed the zone \"name\" on the current time, which could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current zone \"name\". This could lead to confusion when the zone to be displayed was not in the same daylight-savings state as \"now\".", "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could cause confusion when the date to be displayed was not in the DST state even though we explicitly referenced the daylight-savings state as \"now\"."], "original_ll": -4.448699474334717, "sampled_ll": -3.65997576713562, "all_perturbed_sampled_ll": [-3.6537961959838867, -3.7158923149108887, -3.5120351314544678, -3.8674654960632324, -3.956315755844116, -3.5547549724578857, -3.8506898880004883, -4.010305881500244, -3.6592729091644287, -3.508485794067383], "all_perturbed_original_ll": [-4.399876117706299, -4.532767295837402, -4.559512138366699, -4.321683883666992, -4.398043155670166, -4.277270317077637, -4.536169528961182, -4.584491729736328, -4.361002445220947, -4.390182018280029], "perturbed_sampled_ll": -3.728901433944702, "perturbed_original_ll": -4.436099863052368, "perturbed_sampled_ll_std": 0.17364831317293392, "perturbed_original_ll_std": 0.10268186341461194}, {"original": "Add colors to airflow config command (#8404)", "sampled": "Add colors to airflow config command (#8404)The", "perturbed_sampled": ["Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The", "Add colors to airflow config command (#8404)The"], "perturbed_original": ["Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)", "Add colors to airflow config command (#8404)"], "original_ll": -6.870358943939209, "sampled_ll": -7.495344638824463, "all_perturbed_sampled_ll": [-7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463, -7.495344638824463], "all_perturbed_original_ll": [-6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209, -6.870358943939209], "perturbed_sampled_ll": -7.495344638824463, "perturbed_original_ll": -6.870358943939209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "sampled": "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "perturbed_sampled": ["[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] * Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of AFL users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users: (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add s Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Bloomberg's Airflow has been setup and configured"], "perturbed_original": ["[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users ! * This is a reminder that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users ! [Airflow-XXX] Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Gillespie is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate when Bloomberg will publish. * Peter Imberman is Bloomberg's Airflow PoC"], "original_ll": -4.35764741897583, "sampled_ll": -3.9237124919891357, "all_perturbed_sampled_ll": [-3.9237124919891357, -3.9237124919891357, -3.807706356048584, -3.9237124919891357, -3.9237124919891357, -3.9237124919891357, -4.526637077331543, -3.959005117416382, -4.375049114227295, -4.172049522399902], "all_perturbed_original_ll": [-4.35764741897583, -4.650132656097412, -4.35764741897583, -4.35764741897583, -4.35764741897583, -4.027015686035156, -4.369421005249023, -4.35764741897583, -4.35764741897583, -4.623315811157227], "perturbed_sampled_ll": -4.045900964736939, "perturbed_original_ll": -4.38157696723938, "perturbed_sampled_ll_std": 0.22232787659623407, "perturbed_original_ll_std": 0.16120081758428714}, {"original": "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "sampled": "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "perturbed_sampled": ["Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If"], "perturbed_original": ["Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420"], "original_ll": -5.37913703918457, "sampled_ll": -5.801395893096924, "all_perturbed_sampled_ll": [-5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924, -5.801395893096924], "all_perturbed_original_ll": [-5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457, -5.37913703918457], "perturbed_sampled_ll": -5.801395893096924, "perturbed_original_ll": -5.37913703918457, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency", "sampled": "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it is an existing TI Task). The task to detect this is to read and return a task-related object. This object needs", "perturbed_sampled": ["Add TaskInstance Return to correct TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of the given object (it is an existing TI Task). The task to detect this is to read and return a task-related object. This object needs", "Add TaskInstance state to TI . Tools should be colour-blind . Currently there is no way to determine the state of a Tooltip object (it is an existing TI ). One task to detect this is to read and return a task-related object. This object needs", "Add colourblind functionality to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to detect the state of a Tooltip object (it is an existing TI Task). The task to detect this is to read and return a task-related object. To do this a task needs", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the State of a Task (it is an existing TI Task). The task to detect this is to read and return a task-related object. This object needs", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it only knows the state of an existing TI Task). The task to fix this is to read and return a task-related object. This object needs", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of the object (it is an existing state). The task to detect this is to read and return a task-related information. The TaskInstance object needs", "Add TaskInstance state to TI Tooltip to make colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it is an existing TI Task). The task to detect this is to set a callback to return a task-related event. This Tooltip object needs", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it is a TI Task). The only way to detect this is to read and return a task-related object. This object needs", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no easy way to determine the state of a Tooltip object (it is an existing TI Task). The task of the TI Tooltip like this is to read the state of a task-related object. This object needs", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier ! At present there is no way to read and set the state of a Tooltip object (it is an existing TI Task). The task to do this is to read and return a task-related object. This object needs"], "perturbed_original": ["Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance within graph view or tree view for people with colour blindness Approximately 4.5% of the total US population has some form of colour vision deficiency", "Add TaskInstance Names for TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of the task in the graph view or tree view for people with colour vision problems. Approximately 4.5% of people experience some form of colour vision deficiency", "Add TaskInstance state to TI tree and graph: can be resolved for colour blind people too (#8910) Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier So far there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately half people experience some form of colour vision deficiency", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) There is no way to determine the state of a TaskInstance in the graph view or tree view of TI Tooltip. Task instances with colour blindness Approximately 4.5% of people experience an element of colour vision deficiency", "Add TaskInstance state to TI Tooltip to be colour-blind friendly. Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately 4.5% of humans experience some level of colour vision deficiency", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance in task view or tree view for people with colour /vision deficiency. 4.5% of people experience some form of colour vision deficiency", "Add TaskInstance state to TI Tooltip for colour-blind people Currently there is no way to determine the state of a TaskInstance in the graph that displays on the tooltip. Add status to TI Tooltip tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency", "Add TaskInstance state to TI BcoView to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance . Improve graph view or tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no tooltip to display the state of a TaskInstance in tab view or tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency"], "original_ll": -3.9705076217651367, "sampled_ll": -3.9782652854919434, "all_perturbed_sampled_ll": [-4.414066791534424, -4.239084720611572, -3.8709487915039062, -4.16396951675415, -3.8106882572174072, -4.04456090927124, -3.926408290863037, -3.865187883377075, -3.8666954040527344, -3.8736023902893066], "all_perturbed_original_ll": [-3.9965014457702637, -3.8240604400634766, -4.102521896362305, -4.259847640991211, -4.1813178062438965, -3.8935017585754395, -4.046728610992432, -3.591952323913574, -4.4254913330078125, -4.193055629730225], "perturbed_sampled_ll": -4.007521295547486, "perturbed_original_ll": -4.051497888565064, "perturbed_sampled_ll_std": 0.19150677730227889, "perturbed_original_ll_std": 0.22664863138698102}, {"original": "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating.", "sampled": "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to", "perturbed_sampled": ["Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values , URL is relative to base path and not relative to", "if you want ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to", "Chart: Allow ``webserver.base_url`` to be templated (#16126) - Webserver documentation states values of base URL is relative to base path and not relative to", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base DNS, not relative to", "Chart: Allow ``webserver.base_url`` to be set relative to base path. As `config`'s documentation states values of base URL is relative to base path and not relative to", "Chart: Allow ``webserver.base_url`` to be relative? (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base URL <unk>name,'; not relative to", "Chart: Allow ``webserver.base_url`` to contain path values (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to", "Chart: Allow ``webserver.base_url`` to be absolute value. As `config`'s documentation states values of base URL is relative to base path and not relative to", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to"], "perturbed_original": ["Chart: Allow ``webserver.base_url`` to be templated (#16126) As the chart above states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed via template one would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states that templated config files may be passed through `tpl`, one would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through , I would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As this states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through templated, we would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through `tpl`, one would expect <unk>webserver.base_url<unk> to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through `tpl`, one would expect its documentation to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through templating, I would expect `config.webserver.base_url` to also support templating.", "Chart: Allow ``webserver.base_url`` to be templated also. As `config`'s documentation states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating."], "original_ll": -3.485475540161133, "sampled_ll": -4.162430763244629, "all_perturbed_sampled_ll": [-4.620672225952148, -4.0405426025390625, -4.106842994689941, -4.327581882476807, -3.7803783416748047, -4.091899871826172, -4.761417865753174, -4.270689487457275, -4.013881683349609, -4.162430763244629], "all_perturbed_original_ll": [-3.582932949066162, -3.659243583679199, -3.2769808769226074, -3.649827480316162, -3.5337865352630615, -3.4643077850341797, -3.924382448196411, -4.144332408905029, -3.4062106609344482, -3.411101818084717], "perturbed_sampled_ll": -4.217633771896362, "perturbed_original_ll": -3.605310654640198, "perturbed_sampled_ll_std": 0.2771721705669687, "perturbed_original_ll_std": 0.24655594960158952}, {"original": "Move setting of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "sampled": "Move setting of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "perturbed_sampled": ["Move setting of project ID after activating service account (#17866) By Svetlana Khimich <khimich@hc-de.org>", "Move d naming convention and project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Move setting of SIG to the left after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Move setting of project ID after creation of project account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Move setting of project ID after activating : (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Move setting of project on activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", ". of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "for use of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Move setting of project ID after activating project? (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Move setting of project ID after activating service account (#17866) : Khimich <khimich@hc-de.org>"], "perturbed_original": ["Move setting of project ID after activating the project ID. (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Move setting of project ID after activating . (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Move setting of project ID after enabling new user account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Change of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Move setting of Gender and Service Account to service account after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Move setting to Account ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Move setting of project ID after activating service account to another project. Dmytro Khimich <khimich@google.com>", "Move setting of project ID after activating service account (#17866) Co-authored-by: Kimich Szymon <khimich@google.com>", "Move setting of project level to activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Move setting of project ID after activating service account (#17866) : Sergey Khimich <khimich@google.com>"], "original_ll": -4.423308849334717, "sampled_ll": -4.336629390716553, "all_perturbed_sampled_ll": [-4.478142738342285, -4.481937885284424, -4.4054059982299805, -4.095019817352295, -4.332839488983154, -4.323643207550049, -4.326204776763916, -4.1544060707092285, -4.213738441467285, -5.158105373382568], "all_perturbed_original_ll": [-4.125774383544922, -4.352086067199707, -4.240740776062012, -3.973822832107544, -4.339558124542236, -4.236653804779053, -4.6948418617248535, -5.142740726470947, -4.516923904418945, -5.160842418670654], "perturbed_sampled_ll": -4.396944379806518, "perturbed_original_ll": -4.478398489952087, "perturbed_sampled_ll_std": 0.2812985967768642, "perturbed_original_ll_std": 0.385292846509017}, {"original": "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "sampled": "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest aws versions and to be compatible with aws 1.7.0", "perturbed_sampled": ["Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy .h to use the latest aws versions and to be compatible with aws 1.7.0", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update d to reflect latest aws versions and to be compatible with aws 1.7.0", "Add type hints to aws provider * Updated AWS provider * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest server settings and to be compatible with aws 1.7.0", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update d to reflect latest aws versions and to check whether type hints work with aws 1.7.0", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy (aws 1.7.0) to support latest aws versions and to be compatible with aws 1.7.0", "Add type hints to aws provider (#11531) * Added check type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest aws versions and to be ready for aws 1.7.0", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update d to reflect latest aws versions and to be compatible with aws 1.7.0", "Add type hints to aws provider * Added type hints to aws provider * Updated aws provider to reflect latest aws versions and to be compatible with aws 1.7.0", "Add service description type hints to aws provider * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest aws versions and to be compatible with aws 1.7.0", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update to reflect latest aws versions and to be compatible with aws 1.7.0"], "perturbed_original": ["Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for aws provider * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Support s3-json for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Improved implementation of expectation for submit_job * Fix documentation Mik Laj Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update documentation * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to provider (#11531) * Added type hints to provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Added s3 type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix version numbering errors about submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to API Log provider (#11531) * Added type hints to API Log provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update documentation * Fix expectation for submit_job * Fix documentation Manuel Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to aws provider (#11531) * Added type hints to aws provider [#1135] * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add type hints to aws provider (#11531) * Add type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation for s3 Provider Mike L <unk>lvaro Laj Bregu\u0142a <mik-laj@users.noreply.github.com>"], "original_ll": -4.058938980102539, "sampled_ll": -3.500737428665161, "all_perturbed_sampled_ll": [-3.606269359588623, -3.6605987548828125, -3.6249470710754395, -3.824852705001831, -3.3887062072753906, -3.7503461837768555, -3.6605987548828125, -2.957244634628296, -3.57794189453125, -3.4022321701049805], "all_perturbed_original_ll": [-3.894770383834839, -4.035798072814941, -4.108150005340576, -3.9638590812683105, -4.20694637298584, -4.075825214385986, -4.074731826782227, -4.138514995574951, -4.007943153381348, -4.44126558303833], "perturbed_sampled_ll": -3.545373773574829, "perturbed_original_ll": -4.094780468940735, "perturbed_sampled_ll_std": 0.23424490353394498, "perturbed_original_ll_std": 0.14270596122552895}, {"original": "Add verify_ssl config for kubernetes (#13516)", "sampled": "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "perturbed_sampled": ["Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)"], "perturbed_original": ["Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)", "Add verify_ssl config for kubernetes (#13516)"], "original_ll": -4.943172931671143, "sampled_ll": -2.856890916824341, "all_perturbed_sampled_ll": [-2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341, -2.856890916824341], "all_perturbed_original_ll": [-4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143, -4.943172931671143], "perturbed_sampled_ll": -2.856890916824341, "perturbed_original_ll": -4.943172931671143, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "sampled": "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "perturbed_sampled": ["Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker"], "perturbed_original": ["Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)"], "original_ll": -6.472273826599121, "sampled_ll": -6.581944465637207, "all_perturbed_sampled_ll": [-6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207, -6.581944465637207], "all_perturbed_original_ll": [-6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121, -6.472273826599121], "perturbed_sampled_ll": -6.581944465637207, "perturbed_original_ll": -6.472273826599121, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "sampled": "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "perturbed_sampled": ["[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The"], "perturbed_original": ["[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)"], "original_ll": -6.033203125, "sampled_ll": -6.346513748168945, "all_perturbed_sampled_ll": [-6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945, -6.346513748168945], "all_perturbed_original_ll": [-6.033203125, -6.033203125, -6.033203125, -6.033203125, -6.033203125, -6.033203125, -6.033203125, -6.033203125, -6.033203125, -6.033203125], "perturbed_sampled_ll": -6.346513748168945, "perturbed_original_ll": -6.033203125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "sampled": "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "perturbed_sampled": ["Use Python 3 style super classes (#11806) or init() super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA pattern for subclassing objects that are", "Use Python 3 style super classes (#11806) instead of super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "Use Python code for super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for instantiating objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better way to handle subclassing objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better solution is subclassing objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, ) #11806\n\nA better syntax for subclassing objects that are", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA method for subclassing objects that are"], "perturbed_original": ["Use Python 3 style super functions for example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes (#11806) example: ``` self.__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs). ```", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) version of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes to replace ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` .label, validators, **kwargs) ```", "Use Python 3 style supercall (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "Use Python 3 style super classes instead of ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```"], "original_ll": -3.0042622089385986, "sampled_ll": -3.6834604740142822, "all_perturbed_sampled_ll": [-3.970177173614502, -3.8094520568847656, -3.7550041675567627, -3.7008447647094727, -3.7315566539764404, -3.68157696723938, -3.7799911499023438, -3.6834604740142822, -4.1407012939453125, -3.793092966079712], "all_perturbed_original_ll": [-2.779315233230591, -3.289156436920166, -2.9064106941223145, -3.284064292907715, -3.2273528575897217, -3.139187812805176, -2.7162928581237793, -3.5629076957702637, -3.063559055328369, -2.5954513549804688], "perturbed_sampled_ll": -3.8045857667922975, "perturbed_original_ll": -3.0563698291778563, "perturbed_sampled_ll_std": 0.13766303249846548, "perturbed_original_ll_std": 0.28742057349098904}, {"original": "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "sampled": "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "perturbed_sampled": ["[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In"], "perturbed_original": ["[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)"], "original_ll": -6.7266011238098145, "sampled_ll": -7.089080810546875, "all_perturbed_sampled_ll": [-7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875, -7.089080810546875], "all_perturbed_original_ll": [-6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145, -6.7266011238098145], "perturbed_sampled_ll": -7.089080810546875, "perturbed_original_ll": -6.7266011238098145, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "sampled": "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "perturbed_sampled": ["[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On"], "perturbed_original": ["[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)"], "original_ll": -6.535831928253174, "sampled_ll": -6.821267127990723, "all_perturbed_sampled_ll": [-6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723, -6.821267127990723], "all_perturbed_original_ll": [-6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174, -6.535831928253174], "perturbed_sampled_ll": -6.821267127990723, "perturbed_original_ll": -6.535831928253174, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "sampled": "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "perturbed_sampled": ["Remove kwargs from Super calls in AWS /EC2 (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "Remove kwargs .basesecrets_create calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to run in backends which", "Remove kwargs from Kwargs in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "Remove kwargs from Super calls in AWS S3. (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want to add kwargs to `BaseSecrets`, for which", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs from API for which", "Remove kwargs from Super calls from Server Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "Remove kwargs from Super Keys and AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want to send kwargs to `BaseSecrets`, for which"], "perturbed_original": ["Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to the end of the call and never need `BaseSecretsBackend` nor `LoggingMixin`", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) Does neither want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) don<unk>t want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "Remove kwargs from Super calls in AWS S3 (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "Remove d Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", ") Refrain from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "Remove kwargs from Super Logging and Base AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want in these cases to have to pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass such details to `BaseSecretsBackend` nor `LoggingMixin`", "Remove d Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`"], "original_ll": -4.820204257965088, "sampled_ll": -5.145382881164551, "all_perturbed_sampled_ll": [-5.040154457092285, -4.902301788330078, -4.929413318634033, -4.835127830505371, -4.790879249572754, -4.6031951904296875, -5.32673978805542, -5.244399070739746, -5.195272445678711, -4.715578556060791], "all_perturbed_original_ll": [-4.981626987457275, -5.311412811279297, -5.393091201782227, -4.743758678436279, -5.460482597351074, -5.191327095031738, -4.820505142211914, -4.602646350860596, -5.169565677642822, -5.460482597351074], "perturbed_sampled_ll": -4.958306169509887, "perturbed_original_ll": -5.1134899139404295, "perturbed_sampled_ll_std": 0.22641579089956806, "perturbed_original_ll_std": 0.2946610860739722}, {"original": "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "sampled": "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "perturbed_sampled": ["[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At"], "perturbed_original": ["[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)"], "original_ll": -5.729770183563232, "sampled_ll": -6.1820526123046875, "all_perturbed_sampled_ll": [-6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875, -6.1820526123046875], "all_perturbed_original_ll": [-5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232, -5.729770183563232], "perturbed_sampled_ll": -6.1820526123046875, "perturbed_original_ll": -5.729770183563232, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now", "sampled": "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "perturbed_sampled": ["[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator test, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * Add gRPCOperator, unit test and added to auto doc (#4917) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "[AIRFLOW-4092] Add - unit test and added to auto doc (#4507) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit"], "perturbed_original": ["[AIRFLOW-4092] Add gRPCOperator, unit test and added to build branch (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to build branch (#4923) * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] Add test as well for handling errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now", "[AIRFLOW-4092] Add an unifying test and added to auto doc (#4923) * [AIRFLOW-4092] Add an unifying test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type _relative because we don't use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * Remove hook dispatcher and auth_type as we don't use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc to fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and add it to doc as we don't use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] Rename dispatcher and auth_type as C:<unk>/lib. // We use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc , and fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and gRPIOperator to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and gRPIOperator to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we do not need it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] Remove dispatcher and auth_type API from controller and don't use it now", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove dispatcher and disabling since we don't use it now"], "original_ll": -3.095273733139038, "sampled_ll": -1.7969061136245728, "all_perturbed_sampled_ll": [-1.8906880617141724, -1.7969061136245728, -1.7969061136245728, -1.7969061136245728, -1.7969061136245728, -1.7969061136245728, -1.7969061136245728, -2.0982022285461426, -1.7969061136245728, -2.448054552078247], "all_perturbed_original_ll": [-2.8771424293518066, -3.0074870586395264, -3.2914113998413086, -3.347402572631836, -3.2893567085266113, -3.4170265197753906, -3.5204977989196777, -3.0468156337738037, -3.042377471923828, -2.8942599296569824], "perturbed_sampled_ll": -1.9015287637710572, "perturbed_original_ll": -3.173377752304077, "perturbed_sampled_ll_std": 0.2034968555821524, "perturbed_original_ll_std": 0.2153800450402031}, {"original": "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - Remove unnecessary specification of default conn ids", "sampled": "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to start_date. - Split the template into two - use @GoogleDags in", "perturbed_sampled": ["Add the following fix to the google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to batch 2 - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example dags template. Changes to Google Cloud Example Dags Template 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to start_date. - Cut template into two - use @GoogleDags in", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - fix the dates, so they are random - Tear down all code related to it - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example dags - batch 2 (#19527) - Remove all code related to start_date - Use catchup=False to close down all code related to start_date. - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example in batch 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to the template file - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - batch 2 - Tear out code related to start_date. - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static template; - Use catchup=False - Tear down all code , other than start_date. - Split the template into two - use @GoogleDags in", "Clean-up template from cloud example dags - batch 2 (#19527) - Replace start_date with start_date - Use catchup=False - Tear down all code related to start_date. - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example dags - Fix for Google Cloud bug (#19527) - Use static start_date - Use catchup=False - Tear down all code related to the dag - Split the template into two - use @GoogleDags in", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use static start_date templates in template - Tear down all code related to start_date. - split template into two - use @GoogleDags in"], "perturbed_original": ["Clean-up of google cloud example dags - batch 2 - Use static start_date - Use catchup=False - Tidy up the chaining ; if necessary change the chaining in some cases - Remove unnecessary specification of default conn ids", "Clean-up of google cloud example dags - batch 2 - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - Implement a specification of default conn ids", "Clean-up of google cloud example - Adding batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - New specification of default conn ids", "Clean-up of google cloud sync service issue: - batch 2 (#19527) - Use static start_date - Use catchup=False - Avoid the chaining of tasks in some cases - Remove unnecessary specification of default conn ids", "Remove extra google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the default tasks in some cases - Remove unnecessary specification of default conn ids", "Clean-up of google cloud crawling - batch 2 (#19527) - Use static start_date - Use catchup=False to speed up the chaining of tasks in some cases - Remove unnecessary specification of default conn ids", "Clean-up of google cloud example dags - batch _init - Use static start_date - Use catchup=False - Fix a bug that prevent the chaining of tasks in some cases - Remove unnecessary specification of default conn ids", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in cases - Remove all use of default conn ids", "Clean-up of google cloud example dags - batch 2 (#19527) - Use start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - Avoid unnecessary specification of default conn ids", "Clean-up of google cloud task queue - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - Eliminate unnecessary specification of default conn ids"], "original_ll": -4.918028831481934, "sampled_ll": -4.533754348754883, "all_perturbed_sampled_ll": [-4.7527337074279785, -4.458693504333496, -4.6897501945495605, -4.487042427062988, -4.660538673400879, -4.609602928161621, -5.077003479003906, -4.6033034324646, -4.774134159088135, -4.348988056182861], "all_perturbed_original_ll": [-5.114742279052734, -5.071982383728027, -4.988377571105957, -4.9316887855529785, -5.419076919555664, -4.779364585876465, -4.997264385223389, -4.982895374298096, -5.047786712646484, -4.723888397216797], "perturbed_sampled_ll": -4.646179056167602, "perturbed_original_ll": -5.005706739425659, "perturbed_sampled_ll_std": 0.19168063274022376, "perturbed_original_ll_std": 0.18054079252863842}, {"original": "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter which allows users to get output in form of table, json or yaml.", "sampled": "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "perturbed_sampled": ["Refactor plugins command output using AirflowConsole (#13036) - refactors the airflow plugins command to be used with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "refactor for command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that custom functions are automatically added. This allows code generated by", "Refactor plugins command output logic (#13036) This PR refactors the airflow plugins command to be compatible with 'output' . This means that new functions are automatically added. This allows code generated by", "Refactor plugins command using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that plugins are automatically added. This allows code generated by", "Refactor plugins command output using AirflowConsole (#13036) This modifies the plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "Refactor airflow plugins as output using an input parameter. This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means new functions are automatically added. See the PR for example code generated by", "Refactor plugins command output parameter (#13036) : This refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "Refactor plugins command for 'output' parameter in AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions can be added. This allows code generated by", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with AirflowConsole using a parameter, which means no new functions are automatically added. This allows code generated by"], "perturbed_original": ["Refactor plugins command output using AirflowConsole (#13036) This PR refactors the pluggin command to be compatible with 'output' parameter which allows users to output in form of table, json or yaml.", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow console plugin 'command' expression code to be compatible with 'output' parameter which allows users to get output from command in form of table, json or yaml.", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter which allows you to get output in form of json or yaml.", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be more consistent with AirflowConsole by adding 'output' parameter which has the ability to get output in form of table, json or yaml.", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' which allows users to get output in form of argmp or yaml.", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the plugins command to be compatible with 'output' parameter . Allow users to get output in form of table, json or yaml.", "Refactor plugins command output using AirflowConsole (#13036) This PR refines airflow plugins command to be compatible with 'output' parameter which allows users to get output in form of table, json or yaml.", "Refactor plugins command output using AirflowConsole . PR Refactor airflow plugins command to be compatible with 'output' parameter which allows users to get output in form of table, json or yaml.", "Refactor the command system compatible with output using AirflowConsole (#13036) This PR refactors the airflow plugins command system compatible with 'output' parameter which allows users to get output in form of table, json or yaml.", "Refactor plugins for using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' . This new command allows users to get output in form of table, json or yaml."], "original_ll": -4.554455280303955, "sampled_ll": -4.462299346923828, "all_perturbed_sampled_ll": [-4.471075057983398, -4.568077564239502, -4.547204494476318, -4.5075836181640625, -4.423219203948975, -4.104783058166504, -4.526181221008301, -4.4669108390808105, -4.008937358856201, -4.309866428375244], "all_perturbed_original_ll": [-4.5666279792785645, -4.35477352142334, -4.476865768432617, -4.0474724769592285, -4.913764953613281, -4.641936779022217, -4.8066301345825195, -4.96714448928833, -4.424205303192139, -4.4384026527404785], "perturbed_sampled_ll": -4.393383884429932, "perturbed_original_ll": -4.563782405853272, "perturbed_sampled_ll_std": 0.1830989854800812, "perturbed_original_ll_std": 0.2651974707940846}, {"original": "Fix task search function in Graph view (#15901)", "sampled": "Fix task search function in Graph view (#15901)One", "perturbed_sampled": ["Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One", "Fix task search function in Graph view (#15901)One"], "perturbed_original": ["Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)", "Fix task search function in Graph view (#15901)"], "original_ll": -6.499565124511719, "sampled_ll": -7.43597412109375, "all_perturbed_sampled_ll": [-7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375, -7.43597412109375], "all_perturbed_original_ll": [-6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719, -6.499565124511719], "perturbed_sampled_ll": -7.43597412109375, "perturbed_original_ll": -6.499565124511719, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "sampled": "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "perturbed_sampled": ["Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The"], "perturbed_original": ["Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)"], "original_ll": -6.875676155090332, "sampled_ll": -7.009141445159912, "all_perturbed_sampled_ll": [-7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912, -7.009141445159912], "all_perturbed_original_ll": [-6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332, -6.875676155090332], "perturbed_sampled_ll": -7.009141445159912, "perturbed_original_ll": -6.875676155090332, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in to get_conn - add salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "sampled": "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description to clarify some context (and not", "perturbed_sampled": ["[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor and update docs - change sign_in_form_name to sign_in_form(data=user=@user , error=@error) - improve link link description to clarify some context (and not", "[AIRFLOW-3993] Add tests for sign_in_form (#4829) - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve user description to clarify some context (and not", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in_form_name to \"\" (e.g. secret=@sec, error=@error) - improve link link to clarify some context (and not", "[AIRFLOW-3993] Add the salesforce hook (#4829) - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, sign_in_form_name) - improve link link description to clarify some context (and not", "[AIRFLOW-3993] Add tests to sign_in_form hook (#4829) - refactor code - update docs - update hook to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description to clarify some context (and not", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - better test examples - improve function - new option to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description to clarify some context (and not", "[AIRFLOW-3993] Add tests for input validation (#4829) - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user +, error=@error) - improve link link description to clarify some context (and not", "[AIRFLOW-3993] Add tests for salesforce hook s - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description , making it use some context (and not", "[AIRFLOW-3993] Add one more salesforce hook (#4829) - refactor code - update docs - change form to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description to clarify some context (and not", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code to read from docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, error=@error) - change link link description to clarify some context (and not"], "perturbed_original": ["[AIRFLOW-3993] Add tests for salesforce hook (#4829) - add sign_in hook to hooks - update docs for sign_in to get_conn - add salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in to get_conn - add salesforce hook test packages - add note to UPDATING.md - send fix package to mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in function to be more explicit when signing in - add salesforce to devel_all - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - fix code - update docs - change sign_in to get_conn - add salesforce to devel_all - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change from get_conn s_packages for salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook s - refactor code - update docs - change sign_in to get_conn - add salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in to get_conn - add pull request for devel_all packages - add note to log - Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in to get_conn - add salesforce to devel_all packages - Add a note to UPDATING.md mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code base - add helper docs - add salesforce to get_conn - add salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - add hook to get_conn - add salesforce to hooks - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>"], "original_ll": -3.955713987350464, "sampled_ll": -4.022988796234131, "all_perturbed_sampled_ll": [-4.247507095336914, -3.5967514514923096, -4.4706807136535645, -3.9624509811401367, -4.050339221954346, -4.516998291015625, -4.058938026428223, -4.234475612640381, -4.3596014976501465, -4.0167670249938965], "all_perturbed_original_ll": [-3.945749282836914, -3.950730562210083, -3.826023578643799, -3.979627847671509, -4.16439962387085, -3.992661237716675, -3.997025728225708, -4.024447917938232, -3.9061291217803955, -3.820694923400879], "perturbed_sampled_ll": -4.151450991630554, "perturbed_original_ll": -3.9607489824295046, "perturbed_sampled_ll_std": 0.2598679535420665, "perturbed_original_ll_std": 0.0944969347676538}, {"original": "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "sampled": "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "perturbed_sampled": ["[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The"], "perturbed_original": ["[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)"], "original_ll": -5.189088344573975, "sampled_ll": -5.456293106079102, "all_perturbed_sampled_ll": [-5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102, -5.456293106079102], "all_perturbed_original_ll": [-5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975, -5.189088344573975], "perturbed_sampled_ll": -5.456293106079102, "perturbed_original_ll": -5.189088344573975, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "sampled": "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of extra options; this bug gives extra options to the URL to which I have", "perturbed_sampled": ["Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of extra options; this bug gives extra options to the browser, in addition to which I have", "Extend HTTP extra_options to LivyHook s (#14816) The LivyHook used by the LivyOperator has a lot of extra options; this bug gives extra options to this hook, to which I have", "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of extra options; the hook also gives extra information about the URL to which I have", "Extend HTTP extra_options to HTTP operator (#14816) The LivyHook used by the LivyOperator has a list of HTTP extra options; this bug gives extra options to the URL to which I have", "Extend HTTP to LivyHook and operator (#14816) The LivyHook used by operator has a lot of extra options; this bug gives extra options to the URL to which I have", "Extend HTTP connection to URL with LivyHook and operator (#14816) The LivyHook used by the LivyOperator has lots of extra options; this bug gives extra options to the URL to which I have", "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a URL that provides extra options. This bug gives extra options to the URL to which I have", "Extend URL options to LivyHook and operator (#14816) The LivyHook used in the LivyOperator has a lot of extra options; this bug gives extra options to the URL to which I have", "Extend URL URL to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of extra options; this bug gives extra options to the URL to which I have", "Extend HTTP extra_options to LivyHook and URL. The LivyHook used by the URL in my site has a lot of extra options; this bug gives extra options to the URL to which I have"], "perturbed_original": ["Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method method. But I saw no way to extend its functionality from the actual operator itself.", "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the operator itself.", "Add extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method but there is no way to use them from the actual operator itself.", "Extend extra_options functionality to LivyHook and operator (#14816) The hook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "Extend extra_options to LivyHook and operator (#14816) The LivyHook used by operator.start() has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "Extend HTTP extra_options to LivyHook and operator . LivyHook has extra_options in its main run_method and the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator provides extra options in its main run_method but there's no way to add them from the actual operator itself.", "Extend HTTP extra_options to LivyHook using the LivyOperator. (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "Extend HTTP extra_options to LivyHook . (#14816) The LivyHook used by the LivyOperator has extra_options when using its main run_method but there's no way to use them from the actual operator itself.", "Extend HTTP extra_options to LivyHook and operator (#14816) The different HTTP options provided by the LivyOperator has options via its main run_method but there's no way to use them from the actual operator itself."], "original_ll": -4.13319206237793, "sampled_ll": -4.3155927658081055, "all_perturbed_sampled_ll": [-4.234919548034668, -4.153022766113281, -4.342316150665283, -4.789228439331055, -4.568983554840088, -4.356457710266113, -4.412477016448975, -4.302102088928223, -4.299620628356934, -4.12925910949707], "all_perturbed_original_ll": [-4.221363067626953, -4.079519271850586, -4.001466751098633, -4.3632097244262695, -4.19110107421875, -3.533433675765991, -4.306511402130127, -3.9246346950531006, -4.23347282409668, -4.820206165313721], "perturbed_sampled_ll": -4.358838701248169, "perturbed_original_ll": -4.167491865158081, "perturbed_sampled_ll_std": 0.1870028574015172, "perturbed_original_ll_std": 0.3142918653342389}, {"original": "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "sampled": "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "perturbed_sampled": ["[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:"], "perturbed_original": ["[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)"], "original_ll": -5.838682651519775, "sampled_ll": -5.734640598297119, "all_perturbed_sampled_ll": [-5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119, -5.734640598297119], "all_perturbed_original_ll": [-5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775, -5.838682651519775], "perturbed_sampled_ll": -5.734640598297119, "perturbed_original_ll": -5.838682651519775, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "sampled": "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "perturbed_sampled": ["[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This"], "perturbed_original": ["[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)"], "original_ll": -6.320858955383301, "sampled_ll": -6.596794605255127, "all_perturbed_sampled_ll": [-6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127, -6.596794605255127], "all_perturbed_original_ll": [-6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301, -6.320858955383301], "perturbed_sampled_ll": -6.596794605255127, "perturbed_original_ll": -6.320858955383301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use DAG_ACTIONS constant. (#16232)", "sampled": "Use DAG_ACTIONS constant. (#16232)About", "perturbed_sampled": ["Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About", "Use DAG_ACTIONS constant. (#16232)About"], "perturbed_original": ["Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)", "Use DAG_ACTIONS constant. (#16232)"], "original_ll": -5.436949253082275, "sampled_ll": -6.420441150665283, "all_perturbed_sampled_ll": [-6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283, -6.420441150665283], "all_perturbed_original_ll": [-5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275, -5.436949253082275], "perturbed_sampled_ll": -6.420441150665283, "perturbed_original_ll": -5.436949253082275, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clean up airflow.contrib in Kubernetes docs (#9551)", "sampled": "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "perturbed_sampled": ["Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Clean up airflow.contrib in Kubernetes docs (#9551)\""], "perturbed_original": ["Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Clean up airflow.contrib in Kubernetes docs (#9551)"], "original_ll": -5.611946105957031, "sampled_ll": -6.084292888641357, "all_perturbed_sampled_ll": [-6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357, -6.084292888641357], "all_perturbed_original_ll": [-5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031, -5.611946105957031], "perturbed_sampled_ll": -6.084292888641357, "perturbed_original_ll": -5.611946105957031, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is easy for us to maintain compatibility, so we should", "sampled": "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is there. It should be merged into a commit that", "perturbed_sampled": ["Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using that implementation directly, but it is there. It should be merged into trunk at that", "Add back-compat layer to clear_task_instances (#16582) is unlikely that anyone is using this function directly, but it is there. These two commits should be merged into a commit that", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is there. It should be merged in the commit that", "Add back-compat layer call (#16582) It is unlikely that anyone is using this function , but it is there. It should be merged into a commit that", "Add back-compat ibility for clear_task_instances (#16582) in VCS. It is unlikely that anyone is using this function directly, but it is there. It should be merged into a commit that", "Add back-compat layer to clear_task_instances (#16582) It seems unlikely that anyone is using this function directly, but it is there. It should be merged into the main codebase after that", "Add back-compat layer for debugging functions (#16582) It is unlikely that anyone is using this function but it is there. It should be merged into a commit that", "Add back-compat layer to clear_task_instances (#16582) It doesn't seem that anyone is using this function directly, but it is there. Probably can be merged into a commit that", "Add back-compat layer to clear_task_instances (#16582) It doesn't look to me that anyone is using this function directly, but it is there. It should be translated into a commit that", "Add back-compat layer to clear_task_instances (#16582) : it's very unlikely that anyone is using this function directly, but it is the only thing that should be merged into a commit that"], "perturbed_original": ["Add back-compat layer to back-compat. It is unlikely that anyone is using git directly, but it is easy for us to maintain compatibility, so we should", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but its easy for us to maintain . And I thought we should", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this in their environment, but it is easy for us to maintain it and we should", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this feature, but it is also useful for us to maintain compatibility, so we should", "Add back-compat layer to clear_task_instances . It is unlikely that Apple will is using this function directly, but it is easy for us to maintain compatibility, so we should", "Add #1898 to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is easy enough to maintain compatibility, so we should", "Move a layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, and it is easy for us to maintain compatibility, so we should", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone will use this function directly, but it seems important for us to maintain compatibility, so we should", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that users would be using this function directly, but it is easy for us to maintain compatibility, so we should", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this directly, but it is easy enough to maintain compatibility, so we should"], "original_ll": -3.894094467163086, "sampled_ll": -3.898669719696045, "all_perturbed_sampled_ll": [-4.104280471801758, -4.349344730377197, -3.921391725540161, -4.123264789581299, -4.059682369232178, -3.7936575412750244, -4.032174587249756, -3.9874155521392822, -3.9070723056793213, -3.937741756439209], "all_perturbed_original_ll": [-3.3996634483337402, -4.403683662414551, -3.938274383544922, -3.9328854084014893, -3.919111967086792, -4.233418941497803, -4.016439437866211, -3.9290146827697754, -3.857395887374878, -4.021270275115967], "perturbed_sampled_ll": -4.021602582931519, "perturbed_original_ll": -3.9651158094406127, "perturbed_sampled_ll_std": 0.14472986002825247, "perturbed_original_ll_std": 0.2462995394288389}, {"original": "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "sampled": "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "perturbed_sampled": ["[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's"], "perturbed_original": ["[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)"], "original_ll": -6.205388069152832, "sampled_ll": -6.484692573547363, "all_perturbed_sampled_ll": [-6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363, -6.484692573547363], "all_perturbed_original_ll": [-6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832, -6.205388069152832], "perturbed_sampled_ll": -6.484692573547363, "perturbed_original_ll": -6.205388069152832, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "detect incompatible docker server version in breeze (#9042)", "sampled": "detect incompatible docker server version in breeze (#9042)The", "perturbed_sampled": ["detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The", "detect incompatible docker server version in breeze (#9042)The"], "perturbed_original": ["detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)", "detect incompatible docker server version in breeze (#9042)"], "original_ll": -6.299882888793945, "sampled_ll": -6.861575603485107, "all_perturbed_sampled_ll": [-6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107, -6.861575603485107], "all_perturbed_original_ll": [-6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945, -6.299882888793945], "perturbed_sampled_ll": -6.861575603485107, "perturbed_original_ll": -6.299882888793945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make them available for python tools. Also /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. It also contains one small change for Linux users - the files created by docker gcloud are created with root user so in order to fix that the directories mounted from the host are fixed when you exit the tool - their ownership is changed to be owned by the host user", "sampled": "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app shows a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In general the tool will ask about the running OS during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "perturbed_sampled": ["Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud tool CI command will give you a list of open CI jobs and the tool will highlight them automatically. * More info about the app on the cloud wiki page * More info about the app on the cloud wiki page * \"Automated Continuous Integration\" can be selected upon installing the cloud tool. In general the tool will ask about its OS during installation. We also support the CI workflow using version control as in Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app shows a list of open CI jobs and the app can highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In general the tool provides information about the running jobs, including the tools available for installation. We also support the CI workflow s based on Jenkins and Git-Flow. * More info about automated continuous integration on DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud tool will show a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page The default setting is \"Automated Continuous Integration\", which can be selected through the cloud tool. Also the tool knows about the running OS during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Updates on the plugin for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app can store a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the tool on DevOps wiki page * \"Automated Continuous Integration\" can now be installed through the cloud tool. In general the tool will ask about the running OS during installation. We also support the automatic deployment using Jenkins . * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app shows a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about CI containers on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the app. In general the tool will ask about the running Jenkins server installation. We also support the CI workflow using Jenkins and VTS. * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud CI container contains a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In general the tool will ask for the CI container running OS during the setup. The tool can also be used in CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) Improved cloud tool available in the trimmed down CI container The tools now have more support for trimmed down CI containers The cloud app shows a list of open CI servers and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In the trimmed down container the tool will ask about the running OS during installation. We also support the CI workflow s Reactive Code and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app shows a list of open commands and the tool will highlight them . * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In general the cloud tool will ask about the environment used during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The cloud tool will have sheers: the cloud app shows a list of all the open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be used with the cloud tool. In general the tool will ask about the automation during installation. We can set up and update the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container (#9166) Some CI jobs now have sheers: the cloud app shows a list of open CI jobs and the tool will highlight them automatically. * More info about the app on the web page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the configuration. In general the tool will ask about the appropriate setup options during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about continuous integration at DevOps wiki page * We added support for"], "perturbed_original": ["make cli tool available in the trimmed down CI container (#9167) * remove cli tool available in the trimmed down CI container (#9167) * docker tools now have shebangs which make them available for python tooling. /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to access directly to/from the host Airflow. It also contains one small change for Linux users - the files created by docker gcloud are created with root user so in order to fix that the directories mounted from the host are fixed when you exit the tool - their ownership is changed to be owned by the host user", "Improved cloud tool available in the trimmed down CI container . Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make them available for python -generated code. The file /opt/airflow is now mounted from the host CI container which makes it possible for the tools to copy files directly to/from the sources of Airflow. It also contains one small change for Linux users - the files shared by firefox from docker gcloud are created with root user so in order to fix that the files copied from the host to the container are also fixed when you exit the tool - their ownership is changed to be owned by the host user", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make them available for python tools. Also the tools can now mounted from the host Airflow sources which makes it easy for the tools to copy files directly to/from the sources of Airflow. It also contains one bugfix for Linux users - the files created by gcloud are created with the host user as a user, so in order to prevent conflicts with Airflow, the directories created by the host are fixed when you exit the tool - their ownership is assumed to be owned by the host user", "Improved cloud tool available in the trimmed down CI container (#9167) * This release fixes cloud tool available in the trimmed down CI container. The tools now have shebangs which make them interact with the python tools. Also /opt/airflow is now mounted from the host to the sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. It also contains one small change for Linux users - the files created by docker gcloud are created with root user so it has to fix that the directories mounted from the host are fixed when you exit docker - their ownership is changed to be owned by the host user", "Improved cloud tool available in the trimmed down CI container Change Log: Improved cloud tool available in the trimmed down CI container The tools now use airflow containers to use the container and which make them available for python tools. Each container is now mounted from the Airflow sources which makes it possible for the tools to copy files from the container to the sources of Airflow. It also contains one small change for Linux users - the directories in the containers created by the tools are created with root user so in order to fix that the directories mounted from the host are fixed when you exit the tool - their ownership is changed to be owned by the host user", "cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container (#9589) * Cloud tools now have shebangs which make them available for python tools. Also /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to copy files directly to/from the sources of the Airflow source. This release also contains one small change for Linux users : The files created by docker tool are created with root user so in order to fix that the directories mounted from the host are fixed when you exit the tool - their ownership is changed to that owned by the host user", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which are available for python tools. The Docker is now mounted from the host Airflow . This makes it possible for the tools to copy files directly to/from Airflow and thus avoid running on each instance of Airflow. It also contains one small change for Linux users - the files created by docker gcloud are created with root user so in git using the Linux fix that the directories mounted from the host are not deleted when you exit the tool - their ownership is changed to be owned by the host user", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The trimmed down CI container contains some additional new containers that have been added to make them available for python tools. Also /opt/airflow is enabled to copy files from the host Airflow sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. It also contains one small bummer for Linux users - the files created by docker gcloud are owned by the root user so in order to fix that the directories mounted from the host are fixed when running the tool the ownership is changed to be owned by the host user", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make them more familiar with python tools. Also /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to copy files to/from the sources of Airflow. It also contains one small change for Linux users - the files created by docker gcloud are not owned by the root user so in order to fix that the files created by gcloud and shared from the host are fixed when you exit docker gcloud - their ownership is changed to be owned by the root user", "* Make any tool available in the trimmed down CI container (#9167) * Make any tool available in the trimmed down CI container The tools now are mounted from Airflow which make them available for airflow. Also /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. It has just one small change for Linux users - the files created by docker gcloud are created with root user so in order to fix that the directories mounted from the host are fixed when you install the tool - their users have to be changed to be owned by the host user"], "original_ll": -3.827514886856079, "sampled_ll": -3.1485157012939453, "all_perturbed_sampled_ll": [-3.3780460357666016, -3.178438186645508, -3.229797124862671, -3.1293201446533203, -3.2764649391174316, -3.1445608139038086, -3.243849754333496, -3.192784070968628, -3.0951027870178223, -3.2558720111846924], "all_perturbed_original_ll": [-3.7115516662597656, -3.8143506050109863, -3.6771810054779053, -3.834320306777954, -3.792273998260498, -3.781264305114746, -3.8525843620300293, -3.6424999237060547, -3.5357935428619385, -3.5840373039245605], "perturbed_sampled_ll": -3.212423586845398, "perturbed_original_ll": -3.7225857019424438, "perturbed_sampled_ll_std": 0.07843833624547299, "perturbed_original_ll_std": 0.10430184354025095}, {"original": "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "sampled": "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "perturbed_sampled": ["fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For"], "perturbed_original": ["fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "fix bug of SparkSql Operator log going to infinite loop. (#19449)"], "original_ll": -6.049223899841309, "sampled_ll": -6.560698509216309, "all_perturbed_sampled_ll": [-6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309, -6.560698509216309], "all_perturbed_original_ll": [-6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309, -6.049223899841309], "perturbed_sampled_ll": -6.560698509216309, "perturbed_original_ll": -6.049223899841309, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove unused 'context' variable in task_instance.py (#14049)", "sampled": "Remove unused 'context' variable in task_instance.py (#14049)I", "perturbed_sampled": ["Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I", "Remove unused 'context' variable in task_instance.py (#14049)I"], "perturbed_original": ["Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)", "Remove unused 'context' variable in task_instance.py (#14049)"], "original_ll": -4.48358154296875, "sampled_ll": -5.201879024505615, "all_perturbed_sampled_ll": [-5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615, -5.201879024505615], "all_perturbed_original_ll": [-4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875, -4.48358154296875], "perturbed_sampled_ll": -5.201879024505615, "perturbed_original_ll": -4.48358154296875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "UPDATING.md for changes included in 2.1.1 (#16615)", "sampled": "UPDATING.md for changes included in 2.1.1 (#16615)The", "perturbed_sampled": ["UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The", "UPDATING.md for changes included in 2.1.1 (#16615)The"], "perturbed_original": ["UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)", "UPDATING.md for changes included in 2.1.1 (#16615)"], "original_ll": -4.0340070724487305, "sampled_ll": -4.425573348999023, "all_perturbed_sampled_ll": [-4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023, -4.425573348999023], "all_perturbed_original_ll": [-4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305, -4.0340070724487305], "perturbed_sampled_ll": -4.425573348999023, "perturbed_original_ll": -4.0340070724487305, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Google Memcached hooks - improve protobuf messages handling (#11743)", "sampled": "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "perturbed_sampled": ["Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Google Memcached hooks - improve protobuf messages handling (#11743)\""], "perturbed_original": ["Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Google Memcached hooks - improve protobuf messages handling (#11743)"], "original_ll": -5.319725513458252, "sampled_ll": -5.819211959838867, "all_perturbed_sampled_ll": [-5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867, -5.819211959838867], "all_perturbed_original_ll": [-5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252, -5.319725513458252], "perturbed_sampled_ll": -5.819211959838867, "perturbed_original_ll": -5.319725513458252, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "sampled": "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second issue being that a task instance may have been closed.) Fixes a crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused", "perturbed_sampled": ["Fix es an error which may sometimes occur when the user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not starting in a supported way and the other being that a task instance may have been closed.) Fixes a crash caused by closing task instances when the application is visible\n\nFix bug which sometimes caused", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) Needed to fix the following error in the log message: [ERROR] The application does not start because of a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second being that a task instance may have been closed.) Fixes a crash caused by task instances when the application view is not visible\n\nFix bug which sometimes caused", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being too recent. This is to fix a crash caused by user creating a task instance Result in the error / log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second likely issue being that a task instance may have been closed.) Fixes a crash caused by deleting task instances when the instances are not visible\n\nFix bug which sometimes caused", "Fix crash when user was viewing \"Task Instance Details\" caused by start_date being None (#14416) Fix crash caused by user being unable to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two issues, the first being that the application is not running in a supported way and the second issue being that a task instance may have been closed.) Fixes a crash caused by closing task instances if the Task Instance Details are not visible\n\nFix bug which sometimes caused", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this crash has been reported from two causes, one being that the application is not running in a supported way and the other being that a task instance has been closed.) Fixes a crash caused by closing task instances when the application is not visible\n\nFix a crash sometimes caused", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is due to the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way This has two causes, one being that the application is not running in a supported way and the second issue being that a task instance has been created. Fix a crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused", "Fix crash when user clicks on \"Edit Details\" caused by start_date being None (#14416) This is to fix the following error in the error message: [ERROR] The application does not have a task instance, therefore application cannot begin in a supported way\n\n(this has two issues, the one being that the application is not running in a supported way and the second issue being that a task instance may have been closed.) Fixes a crash caused by closing the app when the application is closed. Add to the list the tasks visible\n\nFix bug which sometimes caused", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error which is in the log message: [ERROR] The runtime does not have a task instance, and therefore it cannot start by starting in a supported way\n\n(this has two causes, one being that the program is not running in a supported way and the second issue being that a task instance may have been closed.) Fixes a crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused", "Fix crash when clicking on \"Task Instance Details\" caused by the Task being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have the permission to run on a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that a task instance is not running in a supported way and the second issue being that a task instance may have been closed.) Fix crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused", "Fix crash when user clicks on \"Task s\" Fix bug which was caused by start_date being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second issue being that a task instance may have closed or been closed.) Fixes a crash caused by closing the task instance when the application is not visible\n\nFix bug which sometimes caused"], "perturbed_original": ["Fix crash when user clicks on \"Task Instance Details\" caused by bug None (#14416) This is to fix the following error message when a user clicks on 'Task Instance Details' for a TaskInstance that has not yet run. This error is produced when a TaskInstance has not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, and the previous TaskInstance was marked success without running. This error is caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. 1. The previous TaskInstance has not yet run because scheduler and user schedules are not enabled. 2. The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error message (#14407) when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. The previous TaskInstance has not yet run because its dependencies are not up to date. The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by dependency bug in None (#14416) This is to fix the following error that happens when a user clicks 'Task Instance Details' for TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This bug was caused when we were doing It alo 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run The dependencies for TaskInstance are not yet met The previous TaskInstance has not yet run After the TaskInstance is busy, the TaskInstance was marked success without running. This bug was merged from #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None . This issue is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This error was caused by #12910. It affects version 2.0.6 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by TaskInstance Not Run None (#14416) This is to fix the following error that happens when a user clicks 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The TaskInstance has not yet run because its dependencies are not yet met The TaskInstance has not yet run because scheduler is busy, the TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is a fix for the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance has not yet run. The previous TaskInstance has not yet run because dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, then run TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "The following error happens when user clicks on \"Task Instance Details\" : start_date being None (#14416) This is to fix the following error that happens when a user clicks on 'Task Instance Details' of a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None . The fix is to fix the following error that happens when a user clicks on 'Task Instance Details' . A TaskInstance that has previous TaskInstance does not run. E.g. The previous TaskInstance has not yet run since its dependencies are not yet met The previous TaskInstance has not yet run because it is busy, the previous TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1."], "original_ll": -3.191002130508423, "sampled_ll": -3.1482508182525635, "all_perturbed_sampled_ll": [-3.2669267654418945, -3.2937912940979004, -3.3188607692718506, -3.1979477405548096, -3.1239490509033203, -3.1025354862213135, -3.4045722484588623, -3.3238871097564697, -3.113818407058716, -3.189154863357544], "all_perturbed_original_ll": [-2.9214284420013428, -3.1125271320343018, -3.1008126735687256, -3.3157083988189697, -3.252204179763794, -3.2083568572998047, -3.2853293418884277, -3.2123141288757324, -3.2474262714385986, -3.2214245796203613], "perturbed_sampled_ll": -3.233544373512268, "perturbed_original_ll": -3.187753200531006, "perturbed_sampled_ll_std": 0.09807223161381234, "perturbed_original_ll_std": 0.10950636762569431}, {"original": "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "sampled": "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "perturbed_sampled": ["Fix command to run from main command at start of breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "Fix command to run tmux with breeze in BREEZE.rst ; <unk>build<unk> --start-airflow` now builds the", "Fix command to run tmux with breeze . (#10837) (#11340) `breeze --start-airflow` now builds the", "how to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "Add support to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "Fix to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "Fix command to run tmux with breeze in <unk>master<unk>: `breeze --start-airflow` now builds the", "Fix command to build airflow with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "Fix command to run tmux .exe in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "Fix command to run tmux with breeze in BREEZE.rst (#11340) The installer to now builds the"], "perturbed_original": ["Fix command to run tmux with breeze in BREEZE.rst <unk> <unk>breeze --start-airflow` -> `breeze start-airflow`", "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze start-airflow<unk> `breeze start-airflow`", "Fix command to start tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "Fix command to run tmux () using the current user specified in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "Fix command to run when breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "Fix command to run tmux with start-airflow. BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "Fix command to run from breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "Fix for BREEZE that could run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "Fix command to run tmux with breeze in BREEZE.rst : <unk>tmux start-airflow --start-airflow` -> `breeze start-airflow`", "Fix command to run tmux with breeze in airflow state. - `breeze --start-airflow` -> `breeze start-airflow`"], "original_ll": -3.772520065307617, "sampled_ll": -4.41995096206665, "all_perturbed_sampled_ll": [-4.531167030334473, -5.208364009857178, -4.258875846862793, -4.198347568511963, -4.355233669281006, -4.433350563049316, -4.878920555114746, -4.565796375274658, -4.578123569488525, -5.2350664138793945], "all_perturbed_original_ll": [-3.9251365661621094, -4.205638885498047, -3.7661328315734863, -3.781623363494873, -3.9168291091918945, -3.445462226867676, -3.7807931900024414, -3.845330238342285, -4.033476829528809, -3.481318950653076], "perturbed_sampled_ll": -4.624324560165405, "perturbed_original_ll": -3.81817421913147, "perturbed_sampled_ll_std": 0.34886320402887894, "perturbed_original_ll_std": 0.21822524501197307}, {"original": "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "sampled": "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "perturbed_sampled": ["[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've"], "perturbed_original": ["[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)"], "original_ll": -6.269988059997559, "sampled_ll": -6.512642860412598, "all_perturbed_sampled_ll": [-6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598, -6.512642860412598], "all_perturbed_original_ll": [-6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559, -6.269988059997559], "perturbed_sampled_ll": -6.512642860412598, "perturbed_original_ll": -6.269988059997559, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI modules via custom .pypirc file. This might allow to install dependencies from in-house, vetted registry of PyPI", "sampled": "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, pyPI_source, to look", "perturbed_sampled": ["Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This patch adds a capability to allow customization on the installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds the capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different from regular sources. You use the command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use a special function, called pyPI_source, to look", "Add capability of customising PyPI sources : Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, the command gives you specific look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds functionality for customising installation of PyPI sources, the functionality is a kin to regular Python, and you can use the command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources (#11385) This patch adds capability of customising installation of PyPI sources. This functionality is a bit different to regular sources. Use the command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different from that available in regular download of PyPI sources. Use the command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds the capability of customising installation of PyPI sources, the functionality is a bit more basic and reflects the functionality of regular sources. You can use our native command, pyPI_source, to look", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources. Since we change the capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, pyPI_source, to look"], "perturbed_original": ["Add capability of customising PyPI sources (#11385) Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources which might allow ordering of packages via custom .pypirc file. This might allow to install dependencies from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI by creating custom .pypirc file. This will allow to install dependencies from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This should add capability of customising installation of PyPI modules via custom .pypirc file. This might allow developers to specify dependencies from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources Change Summary: Add capability of customising PyPI sources This change adds capability to provide custom installation of PyPI modules via custom .pypirc file. This might allow to install dependencies from in-house, vetted registry of PyPI", "* Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI modules via custom .pypirc file. This might allow to select particular package from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI modules via custom .pypirc file. This might allow to install some application that will be in in-house, vetted registry of PyPI", "Add capability of custom PyPI sources (#11385) * Add capability of custom PyPI sources This change adds capability of customising installation of PyPI modules via custom .pypirc file. This might allow to install PyPI modules from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources (#11385) * This change adds capability of customising installation of PyPI modules using .pypirc file. This will allow to install dependencies from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This adds capability of customising installation of PyPI dependencies via custom .pypirc file. This might allow to install dependencies from in-house, vetted registry of PyPI", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This adds capability of customising installation of PyPI modules via custom installation directory and git file. This might allow to install dependencies from in-house, vetted registry of PyPI"], "original_ll": -3.6776766777038574, "sampled_ll": -3.3592031002044678, "all_perturbed_sampled_ll": [-3.3543989658355713, -3.303985834121704, -3.3426413536071777, -3.384824514389038, -3.6251492500305176, -3.6507728099823, -3.133713722229004, -3.335439443588257, -3.5278713703155518, -3.570518732070923], "all_perturbed_original_ll": [-3.662874698638916, -3.5297563076019287, -3.7663495540618896, -3.688499689102173, -3.706726312637329, -3.831002712249756, -3.6925466060638428, -3.347214460372925, -3.646782875061035, -3.9622583389282227], "perturbed_sampled_ll": -3.422931599617004, "perturbed_original_ll": -3.6834011554718016, "perturbed_sampled_ll_std": 0.15609076236544908, "perturbed_original_ll_std": 0.15667550847356687}, {"original": "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "sampled": "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "perturbed_sampled": ["Update link to match what is in pre-commit (#16408) [The k8s schema that has been used for chart pytest has now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber * Source License: http://github.com/davidhochhuber License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "is fixed to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now been changed to the stable k8s schema repository] Code of Conduct: 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: 2015 David Hochhuber <dhochhuber@google.com> License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that had been used for chart pytest has now now been changed to the stable schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update link to match documentation in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> (c) 2015 David Hochhuber Open source: http://opensource.org/licenses/MIT /GRAS (c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl Rules of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber Reuse MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber (c) c)\n\n1.", "Update link to match the changes in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl Code of Conduct: (c) 2015 David Hochhuber <dhochhuber@google.com> License: Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been in the chart file has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <unk>davidhochhuber@github.com> MIT Open Source License (c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "and also updated to match what is in pre-commit (#16408) [The schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> License: MIT Open -Source License Source: David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update link to match what is in the k8s repository (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl [New] (c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> License: MIT /Brewer License http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1."], "perturbed_original": ["Update link is what is in pre-commit (#16408) [The git repository that has been used for chart pytest has gone through a major upgrade with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in 1.18.1], this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit (#16408) [The k8s chema that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no planned updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow has actually merged a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this is intended to be [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no updates beyond 1.18.1, and [PRs for updates are being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is doing a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change should be for [a newer and updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit . A k8s schema repository that has been used from backend to pytest has gone stale with no updates [for almost 1 year in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no versions beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in the 1.18.1 stable release. [The k8s schema repository that has been used for chart pytest has gone stale with no updates in 16 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit (#16408) [The old repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork of Schema, so we should maintain the old one since 1.17.1 currently uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit (#16408) [The existing repository that has been used for chart pytest has gone stale without updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, so the current link for updates to 1.18.0 [is now in place and being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "be updated to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). The k8s schema is using a more stable fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this patch matches [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit , to find k8s schema repository that has been used for pyktk and pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so update: The latest version uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update link to match what is in pre-commit (#16408) [The k8s dk that has been used for chart pytest has gone away with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). K8sdk is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)"], "original_ll": -2.924253225326538, "sampled_ll": -2.0435242652893066, "all_perturbed_sampled_ll": [-2.103219985961914, -2.189155101776123, -2.201277732849121, -2.284029483795166, -2.592827081680298, -2.2243335247039795, -2.024012327194214, -2.4429373741149902, -2.416365146636963, -2.353968620300293], "all_perturbed_original_ll": [-2.9473752975463867, -3.0540428161621094, -2.9709866046905518, -2.952894449234009, -2.8049211502075195, -3.004077911376953, -2.9367129802703857, -2.8573708534240723, -3.0544660091400146, -2.996905565261841], "perturbed_sampled_ll": -2.2832126379013062, "perturbed_original_ll": -2.9579753637313844, "perturbed_sampled_ll_std": 0.16208603165657035, "perturbed_original_ll_std": 0.0751577166810544}, {"original": "Update to latest pygrep pre-commit hook (#8489)", "sampled": "Update to latest pygrep pre-commit hook (#8489)A", "perturbed_sampled": ["Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A", "Update to latest pygrep pre-commit hook (#8489)A"], "perturbed_original": ["Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)", "Update to latest pygrep pre-commit hook (#8489)"], "original_ll": -4.832974433898926, "sampled_ll": -5.407581329345703, "all_perturbed_sampled_ll": [-5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703, -5.407581329345703], "all_perturbed_original_ll": [-4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926, -4.832974433898926], "perturbed_sampled_ll": -5.407581329345703, "perturbed_original_ll": -4.832974433898926, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "sampled": "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "perturbed_sampled": ["Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in TestScope.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy content in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator path in test_zip.inc (#13734) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator by <unk>ttest<unk> in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated path in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.zip with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator path in file() (#13628) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in test_zip.inc with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Replace deprecated dummy file in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated"], "perturbed_original": ["Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with new path: = from airflow.operators.dummy import DummyOperator ```", "Deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.dummy import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` >> from airflow.operators.dummy import DummyOperator ```", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` _<unk> from airflow.operators.dummy import DummyOperator ```", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy_operator import DummyOperator ```", "Replace deprecated dummy operator path in <unk> Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "Remove deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "Replace deprecated dummy operator path . (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```"], "original_ll": -3.2934682369232178, "sampled_ll": -2.4475862979888916, "all_perturbed_sampled_ll": [-2.675032615661621, -2.4111461639404297, -2.361422300338745, -2.4072587490081787, -2.8110647201538086, -2.4273202419281006, -2.4358417987823486, -2.6798255443573, -2.7650020122528076, -2.301589250564575], "all_perturbed_original_ll": [-3.4153358936309814, -3.1541588306427, -3.5773932933807373, -3.305920124053955, -3.453556537628174, -3.1360738277435303, -3.4954822063446045, -3.2531325817108154, -3.414233446121216, -3.350188732147217], "perturbed_sampled_ll": -2.5275503396987915, "perturbed_original_ll": -3.355547547340393, "perturbed_sampled_ll_std": 0.17516732787988737, "perturbed_original_ll_std": 0.13679482413672964}, {"original": "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "sampled": "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "perturbed_sampled": ["Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted"], "perturbed_original": ["Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)"], "original_ll": -6.64837121963501, "sampled_ll": -7.210554599761963, "all_perturbed_sampled_ll": [-7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963, -7.210554599761963], "all_perturbed_original_ll": [-6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501, -6.64837121963501], "perturbed_sampled_ll": -7.210554599761963, "perturbed_original_ll": -6.64837121963501, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "sampled": "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "perturbed_sampled": ["[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This"], "perturbed_original": ["[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)"], "original_ll": -5.231442928314209, "sampled_ll": -5.529385566711426, "all_perturbed_sampled_ll": [-5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426, -5.529385566711426], "all_perturbed_original_ll": [-5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209, -5.231442928314209], "perturbed_sampled_ll": -5.529385566711426, "perturbed_original_ll": -5.231442928314209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "sampled": "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "perturbed_sampled": ["Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: eb-op. (#19644)", "Removed hardcoded connection to DbApiHook if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) - Matthew Sapere (#19644)", "Removed hardcoded connection types. Check if hook is configured for DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "Removed the following types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "Removed hardcoded connection type if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: James J. (#19644)", "Removed hardcoded connection types. Check if hook is instance of DbApiHook. Michael Hendry, Domenic Sapere (#19644)", "Removed hardcoded connection on request if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "Removed ambiguity in the hook types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)"], "perturbed_original": ["Removed hardcoded connection types. Each hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Removed hardcoded connection types. Check if hook is instance -level. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "of the set of connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) by Denis Kazanzhy <dkazanzhy@demandbase.com>", "Removed hardcoded hook class. // Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Removed hardcoded connection types. Check if hook is instance of class. Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Removed hardcoded connection types. New hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Removed hardcoded connection types. Check ed whether there is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Removed multiple hook types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "is defined in connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>"], "original_ll": -4.763741493225098, "sampled_ll": -4.207995891571045, "all_perturbed_sampled_ll": [-4.9401655197143555, -3.801166296005249, -4.7356696128845215, -4.33150577545166, -4.208250522613525, -4.294366836547852, -4.602383613586426, -4.9253363609313965, -4.3252763748168945, -4.165726661682129], "all_perturbed_original_ll": [-4.659881114959717, -5.243812084197998, -4.7890849113464355, -5.1868720054626465, -4.67664909362793, -4.957124710083008, -4.745691299438477, -5.048692226409912, -4.712170600891113, -5.052317142486572], "perturbed_sampled_ll": -4.432984757423401, "perturbed_original_ll": -4.9072295188903805, "perturbed_sampled_ll_std": 0.34383098507653603, "perturbed_original_ll_std": 0.20675799413245932}, {"original": "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "sampled": "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "perturbed_sampled": ["Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed"], "perturbed_original": ["Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>"], "original_ll": -4.778177261352539, "sampled_ll": -4.294705390930176, "all_perturbed_sampled_ll": [-4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176, -4.294705390930176], "all_perturbed_original_ll": [-4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539, -4.778177261352539], "perturbed_sampled_ll": -4.294705390930176, "perturbed_original_ll": -4.778177261352539, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214", "sampled": "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for the Azure CLI", "perturbed_sampled": ["Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted . Azure CLI is now able to airflow db migrations (@gwern)\n\n: Added from the Azure CLI", "Log migrations info in the console. (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db p. Added support for the Azure CLI", "Log migrations info in consisten way (#13458) : Adding permissions migration changes logging handlers so each next migration is differently formatted . Add support for Azure CLI to log airflow db migrations (@gwern)\n\n: Added support for the Azure CLI", "Log migrations info in consisten way (#13458) Resource based permissions for logging handlers so each next migration is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for data export from CLI", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next log is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for the new Airflow Web CLI", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db migrations (#13456) Improve support for Azure CLI", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is properly formatted when doing airflow (#15476) - Added (@gwern)\n\n: Added support for the Azure CLI", ". // Debug info in consisten way (#13458) Resource based permissions migration changes logging handlers so each log file is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for the Azure CLI", "Log migrations info in the Azure console (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db migrations (@gwern)\n\n: Added migrations info in the Azure CLI", "Log ging and logging can be stored in consisten way (#13458) Resource based permissions migration changes logging for each next migration is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for the Azure CLI"], "perturbed_original": ["Log migrations info now the old styled way (#13458) The schema permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214", "Log file in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow . This commit fixes this behavior. closes: #13214", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration file is formatted when doing the reset. This commit fixes this behavior. closes: #13214", "logging info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db reset. This commit fixes this. closes: #13214", "log and configuration info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted then the last airflow db reset. This commit fixes this behavior. closes: #13214", "Log migrations info in consisten way (#13458) Resource set after migration changes logging handlers so each next migration is differently formatted , after airflow db reset. This commit fixes this behavior. closes: #13214", "Log migrations info in consisten way . If time based permissions migration changes logging handlers logging to the next migration is differently formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214", "Log migrations in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration logger doesn't get formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214", "Log migrations info . in different way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing the reset. This commit fixes this behavior. closes: #13214", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers . The next migration log is always formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214"], "original_ll": -6.46252965927124, "sampled_ll": -5.869115352630615, "all_perturbed_sampled_ll": [-5.825168609619141, -6.375878810882568, -5.264878273010254, -5.820796012878418, -5.746359348297119, -6.126185417175293, -5.90078592300415, -5.719038963317871, -5.50293493270874, -5.8449931144714355], "all_perturbed_original_ll": [-6.1557183265686035, -6.669366359710693, -6.063485622406006, -6.6219048500061035, -6.47874116897583, -6.469563961029053, -6.514533996582031, -6.3667168617248535, -5.941183567047119, -6.330556392669678], "perturbed_sampled_ll": -5.812701940536499, "perturbed_original_ll": -6.361177110671997, "perturbed_sampled_ll_std": 0.28873915214806867, "perturbed_original_ll_std": 0.22808843612140348}, {"original": "JIRA and Github issues explanation (#8539)", "sampled": "JIRA and Github issues explanation (#8539)Still", "perturbed_sampled": ["JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still", "JIRA and Github issues explanation (#8539)Still"], "perturbed_original": ["JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)", "JIRA and Github issues explanation (#8539)"], "original_ll": -6.503997802734375, "sampled_ll": -7.256471157073975, "all_perturbed_sampled_ll": [-7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975, -7.256471157073975], "all_perturbed_original_ll": [-6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375, -6.503997802734375], "perturbed_sampled_ll": -7.256471157073975, "perturbed_original_ll": -6.503997802734375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "sampled": "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "perturbed_sampled": ["[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New"], "perturbed_original": ["[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)"], "original_ll": -5.6918487548828125, "sampled_ll": -6.017413139343262, "all_perturbed_sampled_ll": [-6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262, -6.017413139343262], "all_perturbed_original_ll": [-5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125, -5.6918487548828125], "perturbed_sampled_ll": -6.017413139343262, "perturbed_original_ll": -5.6918487548828125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "sampled": "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates", "perturbed_sampled": ["Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog entries and updates", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates", "Add Changelog and Updating for 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates", "Add Changelog & Updating to 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates", "Add Changelog & Updating.md , update log. (#14870) This commit adds Changelog & Updating.md and updates", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds an update to Updating.md and updates", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md as updates", "Add Changelog & Updating.md for 1.10.15 (#14870) This patch adds Changelog & Updating.md and updates", "Add Changelog & Updating.md for 2.6.1. This commit adds Changelog & Updating.md and updates", "Add Changelog and Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates"], "perturbed_original": ["Add Changelog /md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 (#14870) This patch adds Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 (#14870) This will add Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 ! ! Description: This commit adds Changelog & Updating.md for 1.10.15", "Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 (#14870) This patch adds additional info. Add Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog and Updating.md for 1.10.15"], "original_ll": -2.8190317153930664, "sampled_ll": -3.319640636444092, "all_perturbed_sampled_ll": [-3.996091842651367, -3.319640636444092, -3.589916944503784, -3.5217087268829346, -3.476597785949707, -3.78979754447937, -3.599982738494873, -3.265190601348877, -2.853454113006592, -3.379936695098877], "all_perturbed_original_ll": [-3.630324363708496, -2.757938861846924, -2.8003222942352295, -2.8190317153930664, -2.7350969314575195, -3.0036234855651855, -2.989651918411255, -2.8190317153930664, -3.0036234855651855, -2.8678369522094727], "perturbed_sampled_ll": -3.479231762886047, "perturbed_original_ll": -2.94264817237854, "perturbed_sampled_ll_std": 0.2941052207363071, "perturbed_original_ll_std": 0.24822456607520818}, {"original": "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "sampled": "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "perturbed_sampled": ["Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders"], "perturbed_original": ["Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)"], "original_ll": -5.607777118682861, "sampled_ll": -4.800323009490967, "all_perturbed_sampled_ll": [-4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967, -4.800323009490967], "all_perturbed_original_ll": [-5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861, -5.607777118682861], "perturbed_sampled_ll": -4.800323009490967, "perturbed_original_ll": -5.607777118682861, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "sampled": "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "perturbed_sampled": ["[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This"], "perturbed_original": ["[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)"], "original_ll": -6.288146018981934, "sampled_ll": -6.573079586029053, "all_perturbed_sampled_ll": [-6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053, -6.573079586029053], "all_perturbed_original_ll": [-6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934, -6.288146018981934], "perturbed_sampled_ll": -6.573079586029053, "perturbed_original_ll": -6.288146018981934, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "sampled": "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "perturbed_sampled": ["[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP"], "perturbed_original": ["[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)"], "original_ll": -6.40598201751709, "sampled_ll": -6.659811496734619, "all_perturbed_sampled_ll": [-6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619, -6.659811496734619], "all_perturbed_original_ll": [-6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709, -6.40598201751709], "perturbed_sampled_ll": -6.659811496734619, "perturbed_original_ll": -6.40598201751709, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix backwards compatibility with k8s executor_config resources (#11796)", "sampled": "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "perturbed_sampled": ["Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Fix backwards compatibility with k8s executor_config resources (#11796)\""], "perturbed_original": ["Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Fix backwards compatibility with k8s executor_config resources (#11796)"], "original_ll": -5.910852432250977, "sampled_ll": -6.474430561065674, "all_perturbed_sampled_ll": [-6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674, -6.474430561065674], "all_perturbed_original_ll": [-5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977, -5.910852432250977], "perturbed_sampled_ll": -6.474430561065674, "perturbed_original_ll": -5.910852432250977, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "sampled": "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "perturbed_sampled": ["Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files files should be accessible before any files", "Docker context files should be available earlier (#12219) If you want to do an initial docker run with local version, the docker-context-files should be accessible before any files", "Docker context files should be available earlier (#12219) If you want to override constraints with local version, local versions should be accessible before any files", "The docker-context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "Docker files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "Docker context files should be accessible earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "Docker context files should be available earlier (#12219) If you will override constraints with local version, the docker-context-files should be accessible before any files", "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "The docker-context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "Docker context files will be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files"], "perturbed_original": ["Docker context files should be available earlier (#12219) If you want to override constraints with local context files, then your docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier (#12219) If we want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier (#12219) If one wishes to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be earlier (#12219) If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier than expected. If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier . If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier , based on the Docker configuration. If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier (#12219) If you want to override constraints of this version, the docker-context-files should be earlier in the Dockerfile", "Docker context files should be available earlier (#12219) If you want to override constraints in the DOCKer version, the docker-context-files should be earlier in the Dockerfile"], "original_ll": -4.086678504943848, "sampled_ll": -4.257526397705078, "all_perturbed_sampled_ll": [-4.2362494468688965, -3.9861767292022705, -4.623691082000732, -4.207969665527344, -4.4477643966674805, -4.334176063537598, -4.582987308502197, -4.257526397705078, -4.207969665527344, -4.326662063598633], "all_perturbed_original_ll": [-3.9093236923217773, -4.183984756469727, -4.223782062530518, -3.9985694885253906, -3.795109510421753, -3.929823875427246, -3.772350311279297, -4.086678504943848, -4.037775039672852, -3.983788251876831], "perturbed_sampled_ll": -4.321117281913757, "perturbed_original_ll": -3.992118549346924, "perturbed_sampled_ll_std": 0.1803895433220982, "perturbed_original_ll_std": 0.1412317203003616}, {"original": "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "sampled": "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "perturbed_sampled": ["fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>"], "perturbed_original": ["fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>"], "original_ll": -4.668304443359375, "sampled_ll": -4.217140197753906, "all_perturbed_sampled_ll": [-4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906, -4.217140197753906], "all_perturbed_original_ll": [-4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375, -4.668304443359375], "perturbed_sampled_ll": -4.217140197753906, "perturbed_original_ll": -4.668304443359375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Typo fix in TESTING.rst (#19216)", "sampled": "Typo fix in TESTING.rst (#19216)For", "perturbed_sampled": ["Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For", "Typo fix in TESTING.rst (#19216)For"], "perturbed_original": ["Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)", "Typo fix in TESTING.rst (#19216)"], "original_ll": -4.901196002960205, "sampled_ll": -5.675152778625488, "all_perturbed_sampled_ll": [-5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488, -5.675152778625488], "all_perturbed_original_ll": [-4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205, -4.901196002960205], "perturbed_sampled_ll": -5.675152778625488, "perturbed_original_ll": -4.901196002960205, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "sampled": "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "perturbed_sampled": ["[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The"], "perturbed_original": ["[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)"], "original_ll": -4.678685665130615, "sampled_ll": -5.020734786987305, "all_perturbed_sampled_ll": [-5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305, -5.020734786987305], "all_perturbed_original_ll": [-4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615, -4.678685665130615], "perturbed_sampled_ll": -5.020734786987305, "perturbed_original_ll": -4.678685665130615, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "sampled": "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "perturbed_sampled": ["[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\""], "perturbed_original": ["[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)"], "original_ll": -4.77077579498291, "sampled_ll": -5.062777996063232, "all_perturbed_sampled_ll": [-5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232, -5.062777996063232], "all_perturbed_original_ll": [-4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291, -4.77077579498291], "perturbed_sampled_ll": -5.062777996063232, "perturbed_original_ll": -4.77077579498291, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "sampled": "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "perturbed_sampled": ["Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The"], "perturbed_original": ["Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)"], "original_ll": -4.699961185455322, "sampled_ll": -5.13549280166626, "all_perturbed_sampled_ll": [-5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626, -5.13549280166626], "all_perturbed_original_ll": [-4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322, -4.699961185455322], "perturbed_sampled_ll": -5.13549280166626, "perturbed_original_ll": -4.699961185455322, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Simplified GCSTaskHandler configuration (#10365)", "sampled": "Simplified GCSTaskHandler configuration (#10365)The", "perturbed_sampled": ["Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The", "Simplified GCSTaskHandler configuration (#10365)The"], "perturbed_original": ["Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)", "Simplified GCSTaskHandler configuration (#10365)"], "original_ll": -6.538324356079102, "sampled_ll": -7.114240646362305, "all_perturbed_sampled_ll": [-7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305, -7.114240646362305], "all_perturbed_original_ll": [-6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102, -6.538324356079102], "perturbed_sampled_ll": -7.114240646362305, "perturbed_original_ll": -6.538324356079102, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently failing with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "sampled": "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory only for running reviews", "perturbed_sampled": ["Fix run reviews (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. Fix labels for run reviews are now triggered in workflows * Fix labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions (#16596) This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows. * Enable running run reviews (#16555) * Use working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix run reviews This commit was committed on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews in workflows * Use working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) Fix permissions for working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Updated workflow workflows to support run reviews * Label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label s labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Ensure that use the current workspace directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged to #15589. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory for running reviews", "Fix label_when_reviewed_workflow_run permissions * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was made on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Use working directory only for running reviews * Use working directory only for run reviews (#16555) * Use working directory only for running reviews", "Fix label_when_reviewed_workflow_run permissions (#16596) Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews * Use working directory only for running reviews"], "perturbed_original": ["Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is being shared with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 * \"Checking selective builds for the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ I now have to add #16794 help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow is currently failing with: in ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of \"list\" in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix permissions This workflow run is currently failing with: ``` Error: resource not accessible by integration\" Run failed with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I think this will be improved Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently not visible: ``` Run completion token: *** Selective status of the build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) ...\" Resource not accessible by integration ``` so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently failing with: ``` *** issue: Failed with: token: *** name: Selective build check status: in_progress sha: $sha: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not found in airflow integration ``` *** We _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently failing with: ``` Run ./.github/actions/checks-action with: token: false Selective build check status: true status_code: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} \"\": code not accessible by integration team<unk>\"\" I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently failing with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: <unk> {\"summary\": \"Checking to check for the failure of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ the fix should help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Ivanovich <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions issue Fix Fix label_when_reviewed_workflow_run permissions issue run is currently failing with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: \"Checking selective status of the build in https://github.com/apache/airflow/actions/runs/960898933 \" \" \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions * Fix label_when_reviewed_workflow_run permissions This workflow run is using airflow and checks action. Run with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ this will help * ?== error messages. Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is being run with: ``` run with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 <unk> output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ I need your help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>"], "original_ll": -3.6983776092529297, "sampled_ll": -2.4744369983673096, "all_perturbed_sampled_ll": [-3.070267915725708, -2.458782196044922, -2.9197070598602295, -2.621682643890381, -2.6943752765655518, -2.788698434829712, -2.575896978378296, -2.5755934715270996, -2.2515149116516113, -2.4922070503234863], "all_perturbed_original_ll": [-3.84588885307312, -3.7994372844696045, -3.8147318363189697, -3.758511543273926, -3.5082671642303467, -3.8439676761627197, -4.163120269775391, -3.500891923904419, -3.981491804122925, -4.024059772491455], "perturbed_sampled_ll": -2.6448725938796995, "perturbed_original_ll": -3.8240368127822877, "perturbed_sampled_ll_std": 0.22414130103331498, "perturbed_original_ll_std": 0.19742643464695647}, {"original": "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "sampled": "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "perturbed_sampled": ["[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In"], "perturbed_original": ["[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)"], "original_ll": -5.498917102813721, "sampled_ll": -5.810244560241699, "all_perturbed_sampled_ll": [-5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699, -5.810244560241699], "all_perturbed_original_ll": [-5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721, -5.498917102813721], "perturbed_sampled_ll": -5.810244560241699, "perturbed_original_ll": -5.498917102813721, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Highlight code blocks (#6243)", "sampled": "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "perturbed_sampled": ["[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT"], "perturbed_original": ["[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "[AIRFLOW-XXX] Highlight code blocks (#6243)"], "original_ll": -5.9480719566345215, "sampled_ll": -5.615355014801025, "all_perturbed_sampled_ll": [-5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025, -5.615355014801025], "all_perturbed_original_ll": [-5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215, -5.9480719566345215], "perturbed_sampled_ll": -5.615355014801025, "perturbed_original_ll": -5.9480719566345215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errors", "sampled": "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errorsThe", "perturbed_sampled": ["Update Helm Chart docs for docs page (#15957) Updates repo name and chart name and some minor errorsThe", "Update Helm Chart 1.0.0 Helm Map 1.0.0 release (#15957) Updates repo name and chart name and some minor errorsThe", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name to fix some minor errorsThe", "Update Helm Chart docs for 1.0.0 release The repo name and chart name and some minor errorsThe", "Update Helm Chart docs for 1.0.0 release (#15957) The image reference: the name and chart name and some minor errorsThe", "Update Helm Chart docs for 1.0.0 release (#15957) The name and chart name and some minor errorsThe", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and fix errorsThe", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and fixes errorsThe", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo docs with helm chart name and some minor errorsThe", "Update Helm Chart The 1.0.0 release (#15957) Updates repo name and chart name and some minor errorsThe"], "perturbed_original": ["Update Helm Chart docs for 1.0.0 release (#15957) - Fixed errors with the name and chart name and some minor errors", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name names and corrects minor errors", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo with the new helm chart name and some minor errors", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart docs, fixes some minor errors", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name for latest patch Fixed some minor errors", "Fixed broken Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errors", "Update Helm Chart docs for latest releases (#15957) Updates repo name and chart name and some minor errors", "Update Helm Chart docs for 1.0.0 release (#15957) by adding the new helm name and chart name and some minor errors", "Refactor Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errors", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name to new name and some minor errors"], "original_ll": -5.773518085479736, "sampled_ll": -6.035676956176758, "all_perturbed_sampled_ll": [-7.085083961486816, -5.25746488571167, -5.694342613220215, -6.265758037567139, -5.858900547027588, -5.990375995635986, -6.106494426727295, -6.010944366455078, -6.144228458404541, -6.104248523712158], "all_perturbed_original_ll": [-5.157626628875732, -5.6895222663879395, -5.50221586227417, -5.4537200927734375, -5.614230155944824, -5.369035720825195, -6.708090305328369, -5.209647178649902, -5.252150058746338, -5.52789306640625], "perturbed_sampled_ll": -6.051784181594849, "perturbed_original_ll": -5.5484131336212155, "perturbed_sampled_ll_std": 0.43921924815487234, "perturbed_original_ll_std": 0.4204808406662987}, {"original": "Small fixes in Google Cloud Secrets Manager guide (#12105)", "sampled": "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "perturbed_sampled": ["Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's"], "perturbed_original": ["Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "Small fixes in Google Cloud Secrets Manager guide (#12105)"], "original_ll": -6.858067512512207, "sampled_ll": -6.949158191680908, "all_perturbed_sampled_ll": [-6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908, -6.949158191680908], "all_perturbed_original_ll": [-6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207, -6.858067512512207], "perturbed_sampled_ll": -6.949158191680908, "perturbed_original_ll": -6.858067512512207, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "sampled": "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "perturbed_sampled": ["[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This"], "perturbed_original": ["[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)"], "original_ll": -5.081605434417725, "sampled_ll": -5.4234843254089355, "all_perturbed_sampled_ll": [-5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355, -5.4234843254089355], "all_perturbed_original_ll": [-5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725, -5.081605434417725], "perturbed_sampled_ll": -5.4234843254089355, "perturbed_original_ll": -5.081605434417725, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "sampled": "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "perturbed_sampled": ["Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What"], "perturbed_original": ["Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)"], "original_ll": -3.53136944770813, "sampled_ll": -4.0279436111450195, "all_perturbed_sampled_ll": [-4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195, -4.0279436111450195], "all_perturbed_original_ll": [-3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813, -3.53136944770813], "perturbed_sampled_ll": -4.0279436111450195, "perturbed_original_ll": -3.53136944770813, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "sampled": "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "perturbed_sampled": ["refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The"], "perturbed_original": ["refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)"], "original_ll": -4.94626522064209, "sampled_ll": -5.403241157531738, "all_perturbed_sampled_ll": [-5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738, -5.403241157531738], "all_perturbed_original_ll": [-4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209, -4.94626522064209], "perturbed_sampled_ll": -5.403241157531738, "perturbed_original_ll": -4.94626522064209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "sampled": "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "perturbed_sampled": ["[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST"], "perturbed_original": ["[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)"], "original_ll": -4.2683796882629395, "sampled_ll": -4.669395923614502, "all_perturbed_sampled_ll": [-4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502, -4.669395923614502], "all_perturbed_original_ll": [-4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395, -4.2683796882629395], "perturbed_sampled_ll": -4.669395923614502, "perturbed_original_ll": -4.2683796882629395, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix build error while running docker-compose #14868 #14906 * Add docker-compose command to run only files that can connect to docker (#15998) #14414 * Fix", "perturbed_sampled": ["Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix build error while running docker package (#17596) #14906 * Add docker-compose command to run only files that can connect to docker (#15998) #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! Build dockerfile * Check for docker build errors * Fix build error while running docker-compose (#16595) * Add docker-compose to run only files that can connect to docker (#15998) #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix build error while running Dockerfile #14906 * Add docker-compose command to manage /tmp files that are related to container #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix the number of exceptions while running docker-compose #14868 #14906 * Fix the docker-compose command to run only files that can connect to the Docker Hub #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * Create a documentation package for Docker image * Check for docker build errors * Check for docker build errors * Fix build error while running docker-compose -command * Add docker-compose command to run under environment variables that can connect to docker (#15998) #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker file * Compile dockerfile * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker file * Build dockerfile * Fix build error while running docker-compose #14868 #14906 * Add docker-compose command to run only files that can connect to docker (#15998) #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix a docker compile error while running docker-compose #14868 #14906 * Fix the docker-compose command to run only files that can connect to docker (#15998) * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * Create a documentation package for Docker image * Build Docker image * Check for docker build errors * Stop error while running docker-compose #14868 #14906 * Add docker-compose if it can run only files that can connect to docker (#15998) #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile s without accounting for docker build errors * Fix build error while running docker-compose #14868 #14906 * Add docker-compose command to run only files to connect to docker (#15998) #14414 * Fix", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image (#14865) * Build dockerfile * Check for build errors * Fix build error while running docker-compose #14868 * Add docker-compose command to run only files that can connect to docker (#15998) #14414 * Fix"], "perturbed_original": ["Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image (#14763) * fixup! fixup! fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image + suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * fixup! Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code -snippets by Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * Fixup! fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik * Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image * Create a documentation package for Docker image * Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image (#14766) * Fixup! Apply suggestions from code review Co-authored-by: Kaxil Naik : Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply this fix to Docker image * Add code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * fixup! Fix a documentation package for Docker image * fixup! Fix a documentation package for Docker image * fixup! Fix a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * fixup! Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review by Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -2.048685073852539, "sampled_ll": -3.0158321857452393, "all_perturbed_sampled_ll": [-2.967932939529419, -2.9905941486358643, -3.202638864517212, -2.8091959953308105, -2.767388343811035, -2.9622151851654053, -3.0383810997009277, -2.9326295852661133, -3.344470262527466, -2.9332358837127686], "all_perturbed_original_ll": [-1.9774625301361084, -1.7807382345199585, -2.190124273300171, -2.442227602005005, -1.9005122184753418, -2.008620262145996, -2.326077938079834, -2.0410008430480957, -2.058546543121338, -2.2234034538269043], "perturbed_sampled_ll": -2.9948682308197023, "perturbed_original_ll": -2.094871389865875, "perturbed_sampled_ll_std": 0.16216875743515366, "perturbed_original_ll_std": 0.19015971281792035}, {"original": "Bump Boto3 (#7851)", "sampled": "Bump Boto3 (#7851)Tunnel", "perturbed_sampled": ["Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel", "Bump Boto3 (#7851)Tunnel"], "perturbed_original": ["Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)", "Bump Boto3 (#7851)"], "original_ll": -7.283779621124268, "sampled_ll": -6.755589485168457, "all_perturbed_sampled_ll": [-6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457, -6.755589485168457], "all_perturbed_original_ll": [-7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268, -7.283779621124268], "perturbed_sampled_ll": -6.755589485168457, "perturbed_original_ll": -7.283779621124268, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "sampled": "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "perturbed_sampled": ["Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct (#9457) * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add references to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct (#9354). CONTRIBUTING.md", "Add link to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md"], "perturbed_original": ["Add reference to the ASF Code of Conduct (#9453) * Update CHANGING. Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct (#9483) * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct Update CONTRIBUTING.rst", "* Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct (#9456) * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct . PITCH CONTRIBUTING.rst", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct (#9454) * Update CONTRIBUTING.rst"], "original_ll": -3.119962453842163, "sampled_ll": -2.969393253326416, "all_perturbed_sampled_ll": [-2.969393253326416, -2.61616849899292, -2.969393253326416, -3.0583407878875732, -2.969393253326416, -2.7340455055236816, -3.2502453327178955, -3.187849998474121, -2.969393253326416, -2.969393253326416], "all_perturbed_original_ll": [-3.3105599880218506, -2.8070311546325684, -3.3306190967559814, -3.0352842807769775, -2.7478532791137695, -3.119962453842163, -3.119962453842163, -3.119962453842163, -3.925152063369751, -2.708512783050537], "perturbed_sampled_ll": -2.9693616390228272, "perturbed_original_ll": -3.1224900007247927, "perturbed_sampled_ll_std": 0.17738917198597318, "perturbed_original_ll_std": 0.33790864506965396}, {"original": "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "sampled": "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "perturbed_sampled": ["[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]"], "perturbed_original": ["[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)"], "original_ll": -6.357372283935547, "sampled_ll": -5.020843982696533, "all_perturbed_sampled_ll": [-5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533, -5.020843982696533], "all_perturbed_original_ll": [-6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547, -6.357372283935547], "perturbed_sampled_ll": -5.020843982696533, "perturbed_original_ll": -6.357372283935547, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix mypy for exasol and facebook hooks (#20291)", "sampled": "Fix mypy for exasol and facebook hooks (#20291)For", "perturbed_sampled": ["Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For", "Fix mypy for exasol and facebook hooks (#20291)For"], "perturbed_original": ["Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)", "Fix mypy for exasol and facebook hooks (#20291)"], "original_ll": -6.832136154174805, "sampled_ll": -7.407689571380615, "all_perturbed_sampled_ll": [-7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615, -7.407689571380615], "all_perturbed_original_ll": [-6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805, -6.832136154174805], "perturbed_sampled_ll": -7.407689571380615, "perturbed_original_ll": -6.832136154174805, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Standardize AWS Lambda naming (#20365)", "sampled": "Standardize AWS Lambda naming (#20365)With", "perturbed_sampled": ["Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With", "Standardize AWS Lambda naming (#20365)With"], "perturbed_original": ["Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)", "Standardize AWS Lambda naming (#20365)"], "original_ll": -6.150325298309326, "sampled_ll": -6.908303260803223, "all_perturbed_sampled_ll": [-6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223, -6.908303260803223], "all_perturbed_original_ll": [-6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326, -6.150325298309326], "perturbed_sampled_ll": -6.908303260803223, "perturbed_original_ll": -6.150325298309326, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "sampled": "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "perturbed_sampled": ["[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So"], "perturbed_original": ["[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]"], "original_ll": -6.083202362060547, "sampled_ll": -6.39528751373291, "all_perturbed_sampled_ll": [-6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291, -6.39528751373291], "all_perturbed_original_ll": [-6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547, -6.083202362060547], "perturbed_sampled_ll": -6.39528751373291, "perturbed_original_ll": -6.083202362060547, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix elasticsearch breaking the build (#7800)", "sampled": "Fix elasticsearch breaking the build (#7800)The", "perturbed_sampled": ["Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The", "Fix elasticsearch breaking the build (#7800)The"], "perturbed_original": ["Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)", "Fix elasticsearch breaking the build (#7800)"], "original_ll": -7.090944290161133, "sampled_ll": -7.705590724945068, "all_perturbed_sampled_ll": [-7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068, -7.705590724945068], "all_perturbed_original_ll": [-7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133, -7.090944290161133], "perturbed_sampled_ll": -7.705590724945068, "perturbed_original_ll": -7.090944290161133, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` process) of the task instance to the current process ID of the task_runner process when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "sampled": "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using \"dependencies\" as an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "perturbed_sampled": ["Fix impersonation issue with LocalTaskJob s. Starting a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never be loaded from root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using a package as an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "Fix issue with LocalTaskJob (#16852) Running a run_from_host and a run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using $pname to match an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "Fix issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as locale. Fix dependency Injection: Fix dependency injection issues when using \"dependencies\" as an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because the user account is not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using an operator as an identifier in the dependencies definition\n\nFix dependency injection issues with installation-time performance by using", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as a task or project. Dependency Injection: Fix dependency injection by using \"dependencies\" as an identifier in the dependencies definition\n\nFix dependency injection by using dependency injection directly (#149625)\n\nImprove installation-time performance by using", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale issues when using Unicode bindings (#16939) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using the root as an identifier in the dependencies definition\n\nFix dependency injection regression improving performance by using", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not saved properly due to incorrect locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using only an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "Fix impersonation issue . (#16852) Running a task with a root PID fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using \"dependencies\" as an identifier in the dependencies list. Fix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "bindings. Fix issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale binding issues on the remote. Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using \"dependencies\" as an identifier in the dependencies definition\n\nFix dependency injection regression (#154874) Improve performance by using", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling : Task bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection regression Fix dependency injection regression when using the package name as an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using"], "perturbed_original": ["Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes this by matching the parent process ID (the `sudo` process) of the task instance to the current process ID of the task_runner process when we use subprocess. Thanks to Ash Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` process) of the task to the current process ID of the run_as_user process when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by corresponding the Process ID (the `sudo` process) of the task instance to the current process as the task_runner process when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by mapping the parent process ID (the `sudo` process) of the task to the current process ID of the task_runner process when we use impersonation and create local tasks. Author, Alyssa Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with local task_runner processes fails because PIDs are not matched correctly. This change fixes this by properly matching the parent process ID (the `sudo` process) of the task instance to the current process ID of the task_runner process when performing impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with a task instance can cause impersonation issues because PIDs are not matched correctly. This change fixes it by matching the current process ID (the `sudo` process) of the task instance to the current process ID of the task_runner process when performing impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a local task via run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` process) of the task instance to the current process ID of the active process when trying impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (from the `sudo` process) of the task instance to the current process ID of the task_runner process when we use impersonation Co-authored-by: Ashay Kujika <ash_github@firemirror.com>", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (current process) of the task instance to the current process ID of the task instance. This will improve the execution of new tasks when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Fixed the issue with LocalTaskJob (#16852) Running a task with run_as_user fails because their IDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` ) of the task instance to the current process ID of the task_runner process when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>"], "original_ll": -4.265727519989014, "sampled_ll": -3.310912847518921, "all_perturbed_sampled_ll": [-3.626302719116211, -3.4781746864318848, -3.7521982192993164, -3.641895294189453, -3.7149853706359863, -3.7884628772735596, -3.5870673656463623, -3.2900302410125732, -3.504765033721924, -3.5437564849853516], "all_perturbed_original_ll": [-4.215779781341553, -3.989872932434082, -4.60651969909668, -4.234011650085449, -4.145792007446289, -3.997255563735962, -4.242059707641602, -4.282101154327393, -3.967085361480713, -4.243818759918213], "perturbed_sampled_ll": -3.592763829231262, "perturbed_original_ll": -4.1924296617507935, "perturbed_sampled_ll_std": 0.14041031297230191, "perturbed_original_ll_std": 0.17859649713950396}, {"original": "Rename the main branch of the Airflow repo to be `main` (#16149)", "sampled": "Rename the main branch of the Airflow repo to be `main` (#16149)This", "perturbed_sampled": ["Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "Rename the main branch of the Airflow repo to be `main` (#16149)This"], "perturbed_original": ["Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)", "Rename the main branch of the Airflow repo to be `main` (#16149)"], "original_ll": -4.861001014709473, "sampled_ll": -5.263483047485352, "all_perturbed_sampled_ll": [-5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352, -5.263483047485352], "all_perturbed_original_ll": [-4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473, -4.861001014709473], "perturbed_sampled_ll": -5.263483047485352, "perturbed_original_ll": -4.861001014709473, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "sampled": "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "perturbed_sampled": ["[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From"], "perturbed_original": ["[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)"], "original_ll": -5.949138641357422, "sampled_ll": -6.390750408172607, "all_perturbed_sampled_ll": [-6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607, -6.390750408172607], "all_perturbed_original_ll": [-5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422, -5.949138641357422], "perturbed_sampled_ll": -6.390750408172607, "perturbed_original_ll": -5.949138641357422, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "sampled": "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "perturbed_sampled": ["[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There"], "perturbed_original": ["[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)"], "original_ll": -6.585800647735596, "sampled_ll": -6.959344387054443, "all_perturbed_sampled_ll": [-6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443, -6.959344387054443], "all_perturbed_original_ll": [-6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596, -6.585800647735596], "perturbed_sampled_ll": -6.959344387054443, "perturbed_original_ll": -6.585800647735596, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of time to create, and each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "sampled": "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,", "perturbed_sampled": ["Further speed up API performance tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That said, it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support for models custom attributes with custom attribute names (#14662) Improve model model support to allow custom attributes to be applied to multiple Connexion instances,", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask server-side tests for Connexion take a significant amount of time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model has not been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior to allow model attributes to be added to multiple Connexion instances,", "Further speed up Flask API tests with pytest session fixtures (#14746) Creating the Flask API and adding tests takes up a significant portion of my development time. That means it should be possible to reduce the number of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model instance has been created. #14644 Add support for models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for django.model attributes (#14686) Prevent model attributes from being loaded by Django when a model instance has been created.\n\nMake Connexion behavior better with models with default names (#14662) Improve Connexion behavior to allow custom connexion models to be added to multiple Connexion instances,", "Further speed up Connexion API with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model instances (#14746) Prevent model attributes from being generated by Django once a model instance has been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to model instances,", "Further speed up Connexion API in pytest session time (#14609) Creating the Flask API and Connexion take a significant amount of time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Add model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django after the model instance has been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior when there are custom attributes to be added to multiple Connexion instances,", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion API test suites can take a significant amount of development time. That means that it should now be possible to reduce the complexity of those tests and make them quicker on execution. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Flask when a model has been created.\n\nMake Connexion dynamically select models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,", "Further speed up Connexion API tests using pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes , to prevent model attributes from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support models with default names . Add Connexion behavior to allow model attributes to be added to multiple Connexion instances,", "Further speed up Connexion API tests with pytest session fixtures (#14728) Tests for both the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. Include django model-specific query methods and attributes (#14686) Prevent model properties from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior to allow custom model names to be added to the instances,", "Further speed up Connexion API tests with flask fixtures (#14746) Creating the Flask fixtures for Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods that access model attributes (#14686) Prevent model attributes from being inherited by Django when a model instance has been created. #14763 Add support for models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,"], "perturbed_original": ["Setting up Connexion API tests with pytest session fixtures (#14746) Creating unittest modules for testing API and Connexion take a lot of time to create, and each of these test modules creates the Flask module in the AP. It's easy to make them take some time off and set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Further speed up Connexion API and Connexion API creation in each pytest session . Creating the Flask API and Connexion take a significant amount of time to create, and each of the test modules creates the Flask API using the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the initialization time for api_connexion/endpoints down to under a minute for me.", "Further speed up the set of tests with pytest session fixtures (#14746) Creating the app . Api, Endpoint and Connexion take a significant amount of the Flask app to create, and each of these test modules creates the Flask app with the same set of configurations. To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, so each _subclass_ would have it's \"own\" module . This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Further speed up Flask tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of time to create, each of these test modules creates the Flask app with the necessary tests being set up \"initializers\". To make this work we had to break away from the pattern that pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have a module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Further speed up built-in API tests with pytest session fixtures (#14746) Creating the Flask app and API Connexion take a lot of time to create, and each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as it won't work with <unk>configured_app<unk>. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope . This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Further speed up Connexion API tests with plugin fixtures (#14746) Creating the Flask API and configuration configuration require a significant amount of time to create, and create some of them will require a lot of time to perform. Each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest cannot work with TestCase subclasses. The `configured_app` fixture is defined at the module level, each App _subclass_ would have it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of time to create, and each of these test modules creates the Flask app with the pytest session fixtures set up \"initializers\". To make this work we had to switch away from test case based configuration, otherwise pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's own scope fixture. This cuts the test time for api_connexion/endpoints down to a bit over half an hour for me.", "Fix up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a fair amount of time to create, and each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to break away from `unittest.TestCase`, as pytest fixtures were not designed to work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ needs it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Further discussion: Replaced the Connexion API and Connexion session into pytest session fixtures (#14746) Creating test cases or fixtures for the Connexion API and Connexion take a significant amount of time to create, and even more time to run these test cases in the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the test time for my module down to sub-1 minute for me.", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API app in pytest will take a fair amount of time to create, and each of these test modules creates the Flask app with the same set . To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't take TestCase subclasses. The `configured_app` fixture is defined separately in each module scope, so each _subclass_ would have it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me."], "original_ll": -3.846102476119995, "sampled_ll": -3.116626501083374, "all_perturbed_sampled_ll": [-3.3531641960144043, -2.998563528060913, -3.2336807250976562, -3.213902235031128, -3.227529525756836, -3.1969687938690186, -3.245817184448242, -3.3981707096099854, -3.195507049560547, -3.164397716522217], "all_perturbed_original_ll": [-3.733172655105591, -3.7570884227752686, -3.811758041381836, -4.129810333251953, -3.9352962970733643, -3.9219422340393066, -3.810661792755127, -3.8446593284606934, -3.6501128673553467, -3.8815813064575195], "perturbed_sampled_ll": -3.222770166397095, "perturbed_original_ll": -3.8476083278656006, "perturbed_sampled_ll_std": 0.10178624587931034, "perturbed_original_ll_std": 0.12528354405275763}, {"original": "Update external docs URL for Segment (#13645)", "sampled": "Update external docs URL for Segment (#13645)The", "perturbed_sampled": ["Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The", "Update external docs URL for Segment (#13645)The"], "perturbed_original": ["Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)", "Update external docs URL for Segment (#13645)"], "original_ll": -6.923836708068848, "sampled_ll": -7.498605251312256, "all_perturbed_sampled_ll": [-7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256, -7.498605251312256], "all_perturbed_original_ll": [-6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848, -6.923836708068848], "perturbed_sampled_ll": -7.498605251312256, "perturbed_original_ll": -6.923836708068848, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy to worker deployments, in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the", "sampled": "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side worker templates more flexible when adding the use_rerun action or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to delay run start time or delay the start of a run by default.\n\nThis commit", "perturbed_sampled": ["Add support for modifying the process update deployment strategy (#15213) This commit modifies the worker template to allow passing a custom process deployment update strategy. This version is only useful if the worker template exists in deploy override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more flexibility to configure workstations on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side worker templates more flexible when using the use_rerun action or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server-side worker templates to include a \"retry action\" to delay run time or delay the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker states (#15053). This change provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" action instead of \"re-run with new worker available at each run\" (#15052) This commit makes server side worker templates more flexible by adding the use_rerun option to the worker template along with setting the status of the worker to run with (#15050).\n\nSupport for adding \"retry\" actions to server side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to delay run start time or delay the start of a run by default.\n\nThis commit", "Add support for celery worker deployment strategy (#15045) This commit modifies the worker template to allow passing a celery worker deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This allows for more customization on the server side.\n\nSupport for adding \"re-run after each run\" or \"re-run with new worker available at each run\" actions (#15100) This commit makes server side templates more flexible when adding the use_rerun action to check the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding retry-start actions to server-side worker templates (#15044) This commit allows server side templates to add a \"retry action\" to delay the run start time or delay the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only available if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at run start\" actions (#15100) This commit allows server side worker templates to include a \"re-run\" action when adding the worker or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" action to server-side worker template (#15050) This commit allows server side templates to include a \"retry action\" at run start time or after the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the server-side worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker templates support an override mode.\n\nSupport for supporting more types of workers (#15066) This commit updates the server side worker template to support more worker types and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new target available at each run\" actions (#15100) This commit makes server side worker templates more flexible when adding the use_rerun action or setting the execution date of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to delay the start time or delay the re-start of a run by default.\n\nThis commit", "Add support for modifying the default deployment strategy (#15213) This commit modifies the worker template to support a non-default deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization on the server side.\n\nSupport for the \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) Support for re-running worker templates (#15214). This commit makes server side worker templates fully compatible when adding the use_rerun action or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding retry actions to server-side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to delay run start actions. This makes worker templates delay the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy through the worker template. This new version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit allows server side templates to use the worker template to support multiple worker types and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side templates more flexible when adding the use_rerun action to change the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to worker templates (#15100) This commit allows server side templates to add the \"retry action\" to worker templates after the start time or the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if your worker template supports an override mode.\n\nSupport for multiple types of workers (#15479) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization options on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side worker template modifications more flexible when adding a re-run action or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding retry actions to server-side worker templates (#15044) This commit allows server side templates to define a \"retry action\" to set a new worker start time or delay the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow a non-default deployment update strategy. This is only useful when the worker template supports an override mode.\n\nSupport for multiple types of worker (#15037) This commit modifies the server side worker templates to support multiple server and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side worker templates more flexible when adding the use_rerun action or setting the start time of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to change the start time of the worker after every run. This will add flexibility to the start of a run by default.\n\nThis commit", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker template supports override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization on the server side. Support for adding \"re-run after each run\" and \"re-run with new worker available at each run\" actions (#15039) This commit makes server-side worker templates easier to customize when using the use_rerun action or setting the status of the worker to \"upcoming\" Support for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server templates to include a \"retry action\" that will either reset the run start time or delay the start of a run by default.\n\nThis commit"], "perturbed_original": ["Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy to worker deployments, in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable on rolling deployments of celery workers. This allows rolling deployments to launch one set of replicas before the last set goes away. This allows new workers to pick up work as quickly as possible, rather than the default deployment setup, which is 1 at a time. `maxSurge` is the number of pods that are scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 replicas, and then scale down the old worker count of 3 to 4 in the", "Add support for celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment strategy to specific worker deployment, in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. This will allow new workers to pick up replicas as quickly as possible, rather than the current default which is 1 per replicate each time. `maxSurge` is a parameter, which specifies the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example, if you set the `maxSurge` to 100% and had 4 replicas, when a deployment started it would launch 4 new workers and then scale down the old ones as the", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment strategy to worker deployments, in particular celery workers. The values have modified to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old ones swan away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time. <unk>maxSurge<unk> is the number of pods that will be scaled up beyond the previous count during a rolling deploy. You can specify values in units or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 pods and then scale down the old ones as the", "Add support for modifying celery worker template parameters (#15213) which modifies the worker template to allow a new update strategy to deploy deployments and in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which launches duplicates one at a time. `maxSurge` is the number of pods that will be scheduled to launch based on the current replica count during a rolling deploy. Users can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a new roll up was started it would launch 4 new pods and then scale down the old ones as the", "Add support for modifying celery worker deployment strategy This commit modifies the worker template to support passing a non-default deployment update strategy to worker deployments, in order to help optimize new & existing workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of the worker to launch the new set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, as an alternative to the current default which is 1 at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 pods when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the", "Add support for specific deployment strategies in the worker deployment template. This commit modifies the worker template to allow passing a non-default deployment update strategy to worker deployments, in particular celery workers. Additional values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch more than the current set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, instead of the current default which is 1 second delivery time. `maxSurge` is the number of workers that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values . For example if you set the value to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow a non-default deployment update strategy for rolling deployments, in particular celery workers. The values have been modified to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. Allowing the new workers to scale up work as quickly as possible, rather than a default which is 1 at a time. `maxSurge` defines the maximum number of pods that will be scheduled beyond the replica count in a rolling deploy. Useful to specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the number of pods as the", "Add support for modifying a deployment strategy (#15213) This commit modifies the deployment strategy in the deploy.py to allow passing a non-default deployment strategy to worker deployments, in particular celery . The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the existing set goes away. Allowing the new workers to pick up work as quickly as possible, thus lowering the overall deployment time, beyond the current default which is 1 at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can set this to any specific values or percentages. For example, if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the", "Init Script for modifying celery deployment update strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy to the worker template, in particular celery workers. The values have been set to 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to deploy the full set of replicas before the old ones fade away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific amounts or percentages. For example if you set the `maxSurge` to 100% for 4 replicas, when a rolling deployment started it would schedule all 4 new pods and then scale down to 3 old ones as the", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a rolling deployment update strategy to roll deployments, in particular celery . The resource values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. Allowing the new ones to pick up work as quickly as possible, rather than the old set which goes away one at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica s in a rolling deploy. You can specify specific values or percentages. For example if the target implementation set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch those 4 extra pods and then scale down the old ones as the"], "original_ll": -3.409484624862671, "sampled_ll": -2.601733684539795, "all_perturbed_sampled_ll": [-2.6094627380371094, -2.6224186420440674, -2.686663866043091, -2.5471322536468506, -2.7113301753997803, -2.6242871284484863, -2.626666307449341, -2.6491174697875977, -2.575734853744507, -2.760303497314453], "all_perturbed_original_ll": [-3.3645620346069336, -3.2703475952148438, -3.529677391052246, -3.4227161407470703, -3.434976577758789, -3.498251438140869, -3.4210894107818604, -3.366853952407837, -3.558992624282837, -3.5838630199432373], "perturbed_sampled_ll": -2.641311693191528, "perturbed_original_ll": -3.445133018493652, "perturbed_sampled_ll_std": 0.06018729283572061, "perturbed_original_ll_std": 0.09302276342939542}, {"original": "Add fudament for API based on connexion (#8149)", "sampled": "Add fudament for API based on connexion (#8149)So", "perturbed_sampled": ["Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So", "Add fudament for API based on connexion (#8149)So"], "perturbed_original": ["Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)", "Add fudament for API based on connexion (#8149)"], "original_ll": -6.420657157897949, "sampled_ll": -7.135887145996094, "all_perturbed_sampled_ll": [-7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094, -7.135887145996094], "all_perturbed_original_ll": [-6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949, -6.420657157897949], "perturbed_sampled_ll": -7.135887145996094, "perturbed_original_ll": -6.420657157897949, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Cleanup KubernetsPodOpertor tests (#15475)", "sampled": "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "perturbed_sampled": ["Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup"], "perturbed_original": ["Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)", "Cleanup KubernetsPodOpertor tests (#15475)"], "original_ll": -7.8532633781433105, "sampled_ll": -7.018958568572998, "all_perturbed_sampled_ll": [-7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998, -7.018958568572998], "all_perturbed_original_ll": [-7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105, -7.8532633781433105], "perturbed_sampled_ll": -7.018958568572998, "perturbed_original_ll": -7.8532633781433105, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add example DAG and system test for MySQLToGCSOperator (#10990)", "sampled": "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "perturbed_sampled": ["Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Add example DAG and system test for MySQLToGCSOperator (#10990)A"], "perturbed_original": ["Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Add example DAG and system test for MySQLToGCSOperator (#10990)"], "original_ll": -6.152090072631836, "sampled_ll": -6.518339157104492, "all_perturbed_sampled_ll": [-6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492, -6.518339157104492], "all_perturbed_original_ll": [-6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836, -6.152090072631836], "perturbed_sampled_ll": -6.518339157104492, "perturbed_original_ll": -6.152090072631836, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix PyPI spelling (#13864)", "sampled": "Fix PyPI spelling (#13864)WASHINGTON,", "perturbed_sampled": ["Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,", "Fix PyPI spelling (#13864)WASHINGTON,"], "perturbed_original": ["Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)", "Fix PyPI spelling (#13864)"], "original_ll": -7.816772937774658, "sampled_ll": -8.67366886138916, "all_perturbed_sampled_ll": [-8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916, -8.67366886138916], "all_perturbed_original_ll": [-7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658, -7.816772937774658], "perturbed_sampled_ll": -8.67366886138916, "perturbed_original_ll": -7.816772937774658, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "sampled": "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "perturbed_sampled": ["[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's"], "perturbed_original": ["[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)"], "original_ll": -5.924832820892334, "sampled_ll": -6.113379001617432, "all_perturbed_sampled_ll": [-6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432, -6.113379001617432], "all_perturbed_original_ll": [-5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334, -5.924832820892334], "perturbed_sampled_ll": -6.113379001617432, "perturbed_original_ll": -5.924832820892334, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.", "sampled": "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.A", "perturbed_sampled": ["[AIRFLOW-4422] Add stats (#5453) Add stats to record pool utilization such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record , such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization reporting.DESCRIPTION Add stats to record pool utilization such as open slots and used slots.A", "[AIRFLOW-4422] Add slot stats (#5453) Add stats to record pool utilization such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization in terms of open slots and used slots.A", ". Add slot utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to pool utilization such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization stats Add stats to record pool utilization such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record Pool utilization, such as open slots and used slots.A", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as the number of cards inserted and used slots.A"], "perturbed_original": ["[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization by record pool utilization including open slots and used slots.", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record of pool utilization, such as open slots and used slots.", "[AIRFLOW-4422] Pool usage (#5453) Add stats to record pool utilization such as open slots and used slots.", "[AIRFLOW-4422] Pool utilization stats : Add the Pool utilization stats to record pool utilization such as open slots and used slots.", "Added pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.", "[AIRFLOW-4422] Pool utilization statistics. Add stats to record pool utilization such as open slots and used slots.", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as number of used slots and used slots.", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization for both open slots and used slots.", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.", "[AIRFLOW-4422] Add stats (#5453) Add stats to record pool utilization such as open slots and used slots."], "original_ll": -5.5927534103393555, "sampled_ll": -5.786152362823486, "all_perturbed_sampled_ll": [-5.846164703369141, -6.183856010437012, -5.607089042663574, -5.855631351470947, -5.63323974609375, -5.951909065246582, -5.662703990936279, -6.135513782501221, -5.760870933532715, -5.651190757751465], "all_perturbed_original_ll": [-5.737074851989746, -5.193274974822998, -5.631934642791748, -5.714838981628418, -5.281574249267578, -5.599587440490723, -5.341476917266846, -5.616407871246338, -5.5927534103393555, -5.641607284545898], "perturbed_sampled_ll": -5.828816938400268, "perturbed_original_ll": -5.535053062438965, "perturbed_sampled_ll_std": 0.19684060939915957, "perturbed_original_ll_std": 0.18069967845790438}, {"original": "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 To be reviewed by : Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "sampled": "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a new command line argument parser to parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x releases which led to an unexpected error message when the script was executed from a Python3.x version that", "perturbed_sampled": ["Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a command line argument parser to parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Update d an example illustrating the new feature after removing a subpkg * Fix a rare regression in Python 3.0 which led to an unexpected error message when the command was executed from a Python3.x version that", "Rewrite handwritten argument parser in prepare(lib). [@lmoo]. * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Write a new command line argument parser for evaluating the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x releases which led to an unexpected error message when the script was executed from a Python3.x version that", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a new command line argument parser to parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Documentation on an example illustrating the parser on a subpkg * Fix a rare regression with the most recent Python 3.x releases which led to an unexpected error when the script was executed from a Python3.x version that", "Rewrite handwritten argument parser in Python 3.4 * Rewrite handwritten argument parser in Python 3.4 ** Documentation on a new command line argument parser to parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an alternative way to install the new parser on a subpkg * Fix a rare regression in Python 3.x releases which led to an error message if a bash script was executed from a Python3.x version that", "Rewrite handwritten argument parser with handle (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Update documentation on a new command line argument parser that extracts the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x that led to an unexpected error message when the script was executed from a program that", "Rewrite handwritten argument parser in prepare_provider_packages * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a new command-line argument parser ** To parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example of using new () in a subpkg * Fix a rare regression in Python 3.x releases which led to an unexpected error message when the script was executed from a Python3.x version that", "Rewrite argument parser in prepare_provider_packages.py (#13234) * Rewrite parameter parser in prepare_pkg.py [@sjoerd]. ** Documentation on a new command line argument parser to parse the argument from the command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x releases which led to an unexpected error when the script was executed from a Python3.x version that", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). py [** Update Documentation on a new command line argument parser to handle argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a command line ** Fix a rare regression in Python 3.x releases due to an unexpected error message when the script was executed from a Python3.x version that", "Rewrite handwritten argument parser in prepare * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a new command line argument parser that can be used to read the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser in the test subpkg * Fix a rare regression in Python 3.x that led to an unexpected error message when the script was executed from a Python3.x version that", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on the command line parser . To read the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser for a subpkg * Fix a rare regression in Python 3.x releases that led to an unexpected error message when the script was executed from a Python3.x version that"], "perturbed_original": ["Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv with argparse. - Replaced positional argument for position and optional argument. <unk># 13069 To be reviewed by : Kamil, Jarek. * Modified help text for each new function as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argv manipulation - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 To be reviewed by : Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by the Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI method in prepare_provider_packages.py to a separate function for modularity and code cleanup as suggested by the Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py . Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced handwritten arguments with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 To be reviewed by Kamil and Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argparse. - Replaced positional argument for PACKAGE with parameter. Issue : 13069 To be reviewed by : Kamil, Jarek. * Modified help text using textedit as suggested by Kamil. Signed-off-by: Debodirno Chandra * Moved argv subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) Issue Description: Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced explicit argument parser manipulation with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue Summary: To be reviewed by : Debodirno Chandra * Modified help text for prepare_provider_packages as suggested by : Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argparse. - Replaces CLI argument for PACKAGE with optional argument. Issue : To be reviewed by : Kamil, Jarek. * Modified the structure for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI argument in prepare_provider_packages.py to a separate function for modularity and code reuse. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py - Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with default argument parser - Replaced positional argument for PACKAGE with optional argument. Issue : to be reviewed by : Kamil, Jarek. * Modified help er function in prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate file for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 4 reviewed by : Kamil * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Move CLI script prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py to support sys.argv manipulation . Change : - Replaced positional argument for PACKAGE with optional argument. Issue : To be reviewed by Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for manual code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) - Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced handwritten argument parser with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 To be reviewed by Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for easy code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>"], "original_ll": -2.868077516555786, "sampled_ll": -3.188203811645508, "all_perturbed_sampled_ll": [-3.4663288593292236, -3.476292133331299, -3.2060091495513916, -2.9841294288635254, -3.541076898574829, -3.3726329803466797, -3.189221143722534, -3.248640537261963, -3.3842835426330566, -3.3965651988983154], "all_perturbed_original_ll": [-3.0701348781585693, -3.0129802227020264, -2.9907402992248535, -3.3450846672058105, -2.8310492038726807, -2.891223669052124, -2.9589784145355225, -2.954845428466797, -2.8959808349609375, -2.8578171730041504], "perturbed_sampled_ll": -3.3265179872512816, "perturbed_original_ll": -2.980883479118347, "perturbed_sampled_ll_std": 0.15970366006708717, "perturbed_original_ll_std": 0.13990584465538744}, {"original": "Make models/taskinstance.py pylint compatible (#10499)", "sampled": "Make models/taskinstance.py pylint compatible (#10499)Still", "perturbed_sampled": ["Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still", "Make models/taskinstance.py pylint compatible (#10499)Still"], "perturbed_original": ["Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)", "Make models/taskinstance.py pylint compatible (#10499)"], "original_ll": -5.924649238586426, "sampled_ll": -6.753892421722412, "all_perturbed_sampled_ll": [-6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412, -6.753892421722412], "all_perturbed_original_ll": [-5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426, -5.924649238586426], "perturbed_sampled_ll": -6.753892421722412, "perturbed_original_ll": -5.924649238586426, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "sampled": "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "perturbed_sampled": ["[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\""], "perturbed_original": ["[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)"], "original_ll": -6.061471462249756, "sampled_ll": -6.3326826095581055, "all_perturbed_sampled_ll": [-6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055, -6.3326826095581055], "all_perturbed_original_ll": [-6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756, -6.061471462249756], "perturbed_sampled_ll": -6.3326826095581055, "perturbed_original_ll": -6.061471462249756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "sampled": "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "perturbed_sampled": ["[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About"], "perturbed_original": ["[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)"], "original_ll": -5.8410468101501465, "sampled_ll": -6.46656608581543, "all_perturbed_sampled_ll": [-6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543, -6.46656608581543], "all_perturbed_original_ll": [-5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465, -5.8410468101501465], "perturbed_sampled_ll": -6.46656608581543, "perturbed_original_ll": -5.8410468101501465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "sampled": "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "perturbed_sampled": ["[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (increased usability) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) AirFLOW-4970 Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration * Bugfix * [AIRFLOW-4970] Add Google Campaign Manager integration"], "perturbed_original": ["[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) : Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * AirFLOW -- Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * Airflow-4970 Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration"], "original_ll": -3.6449153423309326, "sampled_ll": -3.6449153423309326, "all_perturbed_sampled_ll": [-3.6449153423309326, -3.6045639514923096, -3.6449153423309326, -4.850269317626953, -3.6449153423309326, -3.6449153423309326, -4.523898124694824, -3.6449153423309326, -4.850269317626953, -3.6470179557800293], "all_perturbed_original_ll": [-4.833765506744385, -3.6449153423309326, -3.6449153423309326, -4.968268871307373, -3.6449153423309326, -4.356425762176514, -3.600027561187744, -3.6449153423309326, -3.6449153423309326, -3.6449153423309326], "perturbed_sampled_ll": -3.970059537887573, "perturbed_original_ll": -3.962797975540161, "perturbed_sampled_ll_std": 0.5121339980805986, "perturbed_original_ll_std": 0.515984261539512}, {"original": "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "sampled": "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "perturbed_sampled": ["[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\""], "perturbed_original": ["[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "[AIRFLOW-6326] Sort cli commands and arg (#6881)"], "original_ll": -6.4958271980285645, "sampled_ll": -6.957827568054199, "all_perturbed_sampled_ll": [-6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199, -6.957827568054199], "all_perturbed_original_ll": [-6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645, -6.4958271980285645], "perturbed_sampled_ll": -6.957827568054199, "perturbed_original_ll": -6.4958271980285645, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to use same version as the image but it should be unified (and in 1.10 when trying to build 2.7 image it would fail).", "sampled": "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the module will be added to a shared library it fails", "perturbed_sampled": ["Default path is used when building image (#13285) For image build the python version in image files is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the value of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the module will be added to test_path or library it fails", "Default name is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path will be used instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking that a module will be added to a module pack it fails", "Default python version is used when building image files (#13435) When image build fails, the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version used in image files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking which module will be added to a shared library it fails", "Default python version is not passed when building image (#13285) For image build , a Python version is passed via PYTHON_MAJOR_VERSION, which is different than the version number of the image files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the module will be added to a shared library it fails", "Default python version is used when building libraries. For now, the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When calling python test_path it fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if an extension will be added to a shared library it fails", "Default python version is used when building image . During image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path _scan should be used instead of the build path (#13627) (#12945) for using static keywords in images (#14059) When checking if the module will be added to a shared library it fails", "Default python version is used when building image (#13285) For image build the python version is passed to the build which is different than the version in .py files (#13515) When image build fails, the output path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the module .py can be added to a shared library it fails", "Default python version is used when building images. For image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) Warning: check will not be added to shared libraries (#14059) When checking that a module will be added to a shared library it fails", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When checking if the image build or image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for shared libraries (#14059) When checking if the module will be build with a shared library it fails", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_VERSION, which may be different than the version in .py files (#13515) When image build fails, the output of test_path is returned instead of the build path (#12945) for shared libraries (#14059) When checking which module will be added to a shared library it fails"], "perturbed_original": ["Default python version is passed when building image (#13285) ! For the normal Master/2.0 build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of image build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to use same version as the image but for image building it\u2019s time to be unified (and in 1.10 when trying to build 2.7 image it would fail).", "Default python version is used when building master/2.0 image (#13285) For image the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part in the image build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to have the same version as the image but it should be unified (and in 1.10 when trying to build 2.7 image it would fail).", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses python installed on Master as default and it is fine for Master/2.0 to use same version as the Image build but for image build it should be unified (and in 1.10 when using image build 2.7 it would fail).", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses python installed on host. This is fine for our scenario but in order to use this as the default it should be unified (and in 1.10 when trying to build image it would fail).", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (like airflow package) that uses python installed on host. This is fine for Master/2.0 to use same version as the 2.7 image but we can't have it be unified (and for this reason when trying to build 2.7 image it would fail).", "Default python version is used when building image (#13285) For image build the python version used in image is set on master via PYTHON_MAJOR_MINOR_VERSION . There is a part of the build (preparing airflow package) that uses python installed on host. It would be fine for Master/2.0 to use same version for build image but it should be unified (and in 1.10 when trying to build 2.7 image it would fail).", "Default python version passed when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow ...) which uses python installed on host. This is fine for Master/2.0 as there is not a issue if host is same version as the image but it should be unified (and in 1.10 when trying to build in the old way it would fail).", "Default python version is used as default across image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a dependency between that and it seems that the build (preparing airflow package) is run on version of python installed on host. This is fine for Master/2.0 to use same version as the image but the image build will not be unified (and in 1.10 when trying to build 2.7 image it would fail).", "Default python version is used when building image (#13285) I tried earlier to build the image where version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to use default version as the image but it should be not necessary for 2.7 image (because in 1.10 when trying to build 2.7 image it would fail).", "Default python version is used when building image (#13285) For image build the python version of master is pre-determined via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses 2.7 on host. This would be good for Master/2.0 to use same version as the image but it should be unified (and in 1.10 when it try to build 2.7 image it would fail)."], "original_ll": -3.6570262908935547, "sampled_ll": -3.1501235961914062, "all_perturbed_sampled_ll": [-3.2581465244293213, -3.229282855987549, -3.051719903945923, -3.211683988571167, -3.263725757598877, -3.5219173431396484, -3.378495931625366, -3.159735918045044, -3.1281278133392334, -3.1014890670776367], "all_perturbed_original_ll": [-3.912590265274048, -3.5353822708129883, -3.7550899982452393, -3.60075306892395, -3.569685220718384, -3.6074748039245605, -3.732893705368042, -3.695676326751709, -3.653214693069458, -3.66440749168396], "perturbed_sampled_ll": -3.2304325103759766, "perturbed_original_ll": -3.6727167844772337, "perturbed_sampled_ll_std": 0.1315843404716738, "perturbed_original_ll_std": 0.10384313920431178}, {"original": "Check python version before starting triggerer (#18926)", "sampled": "Check python version before starting triggerer (#18926)One", "perturbed_sampled": ["Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One", "Check python version before starting triggerer (#18926)One"], "perturbed_original": ["Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)", "Check python version before starting triggerer (#18926)"], "original_ll": -6.845289707183838, "sampled_ll": -7.593542098999023, "all_perturbed_sampled_ll": [-7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023, -7.593542098999023], "all_perturbed_original_ll": [-6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838, -6.845289707183838], "perturbed_sampled_ll": -7.593542098999023, "perturbed_original_ll": -6.845289707183838, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "sampled": "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "perturbed_sampled": ["Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>"], "perturbed_original": ["Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>"], "original_ll": -4.542260646820068, "sampled_ll": -4.120544910430908, "all_perturbed_sampled_ll": [-4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908, -4.120544910430908], "all_perturbed_original_ll": [-4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068, -4.542260646820068], "perturbed_sampled_ll": -4.120544910430908, "perturbed_original_ll": -4.542260646820068, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was not.", "sampled": "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was no", "perturbed_sampled": ["Remove : Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was no", "Remove Brent from Collaborators (#18182) Brent is already a committer so I\u2019m just removing this entry here. It was needed only when he was no", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was done back when he was no", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this list as a whole. It was needed only when he was no", "Remove Brent from Collaborators (#18182) : Brent is already a committer so we don't this entry here. It was needed only when he was no", "Remove Brent from Collaborators (#18182) Brent is already debated so we don't this entry here. It was needed only when he was no", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was added when he was no", "Remove Brent as Committer (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was no", "Remove Brent from Collaborators (#18182) Brent is already a committer so there's no need to make this entry here. It was needed only when he was no", "Remove Brent from Collaborators (#18182) Brent is already a committer so we never needed his entry here. It was needed only when he was no"], "perturbed_original": ["Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was in fact a time when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry anymore. This was needed only when he was not.", "Remove Brent from commits. Brent is already a committer so we don't this entry here. It was needed only when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was a commit when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so we removed his entry here. It was needed only when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so don't this entry here. It was needed only when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed , but he was not.", "Remove Brent from Collaborators (#18182) Brent is already in collaborations so we don't this entry here. It was needed only when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was there for Brent's commits and only when he was not.", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was not."], "original_ll": -4.801669120788574, "sampled_ll": -4.851532936096191, "all_perturbed_sampled_ll": [-4.97227144241333, -5.196328163146973, -4.656074047088623, -4.547585964202881, -4.802281379699707, -5.3737568855285645, -4.731512069702148, -4.646214962005615, -4.287915229797363, -4.763675212860107], "all_perturbed_original_ll": [-4.558149814605713, -4.741705894470215, -4.7947916984558105, -4.688475608825684, -4.636351108551025, -4.968733787536621, -4.849677085876465, -5.035050392150879, -4.713690280914307, -4.801669120788574], "perturbed_sampled_ll": -4.797761535644531, "perturbed_original_ll": -4.77882947921753, "perturbed_sampled_ll_std": 0.2987011526364083, "perturbed_original_ll_std": 0.13794684688596015}, {"original": "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "sampled": "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "perturbed_sampled": ["[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or"], "perturbed_original": ["[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)"], "original_ll": -4.827899932861328, "sampled_ll": -5.03603982925415, "all_perturbed_sampled_ll": [-5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415, -5.03603982925415], "all_perturbed_original_ll": [-4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328, -4.827899932861328], "perturbed_sampled_ll": -5.03603982925415, "perturbed_original_ll": -4.827899932861328, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "sampled": "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "perturbed_sampled": ["Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still"], "perturbed_original": ["Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)"], "original_ll": -4.170490741729736, "sampled_ll": -4.739984035491943, "all_perturbed_sampled_ll": [-4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943, -4.739984035491943], "all_perturbed_original_ll": [-4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736, -4.170490741729736], "perturbed_sampled_ll": -4.739984035491943, "perturbed_original_ll": -4.170490741729736, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well.", "sampled": "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the `exit` clause inside", "perturbed_sampled": ["Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is deleted only during execution of the `exit` clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the user_id variable is available only during execution of the `exit` clause inside", "Docs: Clarify behavior of function at runtime. Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the `exit` clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the failure clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is pushed during execution of the `exit` clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the fail clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` function is available only during execution of the `exit` clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution During the `exit` clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` function is available only during execution of the `exit` clause inside", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is called only during execution of the `exit` clause inside"], "perturbed_original": ["Docs: Clarify the importance of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the behavior only applies to worker failures, not task failures as well.", "This change modifies the behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag is tied to worker failures, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag is used only to remove worker pods in response to worker failures, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that behavior of this flag only applies to worker failures, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, but not to pod failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failure, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that this flag only applies to worker failures, not task failures as well.", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only works on worker failures, not task failures as well."], "original_ll": -2.8407065868377686, "sampled_ll": -2.932061195373535, "all_perturbed_sampled_ll": [-2.991611957550049, -3.8863589763641357, -3.7053675651550293, -3.0197536945343018, -3.088087558746338, -3.0620734691619873, -2.888294219970703, -3.1526966094970703, -2.888294219970703, -2.9661948680877686], "all_perturbed_original_ll": [-2.8419950008392334, -3.6215286254882812, -2.92024302482605, -2.966339349746704, -2.860455274581909, -3.7746729850769043, -2.7973766326904297, -2.8903512954711914, -3.631565809249878, -2.9394826889038086], "perturbed_sampled_ll": -3.1648733139038088, "perturbed_original_ll": -3.124401068687439, "perturbed_sampled_ll_std": 0.3275411494668688, "perturbed_original_ll_std": 0.3659520618278032}, {"original": "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the worker pods that execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "sampled": "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels rather than running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "perturbed_sampled": ["[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels , allow task definitions to specify labels rather than running a specific kubernetes user instead of running a definition for an entire pod. A new label for", "[AIRFLOW-4739] Add the ability to arbitrarily define kubernetes worker pod labels (#5376) Allow the user to specify labels rather than running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow worker pod definitions to specify labels rather than running a specific kubernetes user instead of running a user for the entire pod. A new label for", "[AIRFLOW-4739] Add ability to define kubernetes worker pod labels (#5376) Allow task definitions to specify labels for running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "An ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to run as pod labels rather than running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "[AIRFLOW-4739] Add ability to specify kubernetes worker pod labels (#5376) Allow task definitions to specify labels rather than running pod specific. Labels added for kubernetes user instead of running a definition in the entire pod. A new label for", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes pod. [AIRFLOW-5377] Task definition labels (#5376) Allow task definitions to specify labels for tasks running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) : Allow pod definitions to specify labels rather than running a specific kubernetes user ID and running a definition in the entire pod. A new label for", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels rather than running a specific pod. This instead of running a task that is not appropriate for the entire pod. A new label for", "[AIRFLOW-4739] Add support to arbitrarily define kubernetes worker pod s. Allow task definitions to specify labels rather than running a specific kubernetes user instead of running a definition in the entire pod. A new label for"], "perturbed_original": ["[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod s. Allow the command line command to specify labels on the worker pods that execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the worker pods that execute that task, by specifying an extra field in executor_config like this: {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes executor task labels (#5376) Allow task definitions to specify labels on the worker pods that are run next to the task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the worker pods that execute a task by specifying an extra entry in the executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add the ability to arbitrarily define kubernetes executor task labels (#5376) Allow task definitions to specify labels on the worker pods that execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels for worker pods that will execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the worker pods that execute that task by specifying an extra field in executor_config : `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allows executor task definitions to specify labels on the worker pods that execute that task by specifying an optional field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pods for tasks (#5376) Allow task definitions to specify labels on the worker pods of that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the pods that execute them by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`"], "original_ll": -3.903278350830078, "sampled_ll": -4.159265041351318, "all_perturbed_sampled_ll": [-4.237339019775391, -4.059124946594238, -4.01303768157959, -4.069558620452881, -3.960303783416748, -4.359337329864502, -3.941535472869873, -4.257771968841553, -4.277933120727539, -4.205346584320068], "all_perturbed_original_ll": [-3.902639627456665, -4.341277599334717, -3.82790207862854, -3.880923271179199, -3.876082181930542, -3.882504463195801, -3.8794517517089844, -3.8452231884002686, -3.8840174674987793, -3.9307026863098145], "perturbed_sampled_ll": -4.138128852844238, "perturbed_original_ll": -3.925072431564331, "perturbed_sampled_ll_std": 0.13928954983945602, "perturbed_original_ll_std": 0.14124717081370222}, {"original": "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "sampled": "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of DAG-API to #7627.", "perturbed_sampled": ["[AIRFLOW-7080] Adds API endpoint to return a DAG's name (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an experimental revision of DAG-API to #7627.", "[AIRFLOW-7080] Adds an additional endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire section of the experimental DAG-API to #7627.", "[AIRFLOW-7080] Adds API endpoint to the experimental DAG-API to get information about a DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved the core group of DAG-API to #7627.", "[AIRFLOW-7080] Adds API endpoint to support the DAG's paused state (#7737) Added additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of DAG-API to #7627.", "[AIRFLOW-7080] Adds API endpoint to return a DAG state (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of API endpoints from #7739 to #7627.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state - add an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of APIs to #7627.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (drag-7082) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved the experimental group of DAG-API to #7627.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API that covers the entire group of endpoints in #7627.", "[AIRFLOW-7080] Adds API endpoint to support DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API and commits the entire group of DAG-API to #7627.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an API endpoint to return a DAG's active state to the DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of DAG-API to #7627."], "perturbed_original": ["[AIRFLOW-7080] Adds API endpoint to return DAG paused state (#7737) Adds an additional endpoint to the experimental API to return paused state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state. Adds an additional endpoint to the API to return the paused state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to return the paused state (#7737) Added an additional endpoint to the experimental API to return the paused state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an endpoint to the BUILD API to return the paused state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to Experimental to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint in the experimental API to return the suspended state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state. Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "[AIRFLOW-7080] Adds an additional endpoint to the experimental API to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "[AIRFLOW-7080] Add an additional endpoint to the experimental API to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the append-pause API to return the paused state of a DAG."], "original_ll": -4.299529075622559, "sampled_ll": -4.042131423950195, "all_perturbed_sampled_ll": [-3.9657952785491943, -3.831112861633301, -3.6174306869506836, -4.175403118133545, -3.845689535140991, -4.451114177703857, -4.280528545379639, -4.793585300445557, -4.621741771697998, -3.5895068645477295], "all_perturbed_original_ll": [-4.505472660064697, -4.062127590179443, -4.6369309425354, -4.34223747253418, -4.339486122131348, -4.407826900482178, -4.190506935119629, -3.6725196838378906, -3.5612070560455322, -4.394241809844971], "perturbed_sampled_ll": -4.117190814018249, "perturbed_original_ll": -4.211255717277527, "perturbed_sampled_ll_std": 0.3950112090035838, "perturbed_original_ll_std": 0.3330098932237053}, {"original": "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - Fix functionality last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG in DB - Change name last_scheduler_run to last_parsed_time, to better reflect what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for DB (from @makr) - add more debug output into DB options (#14572) #14579 - add a couple of new command lines. #14788 - add \"dbl\" and 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - new output of DB check is also output into a string, that matches", "perturbed_sampled": ["Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to build path (#14589) - add 'all' command (#14436) - add debug tools (from @makr) (#14572) - set _start_time to a new time, for DB (from @makr) - add more debug output to build options (#14572) #14579 - add a few lines of new command lines. #14788 - add 'all' and 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - new output of DB build options also output into a string, that matches", "Rename d last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db build, new debug lines - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for DB (from @makr) - add more debug output to options (#14572) - add a couple of new command lines. #14788 - add \"dbl\" and some command lines to DB build, to help debug DB build. #14728 - add \"all\" option in options file (#15008) #14537 - new output of DB check is also output into a string, that matches", "- put db_time into last_parsed_time, and ensure it's updated in DB (#14581) - add if db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for DB (from @makr) (#14583) - add more debug output into DB options (#14572) #14579 - add a couple of new command lines. #14788 - add a couple of new 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - new output to check . - convert DB constructor output into a string, that matches", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command line to build (#14591) - add debug support (from @makr) (#14572) - set last_run to null for the first time (from @makr) - add more debug ging/help features to DB options (#14572) - add a couple of new commands in DB configure #14788 - add \"dbl\" and 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - new output of DB check is also to a string, that matches", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for debugging(from @makr) - add more testable commands into DB (#14469) #14579 - add a couple of new command lines. #14788 - add \"dbl\" and 'all' commands to DB build, to improve compatibility with DBConfig (#14563) #14728 - add :log option in options file (#15008) #14537 - new output of status is also output into a string, that matches", "Rename last_scheduler_run into last_parsed_time, and ensure that is in DB -- add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - remove _start_time to null for DB (from @makr) - add more debug output into DB options (#14572) #14579 - add a couple of new command lines. #14788 - add new 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#14567) - new output of DB check is also available - add a string, that matches", "Rename last_scheduler_run s and ensure it's updated in the next time run. #14793 - add \"dbl\" command to DBConfig file path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#15012) #14793 - set _start_time to null for DB Config (#15417) - add more debug output into DB options (#14572) #14579 - add a couple of new command lines. #14788 - add \"dbl\" and 'all' command lines to DBConfig to help debug DBConfig (#14563) #14728 - add \"all\" option options file (#15008) #14537 - new output of DB check is also output into a string, that matches", "Rename last_scheduler_run into a file to ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for DB build (#14572) - add more debug output into DB options (#14572) #14579 - add a new debug message into DB options and new command lines. #14788 - add \"dbl\" and 'all' command lines to DB build, to help debug DBConfig . #14719 - add \"all\" option in options file (#15008) #14537 - make debug output simpler. #14944 - check DB configuration output into a string, that matches", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from #4612) - set _start_time to null for debug mode (and @makr) - add more debug output /config options (#14572) #14579 - get a couple of new command line options (#14277) #14788 - add \"dbl\" option to command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - now the output of DB check is also output into a string, that matches", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in the project path. #14794 - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @jamiej) - set _start_time to null for DB (from @makr) - add more debug output (#14445) #14798 - add custom run options (#14572) #14579 - add a couple of new command lines. #14788 - add \"dbl\" and 'all' command for DB build, to work with DBConfig (#14563) #14728 - add \"all\" option to configuration file (#15008) #14537 - new output mode: check is also output into a string, that matches"], "perturbed_original": ["Rename column name to last_parsed_time, and ensure it's updated in updated_date - Column last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last _parsed_time of each DAG in DB - Change name to last_parsed_time, to fit with what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts can perform, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run to last_parsed_time, and ensure it's updated in DB (#14581) - Fix functionality last_scheduler_run is created in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of DAG in DB . Rename the name last_scheduler_run to last_parsed_time, to better reflect what it is now. Migration script is added, and codebase is fixed for migrate. To ensure the migration scripts can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with one per column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - A column that contains functionality last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of a dag. Update in DB - Change name last_scheduler_run to last_parsed_time, to better reflect what it does now. Rename it so it is updated in DB when codebase is updated - To ensure this is done in scripts , we have to limit the permission set in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - Fix functionality last_scheduler_run was missed during the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of a dag in DB . I changed the name last_scheduler_run in the code to better reflect what it is there to support: Migration script is added, and codebase is updated - To ensure the migration scripts can work, we have to change the columns needed in the MigrationScript object, and ensure the migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run into last_parsed_time, and ensure it's present in DB (#14581) - Fix functionality last_scheduler_run was missed in the process of migrating from log_read() to bulk_write_to_db. This allows multiple DAGs to fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG in DB - Change name last_scheduler_run to last_parsed_time, to better reflect what it does now. Migration script is added, and codebase is improved. To ensure the migration scripts can work, we also limit the permissions in create_dag_specific_permissions(), so migration 2c6edca13270 d3acabcdfaf should work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run into last_parsed_time, and ensure it's in DB (#14581) - A column last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG in DB - Change name last_scheduler_run to last_parsed_time, to better reflect what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts work, we have to limit the columns that can be imported by create_dag_specific_permissions(), so migration 2c6edca13270 can use the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - Fix one step was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. It will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG in DB . Change table name last_scheduler_run into last_parsed_time to better reflect what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts can work, we have to limit number of lines needed in the codebase. An intermediate migration 2c6edca13270 can work to update the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - Fix functionality last_scheduler_run was missed in the migration scripts when migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and prevent users from fetching the last schedule time of each DAG in DB - Change name last_scheduler_run to last_parsed_time, to better reference what it does now. Migration script is updated when entire codebase is updated - So the migration scripts can handle the same issue, and we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "run into last_parsed_time, so it's updated codebase will work. (#14581) - Fix functionality last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and will fail from checking the last schedule time of each DAG in DAG table. Change name last_scheduler_run to last_parsed_time, to better reflect what it does . (#14585) Migrating script is added, and codebase is updated . To ensure the migration scripts can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Rename last_scheduler_run to last_parsed_time, and ensure it's updated in DB (#14581) - The last_scheduler_run column is in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG . Fix - We renamed last_scheduler_run to last_parsed_time, to better reflect what it does now. DB rename is added, and codebase is updated - To ensure the new DB codebase can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -3.4800262451171875, "sampled_ll": -2.965261220932007, "all_perturbed_sampled_ll": [-3.0520846843719482, -3.292189836502075, -3.243403196334839, -3.152705192565918, -3.1636037826538086, -3.223069667816162, -3.178593158721924, -3.1615281105041504, -3.294952869415283, -3.2238845825195312], "all_perturbed_original_ll": [-3.604717254638672, -3.4282758235931396, -3.513821601867676, -3.668355941772461, -3.662874698638916, -3.4230780601501465, -3.634904623031616, -3.5261971950531006, -3.5925850868225098, -3.3431429862976074], "perturbed_sampled_ll": -3.198601508140564, "perturbed_original_ll": -3.5397953271865843, "perturbed_sampled_ll_std": 0.06918145637134052, "perturbed_original_ll_std": 0.10644950151607553}, {"original": "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. The --rm flag is added.", "sampled": "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "perturbed_sampled": ["[AIRFLOW-6511] Remove BATS container, fix. (#7103) The containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed in the build but you have to remove them with `docker rm`. This should fix any bugs you", "Add the BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) \" containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker remove>. This should fix any bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This should clear up the bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This is your current project. Please report any bugs you", "[AIRFLOW-6511] Remove docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and I needed to remove them with `docker rm`. This should fix any bugs you", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were added via Docker and you have to remove them with `docker rm`. This should fix any bugs you"], "perturbed_original": ["[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and I want to remove them with `dockery system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and I had to remove them with `dockery system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them manually through <unk>dfs system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were created and you have to remove them with `dockery system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery system commands. Here the --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed immediately. You have to remove them with `dockery system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. The --rm flag should be added.", "[AIRFLOW-6511] Remove Dockery containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. The --rm flag is added.", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. A clean bin flag is added."], "original_ll": -4.8016862869262695, "sampled_ll": -4.509748458862305, "all_perturbed_sampled_ll": [-4.609539985656738, -4.392770290374756, -4.19801664352417, -4.803386211395264, -4.744779586791992, -4.515322685241699, -4.505546569824219, -4.234957695007324, -4.550200939178467, -4.512576580047607], "all_perturbed_original_ll": [-4.809858322143555, -4.7749738693237305, -5.2858476638793945, -4.84409236907959, -4.89278507232666, -4.788341999053955, -4.958450794219971, -4.703298568725586, -4.619983673095703, -5.0480265617370605], "perturbed_sampled_ll": -4.506709718704224, "perturbed_original_ll": -4.872565889358521, "perturbed_sampled_ll_std": 0.18422607850467004, "perturbed_original_ll_std": 0.179773010534869}, {"original": "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation mechanism this happens frequently enought and it is easy enough to fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "sampled": "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path to add a pip dependency (#13805) + Support for using \"py\" for path/filename name argument for cwd() function (#13979) + Add a -n option to pip install --help to get all available help files (#13952) + Added config.py to install pips from sources. * Added support for Windows + added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "perturbed_sampled": ["Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path for a pip dependency (#13805) + Support for using \"py\" for path/filename name argument for cwd() function . * Add a -n option to pip .conf to get all existing files, not only the recent files (#13952) + Added config.py to install external sources. * Added support for Windows + added Python 2.7.x dependencies, added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "Open id dependency (#13714) * Adds a pip2 dependency option (#13723) * Seems that python3-openid dependency is not supported by tools/packages. * New code path to add a pip dependency (#13805) + Support for using \"py\" for path/filename name + New cwd() function (#13979) + Add --help option to pip install --help to get all available help files (#13952) + Added config.py to install pips from sources. * Installed for Windows + added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "Add open id dependency (#13714) * New dependency requirement Seems that python3-openid dependency is not properly used with some tools/packages. * New code path to add a pip 1 dependency + Support use of \"py\" for path/filename name argument for cwd() function (#13979) + Add a -n option to pip install --help to get all available help files (#13952) + Added config.py to install pips from sources. * New support for pip3 * Newly added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that problem is not properly solved by tools/packages. * New option to add a pip dependency (#13805) + Support for using \"py\" for path/filename name argument for cwd() function (#13979) + Add s a \"-h\" option to pip install --help to get all available help content. + Added config.py to install pips from sources. * Added support for Python 3 * Updated dependencies, added Python 2.7.x * Added a pip2 dependencies option. (#13898) * Pip install now always fails + Support", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not solved by tools/packages. * New psp2 to add a pip dependency to pip2 (#13952) + Support for using \"py\" for path/filename name argument for cwd() function (#13979) + Add help option to pip install --help to get all available help files (#13952) + Added config.py to install pips db * Added support for Windows + added Python 2.7.x * Added a pip2 dependencies option. * Pip install now always fails + Support", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path of a pip dependency (#13805) + Support for using \"py\" for the argument for cwd() function (#13979) + Add a -n option to install --help to get all available help files (#13952) + Allows to install pips from sources. * Fixed for Windows + Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "Add open id dependency (#13714) * python3-openid dependency now seems to be correctly solved * Fixed bug that python3-openid dependency is not properly solved * Added a PyOpenID dependency to * New code path to ppp + new pip dependency (#13805) + Support for using \"py\" for path/filename name argument for module paths (#13979) + Add a -n option to pip install --help to get all available help. (#13952) + Added config.py to install pips from sources. * Added support for Windows + added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that the dependency is not properly solved by tools/packages. * New code path to add a pip dependency (#13805) + Fixed using \"py\" for path/filename name argument for cwd() function (#13979) + Add a -n option to pip install --help to get all available help files (#13952) + Added config.py to install pip2 sources. * Added support for Windows + added Python support. + Added a pip2 dependencies option. (#13898) + Download and install now (#13888) + Support", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path to add github dependency (#13805) + Support for using \"py\" for path/filename name argument for cwd() function (#13979) + Added a -n option to pip install --help to get all available help files (#13952) + Support for using config.py to install pips from sources. * Added support for Windows + added build option now * Added a pip2 dependencies option. (#13898) + Pip install now always gives source directory when exiting. * Support", "Add open id dependency by default * Adds python3-openid dependency: It appears that python3-openid dependency is not properly solved by tools/packages. * New code path to add a pip dependency (#13805) + Support for using \"py\" as the second argument to the name argument of a dependency function (#13979) + Add a -n option to pip install --help to get all available help files (#13952) + Added config.py to install pip from open sources. * Added support for Windows + added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install --compat fails + Support"], "perturbed_original": ["Add open id dependency (#13714) * Adds python3-openid dependency We do not have any reason to complain that python3-openid dependency is properly solved by tools like poetry (it is properly resolved by pip). The reason for it is that old version of python3-openid is installed when poetry is used and for example when initdb is run. While we do not treat poetry as an official installation mechanism this happens frequently enought and it is easy enough to fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is handled by pip). The result is that the wrong version of python3-openid is installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation , this happens frequently enought and it is easy enough to fix that we can add this dependency to make it a bit more convenient for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools .py (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors when poetry is run. While poetry does not use this dependency, it's not an official requirement. However, this happens frequently enought and it is easy enough to fix that we can add this dependency to poetry and make life easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "Add open id dependency (#13714) * Add python3-openid requirement Seems that python3-openid is missing and in some cases poetry is not properly installing dependencies for tools like poetry (it is properly resolved by pip). One possible problem is that old version of openid gets installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation mechanism this happens frequently enought and it is easy enough to fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid has a known issue that is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and new version initdb is run. While we do not use poetry as an open-id mechanism this happens frequently enought and it is easy enough to fix that we can add this to the requirements and make poetry check for that. Related to #13711 #13558 #13149 * Update setup.cfg", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it was resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. While we do not use poetry in our official project, this happens frequently enought and it is easy enough to fix that . We need to add this dependency to make it easier for poetry -users to use poetry directly. #13149 * Update setup.cfg", "Add open id and poetry dependency (#13714) * Adds package Seems that python3-openid dependency is not properly resolved by tools like poetry (it is properly resolved by Poetry). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation mechanism this happens frequently and it is easy enough to fix that we can add this dependency to make it easier for poetry users. #13737 to #13711 #13558 #13149 * Update setup.cfg", "Add s poetry dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. Although we do not use poetry as the official installation tool this happens frequently enought and it is easy enough to fix that we can add this dependency to make life easier for poetry users. #3358 #13711 #13558 #13149 * Update setup.cfg", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors were reported on the startup when poetry is run. While we do not use poetry as an official installation mechanism this happens frequently and it is easy enough to fix that we can add this to make it easier for poetry users. Related to #13711 #13558 #13149 13529 Issue: poetry setup.cfg", "s that open id does need. * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of Python is installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation mechanism this happens sometimes and it is easy enough is fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg"], "original_ll": -3.8997714519500732, "sampled_ll": -3.5295112133026123, "all_perturbed_sampled_ll": [-3.798344373703003, -3.466928720474243, -3.7971673011779785, -3.7204198837280273, -3.7367706298828125, -3.739936590194702, -3.4693808555603027, -3.7806882858276367, -3.7362866401672363, -3.499953508377075], "all_perturbed_original_ll": [-3.7361299991607666, -3.7768733501434326, -3.711394786834717, -3.9885787963867188, -3.891622543334961, -3.8578953742980957, -4.047359943389893, -3.959393262863159, -4.069563388824463, -4.276062488555908], "perturbed_sampled_ll": -3.674587678909302, "perturbed_original_ll": -3.9314873933792116, "perturbed_sampled_ll_std": 0.13090933938869534, "perturbed_original_ll_std": 0.16515972610409507}, {"original": "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "sampled": "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also changes documentation output, in particular with respect to the default configuration", "perturbed_sampled": ["Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to doc creation. This also changes documentation output, but this only happens with respect to the default configuration", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of fix: do this - fixed the bug related to `download`. This also changes documentation output, in particular with respect to the default configuration", "Doc: Fix the script for downloading sources (#18179) - Follows issue 3429. Closer Lua of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also changes documentation output, in particular with respect to the default configuration", "Doc: Use ``closer.lua`` script for closing sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This bug fixes documentation output, in particular with respect to the default configuration", "Doc: Use ``closer.lua`` script for downloading package files - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also leads to the incorrect output, in particular with respect to the default configuration", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also improved the performance of the test output, in particular with the the default configuration", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows the path of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also changes documentation output, in particular with respect to the default configuration", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also is required for output, in particular with respect to the default configuration", "Doc: Use ``closer.lua`` when downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug in `download`. This also changes documentation output, in particular with respect to the default configuration", "Doc: Use ``closer.lua`` script for downloading packages - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to the git version for version #3 (#17836) which also changes documentation output, in particular with respect to the default configuration"], "perturbed_original": ["Doc: Added script for downloading sources (#18179) - Follows first point of Open PR by recommending to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use s for downloading sources - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the dependency substitution does not work for Python3 (Bugfix PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page s/ `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug that current version substitution does not work for the Sphinx (e.g. see PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes where the current version substitution does not work for Hyperlinks (see https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use SVN mirrors - Fixes bug as Sphinx version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "- Added ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (see https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - to switch the first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - to use the source substitution, as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "Doc: Use ``closer.lua`` script for Source-Release (#18179) - Correct point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)"], "original_ll": -3.535367488861084, "sampled_ll": -3.2464778423309326, "all_perturbed_sampled_ll": [-3.339510440826416, -4.5960164070129395, -3.5795986652374268, -3.2359232902526855, -3.2443344593048096, -3.311145067214966, -3.191345691680908, -3.3449652194976807, -3.2135086059570312, -3.4728777408599854], "all_perturbed_original_ll": [-3.7165374755859375, -3.6171295642852783, -3.5092315673828125, -3.6044139862060547, -3.3654510974884033, -3.454572916030884, -3.812748432159424, -3.5043575763702393, -3.5411665439605713, -3.476590871810913], "perturbed_sampled_ll": -3.452922558784485, "perturbed_original_ll": -3.5602200031280518, "perturbed_sampled_ll_std": 0.3980032311196624, "perturbed_original_ll_std": 0.12472814308983711}, {"original": "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in CI. The image was significantly smaller, but then for local development and testing you needed both full CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs in Travis, it increased time needed by developers to have a working environment and to keep it updated every time it was needed (by minutes) Also having two separately managed images made it rather complex to join some of the Travis CI jobs (there is a follow-up change with getting rid of Checklicence image). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "sampled": "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in the main task, and hence there was no effect on other tasks. This may have been because the main task had very limited execution time, but the results from the other tasks suggest that it had a more substantial effect. In the task with a small gain, you may need to adjust the speed of a small decrease of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for providing the details of this improvement, both in terms of how it affects execution time while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also received permission from my", "perturbed_sampled": ["[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in the main task and hence there was no real difference in the results from the other tasks. This may have been because the main task has very limited execution time, but the results from the other tasks suggest that it has a better more substantial effect. In the task with a small gain, you may need to adjust the speed of a small decrease of image size to increase the speed of executing the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for sending me the details of this improvement, both in terms of how it affects execution time when in progress and how he actually applies it to the main task. The speed gain is in both cases about a 1:1 increase over drawing the object. I also received some information regarding my", "[AIRFLOW-5830] Get rid of slim line image (#6834) The slim image gave only very small gain on executing the results of the main task, and hence there was no effect on other tasks. The small gain may have been because the main task had very limited execution time, but the results of the other tasks suggest that it may have had a more substantial effect. In the task with a small gain, you may need to adjust the speed of a small decrease of image size to increase the accuracy of the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the size of the draw (#6644) I would like to thank Richard on the project for providing details of this improvement, both in terms of how it affects execution time while the object remains in place and how he actually performed the simulation. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also received permission from my", "] Get rid of slim image (#6494) The slim image gave only very small gain on executing the fastest tasks. I have only checked the results for the main task, but unfortunately there was no effect on other tasks. This may have been because the main task had longer execution time, but the results from the other tasks suggest that it had a more substantial effect. In the task with a small image, I think you may need to adjust the speed of a small decrease of image size to increase the accuracy in the results. This may help with speed ing up some of the older work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for providing excellent details of this improvement, both in terms of how it affects the process while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing using the current process. I also received permission from my", "[AIRFLOW-5830] Get rid of the image as slim as possible (#6494) The slim image gave a small gain on executing the tests in one main task, and hence there was no effect on other tasks. This may have been because the main task had a small gain in execution time, but the results from other tasks suggest that it had a more substantial effect. In the task with a small gain, you may need to adjust the speed of a certain number of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project of this implementation for reporting back to me the details of this improvement, both in terms of how it affects execution time while in progress and how he actually performed the changes. The speed gain is in most cases about a 1:1 factor improvement over drawing the object. I may reproduce this work with permission from my", "[AIRFLOW-5830] Get rid of the slim image (#6494) The slim image gave only a small gain on executing the tests in the main task, and hence there was no effect on other tasks. This may have been because the main task had very limited execution time, but the results from the other tasks suggest that it had a more substantial effect. If you find this helps in the main task with a small gain, you may need to adjust the images with a small decrease of image size to increase the effectiveness of the results. This may also help when the speed increases during work times.\n\n\n[AIRFLOW-5740] Increase speed of the draw (#6644) I would like to thank Richard on the project for providing the data to support the results of this improvement, both in terms of how he measured execution time while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement in speed of performance of the object. I also received permission from my", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave a very small gain on executing the tests in the main task, and hence there was no effect on the other tasks. This may have been because the main task had very limited execution time, but the results in the other tasks suggest that it had a more substantial effect. In using a separate task with no performance gain, you may want to adjust the speed of a small decrease of the drawing order, to increase the speed for getting the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for providing the details of this improvement, both in terms of how it affects execution time while in the main task, and also how he added the speed gain while in the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also received permission from my", "[AIRFLOW-5830] Get rid of the \"slim\" image (#6494) The slim image gave a small gain on executing the tests in the main task, and hence there was no effect on other tasks. This could have been because the results of the main task had very limited effect on tests as expected, but the results from the other tasks suggest that it had a more substantial effect. In the task with a small gain, maybe we need to adjust the speed of a small decrease of the draw speed to increase the accuracy in the results. This may help with increases during work times.\n\n\n[AIRFLOW-5740] Better speed of the draw (#6644) I would like to thank Richard on the project for providing the details of this improvement, both in terms of how he was able to measure the effect on the execution time during progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I have had permission from my", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing time while in the main task, and hence there was no effect on other tasks. This may have been because the task had very limited execution time, but the results from the testing I did suggest that it had a more robust effect. In order to achieve more accurate results with a small gain, you may need to adjust the speed of draw, and decrease of image size to increase the accuracy in the results. This may help with increasing the accuracy during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for providing the details of this improvement, both in terms of how it affects execution time while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also appreciate the results from my", "[AIRFLOW-5830] Get rid of slim ming effects. The slim ming effect caused only very small gain on executing the tests in the main task, and hence there was no effect on other tasks. This may have been because of the main task 's very limited execution time, but the results of the tests on the other tasks suggest they had a more significant effect. In the task , to reduce the very small gain, you may want to adjust the speed of a small decrease of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the drawing functions. I would like to thank Richard on the project for providing the details of this improvement, both in terms of how it was calculated, how much it improved the time while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also received permission from my", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave a very small gain on the speed tests in the main task, and hence there was no effect on other tasks. This may have been because the main task had very limited execution time, but the results from the other tasks suggest that it had a more substantial effect. In the future it may be a small adjustment to try to increase the effect on other tasks. I may need to adjust the speed of a small decrease of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase speed of the draw (#6644) I want to thank Richard on the project for providing the details of this improvement, both in terms of how it affects drawing while in progress and how he actually performed the task. The speed gain is quite significant in most cases about a 1:1 factor improvement over drawing the object. I also received permission from my"], "perturbed_original": ["[AIRFLOW-5830] Get rid of slim image (#6494) : Slim image gave only very small gain on executing the tests in Travis CI. The loss on doing Travis CI tests was significantly smaller, but then for local development and testing you needed both full CI and SLIM-CI image. This made the scripts and docker image needlessly complex - in the case of coming Production image it turned to be premature optimisation really. While it sped-up (slightly , about 10-20 seconds) some static check jobs in Travis, it increased time needed by developers to have a working environment and to keep it updated every time it was needed . Also having two separately managed images made it rather difficult to join some of the Travis CI resources together (see here - This is a follow-up to the previous issue about getting rid of Checklicence image). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of the scripts in the implementation of production image.", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in CI. The image was smaller, but then for local development and testing you needed to have Travis CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs in Travis, it was still needed by developers to have a reliable development environment and to keep it updated every time it got old (by using Checklicence version). Also having two separately managed images made it rather difficult to join some of the Travis CI jobs (there is possible to achieve this with this change with getting rid of Checklicence image). With this change both static checks and tests are executed using single image. This opens doors for further automation of the scripts and easier implementation compared to separate image.", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests and static checks. The image was significantly smaller, but then for local development and staging people needed both full CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by only a little) some of the CI jobs in Travis, it increased requirement by developers to have a working environment and to keep it updated every time version change was needed . Also having two separately managed images made it rather difficult to join some of the Travis CI jobs (there was also follow-up change with addition of Checklicence image). With latest update both static checks and tests are executed using single image. That also opens the door to further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5830] Get rid of slim image (#6494) Slim image gave only very small gain on executing the tests in CI. The image was used for the static testing, but then for unit tests and testing you needed both full CI and SLIM-CI image. This made the scripts to use this image needlessly complex - especially in the wake of coming of SLIM-CI and it turned to be premature and outdated. While it sped-up (slightly - by 10-20 minutes) the testing of static check jobs (and unit tests), it increased time needed by developers to have a working environment and to keep it updated every time it was needed (by minutes) Also having two separate package managed images made it a pain to join some of the Travis CI jobs (there is a follow-up change with getting rid of Checklicence image). With this change both static and unit tests are executed using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5830] Get rid of SSL. (#6494) The slim image gave only short-term gain on executing checks in CI. The output size was significantly smaller, but then for local development and testing you needed both SLIM-Steam and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turns out to be premature optimisation really. While it sped-up (slightly - by 10-20 %) the executing static check jobs in Travis, it increased time needed by developers to have a working environment and to keep it updated every time it changed. (by minutes) Also having two separately managed images made it rather complex to join them to the Travis CI jobs (there is a follow-up change with getting rid of Checklicence ). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only marginal gain on executing the tests in CI. The image was significantly smaller, but then for local development and testing you needed both full CI and SLIM-CI image. This made integration between test image and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs in Travis, it increased time required for the developers to have a local Jenkins environment and keep it updated every time it was needed . Also having two separately managed images made it rather complex to join some of the local CI jobs - such a scenario was almost impossible before (this is also with getting rid of Checklicence image). With this change both static and dynamic tests will now be handled using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5830] Get rid of Checklicence (#6494) The slim image gave only very small gain on executing the tests in one step. The checklicence image was significantly smaller, but then for local deployment and testing you needed both full CI work and docker image. This made the scripts and docker image needlessly complex . However in the wake of coming back to the docker image it turned to be premature optimisation : even though it sped-up (slightly - about 10 seconds) some of the CI jobs in the server, it also increased time needed by developers to have a working environment for a release and also keep it updated every time it was needed (by minutes) Also having two separately managed images made it rather complex to join some of the Travis CI jobs (there is a follow-up change with getting rid of Checklicence image). With this change both static checks and tests are in single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5830] Get rid of Checklicence image (#6494) The slim CI image was only very small gain on executing the tests in CI. The image was significantly less complex but then for both static checks and testing you needed in Travis CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be a bad optimisation really. While it sped-up (perhaps by 10-20 seconds) some of the jobs in Travis, it increased time needed for Travis CI to have a working environment and to keep it updated every time it was needed (by minutes) Also having two separately managed images made it rather complex to join some of the Travis CI jobs (there is no this change with getting rid of Checklicence image). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of Travis CI and faster access to the rest of production image.", "[AIRFLOW-5830] Get rid of smoocon. (#6494) The slim image gave only very small gain on executing the tests in CI. The file size became significantly smaller, but then for local development and testing we needed both full CI and SLIM-CI image. This made the scripts and testing so needlessly complex - especially in the early version of project. As with the coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs in Travis, it increased time needed by developers to have a working environment and to keep it running all the time it was needed (by minutes) Also having that managed , it made it rather complex to join some of the CI jobs (also as a follow-up to getting rid of Checklicence image). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the static check in CI. The image was useful but then for development and testing you needed both full CI and the SLIM-CI images. It made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs , in fact it increased time needed by developers to have a working environment and to keep it updated every time it was needed (by not having fully managed images ) and it became rather complex to join some of the Travis CI jobs (there is a follow-up change with removal of Checklicence image). With this change both static checks and container CI jobs are executed using production image as it is already supported. That also opens doors for further simplification of the scripts and easier implementation of production image."], "original_ll": -4.367262840270996, "sampled_ll": -3.17258358001709, "all_perturbed_sampled_ll": [-3.145690441131592, -3.189690351486206, -3.3481860160827637, -3.3263745307922363, -3.202122688293457, -3.16131854057312, -3.2343332767486572, -3.2368955612182617, -3.3170042037963867, -3.2534313201904297], "all_perturbed_original_ll": [-4.252213478088379, -4.322110176086426, -4.416444301605225, -4.0734992027282715, -4.373456954956055, -4.433347702026367, -4.243472099304199, -4.289297580718994, -4.405376434326172, -4.366405010223389], "perturbed_sampled_ll": -3.241504693031311, "perturbed_original_ll": -4.317562294006348, "perturbed_sampled_ll_std": 0.06659185094167154, "perturbed_original_ll_std": 0.1032236668081122}, {"original": "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "sampled": "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "perturbed_sampled": ["[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When"], "perturbed_original": ["[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)"], "original_ll": -5.481460094451904, "sampled_ll": -5.807520389556885, "all_perturbed_sampled_ll": [-5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885, -5.807520389556885], "all_perturbed_original_ll": [-5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904, -5.481460094451904], "perturbed_sampled_ll": -5.807520389556885, "perturbed_original_ll": -5.481460094451904, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are defined on DAG. But I just found out that while we were storing the task-level callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we don't display them in the Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", "sampled": "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "perturbed_sampled": ["BugFix: Dag-level Callback Requests were not a suitable solution (#13649) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline, #13712 Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline , #13735 Improved performance when parsing URL query string, #13826 Improved performance when calling in the background (#13861) Fixed an issue with JSON payload type that caused script line to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "API to allow Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow also includes /api/callback.json as a callback to notify an API user of failed calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "BugFix: Callbacks for pending Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts a rsv file as a callback to receive new calls after the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the query (#13861) Fixed an issue with HTML type that caused script outputs to end with a trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a value type when sending JSON", "BugFix: Dag-level Callback Requests were currently not accepted (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared (#13668) In https://github.com/apache/airflow/pull/2874 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the scripts used the correct type for a type parameter in JSON", "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now passes as a callback from new call request to new calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13815 Fixed -v argument to add to the end of a scriptline (#13652) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs not to include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "BugFix: Dag-level Callback Requests were not run time exception by airflow - https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback file that will be run if it starts making new calls before the callstack has cleared . https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in airflow API (#13861) Fixed an issue with JSON payload type that caused outputs to have trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "BugFix: Dag-level Callback Requests were not set correctly In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared . In https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL query string, #13399 Fixed query for URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include URL (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive the response before the page is cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13826 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always end with '+' that caused a potential hang if the server received the wrong value for a type parameter when sending JSON", "if requests from Callback Requests were not run (#13651) In a bug fix, Airflow now accepts /api/callback.json as a callback to receive new request when the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline #13702 Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL query string, preventing tuples from the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server had wrong value for a type parameter when sending JSON", "BugFix: Dag-level Callback Requests were not returning on the Server Side (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 Fixed -pv argument to add to the second parameter of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the second parameter of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON API that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value of the type parameter when sending JSON"], "perturbed_original": ["BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only callback DAG-level requests when they are not part of a DAG. I just found out that while we were storing the task-level callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized JSON was deserialized which meant that DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we don't display them on the Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks if they are defined and sends it to", "BugFix: Dag-level Callback Requests were not running. In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are defined on DAG. But I just found out that while we were storing the callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we will use the custom JSON to store these callbacks as string, as we don't display them in the Webserver and the actual number is not used anywhere in the Scheduler itself. Scheduler will now only run these if the callbacks are defined and the DAG is not running. I've attached a PR to", "BugFix: Dag-level Callback Requests were not run (#13651) In the previous PR, I attempted to only run Callback requests when they are defined on DAG. But I just found out that we were storing the task-level callbacks as String, which were stored inside Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we display them in the DAG and the actual stream is not used by the Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", ": Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests as they are defined on DAG. But I just found out that while we were storing the Callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes that. We don't really store DAG callbacks as string, as we don't display them in the Webserver and the actual contents are not used by the Scheduler itself. Scheduler checks if the callbacks are defined and sends it to", "BugFix: Dag-level callbacks were not run (#13651) In https://github.com/apache/airflow/pull/13163 - c I suggested to only run Callback requests when they are defined on DAG. But I just found out that while we were storing the task-level callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run, hence, this PR fixes it, we have to store DAG level callbacks as string, as we don't display them in the Webserver and the actual contents are not used anywhere in the Scheduler or the Webserver - it just checks if the callbacks are defined and sends it to", "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I stated that I want scheduling system to only run Callback requests when they are defined on DAG. But I found out that while we were storing JSON callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Request was in queue. This meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we don't display them on Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler simply checks if the callbacks are defined and sends it to", "BugFix: DAG Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are created on DAG. But I just found out that while we were storing the task-level callbacks as a Serialized JSON, we were not storing the DAG level callbacks and hence it default to T-level callbacks and the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callback as a T-level string, as we don't display them in the Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", "BugFix: Dag-level Callback Requests were not running. In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are defined as a DAG. But I just found out that while we were storing the callbacks as string in the Webserver, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, no need to store DAG level callbacks now as we are storing them in the Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", "BugFix: Callbacks to DAG Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are called on DAG. But I just found out that while we were already capturing the task-level callbacks as string in Serialized JSON, we were not storing DAG level callbacks and it would simply default to None when the JSON was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we don't display them in the Webserver and the actual contents are not used anywhere in the Webserver or Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are defined on DAG. But I found out that while we were storing the task-level callbacks as string in Serialized JSON, we were not storing the DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as Scheduler display them in the Tasklist view but the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks that callbacks are defined and sends it to"], "original_ll": -3.375293493270874, "sampled_ll": -2.7995567321777344, "all_perturbed_sampled_ll": [-2.9013748168945312, -2.826573610305786, -2.764089345932007, -2.8178887367248535, -2.961986780166626, -3.147076368331909, -2.8456003665924072, -2.8976681232452393, -3.0608866214752197, -2.804595947265625], "all_perturbed_original_ll": [-3.4101333618164062, -3.394670009613037, -3.360553741455078, -3.3789217472076416, -3.5105459690093994, -3.4977147579193115, -3.358435869216919, -3.307142496109009, -3.346348285675049, -3.4248404502868652], "perturbed_sampled_ll": -2.9027740716934205, "perturbed_original_ll": -3.3989306688308716, "perturbed_sampled_ll_std": 0.11548620014906222, "perturbed_original_ll_std": 0.06142816274172756}, {"original": "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed * Version suffix is only used to rename the binary packages not for the version itself * Release process description is updated with the release process * Package version is consistent - leading 0s are skipped in month and day", "sampled": "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to exist on the backports list already released by backports. This release candidate adds those changes and also updates all other packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "perturbed_sampled": ["Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some fixes have turned out to exist on the backports list already released by backports. This release candidate adds those changes and also updates all packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel s (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for this); * libdrm/drm/bluetooth: Ignore the env of glibc_pixmap.h (by Jan Hegerl) when the driver is", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate for all the packages, some changes turned out to exist on the backports list already released by backports. This release candidate adds those changes and will rebuild them with all other packages in the release so they don't need to build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Van Nieuwendijk for reporting this); * gl: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Ubbe), since the driver is", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to exist on the backports list already released by backports. This release candidate adds them to the list and corresponding, and builds all other packages in the release so they don't need backports to build. * libdrm: Fix update fault for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Fixed the issue in #1896 (thanks Jan Hegerl) when the driver is", "Prepare release candidate for backport packages (#8891) After preparing the release candidate and reviewing the packages, some changes turned out to exist on the backports list already , but no longer need backports. This release candidate fixes those changes and also updates all other packages in the release so they don't need backports . * libdrm: Fix the backport for linux 2.6 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl of JAhnemann) causing the error if the driver is", "Prepare release candidate for backport packages (#8891) After the 2020.5.19 release candidate built the packages, some failed packages turned out to exist on the backports list and should have been updated by backports. This release candidate adds those changes and also updates all other packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 and 3.2 (and hopefully other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "Prepare release candidate for backport packages (#8891) After preparing the release candidate and reviewing the packages, some new packages turned out to exist on the backports list , as were those package that had been bugged by other packages. The release candidate adds those changes and also updates all other packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and newer kernels), see #1466 (thanks to Jonathan Heinrich <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", ". Refresh the releases list in this release candidate for backport packages . While preparing this release candidate and reviewing the packages, some changes turned out to exist on the backports list already that didn't need additional backports. This release candidate adds those changes and also updates all other packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and earlier kernels), see #1466 (thanks Jan Hegerl for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate for the packages, some changes turned out to exist on the backports list already released by backports. This release candidate adds those backports. I have also tested out other packages in the list, but they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl for reporting ! * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "Prepare release candidate for packages . When preparing the 2020.5.19 release candidate for the packages, some changes to backports started to exist on the backports list already released by backports. This release candidate adds those changes and also updates all other packages mentioned in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting it). * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "Prepare release candidate for backport s list: After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to exist in the backports list already released by the project. This release candidate adds those changes and also updates all the packages that will be in the release so they don't need backports to build. * libdrm: Fix the use of the linux kernel 3.1 (and possibly later), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is"], "perturbed_original": ["Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to affecting the date. Therefore the date was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed from the release Candidate * Version changes are only used to rename the binary packages not for the packages in the binary directory * * Release process description is updated with the release process * Package s have been consistent - leading 0s are only used in month and day", "Prepare release candidate for backport ing After preparing the 2020.5.19 release candidate and backporting the first packages, some changes were found to be necessary. Therefore the date was changed to 2020.5.20 with the folowing fixes: * Version suffix was hard-coded and added for all packagesa and they were removed * Version suffix is only used to rename the binary packages not for the version itself * Release date in date is updated with the release candidate * Package version is consistent - leading 0s are skipped in month and day", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to be necessary. Therefore the date is changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags _rs are removed and added to other packagesa and they become binary * Version suffix is only used to rename the binary packages not for the version itself * Release process description is updated with the new dates * Package version is used in the release process, but leading 0s are skipped in month and day", "Prepare release candidate and review packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the following changes: * cncf.kubernetes.example_dags were hard-coded and added for all the packages before they were removed * Version _path is only used to rename the binary packages not for the version itself * Release process description is updated with the release process * Package version is consistent - only the packages are skipped in month and day", "Prepare d package release for backport packages (#8891) After preparing the 2020.5.19 package release and reviewing the code, some changes turned out to be necessary. Therefore this release was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all sample dags - for deployment they were removed * Version suffix is only used to rename the binary packages not for the version itself * Release process description is updated with the release process * Package version is consistent - leading 0s after every word in month and day", "Prepare release candidate for backport ing After preparing the 2020.5.19 release candidate and reviewing the packages, some major improvements turned out to be necessary. Therefore the release candidate was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed * Version suffix was used to refer to the version of binary packages not for the version itself * Release process description is now used for all packages included in the release process * Package version is consistent - leading 0s are skipped in month and day", "release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the code, a few changes turned out to be necessary. Therefore the date was changed to February 2020 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed * Version suffix was used to rename the binary packages instead of the version itself * Release process description is updated with the release process * Package version is consistent - leading 0s are replaced with month and day", "Prepare release candidate and backport packages (#8891) After preparing the 2020.5.19 release candidate and backporting several packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the following changes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa gs * Package names were removed * Version suffix is only used to rename the binary packages not for the version itself * Release Candidate is updated with the release process * Package version is consistent - leading 0s are skipped in month and day", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the folowing fixes: * Old names were hard-coded and added for all packagesa and they were removed * Version suffix is only used to rename older packages not for the version itself * Release process date was updated with the release process * Package titles are more consistent - leading 0s are skipped in month and day", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and backport packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added to backport packagesa and they were added to deployments as well * Version suffix is only used to rename the binary , not for the version itself * Release process description is updated with the release process * Package version name - leading 0s are skipped in package name * Define release version per day"], "original_ll": -4.2701568603515625, "sampled_ll": -3.0865206718444824, "all_perturbed_sampled_ll": [-3.3999216556549072, -3.4553537368774414, -3.4134914875030518, -3.0785889625549316, -3.1012609004974365, -3.2092161178588867, -3.196291208267212, -3.2369399070739746, -3.1018924713134766, -3.18735671043396], "all_perturbed_original_ll": [-4.35373067855835, -4.0280232429504395, -4.115736961364746, -3.991485118865967, -4.3113179206848145, -4.070126533508301, -4.365636348724365, -4.135616302490234, -4.149369716644287, -4.146175384521484], "perturbed_sampled_ll": -3.2380313158035277, "perturbed_original_ll": -4.166721820831299, "perturbed_sampled_ll_std": 0.13115915644555456, "perturbed_original_ll_std": 0.12610053205829794}, {"original": "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'.", "sampled": "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';", "perturbed_sampled": ["Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';", "Change render to render_template in plugins.rst file; Configure baseView_baseView as BaseView; Change render to render_template as BaseView object has no attribute 'render';", "Change render to render_template in plugins.rst file. - Re-set render to render_template as BaseView object has no attribute 'render';", "Change render to render_template in plugins.rst (#13560) Changing BaseView object render to render_template as BaseView object has no attribute 'render';", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as wordpress.xml has no attribute 'render';", "Change d base to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no property 'render';", "changing render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';", "Change render to render_template in baseView; Changing render to render_template as BaseView object has no attribute 'render';", "(#13479) Changing render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';"], "perturbed_original": ["Change render to render_template in plugins.rst to change render to render_template as BaseView object has no attribute 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render is not possible as BaseView object has no attribute 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render is not supported as BaseView object has no attribute 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has default 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'.", "Change render to render_template in BaseView. Changing render to render_template as BaseView object has no attribute 'render'.", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object with attribute 'render'.", "Change baseview object render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'."], "original_ll": -4.506881237030029, "sampled_ll": -4.57221794128418, "all_perturbed_sampled_ll": [-4.57221794128418, -3.923923969268799, -4.140810966491699, -4.53657865524292, -4.490584850311279, -5.080259323120117, -4.600030422210693, -4.663689613342285, -4.147941589355469, -4.410982608795166], "all_perturbed_original_ll": [-4.070849418640137, -4.761969566345215, -4.81756591796875, -4.506881237030029, -4.506881237030029, -4.814748764038086, -4.506881237030029, -4.012721061706543, -4.659583568572998, -4.4970879554748535], "perturbed_sampled_ll": -4.456701993942261, "perturbed_original_ll": -4.515516996383667, "perturbed_sampled_ll_std": 0.3088924947599618, "perturbed_original_ll_std": 0.26708867693317456}, {"original": "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "sampled": "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "perturbed_sampled": ["[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The"], "perturbed_original": ["[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)"], "original_ll": -5.850271224975586, "sampled_ll": -6.190381050109863, "all_perturbed_sampled_ll": [-6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863, -6.190381050109863], "all_perturbed_original_ll": [-5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586, -5.850271224975586], "perturbed_sampled_ll": -6.190381050109863, "perturbed_original_ll": -5.850271224975586, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure process.join is called with timeout", "sampled": "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "perturbed_sampled": ["Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix flaky tests in test_worker_as_job_test.py, where the tests fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_utils.py (issue#17378) This PR attempts to fix some issues in test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix certain flaky tests in test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix flaky test in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_local_task_job.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where some of them will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky behavior in test_worker_as_job_test.py, where the test will try to replace empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in local task files with tasks or test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some known issues in test_worker_as_job_test.py, where the test will sometimes be unable to associate the empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some crashing tests in test_worker_as_job_test.py, where tests will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where the test can fail on empty task jobs or job with empty tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)"], "perturbed_original": ["Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests by removing some comments in the tests. This could be fixed by removing assert not process.is_alive() in the tests and making sure process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure it gets called with timeout", "Fix flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and removing assert process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py - This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and by blocking the method on run time when process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests . Instead, we make sure process.join is safe so the test doesn\u2019t timeout", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This task tries to fix some flaky/quarantined tests in test_local_task_job.py by using run_to_await() and not process.is_alive() in the tests and making sure process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py . This PR attempts to fix some quarantined/flaky tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and by providing an explanation of why process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR will fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and to make sure process.join is called with timeout", "Fix quarantined/flaky tests in test_local_task_job.py . This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure process.join is always called before the max timeout"], "original_ll": -3.2500123977661133, "sampled_ll": -2.8332574367523193, "all_perturbed_sampled_ll": [-2.8593475818634033, -3.1017074584960938, -2.8827173709869385, -2.798482894897461, -2.8835086822509766, -2.887845993041992, -2.964442014694214, -3.0228447914123535, -2.970886707305908, -2.824934959411621], "all_perturbed_original_ll": [-3.6076300144195557, -3.226294755935669, -3.4358129501342773, -3.34500789642334, -3.681893825531006, -3.2930893898010254, -3.028820276260376, -3.2774267196655273, -3.348867416381836, -3.217367649078369], "perturbed_sampled_ll": -2.9196718454360964, "perturbed_original_ll": -3.346221089363098, "perturbed_sampled_ll_std": 0.08910448914119284, "perturbed_original_ll_std": 0.18119890777461398}, {"original": "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from status", "sampled": "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from statusA.txt", "perturbed_sampled": ["Fixes quarantine parsing teething issues , fixing issues where: * wrong issue id (from tests) * comment field was copied from statusA.txt", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment B for test copied from statusA.txt", "Fixes quarantine parsing teething problems * wrong issue id (from tests) * comment field was copied from statusA.txt", "Fixes quarantine parsing teething problem. * wrong issue id (from tests) * comment field was copied from statusA.txt", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field is ignoring the message from statusA.txt", "* parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from statusA.txt", "Fixes quarantine parsing teething issues : * wrong issue id (from tests) * comment field was copied from statusA.txt", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from #10) * comment field was copied from statusA.txt", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * status:sudden field was copied from statusA.txt", "Fixes quarantine parsing teething issues : * wrong issue id (from tests) * comment field was copied from statusA.txt"], "perturbed_original": ["Fixes quarantine related issues (#10145) * wrong issue id (from tests) * comment field was copied from status", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field if change is from status", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field missing (used from status", "Fixes quarantine parsing teething issues (#10145) * wrong value for a comment field (from tests) * comment field was copied from status", "Fixes quarantine parsing teething issues (#10145) * wrong issue (check tests) * comment field was copied from status", "Fixes quarantine parsing teething issues (#10145) * wrong comments from the status (from tests) * comment field was copied from status", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field missing from status", "Fixes quarantine parsing teething issues * wrong issue id (from tests) * comment field was copied from status", "Fixes quarantine parsing teething issues (#10145) * changed issue id (from tests) * comment field was copied from status", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was wrong for issues status"], "original_ll": -5.746716022491455, "sampled_ll": -5.737582683563232, "all_perturbed_sampled_ll": [-5.611326217651367, -6.081299304962158, -5.994831085205078, -5.90793514251709, -5.815640449523926, -5.651393413543701, -5.883233070373535, -5.720407485961914, -5.9061665534973145, -5.883233070373535], "all_perturbed_original_ll": [-5.641798496246338, -5.862173557281494, -5.774342060089111, -5.247129440307617, -5.813319206237793, -5.5140910148620605, -5.701254844665527, -6.0765886306762695, -5.770740985870361, -5.803152084350586], "perturbed_sampled_ll": -5.845546579360962, "perturbed_original_ll": -5.720459032058716, "perturbed_sampled_ll_std": 0.1404925861157727, "perturbed_original_ll_std": 0.20999739230810377}, {"original": "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "sampled": "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "perturbed_sampled": ["Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe"], "perturbed_original": ["Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description"], "original_ll": -5.9300127029418945, "sampled_ll": -6.424129009246826, "all_perturbed_sampled_ll": [-6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826, -6.424129009246826], "all_perturbed_original_ll": [-5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945, -5.9300127029418945], "perturbed_sampled_ll": -6.424129009246826, "perturbed_original_ll": -5.9300127029418945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "sampled": "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "perturbed_sampled": ["[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By"], "perturbed_original": ["[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)"], "original_ll": -5.813697338104248, "sampled_ll": -6.207937717437744, "all_perturbed_sampled_ll": [-6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744, -6.207937717437744], "all_perturbed_original_ll": [-5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248, -5.813697338104248], "perturbed_sampled_ll": -6.207937717437744, "perturbed_original_ll": -5.813697338104248, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we recommend everyone to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group of user when starting the container, so the only real difference is that the \"airflow\" unmodifiable files such as python code belong to different group, which has no real value. You can still use whatever group you want for mounted files and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and we also have to remember that if the user belongs to other groups in the host, it will also", "sampled": "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to include port 80", "perturbed_sampled": ["Remove AIRFLOW_GID from Docker images (#18832) We found the AIRFLOW_GID parameter was missing from Docker images for historical reasons, so AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue (#18832), so we need to fix it. #18764 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and AirFlow should be run differently depending on the container host (#18768) We also need to include port 80", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is the original problem that caused a huge security issue (#18832), so we need to fix it. fix : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all . Fix : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all . fix : Docker and docker-machine should run differently on the container host (#18768) We also need to include port 80", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a known non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on hostname. Docker host (#18768) Docker and docker-machine need to use port 80", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for safety reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue (#18832), so we need to fix that issue first (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images to contain around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images to contain around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run at the same time even when multiple programs exist on the container host (#18768) We also need to change the default host to 80", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the default version of AirFlow for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to fix some errors on ports 80", "Remove AirFlow from Docker images (#18747) This parameter was in the images for historical reasons, and now AirFlow needs to be set in CI so it isn't overwritten. This is a non-security issue (#18832), so we need to avoid fixing it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the version of the container host (#18768) We also need to include port 80", "AirFlow is missing from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't detected anymore. This is an old issue (#18832), we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all . Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all . #18746 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to include port 80", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter is added to the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't exposed. This is a non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around several thousand files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around several thousand files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 docker-os and docker-machine should run differently depending on the container . We also need to include port 80", "Remove AIRFLOW_GID from images and config images (#18747) This parameter was in the images for a few reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue , but we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #26730 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to include port 80", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was still used in images for historical reasons, and now it needs to be set to NULL so that it is never overwritten. This is a non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18764) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the container host (#18768) We have changed docker-machine host detection to include port 80"], "perturbed_original": ["Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for many reasons, and for a long time we recommend everyone to use GID=0 in order to make it possible to start the images with Arbitrary UID. Setting different group than 0 has NO effect on AIRFLOW images. You do not actually override the group of user when starting the container, so the only real difference is that the \"airflow\" unmodifiable files such as python code may have a different group, which has no real value. You can use whatever group you want for mounted files , folders and network resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and we try to remember that if the user belongs to other groups in the host, it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was enabled in Docker for images for historical reasons, however for a long time I wanted everyone to use GID=0 in order to make it easier to run the image with correct group of the user. Setting different group than 0 has NO VALUE actually. You can override the group of user when starting the same virtual machine, and the only real difference is that the \"airflow\" unmodifiable files such as python code belong to different group, which has no real value. You can still use whatever group you want for mounted un modifiable resources. Airflow Docker image will work perfectly fine when the group of the user is 0 (and we also have to remember that if the user belongs to a different group in the host, it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was removed from Docker images for historical reasons, however for a long time we recommend everyone to maintain GID=0 in order to make it possible to run the image with Arbitrary UID. Setting GID for Airflow images higher than 0 had NO REAL VALUE actually. You can still override the group of user when starting the container, so the only real difference is that the unmodifiable files such as python code belong to different group, which has no real value. You can still use whatever group you want for python code, configuration files and modifiable resources. Airflow Docker image will work perfectly fine when the main group of user is 0 (and we also have a small patch which can ensure that if the user belongs to other groups in the host, it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we recommend to use GID=0 in order to make it easier to start the image with Arbitrary user in different group . It has NO VALUE actually. You can still override the group of user when starting the container, so the only real difference is that the \"airflow\" unmodifiable files such as python code belong to different group, which has no real value. You can still use whatever group you want for mounted files and containers, the Airflow Docker image will work fine when the main group of the user is 0 (and we also recommend to remember that if the user belongs to other groups in Docker that it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we recommend everyone to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override group of user when starting an image so the only difference is that all \"airflow\" unmodifiable files such as python code belong to different group, but has no effect on the image. You can still use whatever group you want for mounted files and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and we will have to remember whether group the user belongs to other . In this case if there are any unmodifiable files on the host, it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we had to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group in the UID when starting the container, so the only real difference is that mounted unmodifiable files such as some configuration files will belong to the default group, which has no real value. You can choose whatever group you want for mounted resources, which is the only difference for modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is set to 0 (and we need to remember that if the user belongs to other groups in the host, it will also", "Remove AIRFLOW_GID from Docker file. The feature was in the images for historical reasons, however for a long time we recommend everyone to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can set the group of user when mounting with that container, so the only real difference is that the \"airflow\" unmodifiable files such as python code belong to this group, which has no real value. You can use it without limitation if you want mounted with modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is set to 0 but we also have to remember that if the user belongs to other groups in the host, it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID variable in the Docker image is historical : for a long time we recommend to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group of user when starting a image, so the only real difference is that the \"airflow\" unmodifiable files such as the image code belong to different group, which has no real value. You can still use whatever group you want for mounted files and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and of course you have to remember that if the user belongs to different group in the host, it will also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, but for a long time we recommend everyone to use it in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group of the user by starting the container, so the only real difference is the name group. If one want to \"airflow\" unmodifiable files such as .conf etc to belong to different group, the parameter has no real value. You can still use whatever group you want for mounted files or resources. Airflow Docker image will work fine when the main group of the user is 0 (and we also have to remember that if the user belongs to other group of the host, things will work also", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we recommend everyone to use GID=0 in order to make it possible to run Airflow with Arbitrary UID. Setting GID value other than 0 has NO VALUE : Airflow Docker image can automatically set the group of user when starting , so the only difference is that only unmodifiable files such as source code belong to different group, which has no real value. You can still use whatever group you want for all user-specific and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and we also recommend -1). Please remember that if the user belongs to other groups in the host, it will also"], "original_ll": -3.594271421432495, "sampled_ll": -2.159637212753296, "all_perturbed_sampled_ll": [-2.177743434906006, -2.444533348083496, -2.2441608905792236, -2.2932212352752686, -2.1861870288848877, -2.3024909496307373, -2.48173451423645, -2.354168176651001, -2.4473321437835693, -2.2438526153564453], "all_perturbed_original_ll": [-3.550506591796875, -3.4946296215057373, -3.4658942222595215, -3.5133140087127686, -3.6478419303894043, -3.517732620239258, -3.7746593952178955, -3.5297417640686035, -3.5898759365081787, -3.4930965900421143], "perturbed_sampled_ll": -2.3175424337387085, "perturbed_original_ll": -3.5577292680740356, "perturbed_sampled_ll_std": 0.1047395130974335, "perturbed_original_ll_std": 0.08787088192638805}, {"original": "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is repeated very often in discussions on Slack, so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "sampled": "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:", "perturbed_sampled": ["Clarify installation of new packages in docker-compose env (#15433) A problem with installing new packages in the Docker-compose environment is that some packages may not be installed. This behavior has been fixed in the latest build. Newer versions of docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in Docker-compose environment is that some packages may not be installed correctly. This has been fixed in the latest Docker version. Newer versions of docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer Docker version: docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This issue has been fixed in the latest Docker version. Newer Post docker-compose:", "Clarify installation of new packages in Docker (#15433) The problem with installing new packages in the Docker-compose environment is no longer a problem because some packages may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This behavior is fixed in the latest Docker version. Newer versions of docker-compose:", "Clarify installation method for some packages in docker-compose env (#15433) A problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This issue has been fixed in the latest Docker version. Newer versions of docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in Docker-compose environment is that some packages may not get installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This has been fixed in the stable version. Newer versions of docker-compose:", "Clarify installation of new packages in docker-compose env (#15433) The problem when installing new packages in the Docker-compose environment is that the packages may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:"], "perturbed_original": ["Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is repeated very often in discussions on Slack, so I decided to update this tutorial to handle the task nicely. See here: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Install of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is mentioned often in discussions on Slack, so I decided to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is discussed very often in Airflow on Slack, so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose env . The problem with installing new packages in the Docker-compose environment has very good discussions on Slack, so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in docker compose environment is repeated very often in discussions on Slack, so I would like to improve this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose environment. The problem with installing new packages in Docker-compose environment is repeated very often in discussions , so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is repeated many times in discussions on Slack, so I would like to extend this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose env is repeated very often in discussions and so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in Docker-compose (#15433) The problem with installing new packages in the Docker-compose environment is repeated very often in discussions on Slack, so I would like to use this new suggestion to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Clarify installation of new packages in docker-compose env (#15433) - The issue with installing new packages in the Docker-compose environment is repeated very often in my discussions on Slack, so I would like to update the code to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200"], "original_ll": -3.7741124629974365, "sampled_ll": -2.6552650928497314, "all_perturbed_sampled_ll": [-2.7753570079803467, -2.628683567047119, -2.820061206817627, -2.8663649559020996, -2.8570291996002197, -2.7344722747802734, -2.7057180404663086, -2.7367424964904785, -2.696769952774048, -2.723689556121826], "all_perturbed_original_ll": [-3.8515095710754395, -3.7261228561401367, -3.7185587882995605, -3.8067712783813477, -3.970508098602295, -3.777560234069824, -3.753441572189331, -3.8714559078216553, -3.8009591102600098, -3.8046321868896484], "perturbed_sampled_ll": -2.7544888257980347, "perturbed_original_ll": -3.808151960372925, "perturbed_sampled_ll_std": 0.07139133378237877, "perturbed_original_ll_std": 0.07141261141203525}, {"original": "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "sampled": "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to other users from the same device while retaining data with existing credentials - now only changes the", "perturbed_sampled": ["Update persists-credentials (#13401) Previous fix: add persist-credentials #13389 Can add persists-credentials to other users from the same device while retaining data with existing credentials - now only changes the", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to other users from the same device while retaining data with them - now only applies to the", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 and extend persists-credentials to other users of same device while retaining data with existing credentials - now only changes the", "Update persists-credentials (#13401) Previous change to add persist-credentials to data added persists-credentials to support changing user data from the same device while retaining data with existing credentials - now only changes the", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 was incorrect. Upgraded to add persists-credentials to allow multiple credentials from the same device while retaining data with existing credentials - now only changes the", "Update persists-credentials (#13401) Previous change to persist-credentials #13389 wrongly added persists-credentials to other users on the same device while retaining data with existing credentials - now only changes the", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 update persists-credentials to allow synchronization of multiple devices logged in from the same device while retaining data with existing credentials - now only changes the", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to other users from the same account. Update while retaining data with existing credentials . Update only changes the", "Update persists-credentials (#13401) Previous change to add persist-credentials on the same device added persists-credentials to other users from the same device while retaining data with existing credentials - now this is the", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to other users on the same device while retaining data with one of them - now only changes the"], "perturbed_original": ["Update persists-credentials (#13401) Previous change to add to the config wrongly added persists-credentials to python-setup rather than checkout action. Also one of the checkout actions also uses master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 Moved persists-credentials to python-setup rather than perf-setup action. Also one of the checkout actions used master rather than v2 tag.", "Update persists-credentials (#13401) Previous comment - add persist-credentials #13389 Moved persists-credentials to python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also made the checkout actions tags v2 rather than v2 tag.", "Update persists-credentials (#13401) : to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also one of the commits used master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to persist-credentials #13389 wrongly added tags for python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to package #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 . Update persists-credentials to python-setup rather than checkout action. Also one of the checkout actions was changed to master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to add persist-credentials also added checkout action python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Some of the actions used master rather than v2 tag."], "original_ll": -4.701555252075195, "sampled_ll": -4.391566276550293, "all_perturbed_sampled_ll": [-4.351640224456787, -4.2998175621032715, -4.532787322998047, -4.331782341003418, -4.2110819816589355, -4.328134536743164, -4.197380542755127, -4.587430477142334, -4.098433494567871, -4.324219703674316], "all_perturbed_original_ll": [-5.033775806427002, -4.675167560577393, -4.737166881561279, -4.648433685302734, -4.86414098739624, -5.07299280166626, -5.112887382507324, -4.491783618927002, -4.889636516571045, -4.874841690063477], "perturbed_sampled_ll": -4.326270818710327, "perturbed_original_ll": -4.840082693099975, "perturbed_sampled_ll_std": 0.13929317655715512, "perturbed_original_ll_std": 0.19137903820933416}, {"original": "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on if it should run. This changes that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open PRs from ashb/airflow, and this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this for was to find the \"parent\" commit, but there is any easier way we can do that: HEAD^1 with a fetch depth of 2 to the checkout option. So I've removed calculating that and where it is used. If we need to bring it back we should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "sampled": "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test is run in the context of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on lintingtest.py or a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "perturbed_sampled": ["Convert OpenAPI client generation tests to use selective checks (#12092) This was bundled in with the needs-api tests, but was abandoned because of the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test is run in absence of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on a module, a report is generated for the module . The results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a client generation test, and a test for the selecttest library. The test tests the client being compiled as intended, including checking that the compiler doesn't miss headers when the test is run in the context of a specified -h flag. Convert needs into generic. Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When a linting command is run on a file or a module in the module tree, the results of linting/tests and unit tests are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Convert OpenAPI client generation tests to use selective checks (#12092) This test was meant to be included with the existing needs-api test but was abandoned due to the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a selection test, a -hectored selection test , and a selecttest callback. This test is used to ensure that the client code runs as intended, including checking that the compiler doesn't emit incorrect headers when it is run within the context of a specified -h option. #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on lintingtest.py or a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Convert OpenAPI client tests to unit checks (#12092) This test was bundled in with the existing needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for integration testing. It includes a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test is run in the context of a specified -h flag. Test-suite integration with unit testing. Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is in progress. When using the linting command on lintingtest.py or a module in the module tree, the results of linting/tests and assert.equal() are shown in a report, which is useful for unit tests.\n\nAdded the test with the", "Convert selection generation tests to static checks (#12092) This test was bundled in with the existing needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for selection testing. It includes both a selection test, which checks that the client passes selection to the compiler for all compiled classes and test for the selecttest compiler. The test also tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test is run in the command line with a specified -h flag. #8109 #8109 Test-suite has returned from linting test but testing (#8105) Test-suite 's support for linting and unit-tests is back. When using the linting command on lintingtest.py or a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Convert OpenAPI client generation tests into more selective checks . This test was bundled in the existing needs-api test library, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler fails with incorrect headers when the test is run in the context of a specific client. The client must have no -h flag. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on the root of a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, to be used for unit tests.\n\nAdded the test with the", "and client generation in use . (#12092) This test was bundled in with existing needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for client selection testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test code is run in the context of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit testing #8109 Test-suite integration with linting and unit testing #8109#8109 When using the linting command on lintingtest.py , a test module in code tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Convert OpenAPI client generation tests to use selective checks (#12092) This test was used with the existing client generation test, but was abandoned due to the API's lack of testing support. The test uses the RPG library for the testing. It includes both the generation test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't change the .h headers when the test is run in the context of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on lintingtest.py or a test in the module , test results for assert.compare() and assert.equal() are collected into a report, which is used for unit testing the test with the", "Convert OpenAPI client generation tests to selective checks (#12092) This test was bundled in with the existing needs-api tests, but was abandoned due to the API's lack of unit testing support. The test uses the selecttest library for server generation tests. It includes both a selection test, and the test for the selecttest callback. This tests the API is compiled as intended, including checking that the compiler doesn't emit incorrect messages when the test is run in the context of a specified client. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command to compare a file over another file, or a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Convert OpenAPI client generation tests to use selective build. This test was bundled in with the existing needs-api tests, but was abandoned due to the lack of testing support. The test uses the openapi library for the testing. It includes both a selection test, and a test for the selecttest callback. This test focuses on the client being compiled as intended, including checking that the compiler doesn't emit any warnings when the test is run in the context of a specified -h flag. Add Test-suite integration with linting and unit testing (#8105) Integration with linting and unit testing is back. When using the linting command on lintingtest.py as the second module in the OpenAPI tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the"], "perturbed_original": ["Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on if the client is run. We should now update that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was wrong -- at least for me as I don't open PRs from ashb/airflow, but this did lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist. But what we were using this for was to find the \"parent\" commit, but there is any easier way we can do this? Add a commit object with a fetch depth of 2 to the checkout option. So currently we have removed calculating that and where exactly it's being used. If we need to bring it back we should look at the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI client generation test to use selective checks . This test was bundled in with the existing client generation test, but then performed it's _own_ checks on if it should run. This changes that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open PRs from ashb/airflow, and this would give a confusing message: > > my_branch does not exist But all we were using this for was to find the \"parent\" commit, but there is a much easier way that can do that: HEAD^1 with a fetch depth of 2 to the checkout option. This is just another tool with a different behavior, and is being removed calculating that as it is used. If we need to bring it back we should use the output from the `potiuk/get-workflow-origin` action , and it gets the correct value", "Convert OpenAPI client generation tests to use selective checks (#12092) This test is still bundled in with the existing needs-api tests, but then performed it's _own_ checks on if it should run. This changes that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open the API to ashb/airflow, and this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not \"match\". Yes. But all we were using this for was to find the \"parent\" commit, but there is any easier way we can do that: HEAD^1 with the depth set to zero option. So I've removed calculating that value -- it's time to leave it alone. If we want to bring it back we should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI client generation tests to specific checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on the client that should run. This fix that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong -- not for me as I don't open PRs from ashb/airflow, and this lead to the following confusing message: > https://github.com/ashb/airflow.git hub.io/airflow does not exist But all we used this for was to find the \"parent\" commit, but there is any easier way we can do that: HEAD^1 with a fetch depth of 2 is the checkout option. So I've removed calculating that and where it is used. If we need to bring it back we should rewrite the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI needs tests to selective checks (#12092) This had been bundled in with the existing needs-api tests, but then performed it's own testing on if it should run. This changes that . Make selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often missed, at least for me as I don't open PRs from ashb/airflow, and this gave a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this for was to find the \"parent\" commit, but there is any easier way we can fetch HEAD^1 with a fetch depth of 2 to get the tree we want given the search option. So I've removed calculating that and where it is used. If we need to bring it back we should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on if it should run. This changes that to not do this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open PRs from ashb/airflow, so this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were asking for was to find something and commit, but there is any easier way to do that: HEAD^1 with a fetch depth of 2 to the checkout option. So I've removed calculating that and where it is set to. If we want to bring it back , we can use the output from the `potiuk/get-workflow-origin` action -- that seems to be the correct value", "Convert OpenAPI client generation tests to use selective checker This test was merged with the existing needs-api tests, but then we needed to do our _own_ checks on if it should run. This changed everything to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often false, at least for me as I don't open PRs from ashb/airflow, and can lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this for was to find the \"parent\" commit, but there is any easier way we can do that with a fetch depth of 2 to the branch name? So I've removed calculating that and where it is used. I'd need to bring it back because CI_SOURCE_REPO should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI client generation tests into selective checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ check to determine if it should run. This changes that to have only selective cases run through this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open PRs -- and this lead to a confusing output: https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this for was to find a commit, but there is any easier way we can do that: HEAD^1 with a fetch depth of 2 '<unk> checkout option. So I've removed calculating that and where it was used. If we need to bring it back I'll use the branch in the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI client generation tests to use openapi (#12092) This has been bundled in with the existing needs-api tests, but still has it's _own_ checks on if it should run. This changes that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong , at least for me as I don't always get a commit from ashb/airflow, and this lead to the confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist ... what we were using this for was to find the \"parent\" commit, but there is any easier way we can do that: add a fetch of 2 to the checkout option. So I've removed calculating that and where it is used. If we need to bring it up we should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Convert OpenAPI client generation tests to selective tests. This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on if it was correct for the test it was run. This changes that to have selective_ci_checks.sh do this check. But this was often wrong -- at least for me as I had to open PRs from ashb/airflow, and this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not have a parent commit. But all I was using this for was to get the \"parent\" commit, but there is any easier way we can do that: HEAD^1 with a fetch depth of 2 to the checkout option. So I've removed calculating that and where it is used. If we need to bring the parent back -- which we should use the response from the `potiuk/get-workflow-origin` action -- that gets the correct value"], "original_ll": -4.373823642730713, "sampled_ll": -3.0044662952423096, "all_perturbed_sampled_ll": [-3.1200695037841797, -3.1233906745910645, -3.082977533340454, -2.9533700942993164, -3.2351624965667725, -3.1498658657073975, -3.1190106868743896, -3.0644071102142334, -3.0973892211914062, -3.210087299346924], "all_perturbed_original_ll": [-4.23199987411499, -4.376080513000488, -4.303319931030273, -4.431916236877441, -4.41895866394043, -4.304980754852295, -4.200557708740234, -4.502182483673096, -4.284386157989502, -4.2997212409973145], "perturbed_sampled_ll": -3.1155730485916138, "perturbed_original_ll": -4.3354103565216064, "perturbed_sampled_ll_std": 0.07404363479442562, "perturbed_original_ll_std": 0.08970652762892323}, {"original": "Remove redundant code to serialized k8s.V1Pod (#11602)", "sampled": "Remove redundant code to serialized k8s.V1Pod (#11602)The", "perturbed_sampled": ["Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Remove redundant code to serialized k8s.V1Pod (#11602)The"], "perturbed_original": ["Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Remove redundant code to serialized k8s.V1Pod (#11602)"], "original_ll": -6.118261337280273, "sampled_ll": -6.488145351409912, "all_perturbed_sampled_ll": [-6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912, -6.488145351409912], "all_perturbed_original_ll": [-6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273, -6.118261337280273], "perturbed_sampled_ll": -6.488145351409912, "perturbed_original_ll": -6.118261337280273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "sampled": "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "perturbed_sampled": ["Add New Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Add s Support for Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Read it here. HTTP Disk Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Add s Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Add Google Cloud Memorystore Memcached Operators (#10121) Submitted by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "CloudStack Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Add Google Cloud Memorystore Memcached Operators 1. Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Updated: Mar 20, 2018 \u2013 Resolved: It's", "Add Google Cloud Memorystore Memcached Configuration\" Author: Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz Lennart Status: Rejected\n\n1/12/2015 Resolved: It's"], "perturbed_original": ["Add Google Cloud Service to noreply API for Android Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore s (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Platform Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski , Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz Kiedzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <unk>tobiaz@totozz_k<unk>dzierski@gmail.com> & Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google 's Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz Wendlin (#8475) Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Martin Laj <mik-laj@users.noreply.github.com>", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Mik Laj <mik-laj@users.noreply.github.com>"], "original_ll": -3.2125680446624756, "sampled_ll": -2.809814691543579, "all_perturbed_sampled_ll": [-2.8405544757843018, -2.971801280975342, -3.1234703063964844, -2.9994561672210693, -2.7357828617095947, -2.948561429977417, -2.90169620513916, -3.202282190322876, -2.905961036682129, -4.356837272644043], "all_perturbed_original_ll": [-3.2938878536224365, -3.2992300987243652, -3.0914554595947266, -3.9788053035736084, -3.308668851852417, -4.055551528930664, -3.1161999702453613, -3.962817907333374, -3.0427451133728027, -2.9826931953430176], "perturbed_sampled_ll": -3.0986403226852417, "perturbed_original_ll": -3.4132055282592773, "perturbed_sampled_ll_std": 0.43784165824924925, "perturbed_original_ll_std": 0.3983788995850432}, {"original": "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our main \"CI\" workflow (the only one we have at the moment)", "sampled": "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our community and allow for an upcoming build to finish. This is", "perturbed_sampled": ["Cancel queued/running builds on second push to PR (#9050) This uses an action from PR 9050, to cancel any running builds for our community and allow the first push from an upcoming build to finish. This is", "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds in the community and allow for an upcoming push to finish. This is", "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for the second push to PR and also sends a message waiting for an upcoming build to finish. This is", "Cancel ed on second push to PR (#9050) This uses an action from PR to cancel any running builds for our community and allow for an upcoming build to finish. This is", "Cancel queued/running builds on GitHub Returned to PR (#9050) This uses an action from the marketplace to cancel any running builds for our community and allow for an upcoming build on GitHub. This is", "Cancel queued/running builds on second attempt. PR (#9050) This uses an action from the marketplace to cancel any running builds for our community and allow for an upcoming build to finish without cancellation. This is", "Cancel queued/running builds on second push to PR (#9050) : In an update we have updated the marketplace to cancel any running builds for our community and allow for an upcoming build to finish. This is", "cancels builds on second push to PR (#9050) This uses an image in the marketplace to cancel any running builds for our community and allow for an upcoming build to finish. This is", "Cancel queued/running builds on second build. PR (#9050) This uses an action from the marketplace to cancel any running builds for our upcoming release, to allow for an upcoming build to finish. This is", "Cancel ling running builds on second push to PR (#9050) ! This is an action from the marketplace to cancel any running builds for our community and allow for an upcoming build to finish. This is"], "perturbed_original": ["Cancel queued/running builds on second push to PR (#9050) This uses an action in the marketplace to cancel any running builds for our main \"CI\" workflow (the default one we have at the moment)", "Cancel queued/running builds on second push to PR s: The PR workflow uses an action from the marketplace to cancel any running builds for our current PR workflow (the only one we have at the moment)", "Cancel queued/running builds for second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our specific workflow (the only one we have at the moment)", "Cancel queued/running builds before push to GitHub. This uses an action from the marketplace to cancel any running builds for our main \"CI\" workflow (the only one we have at the moment)", "Cancel queued/running builds on second push to PR (#9050) This uses an action from an existing feature that was pushed to cancel queued builds for our main \"CI\" workflow (the only one we have at the moment)", "Cancel queued/running builds on second push to PR (#9050) We are awaiting an action from the marketplace to cancel queued/running builds for our main \"CI\" workflow (the only one we have at the moment)", "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running build queues we have before our main release by the marketplace (the only one we have at the moment)", "Cancel queued/running builds on second push to PR (#9050) Pull request for an action from the CI workflow to cancel any running builds for our main \"CI\" workflow (the only one we have at the moment)", "on first push to PR and terminate all existing builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds in main \"CI\" workflow (the only one we have at the moment)", "Cancel queued/running build for our second push to PR (#9050) This uses an action from the marketplace to cancel queued builds for our main \"CI\" workflow (the only one we have at the moment)"], "original_ll": -4.701043128967285, "sampled_ll": -4.690144062042236, "all_perturbed_sampled_ll": [-4.443472862243652, -4.552449703216553, -4.221487998962402, -5.2406206130981445, -4.739963531494141, -4.623875617980957, -4.775715351104736, -4.814248085021973, -4.528376579284668, -5.263106822967529], "all_perturbed_original_ll": [-4.811279773712158, -4.839412689208984, -4.7687811851501465, -4.417898654937744, -4.562174320220947, -4.315547466278076, -4.668877124786377, -4.748684883117676, -4.764293670654297, -4.447426795959473], "perturbed_sampled_ll": -4.720331716537475, "perturbed_original_ll": -4.6344376564025875, "perturbed_sampled_ll_std": 0.31263544189332504, "perturbed_original_ll_std": 0.1764121074840562}, {"original": "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your cluster on by setting zone to None or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in short form.", "sampled": "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to Dataproc (#5171) When Auto-Zone is applied, all GDT rules", "perturbed_sampled": ["[AIRFLOW-3143] Enable AutoZone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to Dataproc (#5171) When Auto-Zone is applied, all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Auto-Target (#5170) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to check rule for an extra time before applying any additional rules to Dataproc (#5171) When Auto-Zone is applied, all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to enable this. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules in Dataproc (#5171) When Auto-Zone is enabled, apply all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to Dataproc Cluster. (#5169) Set Auto-Zone to all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Turns on and off the ability to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before creating additional rules for Auto-Zone application (#5171) When Auto-Zone is applied, all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let the cluster operate and apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow s you to apply an extra time before applying any additional rules to Auto-Zone. When Auto-Zone is applied, all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was disabled by default.\n\nAllow GDT to wait 2 seconds extra time before applying any auto-zone to Dataproc (#5171) When Auto-Zone is applied, all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default. to apply an Auto-Zone rule before applying any additional rules to Dataproc (#5171) When Auto-Zone is in, GDT rules", "Support Auto-Zone mode (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to the GDT zone. Support Auto-Zone (#5171) When Auto-Zone is applied, all GDT rules", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically select an automatic zone.\n\nSupport Autotargeting (#5175) Allows you to disable Autotargeting in case this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to this zone (#5171) When Auto-Zone is applied, all GDT rules"], "perturbed_original": ["[AIRFLOW-3143] Support Auto-Zone ing (#5169) Allows you to let GCP decide what zone to put your cluster on by setting zone to None or a blank string, making the zone in the cluster auto-selectable. Per the API spec at Amazon. It means that all machineTypeUris have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to build a cluster on by setting zone field to auto zone or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all zones have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your instances on by setting zone to None or a blank string, making the parameter optional. Per the spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, it also means that all machineTypeUris have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone ing (#5169) Allows you to let GCP decide what zone to put your cluster on by setting zone to None or a blank string, making the parameter optional. Per the documentation at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone in GCP. Allows you to let GCP decide what zone to put a machine on by setting zone to None or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, make it clear that all machineTypeUris have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide which zone to put your cluster on by setting zone to None with blank string, making the parameter optional. According to the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zones to put your cluster on by setting zone to a user. Instead of the zone parameter, it displays a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in the zone parameter's text form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your cluster s into. No setting zone for your cluster or a zone, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in short form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your cluster on by setting zone to None or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this method will ensure all machineTypeUris have the same machinetype in short form.", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP automatically decide which zone to create your cluster on by setting zone to None or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in machine form."], "original_ll": -4.201833724975586, "sampled_ll": -3.4625566005706787, "all_perturbed_sampled_ll": [-3.5730862617492676, -3.562577962875366, -3.546342134475708, -3.5893471240997314, -3.548804998397827, -3.855276107788086, -3.4484639167785645, -3.788159132003784, -2.940563917160034, -3.609666585922241], "all_perturbed_original_ll": [-4.905017375946045, -4.034827709197998, -4.230057239532471, -4.289825916290283, -4.1740922927856445, -4.2482099533081055, -4.088704586029053, -4.380866050720215, -4.243705749511719, -4.183412551879883], "perturbed_sampled_ll": -3.546228814125061, "perturbed_original_ll": -4.277871942520141, "perturbed_sampled_ll_std": 0.2317988906937404, "perturbed_original_ll_std": 0.22867948463786864}, {"original": "Resurrect python openapi client generator (#19155)", "sampled": "Resurrect python openapi client generator (#19155)When", "perturbed_sampled": ["Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When", "Resurrect python openapi client generator (#19155)When"], "perturbed_original": ["Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)", "Resurrect python openapi client generator (#19155)"], "original_ll": -6.4396209716796875, "sampled_ll": -7.058577060699463, "all_perturbed_sampled_ll": [-7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463, -7.058577060699463], "all_perturbed_original_ll": [-6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875, -6.4396209716796875], "perturbed_sampled_ll": -7.058577060699463, "perturbed_original_ll": -6.4396209716796875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out at 1200px", "sampled": "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out the topmost", "perturbed_sampled": ["set max tree width to 1200px (#16067) the totalwidth of the tree will depend on the window size like before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree will depend on the window size like before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window and the tree view size as before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, only the root layer will fill out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree will depend on the window size like before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree will still depend on the window size like before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on tree view window size like before, but max out the topmost", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the height of the tree like before, but max out the topmost", "set max tree width to 1200px (#16067) and the width of the tree view will depend on the window size like before, but max out the topmost"], "perturbed_original": ["When we raise the tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on window size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the file size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the width of the tree view will depend on the window size like before, but max out at 1200px", "set screen width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the totalwidth and height of the tree view will depend on the window size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the totalwidth of the tree will depend on the window size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the totalwidth of the tree will depend on the window size like before, but max out at 1200px", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max tree width will still be 1200px", "set max tree width to 1200px (#16067) the totalwidth of tree view will depend on the window size like before, but max out at 1200px"], "original_ll": -4.2503743171691895, "sampled_ll": -4.525360107421875, "all_perturbed_sampled_ll": [-4.470757007598877, -4.470757007598877, -4.372823715209961, -4.525360107421875, -4.357687950134277, -4.470757007598877, -4.355558395385742, -4.7183613777160645, -4.243824005126953, -4.245090484619141], "all_perturbed_original_ll": [-4.312890529632568, -4.434962749481201, -4.316010475158691, -3.951809883117676, -4.267530918121338, -4.175799369812012, -4.1885199546813965, -4.1885199546813965, -3.8100345134735107, -4.463991165161133], "perturbed_sampled_ll": -4.423097705841064, "perturbed_original_ll": -4.2110069513320925, "perturbed_sampled_ll_std": 0.13403192199829533, "perturbed_original_ll_std": 0.19208620311272723}, {"original": "Fix is_terminal_support_colors functtion (#9734)", "sampled": "Fix is_terminal_support_colors functtion (#9734)In", "perturbed_sampled": ["Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In", "Fix is_terminal_support_colors functtion (#9734)In"], "perturbed_original": ["Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)", "Fix is_terminal_support_colors functtion (#9734)"], "original_ll": -5.248140335083008, "sampled_ll": -5.800577163696289, "all_perturbed_sampled_ll": [-5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289, -5.800577163696289], "all_perturbed_original_ll": [-5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008, -5.248140335083008], "perturbed_sampled_ll": -5.800577163696289, "perturbed_original_ll": -5.248140335083008, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode by first (and only installs the GPL library by an optional extra, not by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "sampled": "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the library when a license doesn't apply. This should have been an exception, and it should have been handled with a warning. We now just get an error for this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "perturbed_sampled": ["Added the nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the library when a license doesn't apply. This should have been an exception, and it should have been caught with a warning. We now just get an error for this since the exception is not caught. Fixed the issue of missing a build when `--disable-test` is non-1-argument (-0.766) The build should stop even if `--disable-test` was", "Remove vendored nvd3 and slugify libraries (#9136) We needed them because slugify is made to default to the GPL'd version of the library and this license doesn't apply. This should have been an exception, and it should have been handled with a warning. We now just get an error for this since the exception is not thrown. No problems now. Failed a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the library when a license was specified. This should have been an exception, and it should have been handled with caution. We now just get an error for this since the exception is not raised: -0.747\n\nFix a build error because there is non-1-argument (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to be the default implementation of the library , and so the license doesn't apply. This should have been an exception, and it should have been handled with a warning. We now just get an error for this since the exception is not raised: -0.747\n\nFix for build errors when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 c libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of a library when a license wasn't specified. This should have been an exception, and it should have been handled with a warning. We now just get an error : since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 and slugify libraries (#9136) We pulled them in just because slugify _used_ to default to the GPL'd version for the library when a license warning is issued. This should have been an exception, and it should have been handled without a warning. You may just get an error for this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the library when a license doesn't apply. This should have been raised and we should have been able to pass that error as a warning. We 'll get into the reason for this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of a library when a license doesn't apply. This should have been an exception, and should have been handled with a warning. We now just get an exception for this since the exception is unhandled. -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build didn't work if `--disable-test` was", "Remove vendored nvd3 and svd3 libraries (#9136) and let nvd3 go ahead and use the GPL in them because slugify _used_ to default to the GPL'd version of the library when a license doesn't apply. This should have been raised as an exception, and it should have been handled with a warning. We now just get an error for this since an exception is not raised: -0.747\n\nFix a build when `--disable-test` was not present. (-0.766) The build would fail if `--disable-test` was", "Remove vendored nvd3 gbl libraries (#9136) We pulled in them because slugify _used_ to link to the GPL'd version of the library when a license doesn't apply. This would have been an exception, and it should have been handled with proper error handling. We now just get an error around this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was"], "perturbed_original": ["Remove vendored nvd3 and slugify libraries (#9136) We pulled it because slugify _used_ to default to the GPL'd `unidecode` module, but wilk since Slugify 3.0[1] it has used text-unidecode by first (and only installs the text library by an extra, not by default) so we can now remove it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove the unidecode and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode by first (and only installs the GPL library as an optional extra, not by default) so we can no longer depend on it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is version one of dbt's . Slugify 3.0[1] [1] [1] [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove vendored nvd3 and slugify libraries (#9136) from the repo, and put dbt(9) in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode by first (and only installs the GPL library by an optional extra, by default) so we should not have to use it. This lets us upgreade text-unidecode from 1.2 to Version 1.3, which is the version one of dbt's dependencies requires. https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove vendored nvd3 and slugify libraries (#9136) which have text-unidecode in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it installed text-unidecode by first (and only installs the GPL library by an administrator, not by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of our users needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode first (and only use the GPL library by an optional extra, not by default) so we can 't rely on it. This allows us to upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of our customers needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 1.1 has used text-unidecode by first (and only installs the GPL library by an optional module by default) so we can now use it. This means we have to upgreade text-unidecode from 1.2 to 2.2.0. The initial version of Text Unidecode was 1.3, and that is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove vendored nvd3 and slugify modules. We pulled in them too. Text-unidecode _used_ to depend on the GPL'd `unidecode` module, but since Slugify 3.0[1] it automatically installs text-unidecode by first (and only installs the GPL library by an optional extra, not by default) so we can now rely on it without issues. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "installs both the nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode by first . slugify installs the GPL library by an optional path (unidecode has been included by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to the current 1.3, which is the version for dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove vendored nvd3 and slugify dependencies. We pulled in them anyway. slugify _used_ to default to the GPL'd version, but since 3.0[1] it has used the GPL version first (and only installs the GPL library by an optional extra, not by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "Remove d the unidecode and slugify libraries (#9136) We pulled in them because the package used to default to the GPL'd `unidecode` module, but since 3.0[1] it has used text-unidecode by first (and only installs the GPL library by an optional upgrade, not by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to at least 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300"], "original_ll": -3.7174630165100098, "sampled_ll": -3.3693156242370605, "all_perturbed_sampled_ll": [-3.4905073642730713, -3.5280535221099854, -3.7837657928466797, -3.4080281257629395, -3.595376491546631, -3.59187650680542, -3.6050212383270264, -3.409283399581909, -3.4202535152435303, -3.5947954654693604], "all_perturbed_original_ll": [-3.8201403617858887, -3.4755048751831055, -3.8025858402252197, -3.54402756690979, -3.5367002487182617, -3.6325294971466064, -3.5409533977508545, -3.6363422870635986, -3.682347297668457, -3.577265501022339], "perturbed_sampled_ll": -3.542696142196655, "perturbed_original_ll": -3.624839687347412, "perturbed_sampled_ll_std": 0.11125285044881637, "perturbed_original_ll_std": 0.10914892344004411}, {"original": "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "sampled": "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "perturbed_sampled": ["[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It"], "perturbed_original": ["[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)"], "original_ll": -5.482000827789307, "sampled_ll": -5.832630634307861, "all_perturbed_sampled_ll": [-5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861, -5.832630634307861], "all_perturbed_original_ll": [-5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307, -5.482000827789307], "perturbed_sampled_ll": -5.832630634307861, "perturbed_original_ll": -5.482000827789307, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to send a success indicator when providing data files into", "sampled": "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "perturbed_sampled": ["] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [AIRFLOW-4298] Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [AIRFLOW-4397]: * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable GCP implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [AIRFLOW-4397] Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4328] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Update the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [AIRFLOW-497] Add support for Google Home devices such as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #5166) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for Google Home * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for Google Play * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM]: * Add support for WiFi and Bluetooth * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug"], "perturbed_original": ["[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) : Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party which refuses to send a success indicator when providing data files into", "Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to send a session request request when providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This issue specifies a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to provide a success indicator when providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party that refuses to send a success indicator when providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case for accepting files from a third party vendor who refuses to send a success indicator when providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This consists of adding a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to send a success message after providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add on this commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who provides to send a success indicator when providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to send a success indicator when uploading/uploading data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) Description Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of data from a third party vendor who refuses to send a success indicator when providing data files into", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit adds the GoogleCloudStorageUploadSessionCompleteSensor to address the use case of an API from a third party vendor who refuses to send a success indicator when providing data files into"], "original_ll": -4.3078789710998535, "sampled_ll": -1.6591389179229736, "all_perturbed_sampled_ll": [-2.0513107776641846, -1.971055269241333, -1.7530890703201294, -2.4851205348968506, -1.6841318607330322, -1.985874891281128, -1.6045188903808594, -1.7527774572372437, -1.6540615558624268, -1.9866042137145996], "all_perturbed_original_ll": [-4.684844493865967, -4.817206382751465, -4.362624645233154, -4.282922744750977, -4.369746685028076, -4.25648832321167, -4.792760848999023, -4.161383628845215, -4.727363109588623, -4.136936664581299], "perturbed_sampled_ll": -1.8928544521331787, "perturbed_original_ll": -4.459227752685547, "perturbed_sampled_ll_std": 0.24946446594231847, "perturbed_original_ll_std": 0.25383083558645025}, {"original": "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * updated doc", "sampled": "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * fix some", "perturbed_sampled": ["[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug modal * change SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage -Message config * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log on SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage backend did not show sendMessage (#5067) * add debug log * change SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage does not work normally (#5067) * add debug log * change SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log and fix some crash * move SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug ging * change SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to SendMessage for SendMessage * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * fix some", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod as a workaround * some"], "perturbed_original": ["[AIRFLOW-4265] Lineage backend did not work correctly: * add debug log * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug logging * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * changed messages log * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not work . * add debug log * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not send message (#5067) * add debug log * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug options * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not start immediately! (#5067) * add debug log * change SendMessage to staticmethod * updated doc", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage () from <unk>lm<unk> to <unk>inline * updated doc", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage Request function * updated doc", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * update doc"], "original_ll": -5.632072448730469, "sampled_ll": -5.407741546630859, "all_perturbed_sampled_ll": [-5.583818435668945, -5.716485500335693, -5.801336765289307, -5.348577976226807, -5.208420753479004, -5.268884658813477, -5.849944591522217, -5.175970077514648, -5.407741546630859, -5.461503028869629], "all_perturbed_original_ll": [-5.565539360046387, -5.5635528564453125, -5.896310329437256, -5.760003566741943, -5.6023268699646, -5.66517972946167, -5.542239665985107, -5.562302589416504, -5.941869735717773, -5.553869724273682], "perturbed_sampled_ll": -5.482268333435059, "perturbed_original_ll": -5.665319442749023, "perturbed_sampled_ll_std": 0.2324570631664656, "perturbed_original_ll_std": 0.14186777915203486}, {"original": "Type-annotate SkipMixin and BaseXCom (#20011)", "sampled": "Type-annotate SkipMixin and BaseXCom (#20011)After", "perturbed_sampled": ["Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After", "Type-annotate SkipMixin and BaseXCom (#20011)After"], "perturbed_original": ["Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)", "Type-annotate SkipMixin and BaseXCom (#20011)"], "original_ll": -6.719922065734863, "sampled_ll": -7.235746383666992, "all_perturbed_sampled_ll": [-7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992, -7.235746383666992], "all_perturbed_original_ll": [-6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863, -6.719922065734863], "perturbed_sampled_ll": -7.235746383666992, "perturbed_original_ll": -6.719922065734863, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "sampled": "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "perturbed_sampled": ["[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]"], "perturbed_original": ["[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)"], "original_ll": -5.2277350425720215, "sampled_ll": -4.811791896820068, "all_perturbed_sampled_ll": [-4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068, -4.811791896820068], "all_perturbed_original_ll": [-5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215, -5.2277350425720215], "perturbed_sampled_ll": -4.811791896820068, "perturbed_original_ll": -5.2277350425720215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926)", "sampled": "Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926)Branch:", "perturbed_sampled": ["Docs: Suggest use of Env vars instead of Airflow Vars in best case (#16926)Branch:", "Docs: Suggest use of Env vars instead of Airflow Vars in best practice configuration (#16926)Branch:", "Re: The use of Env vars instead of Airflow Vars in best practises doc (#16926)Branch:", "Docs: Suggest using Env vars instead of Airflow Vars in best practises doc (#16926)Branch:", "Docs: Suggest use of Env Variation of Airflow Vars in best practises doc (#16926)Branch:", "Docs: Suggest use of ACFs instead of Airflow Vars in best practises doc (#16926)Branch:", "Docs: Suggest use of Env vars instead of Airflow Vars in best practice. (#16926)Branch:", "Docs: Suggest Airflow Env vars instead of Airflow Vars in best practises doc (#16926)Branch:", "Docs: Suggest use of Env vars instead of Env Vars in best practises doc (#16926)Branch:", "Docs: Suggest use of flow vars instead of Airflow Vars in best practises doc (#16926)Branch:"], "perturbed_original": ["Docs: Suggest use of Env vars instead of Airflow Vars in best practice (#16926)", "Docs: Suggest use of Env s as part of Airflow Vars in best practises doc (#16926)", "Docs: Suggest use of Env vars instead of Vars in best practises doc (#16926)", "Docs: Suggest use of Env vars instead of Airflow Vars in best practice? (#16535) (#16926)", "Docs: Suggest use of Env vars instead of Airflow Vars in this doc (#16926)", "Docs: Suggest use of eVariable instead of Airflow Vars in best practises doc (#16926)", ". Note the use of Env vars instead of Airflow Vars in best practises doc (#16926)", "Docs: Suggest use of other vars instead of Airflow Vars in best practises doc (#16926)", "Docs: Suggest use of Env vars instead of Vars in best practises doc (#16926)", "The use of Env vars instead of Airflow Vars in best practises doc (#16926)"], "original_ll": -4.6324944496154785, "sampled_ll": -4.696807384490967, "all_perturbed_sampled_ll": [-4.662246227264404, -4.588470458984375, -5.095785140991211, -4.920347213745117, -5.346606254577637, -5.244009971618652, -4.448451042175293, -4.927426338195801, -4.216948509216309, -4.978504657745361], "all_perturbed_original_ll": [-4.49379825592041, -5.250189781188965, -4.240166187286377, -4.268199443817139, -4.349377155303955, -5.39085054397583, -4.914308547973633, -5.036420822143555, -4.240166187286377, -5.221460342407227], "perturbed_sampled_ll": -4.842879581451416, "perturbed_original_ll": -4.740493726730347, "perturbed_sampled_ll_std": 0.33983332865594257, "perturbed_original_ll_std": 0.44378095856341676}, {"original": "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being built in a separate workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs or PRs with failed jobs, the workflow will add a comment to the PR stating the reason why the PR", "sampled": "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "perturbed_sampled": ["Improve running and canceliling of the PR builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete arguments to improve performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is probably due to the new functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" option, before changing to a package name with an argument (which ignored the argument based on \"--max\" or \"--min\".) (#114317)\n\nSupport the new build.target property . (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regard to both running and canceling. The tool now uses the clean() built function \"build_build\" to improve performance and memory usage, and hence the new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored arguments like \"-m\" or \"--min\".) (#114317)\n\nSupport the new property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible kernel, this does not generate an update file , only uninstall.sh .\n\nproperty", "Improve running and canceliling of PR builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe new builds are now faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions introduced in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112842 when creating packages to use the \"build_build\" command line argument (which ignored the option \"-m\" ). (#114317)\n\nSupport the user setopt property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "Improve running and canceliling of PR builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" due to performance concerns, especially the increased file usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions added in the earlier version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating builds with use the last command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not trigger update.sh , only uninstall.sh .\n\nproperty", "issues related to running and canceliling the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to radically lower the CPU and memory usage. Now builds should be faster on many architectures, notably Linux x86. (This is due to the new clean() and build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR -triggered builds are now better handled when it comes to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" from the build script, resulting in improved build performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() called in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" script option for the --module argument (which ignored argument \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh or uninstall.sh .\n\nproperty", "Improve running status of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe clean() should be faster on non-Linux x86 devices than Linux x86. (This is due to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the \"build_build\" command line argument and the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not run update.sh , only uninstall.sh .\n\nproperty", "Improve running and canceliling of PR-triggered builds. (#11268) The PR is now better handled with regards to running and cancelling the builds. (#111732) Rewrite of obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the additional functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) Introduce a new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not run install.sh , only uninstall.sh .\n\nproperty", "Improve the canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running and canceling. Added the new \"clean()\" function \"build_build\" to improve performance and memory consumption. The new builds should be faster on high-performance systems, notably Linux x86. (This is thanks to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, where creating packages for PR was not allowed when using the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "(#112479) Better handling of the running and canceliling of the PR-triggered builds. The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to save space and memory usage. (#112744)\n\nThe new builds should now run faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating a package with an incompatible target, do not run install.sh , only uninstall.sh .\n\nproperty"], "perturbed_original": ["Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow will inform the PR whose version we are concerned with - that the image is being built with a separate workflow - with an ETA link to that workflow. Thirdly - when running duplicate PRs or PRs with failed jobs, the workflow will add a comment to the PR stating the reason why the PR", "Improve running times and handling of the PR-triggered builds. (#11268) The PR builds are currently being handled with regards to both running (using merge-request) and canceling (using cancel notifications). First of all - the workflow runs are using merged commits on the PR, not the original commit from the PR. Secondly - the workflow run notifies the PR with comment stating that the image is being built in a separate workflow - including the link to cancel the workflow. Thirdly - when canceling duplicate PRs or PRs with failed jobs, the workflow will add a comment to the PR stating the reason why the PR", "Improve running and cancellation of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR builder stating that the image will be built in a separate workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs , even with failed jobs, the workflow will add a note to the PR stating the reason for cancelling the PR", "Improve running and canceliling of the PR-triggered image builds. The PR builds are now better handled with regards to both running (using workflow run notification) and canceling (with cancel notifications). First of all, the workflows are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being built in a separate workflow - including the link to that workflow. Thirdly - when running PRs or PRs with failed jobs, it will add a comment to the PR stating the version of the PR", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds be better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all - the builds should run using merged commit from the PR, not only the PR itself, but the merged commit from the PR. Secondly - PR run notifies the original PR with comment stating that the image is being built in a separate workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs that fail with failed jobs, the workflow should add a comment to the PR stating the reason for the cancelled PR", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using triggers attached) and canceling (with cancel notifications). First of all - the workflows run builds using merged commit from the PR and the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image was built in the workflow - and a link to that workflow. Thirdly - when canceling duplicate PRs or PRs with failed jobs, the workflow will add a comment to the PR stating about why the PR", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled related to both running (using merge-request) and canceling (with cancel notifications). First of all we are aware that the PR triggered is usually the \"new commit \" for the PR, not the original CI attached to the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being built in a separate workflow - including the link to the new media. Thirdly - for duplicate d PRs with failed jobs, the workflow will add a comment to the PR stating the reason why the PR", "Improve running and canceliling of the PR-triggered builds. Automated PR builds are now faster with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly , a PR triggered workflow run notifies the PR with comment stating that a new image is being built in a separate workflow - including the link to that workflow. Finally - when canceling duplicate PRs or PRs with cancel notifications, the workflow will add a comment to the PR stating the reason why the PR", "Improve running and canceliling of the PR builds. (#11268) The PR builds bug has been better handled with workflows both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being built in a new workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs - with failed jobs, the workflow will add a comment to the PR itself stating the reason why the PR", "Improve running and canceliling of the PR-triggered builds. The PR builds are now better handled with regards to both merging (via merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being created in a separate workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs or PRs with multiple merges the workflow will add a comment to the original PR explaining the reason for cancelling the PR"], "original_ll": -3.7442238330841064, "sampled_ll": -2.9868364334106445, "all_perturbed_sampled_ll": [-3.1207826137542725, -3.4107918739318848, -3.0248873233795166, -3.201023578643799, -3.1049630641937256, -3.0640435218811035, -3.1013102531433105, -3.235206365585327, -3.195553779602051, -3.0241756439208984], "all_perturbed_original_ll": [-3.8252134323120117, -3.6193346977233887, -3.7700912952423096, -3.6002745628356934, -3.746242046356201, -3.838679313659668, -4.171467304229736, -3.789154529571533, -3.921908378601074, -3.60382342338562], "perturbed_sampled_ll": -3.148273801803589, "perturbed_original_ll": -3.7886188983917237, "perturbed_sampled_ll_std": 0.11137657757175395, "perturbed_original_ll_std": 0.16389131946481586}, {"original": "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. And that is quite common case.", "sampled": "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. (Thanks to Matt J. H. for", "perturbed_sampled": ["Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets with the same name, for example execution date. Thanks to Matt J. H. for", "Add bucket_name to template fileds in S3 operators (#13973) Without this it is impossible to create buckets using for example execution date. (Thanks to Stephen H. for", "Add : Added a template to handle S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. (Thanks to Matt J. H. for", "Add bucket_name to allow operator to be used in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. Thanks to Matt J. H. for", "Add bucket_name to template for S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. (Thanks to Matt J. H. for", "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to get bucket names using for example execution date. Thanks to Matt J. H. for", "Add bucket_name to template fileds without date operators (#13973) but it's impossible to create buckets using for example execution date. (Thanks to Matt J. H. for", "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example the S3 operator (Thanks to Matt B.) for", "Add bucket_name to template fileds for bucket_name operators (#13973) Without that it's impossible to create buckets using for example execution date. (Thanks to J. H. for", "Add bucket_name to bucket in S3 operators (#13973) Without that it's impossible to create buckets using for example execution . Thanks to Matt J. H. for"], "perturbed_original": ["Add bucket_name to template fileds in WAR files file. (#13973) Without that it's impossible to create columns with for example execution date. And that is quite common case.", "Add the new template fileds in S3 operators (#13973) because it's impossible to create buckets using for example execution date. And that is quite common case.", "Add new template fileds in S3 operators (#13973) Without that it's impossible to create file templates for field fields and fields of template for example execution date. And that is quite common case.", "Add bucket_name to bucket_subject in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. And that is definitely the case.", "Add bucket_name to template fileds . - Fixed the operators (#13973) Without that it's impossible to generate dynamic data using for example execution date. And that is quite common case.", "can only be created according to template fileds in S3 operators file. That means that it's impossible to create buckets using for example execution date. And that is quite common case.", "Add bucket_name to fileds in S3 operators (#13973) Without that it's impossible to create any of buckets with for example execution date. And that is quite common case.", "Add bucket_name to template fileds in S3 operators (#13973) Without the bucket_name, it is impossible to create buckets using for example execution date. This is quite common case.", "Add bucket_name to bucket name in S3 bucket. Without that it's impossible to create buckets using for example execution date. And that is quite common case.", "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create operators that can be used for example execution s because that is quite common case."], "original_ll": -5.528934955596924, "sampled_ll": -5.260386943817139, "all_perturbed_sampled_ll": [-4.820214748382568, -5.266054630279541, -4.8690338134765625, -4.886286735534668, -5.001664161682129, -5.356970310211182, -5.466209411621094, -5.232545375823975, -5.112820625305176, -5.07651948928833], "all_perturbed_original_ll": [-5.487292766571045, -5.565374374389648, -5.204463481903076, -4.9123663902282715, -5.785182952880859, -4.802243232727051, -5.486224174499512, -4.988147735595703, -4.279541492462158, -5.5439677238464355], "perturbed_sampled_ll": -5.108831930160522, "perturbed_original_ll": -5.205480432510376, "perturbed_sampled_ll_std": 0.2078917088626436, "perturbed_original_ll_std": 0.4349139093340786}, {"original": "Refactor create_app in airflow/www/app.py (#9291)", "sampled": "Refactor create_app in airflow/www/app.py (#9291)With", "perturbed_sampled": ["Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With", "Refactor create_app in airflow/www/app.py (#9291)With"], "perturbed_original": ["Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)", "Refactor create_app in airflow/www/app.py (#9291)"], "original_ll": -5.197209358215332, "sampled_ll": -5.8196868896484375, "all_perturbed_sampled_ll": [-5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375, -5.8196868896484375], "all_perturbed_original_ll": [-5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332, -5.197209358215332], "perturbed_sampled_ll": -5.8196868896484375, "perturbed_original_ll": -5.197209358215332, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "sampled": "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "perturbed_sampled": ["Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At"], "perturbed_original": ["Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511"], "original_ll": -5.268479824066162, "sampled_ll": -5.752065181732178, "all_perturbed_sampled_ll": [-5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178, -5.752065181732178], "all_perturbed_original_ll": [-5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162, -5.268479824066162], "perturbed_sampled_ll": -5.752065181732178, "perturbed_original_ll": -5.268479824066162, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter sidecar", "sampled": "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter and", "perturbed_sampled": ["Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An error crept in and was breaking the pgbouncer exporter and", "Colon in my ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter and", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the exporter and", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the exporter and", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon was missing and was breaking the pgbouncer exporter and", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An internal fix crept in and was breaking the pgbouncer exporter and", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept into the sidecar and it was breaking the pgbouncer exporter and", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and now you have to modify the pgbouncer exporter and", "Chart: Fix ``PgBouncer`` exporter naming problems. An extra colon crept in and was breaking the pgbouncer exporter and", "- Broken <unk>Pgblaster<unk> and ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter and"], "perturbed_original": ["Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was deleted. fix pgbouncer exporter sidecar", "Chart: The exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter sidecar", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra exporter sidecar had been added in and was breaking the pgbouncer exporter sidecar", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and broke the pgbouncer exporter sidecar", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was not easily fixed. Fix pgbouncer exporter sidecar", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) : Fixed the problem where a colon crept in and was breaking the pgbouncer exporter sidecar", "Chart: Fix ``PgBouncer`` exporter sidecar. An extra colon crept in and was breaking the pgbouncer exporter sidecar", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) : A colon crept in and was breaking the pgbouncer exporter sidecar", "Chart: pgbouncer exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter sidecar", "* ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter sidecar"], "original_ll": -4.755273342132568, "sampled_ll": -5.021922588348389, "all_perturbed_sampled_ll": [-4.702942848205566, -5.4514546394348145, -5.440662860870361, -5.440662860870361, -4.7996110916137695, -4.8879218101501465, -4.667230606079102, -4.905134201049805, -5.059229373931885, -5.950283050537109], "all_perturbed_original_ll": [-4.806069374084473, -6.079376697540283, -4.090689659118652, -4.693045616149902, -4.660683631896973, -4.732335567474365, -4.837796688079834, -4.862648963928223, -4.918798446655273, -5.214682102203369], "perturbed_sampled_ll": -5.130513334274292, "perturbed_original_ll": -4.889612674713135, "perturbed_sampled_ll_std": 0.3986329981356379, "perturbed_original_ll_std": 0.4781749660638284}, {"original": "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate between task instances even if it were functional.", "sampled": "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "perturbed_sampled": ["Remove datepicker for task instance detail view (#15284) Closes #15261 by removing a timepicker from task detail view and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in the 1.12 beta where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images having a single", "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes an issue with multiple actions from a single", "Remove datepicker for task instance detail : 1. Closes #15261 . Fixes, by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "and Datetimepicker for task instance detail view . Fix #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to an update in the OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "#17044 Updates for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes the crash introduced in OSX where task items were overwritten due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "Remove datepicker for task instance detail view fix #15261 by removing the datetimepicker and replacing it with a time picker. #17026 Fixes (#15270) Fixes a crash introduced in OSX where task objects are not duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "Remove datepicker from instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items do not properly show due to the new OSX version. #17016 Fixes (#15256) Fixes a crash with multiple images showing from a single", "a datepicker for task instance detail view (#15284) Closes the dialog removing the datepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "Remove datepicker for datetimepicker detail view (#15284) Closes #15261 by removing the datetimepicker view and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in the previous build when task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "Remove datepicker for date selection in detail view in #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX Tasks View. #17064 Fixes (#15256) Fixes a crash with multiple images showing from a single"], "perturbed_original": ["Remove s datetimepicker from task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. I don't think it was an effective UX to navigate between task instances even if it were functional.", "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate between task instances even while the link and datepicker were functional.", "Remove datepicker for task instance heading. (#15284) Closes #15261 by removing the datetimepicker and replacing it with the task instance heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think the datetimepicker was an effective UX to navigate between task instances even if it were functional.", "Remove datepicker for task instance . (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to have a datepicker for task instances even if they were functional.", "Remove navigation between task instances from task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The navigation between task instances was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate between task instances even if it were functional.", "Remove datepicker for task instance detail view (#15284) - Added DateTime Detail View by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is easier to remove it rather than fix. Also, I don't think it was an effective UX to navigate through task instances even if it were functional.", "Remove datepicker for task detail view (#15284) Closes #15261 by removing datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. I don't think it was an effective UX to navigate between task instances even if it were functional.", "Remove datepicker from task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a statuspicker. Notes: The datetimepicker was broken. It is simpler to remove it rather than try and fix it. I don't think it was an effective UX to navigate between task instances even if it were functional.", "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I think it was an effective UX to navigate between task instances even if it wasn't necessarily functional.", "Remove datepicker for task instance heading (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate the task instances even if it were functional."], "original_ll": -3.6370608806610107, "sampled_ll": -3.17030668258667, "all_perturbed_sampled_ll": [-3.1179261207580566, -3.138686180114746, -3.5352609157562256, -3.57806134223938, -3.4640777111053467, -3.543769359588623, -3.1482741832733154, -3.395531177520752, -2.9887077808380127, -3.3337652683258057], "all_perturbed_original_ll": [-3.657776355743408, -3.6010966300964355, -3.2992594242095947, -3.4309451580047607, -3.683058261871338, -3.79241681098938, -3.761467695236206, -3.415318489074707, -3.754594564437866, -3.684004545211792], "perturbed_sampled_ll": -3.3244060039520265, "perturbed_original_ll": -3.607993793487549, "perturbed_sampled_ll_std": 0.2006033869557412, "perturbed_original_ll_std": 0.16024448824688067}, {"original": "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "sampled": "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "perturbed_sampled": ["[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W."], "perturbed_original": ["[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)"], "original_ll": -6.068178176879883, "sampled_ll": -6.045558452606201, "all_perturbed_sampled_ll": [-6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201, -6.045558452606201], "all_perturbed_original_ll": [-6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883, -6.068178176879883], "perturbed_sampled_ll": -6.045558452606201, "perturbed_original_ll": -6.068178176879883, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "sampled": "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "perturbed_sampled": ["[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics"], "perturbed_original": ["[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)"], "original_ll": -6.4276957511901855, "sampled_ll": -6.436801910400391, "all_perturbed_sampled_ll": [-6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391, -6.436801910400391], "all_perturbed_original_ll": [-6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855, -6.4276957511901855], "perturbed_sampled_ll": -6.436801910400391, "perturbed_original_ll": -6.4276957511901855, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "sampled": "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "perturbed_sampled": ["AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still"], "perturbed_original": ["AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)"], "original_ll": -6.724536418914795, "sampled_ll": -7.363710880279541, "all_perturbed_sampled_ll": [-7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541, -7.363710880279541], "all_perturbed_original_ll": [-6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795, -6.724536418914795], "perturbed_sampled_ll": -7.363710880279541, "perturbed_original_ll": -6.724536418914795, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "sampled": "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably want the newest", "perturbed_sampled": ["Don't try changing the python image when building on release branches (#15394) They use the same python image as #15397 (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably want the newest", "Don't try to push the python build image when building on release branches (#15394) They use the same build (different version as I already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably may not get the newest", "Don't try to push the current python image when building on release branches (#15394) They use the same python image as master (as already mentioned in previous section) instead of the latest one, resulting in an issue in the .py build script where we want the newest", "Don't try to push the python build image when building on release branches (#15394) They should use the same python image as master (as already mentioned in the post I quoted) instead of the completely latest one, this causes an issue in the .py build script where you probably want the newest", "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in the previous section) instead of the latest one, this causes an issue in the .py build script where you probably have the newest", "is a mistake to push the python build er to use the latest image for building on release branches (#15394) because it uses the same python image as master (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably want the newest", "Don't try to switch to latest python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in previous section) instead of the latest one, which can cause an issue in .py build script where you probably want the newest", "Don't try to push the latest build image when building on release branches , use the same library as master (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably want the newest", "Don't try to push the python build image when building new branches (#15394) They use the same python image as master (as described in previous section) instead of the latest one, this causes an issue in the .py thon files, a place where you probably want the newest", "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in this section) instead of pushing the latest one, this causes an issue in the build script where you probably want the newest"], "perturbed_original": ["Don't try to push the build script on a production image when building on release branches (#15394) They use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "Don't try to push the python build image of the PR on release branches (#15394) They use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building a new branch.", "Don't try to push the python image when building on release branches (#15394) They don't have the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "Don't try and push the python build image when building on release branches (#15394) They use the same python image as we've already mentioned in the comments , so we don't want to try and push the python image when we aren't building the main branch.", "Don't try to push the python build when building on release branches (#15394) They use the same python image as master (as compiled in the same way in the ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "Don't try to push the python build image when we aren't working on release branches . We don't yet use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "Don't try to push the python image.. (#15384) Don't push the python image when building on release branches (#15394) They use the same python image as master (as already mentioned in comments in ci_prepare_prod_image_on_ci.sh) so we shouldn't be trying to try and push the python image when we aren't building the main branch.", "Don't try to push the python build image when building on development branch. (#15394) They use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python build image when we aren't building the main branch.", "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in the acm ci_prepare_prod_image_on_ci.sh) so we don't want to try to push the python image when we aren't building in release branch.", "Don't try to push the python build image when building on release branches . We use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we build on Release branches from the main branch."], "original_ll": -3.4654805660247803, "sampled_ll": -3.6694979667663574, "all_perturbed_sampled_ll": [-3.71730899810791, -3.8730738162994385, -3.7397983074188232, -3.974820375442505, -3.6734936237335205, -3.993999481201172, -3.7984707355499268, -3.7793877124786377, -3.9176337718963623, -3.716036796569824], "all_perturbed_original_ll": [-3.551218032836914, -3.5416836738586426, -3.3995201587677, -3.5335452556610107, -3.5245323181152344, -3.165609359741211, -3.4051215648651123, -3.371641159057617, -3.5963308811187744, -3.3414573669433594], "perturbed_sampled_ll": -3.818402361869812, "perturbed_original_ll": -3.4430659770965577, "perturbed_sampled_ll_std": 0.10859646102334433, "perturbed_original_ll_std": 0.1246915065936448}, {"original": "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "sampled": "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "perturbed_sampled": ["Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer"], "perturbed_original": ["Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`"], "original_ll": -4.177849769592285, "sampled_ll": -4.8453145027160645, "all_perturbed_sampled_ll": [-4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645, -4.8453145027160645], "all_perturbed_original_ll": [-4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285, -4.177849769592285], "perturbed_sampled_ll": -4.8453145027160645, "perturbed_original_ll": -4.177849769592285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "sampled": "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "perturbed_sampled": ["[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved"], "perturbed_original": ["[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)"], "original_ll": -5.315189361572266, "sampled_ll": -5.54821252822876, "all_perturbed_sampled_ll": [-5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876, -5.54821252822876], "all_perturbed_original_ll": [-5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266, -5.315189361572266], "perturbed_sampled_ll": -5.54821252822876, "perturbed_original_ll": -5.315189361572266, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "sampled": "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "perturbed_sampled": ["Fix session_lifetime_minutes config . - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config docs (#12628) - Update versions to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "- Better formatting of config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "in config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` (#12634) - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config issue - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config docs (#12628) - Make changes from 1.10.1 to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config docs (#12628) - Keep `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config docs (#12628) - Update to latest doc version 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor better (#12217) - Fixed logging"], "perturbed_original": ["Fix session_lifetime_minutes , instead of 15 - (#12628) - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config docs - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better syntax using two back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it using two lines per code-block instead of italics", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better document formatting, perhaps using two back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it using <unk>css<unk> for code-block instead of italics", "Fix <unk>version_added<unk> docs (#12628) - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config .c - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it erated docs: Use back-ticks for code-block instead of italics", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it using two back-ticks for italics instead of italics"], "original_ll": -4.726297855377197, "sampled_ll": -4.182541370391846, "all_perturbed_sampled_ll": [-4.539256572723389, -4.140110969543457, -4.124470233917236, -4.3008599281311035, -4.158917427062988, -4.332816123962402, -3.8778669834136963, -4.314409255981445, -4.132164001464844, -4.214454650878906], "all_perturbed_original_ll": [-4.555390357971191, -4.709488391876221, -4.619407653808594, -4.82111120223999, -4.5895819664001465, -4.9918084144592285, -4.650650501251221, -4.906900405883789, -4.853731632232666, -4.664849758148193], "perturbed_sampled_ll": -4.213532614707947, "perturbed_original_ll": -4.736292028427124, "perturbed_sampled_ll_std": 0.16544654442020518, "perturbed_original_ll_std": 0.14016073365006915}, {"original": "Invalidate Vault cached prop when not authenticated (#17387)", "sampled": "Invalidate Vault cached prop when not authenticated (#17387)This", "perturbed_sampled": ["Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This", "Invalidate Vault cached prop when not authenticated (#17387)This"], "perturbed_original": ["Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)", "Invalidate Vault cached prop when not authenticated (#17387)"], "original_ll": -6.747575283050537, "sampled_ll": -7.304325580596924, "all_perturbed_sampled_ll": [-7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924, -7.304325580596924], "all_perturbed_original_ll": [-6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537, -6.747575283050537], "perturbed_sampled_ll": -7.304325580596924, "perturbed_original_ll": -6.747575283050537, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "sampled": "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "perturbed_sampled": ["[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For"], "perturbed_original": ["[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)"], "original_ll": -6.1086835861206055, "sampled_ll": -6.581632137298584, "all_perturbed_sampled_ll": [-6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584, -6.581632137298584], "all_perturbed_original_ll": [-6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055, -6.1086835861206055], "perturbed_sampled_ll": -6.581632137298584, "perturbed_original_ll": -6.1086835861206055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "sampled": "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "perturbed_sampled": ["[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This"], "perturbed_original": ["[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)"], "original_ll": -6.3997931480407715, "sampled_ll": -6.662501811981201, "all_perturbed_sampled_ll": [-6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201, -6.662501811981201], "all_perturbed_original_ll": [-6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715, -6.3997931480407715], "perturbed_sampled_ll": -6.662501811981201, "perturbed_original_ll": -6.3997931480407715, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the meantime, importing the modules and then referring to the classes using modulename.Classname avoids the issue.", "sampled": "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add error flag to prevent auto-adding documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some class docs for some classes", "perturbed_sampled": ["stop rendering some class docs in wrong place (#8095) stop rendering some class docs in wrong place * document strings generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add error flag to prevent auto-adding definitions into docs.yandex.org for this class * documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add a docs for some classes", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add error flag to prevent some classes in wrong place Generated for docs.yandex.org for example documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix the importing errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some validation for some classes", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and also updated in updates.* add error flag that docs generated by auto-adding documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix some config files that auto-adding docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some class docs for classes", "stop rendering some class docs in wrong place (#8095) * fix stop rendering some class docs in wrong place Docs generated for new versions have not yet been updated into the new version they will be treated as new docs and will be updated in updates.* add error flag to update docs generated by updating documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org will only validate against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some class docs for some classes", "stop rendering some class docs in wrong place * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated to documentation/ * add some code to prevent auto-adding documentation/ for docs.yandex.org . When updating, documentation/ produced with documentation/ providers.yandex.org will only be updated to documentation/ instead * fix some errors of docs/documents.yandex.org * Fix some issue with docs.yandex.org in docs/ docstring is missing * add some class docs for some classes", "add some class docs in wrong place (#8095) * add some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add some flag to prevent auto-adding documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org will be updated against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ the missing * add some class docs for some classes", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add some filters to prevent auto-adding of docstring generated for docs.yandex.org for example with documentation/ docstring will only be updated against this document * fix some errors of doc generation for example with documentation/ docs/ docstring is missing * add some class docs for some classes", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* Add a test flag to prevent auto-adding documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org Docs generated for documentation/ will be updated in updates * fix errors of docs/documents.yandex.org for example with docs.yandex.org a docstring is missing * add new docs for some classes", "stop rendering some class docs in wrong place * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add error * prevent auto-adding documentation/ for providers.yandex.org, for example with documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in one or more places documentation is missing * add some class docs for some classes", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs / providers.yandex.org have been updated into documentation/ providers.yandex.org will only be updated as new docs and will be updated against version 4 updates.* add error flag to prevent auto-adding documentation/ for docs.yandex.org in case it disagrees with documentation/ providers.yandex.org will only be updated against version 4 updates.* fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some class docs for some classes"], "perturbed_original": ["stop rendering some class docs in wrong place (#8095) : stop rendering some class docs in wrong place It would seem that for instance the autoapi module could not include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook inside the yandex module, as both classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the meantime, importing the modules and then referring to the classes using modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook . The docs needed were present in an underlying yandex module, as those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the meantime, importing the modules by referring to the classes using modulename.Classname avoids the issue.", "stop rendering some class docs in place (#8095) * Stop rendering some class docs in place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the methods it uses, but I haven't tracked down the root cause. In the meantime, importing the modules and then exporting the classes using modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for many of the classes in airflow.hooks.base_hook.BaseHook in the yandex module, as though the classes had been defined in that module. This is likely a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the meantime, importing the modules and then referring to the classes using modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, since those classes had been imported into that module. This is almost certainly a bug in autoapi or something in my understanding of the libraries it uses, but I haven't yet discovered the root cause. In the meantime, importing the modules and then referring to the classes in the yandex module avoids the issue.", "place * stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place The docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is probably a bug in autoapi or one of the libraries it uses, but I am still hunting down the root cause. In the meantime, importing the modules and linking them to the same base class modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in another module. This is almost certainly an update or in fix to one of the packages that it uses, but I haven't tracked down the root cause. In the meantime, importing the module and then referring to the classes using modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the context of each module, as if those classes had been defined in that module. This is almost certainly caused by something in the source code of one of the libraries it uses, but nobody has tracked down the root cause. In the meantime, importing the modules and then referring to the classes using modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place (#8095) to stop rendering some class docs in wrong place Docs generated by autoapi incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the end, naming the modules and then referring to the module using modulename.Classname avoids the issue.", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes are automatically defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the issue. In the meantime, importing the modules and referencing them. to the class name using modulename.Classname should fix the issue."], "original_ll": -3.5842390060424805, "sampled_ll": -2.805124044418335, "all_perturbed_sampled_ll": [-3.1411972045898438, -2.9741528034210205, -3.0106873512268066, -3.2479007244110107, -2.8547470569610596, -2.806642770767212, -3.4338488578796387, -2.9120302200317383, -2.889477014541626, -3.0037450790405273], "all_perturbed_original_ll": [-3.490382194519043, -3.6798179149627686, -3.6640100479125977, -3.705918550491333, -3.580554723739624, -3.6169557571411133, -3.6896519660949707, -3.571362018585205, -3.5109028816223145, -3.6766180992126465], "perturbed_sampled_ll": -3.0274429082870484, "perturbed_original_ll": -3.6186174154281616, "perturbed_sampled_ll_std": 0.18474871848081975, "perturbed_original_ll_std": 0.07319068378599848}, {"original": "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "sampled": "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "perturbed_sampled": ["[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I"], "perturbed_original": ["[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)"], "original_ll": -5.792147636413574, "sampled_ll": -6.281447887420654, "all_perturbed_sampled_ll": [-6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654, -6.281447887420654], "all_perturbed_original_ll": [-5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574, -5.792147636413574], "perturbed_sampled_ll": -6.281447887420654, "perturbed_original_ll": -5.792147636413574, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "sampled": "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "perturbed_sampled": ["[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I"], "perturbed_original": ["[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)"], "original_ll": -5.860006809234619, "sampled_ll": -6.2007269859313965, "all_perturbed_sampled_ll": [-6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965, -6.2007269859313965], "all_perturbed_original_ll": [-5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619, -5.860006809234619], "perturbed_sampled_ll": -6.2007269859313965, "perturbed_original_ll": -5.860006809234619, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "sampled": "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "perturbed_sampled": ["Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to"], "perturbed_original": ["Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>"], "original_ll": -4.2777862548828125, "sampled_ll": -3.9892313480377197, "all_perturbed_sampled_ll": [-3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197, -3.9892313480377197], "all_perturbed_original_ll": [-4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125, -4.2777862548828125], "perturbed_sampled_ll": -3.9892313480377197, "perturbed_original_ll": -4.2777862548828125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run something with the write credentials. We should not override the in_container scripts, however because they become part of the image, so we should use those that came with the PR. That's why we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. We've made sure that those scripts in ci are self-contained and they do not need reach outside", "sampled": "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an unexpected conflict when we build the image from the command line. Thanks @joshmcdaniel for the report :) #10368 [master] add support for custom-generated image paths for imgTags (#10919) Fixed a possible crash when building Images with the wrong set of images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building images using the -c option without directories (#10925) #10856 [master] add -O option for Image::build_with_dir and", "perturbed_sampled": ["Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the image file from the command line in Windows (from in-place to in-process!) It might help avoid an unexpected conflict when we build the image from the command line. Thanks @joshmcdaniel for the crash report :) #10860 Update #10962 add support for custom-generated image paths for imgTags (#10919) Fixed a possible crash when building Images with the wrong set of images when using empty build in -g /build option. Update #10915 fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue with missing images when building images using the -c option for directories (#10925) Update #11067 add -O option for Image::build_with_dir and", "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an unexpected crash when we build the image from the Windows terminal. Thanks @joshmcdaniel for the feedback! #10368 [master] added support for custom-generated image paths for imgTags (#10919) Fixed a possible crash when building Images with the wrong root directory for images when doing an empty build in -g (#10924) [master] fix crash by not detecting path since #10943 (#10958) #10914 Update #10901 : Fix Image::link() failing (#10927) #10860 Update #11065 : fix crash in Image::build_with_dir when building images using the -c option without directories (#10925) #10856 [master] add -O option to Image::build_with_dir and", "Do not run in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an unexpected conflict when you build the image from the command line. Thanks @joshmcdaniel for the report . [master] add support for custom-generated image paths for imgTags - fix for a possible crash when building with the wrong environment. [master] fix crash by not detecting images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 [master] fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building raw images when using the -c option without directories (#10925) #10856 [master] add support for the -O option for Image::build_with_dir and", "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from run to in-process!) It might cause an issue sometimes when we build the image from the command line. Thanks @joshmcdaniel for the report :) #10368 [master] add support for custom-generated image paths for imgTags (#10919) Fixed a possible crash when building Images with the wrong set of icons by doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#1090) #10860 [master] Update #11065 Fix issue in Image::build_with_dir when building images without -c option in -F (#10925) #10856 [master] add -O option for Image::build_with_dir and", "Do not override it when building the image Now in #10368, we've changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an unexpected change when we build the image from the command line. Thanks @joshmcdaniel of #jms, for the spec bug report :) #10368 [master] add support for custom-generated image in imgTags (#10919) Fixed a possible crash when building Images with the wrong set of images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 [master] fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building images using the command line without scripts #10856 [master] add -O option for Image::build_with_dir and", "Do not use the same target for all install scripts when building the image (#10442) After #10368, we've changed the way we build the image on Windows (from in-place to in-process!) It might help avoid an unexpected conflict when we build the image from the command line. Thanks @joshmcdaniel for the bug report :) #10368 Add support for custom-generated image paths for imgTags (#10919) Fixed #10945 crash when building Images with the wrong set of images on an empty build in -g (#10924) [master] Fix image build conflict by not detecting \"navigate_image\" in #10943 (#10958) #10914 Update #10901 fix for Image::link() failing on command line in #10945 Update #11065 Fix issue in Image::build_with_dir when building images using the -c option without directories (#10925) #10856 [master] add -O option for Image::build_with_dir and", "Do not override configuration option when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to build through configuration). It might help avoid an unexpected conflict when we build the image from the command line. Thanks to @joshmcdaniel for the report :) #10368 [master] Add and remove support for custom-generated image paths from -d (#10919) Fixed a possible crash when dealing with the wrong set of images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for building on Linux #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building images with -c option without directories (#10925) #10856 [master] add -O option to build images on Windows and", "Do not override in_container scripts when building the image. After #10916 it was reported that we have changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an ugly crash when we build the image from the command line. Thanks @joshmcdaniel for the report :) #10940 [master] add support to dynamic image paths for imgTags (#10919) Fixed a possible crash when building Images using the wrong set of source images by doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #10831 Fix issue in Image::build_with_dir when building images using -g option without directories (#10925) #10856 [master] add -O option for Image::build_with_dir and", "Do not override Windows properties when building an image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-process!) to help avoid an unexpected conflict when we build the image from the command line. Thanks for the report :) #10368 [master] add support for custom-generated image paths in image::build_with_dir (#10919) Fixed a possible crash due to building Images with the wrong set of images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building using the command line without directories (#10925) #10856 Release #10878 Add support for -O option for Image::build_with_dir and", "Do not override in_container scripts when building images (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-place). It might help avoid an unexpected conflict when we build the image with the command line. Thanks @joshmcdaniel for extending this. :) #10368 [master] add support for custom-generated image paths for imgTags (#10919) Fixed a possible crash in Images with the wrong set of paths doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for #10942 (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir of images using the -c option without directories (#10925) #10856 [master] add -O option to image.mime and"], "perturbed_original": ["Do not override in_container scripts when building CI images (#10442) After #10368, we've changed the way we build the PR from CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master , which should give roque PR a greater possibiility to run something with the write credentials. We should not override the in_container scripts, however because they are part of the image, so we should use those that came with the PR. That's why we have to make sure that we have the scripts stored within the \"in_container\" folder instead of the \"ci\" folder and only override the \"ci\" folder with the one from master. We are made sure that those scripts in ci are self-contained and they do not need reach outside", "Do not override in_container scripts when building the image (#10442) : we've changed the way we build the images on CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master. This was done to not give roque PR authors the possibiility to run something with the write credentials. We should not override the in_container scripts, however because they become independent of the image, so we should use those that came from the PR. So we have to move all the scripts out of the \"master\" folder, and only override the \"ci\" folder with the ones that were taken directly from master. We've realized that those scripts in ci are self-contained and they do not need reach outside", "Do not use external scripts when building your image . In #10368, we've changed the way we build the images on CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility of doing something with the write credentials. We should not override the in_container scripts, however because they become part of the image, so we should use those that came with the image. This is why we have kept the scripts out of the \"ci\" folder. We need now only override the \"ci\" folder with the one from master. We've made sure that those scripts in ci are self-contained , so we do not need reach outside", "Do not override in_container scripts when building the image (#10442) After #10368, we've changed how we build the images on CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run a PR without the write credentials. We should not override the other scripts however because they become part of the PR code, so we have to build from those that came with the PR. That's why we have to move the scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. We've made sure that those scripts in ci are under the proper account and they do not reach outside", "Do not override in_container scripts in the image (#10442) After #10368, we've changed the way we build the images . We are overriding the scripts that we use to build the image with the scripts taken from master to not give sbp authors the possibiility to run something with the write credentials. We should not override the in_container scripts, however because they become part of the image, so we should use those that came with the PR. In addition to this, we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. This will make sure that those scripts created in that folder are self-contained and they don't need reach outside", "We override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the PR on CI. We are overriding the ci scripts we use to build the in_container image with the scripts taken from master to not give PR authors the possibiility to run something with the write credentials. We don't really want to override the in_container scripts, however because they become part of the PRs' we should use those that came with the PR. That's why we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the scripts from master. We've made sure that those scripts inside are self-contained and they do not need reach outside", "Do not override in_container scripts when building the images. After #10368, we've changed the way we build images on CI. We are overriding the \"master\" scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run something with the write credentials. We can not override the in_container scripts, however because they become part of the image, so we should use those that came with the image. That's why we have to move the \"in_container\" scripts out of the master folder and only share the \"ci\" folder with the one from master. We've made sure that those scripts in ci are self-contained and they do not need reach outside", "Do not override in_container scripts when building the image . In #10368, we've changed the way we build the image in CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run something with the write script. We should use the in_container scripts, however because those is part of the image, so we should use those that came with the PR. That's why we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. We've realized that those scripts in ci are not part of the CI process and do not need reach outside", "Do not override in_container when building the image (#10442) After #10368, we've changed the way we build the images on CI. We are replacing all the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run something with invalid credentials. We should not override the master scripts, however , instead, the master scripts become part of the image, so we should use those that came with the image. That's why we decided to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. We've made sure the scripts in ci are self-contained scripts but do not need reach outside", "Do not override in_container scripts that become part of the image (#10442) After #10368, we've changed the way we build the images on CI. We are overriding the ci scripts that we use to build images with the scripts from master to not give roque PR authors the chance to run something with the write credentials. We should not override the in_container scripts, after they become part of the image, so we should use those that came with the PR. That's why we 've decided to move the \"in_container\" folder out of the image and only override the \"ci\" folder with the one from master. We've made sure that those scripts in ci are self-contained and they do not need reach outside"], "original_ll": -3.3929834365844727, "sampled_ll": -2.9961636066436768, "all_perturbed_sampled_ll": [-3.204349994659424, -3.0570015907287598, -2.9985275268554688, -3.1105504035949707, -3.2246623039245605, -3.183784246444702, -3.176515579223633, -3.1277143955230713, -2.942716121673584, -3.2645010948181152], "all_perturbed_original_ll": [-3.4394147396087646, -3.4487555027008057, -3.502804756164551, -3.5835750102996826, -3.3969039916992188, -3.4388139247894287, -3.4644060134887695, -3.486971139907837, -3.4999358654022217, -3.4590296745300293], "perturbed_sampled_ll": -3.129032325744629, "perturbed_original_ll": -3.4720610618591308, "perturbed_sampled_ll_std": 0.097825885714836, "perturbed_original_ll_std": 0.0479282339755135}, {"original": "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts against 200) So no need to check status code explicitly if either of these two methods are used to check the response.", "sampled": "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected with non-json queries, due to incorrect test assertions being marked as wrong on the test runner path. The test cases have also been", "perturbed_sampled": ["Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected , despite the test cases running queries, due to incorrect test cases and test methods marked as wrong on the runner path. The test cases have also been", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't return the expected response from queries, due to incorrect test assertions being marked as wrong on the test runner . Correct test cases have also been", "Remove d in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response' (https://github.com/libjunit/junit/issues/12176) still don't pass as they require non-json queries, due to incorrect test assertions being marked as wrong on the test runner path. The test cases have also been", "Unknown asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected with non-json queries, due to incorrect test assertions being passed via wrong paths and other test arguments being specified wrong on the test runner path. The test cases have also been", "Remove redundant tests from tests/www/test_views.py (#12176) Methods like 'search_content' and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected with non-json queries, due to incorrect test assertions being marked as wrong on the test cases. The test cases have also been", "Remove redundant asserts in tests/www/test_views.py . 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't work as expected with non-json queries, due to the test assertions being marked as wrong on the test runner path. The test cases have also been", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' and Methods 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still work as expected for non-json queries, due to incorrect test assertions being marked as wrong on the test runner path. The test cases have also been", "Remove redundant asserts in tests/www/test_views.py (#12176) ' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as they only test non-json queries, due to incorrect test assertions being marked as redundant on the test runner path. The test cases have also been", "wrong test asserts in tests/www/test_views.py (#12176) Methods 'check_a_string' and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected with non-json queries, due to incorrect test assertions being marked as wrong on the test path. The test cases have also been", "test asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) don't pass as expected with non-json queries, due to incorrect test assertions , marked as wrong on the test runner path. The test cases have also been"], "perturbed_original": ["Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check in response without added asserts . So no need to check status code explicitly if either of these two methods are used to check the response.", "of the status code. To avoid these asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts against 200) So no need to check status code explicitly if either of those methods are used to check the response.", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts not to do this). So no need to check the response status code explicitly if either of these two methods are used to check the response.", "Remove redundant code from tests/www/test_views.py (#12176) - these already take care of status code check (by default asserts against 200) So no need to check status code explicitly if either of these two methods are used , it would validate the response.", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by comparing 0 against 200) So no need to check status code explicitly if either of the methods checked does not care about status code if only 'check_content' methods are used to check the response.", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts against 200) So no need to check for the status code explicitly if all these methods are used to check the response.", "Remove redundant asserts against status code for status code (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts against 200) So no need to check status code , either of these two methods are used to check the response.", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already include some sort of status code check (by checking 1 against 200) So no need to check status code explicitly if either of these two methods are used to check the response.", "Remove redundant asserts if needed. (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take the status code check (by default asserts against 200) So no need to check status code explicitly if either of these two methods is used to check the response.", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code (which default asserts against 200) So , it is unnecessary to check code explicitly if either of these two methods are used to check the response."], "original_ll": -3.6087636947631836, "sampled_ll": -2.2107129096984863, "all_perturbed_sampled_ll": [-2.420772075653076, -2.2285141944885254, -2.4184350967407227, -2.4073634147644043, -3.011030435562134, -2.2225637435913086, -2.7216830253601074, -2.720156669616699, -2.9677894115448, -2.341840982437134], "all_perturbed_original_ll": [-3.6871719360351562, -3.648444175720215, -3.619816541671753, -4.483990669250488, -3.6220107078552246, -3.62052059173584, -3.7087745666503906, -3.5995893478393555, -3.8091862201690674, -3.95387601852417], "perturbed_sampled_ll": -2.546014904975891, "perturbed_original_ll": -3.775338077545166, "perturbed_sampled_ll_std": 0.2745211787639054, "perturbed_original_ll_std": 0.2580829945952524}, {"original": "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "sampled": "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler will exit with a false exit code, as is normal (although I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the last exit code is false. In any case, it looks like we", "perturbed_sampled": ["Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` is not used in Airflow, the scheduler will exit with a false exit code, as is normal (although I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the last exit code is not any older than that). It looks like we", "Pass over. Tom Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler will exit with a false exit code, as is normal (although I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the exit code is false. In any case, it looks like we", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` has been used in Airflow, the scheduler will exit with a false exit code, as is normal (although I should clarify, the last check that comes for this feature was done in Airflow , and thus the last exit code is false. In that case, it looks like we", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler process failed with a false exit code, as described below (although I should note that the last check that was done against this feature was done in Airflow 7.28, and thus the last exit codes you might get is false. In any case, it looks like we", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` isn't currently used in Airflow, the scheduler will exit with a false exit code, just like normal (although I should note that the last check that comes for this feature was in Airflow 7.28, and thus the last exit code is false. In any case, I guess we will handle it like we", "Pass SchedulerJob.subdir to Dagbag (#13291) Because the scheduler subdir is not used in Airflow, the scheduler will exit with false exit code, as described above (although I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the last valid exit code is false. In any case, it looks like we", "Pass SchedulerJob.subdir to Dagbag . If the `SchedulerJob.subdir` was not used in Airflow, the scheduler will exit with the following exit code, as is normal (although I should note that the last check that comes for this feature was in Airflow 7.28, and thus the last exit code is false. ) In either case, it looks like we", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler was terminated with a false exit code, as is normal (although please note that the last check that comes for this feature was done in Airflow, and thus the last exit code was indeed false. In any case, it looks like we", "Pass SchedulerJob.subdir to Dagbag , if `SchedulerJob.subdir` was not used previously, the scheduler job was checked with a false exit code, as is often the case. I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the last exit code is false. In any case, it looks like we", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in this case, scheduler will exit with a false exit code, as is normal (although I should note that the first time that comes for this feature was done in Airflow ). thus the last exit code is false. In any case, it should look like we"], "perturbed_original": ["Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would pass the DAGs to Airflow and use everything from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py --setjobs=0 --durations=0 Before: 9 passed, 120 deselected, 1 warning in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "Pass ed Dagbag (#13291) Because `SchedulerJob.subdir` was used in Airflow 2.0, once the job would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would run, it would serialize all the DAGs to the DB from the scheduler. root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 130 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 12 warnings in 10.56s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag (#13291) When this pattern was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from the same subdir. root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances After: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passes, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag (): Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, with 2 warnings in 22.11s After: 9 passed, with 2 warnings in 10.56s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used by Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest -gc:f test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= Before: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was used in the class, whenever SchedulerJob() would be initialized, it would serialize all the jobs that it would read from the DB from the job file. root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "Pass SchedulerJob.subdir on a single job. (#13291) While this was not used in Airflow 2.0, whenever SchedulerJob() was first initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag (#13291) Because Airflow supports Dagbags not used in schedulerjob(), whenever SchedulerJob() would be initialized, it would read the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# test_dag_file_processr -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "Pass SchedulerJob.subdir to Dagbag s if desired Since `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. As for the first: pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 10.56s After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```"], "original_ll": -3.684197187423706, "sampled_ll": -3.6977012157440186, "all_perturbed_sampled_ll": [-3.6522974967956543, -4.028467178344727, -3.890302896499634, -3.770399570465088, -3.7687554359436035, -3.920989513397217, -3.793492078781128, -3.725931406021118, -3.7626094818115234, -4.113259315490723], "all_perturbed_original_ll": [-3.741682291030884, -3.746961832046509, -3.717849016189575, -3.907336950302124, -3.7599101066589355, -3.88820481300354, -3.795880079269409, -3.599888801574707, -4.165630340576172, -3.746549367904663], "perturbed_sampled_ll": -3.8426504373550414, "perturbed_original_ll": -3.8069893598556517, "perturbed_sampled_ll_std": 0.13620797549799735, "perturbed_original_ll_std": 0.14490920124577727}, {"original": "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "sampled": "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "perturbed_sampled": ["[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The"], "perturbed_original": ["[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)"], "original_ll": -5.665452480316162, "sampled_ll": -6.117630481719971, "all_perturbed_sampled_ll": [-6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971, -6.117630481719971], "all_perturbed_original_ll": [-5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162, -5.665452480316162], "perturbed_sampled_ll": -6.117630481719971, "perturbed_original_ll": -5.665452480316162, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "sampled": "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "perturbed_sampled": ["Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2"], "perturbed_original": ["Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155"], "original_ll": -4.144833564758301, "sampled_ll": -3.8231377601623535, "all_perturbed_sampled_ll": [-3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535, -3.8231377601623535], "all_perturbed_original_ll": [-4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301, -4.144833564758301], "perturbed_sampled_ll": -3.8231377601623535, "perturbed_original_ll": -4.144833564758301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Persist tags params in pagination (#15411)", "sampled": "Persist tags params in pagination (#15411)What's", "perturbed_sampled": ["Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's", "Persist tags params in pagination (#15411)What's"], "perturbed_original": ["Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)", "Persist tags params in pagination (#15411)"], "original_ll": -6.270488262176514, "sampled_ll": -6.770099639892578, "all_perturbed_sampled_ll": [-6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578, -6.770099639892578], "all_perturbed_original_ll": [-6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514, -6.270488262176514], "perturbed_sampled_ll": -6.770099639892578, "perturbed_original_ll": -6.270488262176514, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "sampled": "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "perturbed_sampled": ["[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There"], "perturbed_original": ["[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)"], "original_ll": -5.87980318069458, "sampled_ll": -6.262747287750244, "all_perturbed_sampled_ll": [-6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244, -6.262747287750244], "all_perturbed_original_ll": [-5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458, -5.87980318069458], "perturbed_sampled_ll": -6.262747287750244, "perturbed_original_ll": -5.87980318069458, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "sampled": "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "perturbed_sampled": ["Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug"], "perturbed_original": ["Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>"], "original_ll": -4.834514617919922, "sampled_ll": -4.32852029800415, "all_perturbed_sampled_ll": [-4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415, -4.32852029800415], "all_perturbed_original_ll": [-4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922, -4.834514617919922], "perturbed_sampled_ll": -4.32852029800415, "perturbed_original_ll": -4.834514617919922, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "sampled": "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a day or more. In that case this function will hang", "perturbed_sampled": ["Increase time limit for helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a few minutes or more. In that case this function will hang", "Increase time for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a half hour or more. In that case this function will hang", "Increase time for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public RC by half a day or more. In that case this function will hang", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time to complete the job for Public Runners by a day . That is if this test cannot wait. In that case this function will hang", "Increase d Time for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a day or more. In that case this function will hang", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the time limit for the job for Public Runners by 1 day or more. In that case this function will hang", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners which can sometimes last a day or more. In that case this function will hang", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the Team or the Public Runners by a day or more. In that case this function will hang", "Increase time limit for Helm chart unit test (#20525) : the time limits for helm chart unit tests exceed the allocated time for the job for Public Runners by a day or more. In that case this function will hang", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the limit for the job for Public Runners by a day or more. In that case the function will hang"], "perturbed_original": ["Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the max time for this test for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit for Helm chart unit tests. Sometimes the duration of unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit on helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the maximum time allowed for the job for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm charts unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% <unk> 2% \u2013 1.5X% <unk> 3% <unk> 25%) This change increases the limit.", "Increase time limit for Helm chart unit tests (#20525) Currently the helm chart unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% ). This change increases the limit.", "Increase time limit for Helm chart unit . Sometimes the helm chart unit will exceed the allocated time for the job for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time limit in the job schedule of Dev Runners by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the estimated time for the job for every test by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit for Helm chart unit test. (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for each unit test by a small margin. (9X% tests successful). This change increases the limit.", "Increase time limit for Helm chart unit tests (#20525) Sometimes Helm chart unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% of time was assigned.) This change increases the limit."], "original_ll": -5.284729480743408, "sampled_ll": -4.908852577209473, "all_perturbed_sampled_ll": [-4.715176105499268, -4.89876651763916, -5.057496547698975, -5.121810436248779, -5.199095726013184, -4.743356704711914, -4.936720371246338, -4.92362117767334, -4.753301620483398, -4.744329929351807], "all_perturbed_original_ll": [-5.212357044219971, -5.253408432006836, -5.016079425811768, -4.570063591003418, -5.168361186981201, -5.378949165344238, -5.147376537322998, -5.071576118469238, -4.981330871582031, -5.077805995941162], "perturbed_sampled_ll": -4.909367513656616, "perturbed_original_ll": -5.087730836868286, "perturbed_sampled_ll_std": 0.16399519167575347, "perturbed_original_ll_std": 0.2055824571861023}, {"original": "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "sampled": "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "perturbed_sampled": ["Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file."], "perturbed_original": ["Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license"], "original_ll": -3.4906115531921387, "sampled_ll": -3.350512981414795, "all_perturbed_sampled_ll": [-3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795, -3.350512981414795], "all_perturbed_original_ll": [-3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387, -3.4906115531921387], "perturbed_sampled_ll": -3.350512981414795, "perturbed_original_ll": -3.4906115531921387, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "sampled": "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "perturbed_sampled": ["[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On"], "perturbed_original": ["[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)"], "original_ll": -6.128391742706299, "sampled_ll": -6.423715114593506, "all_perturbed_sampled_ll": [-6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506, -6.423715114593506], "all_perturbed_original_ll": [-6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299, -6.128391742706299], "perturbed_sampled_ll": -6.423715114593506, "perturbed_original_ll": -6.128391742706299, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "sampled": "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "perturbed_sampled": ["Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling"], "perturbed_original": ["Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)"], "original_ll": -4.6624040603637695, "sampled_ll": -4.973670959472656, "all_perturbed_sampled_ll": [-4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656, -4.973670959472656], "all_perturbed_original_ll": [-4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695, -4.6624040603637695], "perturbed_sampled_ll": -4.973670959472656, "perturbed_original_ll": -4.6624040603637695, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fix broken link in experimental API deprecation headers (#13547)", "sampled": "fix broken link in experimental API deprecation headers (#13547)\"", "perturbed_sampled": ["fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\"", "fix broken link in experimental API deprecation headers (#13547)\""], "perturbed_original": ["fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)", "fix broken link in experimental API deprecation headers (#13547)"], "original_ll": -5.491623878479004, "sampled_ll": -6.088904857635498, "all_perturbed_sampled_ll": [-6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498, -6.088904857635498], "all_perturbed_original_ll": [-5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004, -5.491623878479004], "perturbed_sampled_ll": -6.088904857635498, "perturbed_original_ll": -5.491623878479004, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error", "sampled": "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "perturbed_sampled": ["add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GoogleSheet (#9067) * add separate example dags and system tests for Google", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag s and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "add separate example dags and system test for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for Google (#9066) * add separate example dags and system tests for Google", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system tests for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "* add separate example dags and system tests for Google (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dag and system tests for Google", "add separate example dag and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * add separate example dags and system tests for Google", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleObject * add separate example dags and system tests for Google", "* add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system tests for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google"], "perturbed_original": ["add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system tests for GCSToGoogleSheetsOperator * remove one of the missing example dags * fix doc error", "add separate example dags and system tests for GCSToSheetsOperator * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dags and system test for GCSToGoogleSheetsOperator * fix doc error * add system test from missing example dags * fix doc error", "add separate example dags and system tests for both classes (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * include test for code error", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * remove two missing example dags * fix doc error", "s add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag s and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error", "add separate example dags and system tests for GCSToGoogleSheetsOperator * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing header * fix doc error", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for Operation * remove gcs_to_sheets from missing example dags * fix doc error", "add separate example dags and system tests for this component * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from the examples and test dags * fix doc error"], "original_ll": -4.384793758392334, "sampled_ll": -3.5069780349731445, "all_perturbed_sampled_ll": [-4.043740272521973, -3.5757012367248535, -4.359062671661377, -3.4266645908355713, -3.9105184078216553, -3.6311352252960205, -3.9675731658935547, -4.352410316467285, -3.4045536518096924, -3.4266645908355713], "all_perturbed_original_ll": [-4.273463249206543, -4.614798545837402, -4.019099712371826, -5.026857852935791, -4.401907920837402, -4.4684062004089355, -4.533666133880615, -5.220561504364014, -5.070895195007324, -4.29698371887207], "perturbed_sampled_ll": -3.8098024129867554, "perturbed_original_ll": -4.592664003372192, "perturbed_sampled_ll_std": 0.35077250142174776, "perturbed_original_ll_std": 0.3722331477195575}, {"original": "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this method is inherently insecure in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Added explanation about it and explicit warning against this.", "sampled": "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works for us, but it will make the output of docker compose less human readable. We can find an easy", "perturbed_sampled": ["Adds warning about using docker compose/install of packages (#16935) While we are supporting installing packages on our helm through docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works for building only, it will make the output of docker compose less human readable. We can find an easy", "Adds warning about using dynamic installation of packages (#16935) While it supports supporting installing packages dynamically in our helm chart by the docker compose while testing, this has a few issues . We can add some dependencies: The current docker compose works for us, but it will make the output of docker compose much less human readable. We can find an easy", "Adds warning about dynamic installation of packages (#16935) : Since we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a lot of value since we can add warnings when we are testing. The current docker compose works for us, but it will make the output of docker compose less human readable. We can find an easy", "Adds warning about using docker compose to update the content of packages list. If we are supporting installing packages dynamically in our containers with docker run and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works for us, but it makes the output of docker run and the output of docker compose less human readable. We can find an easy", "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages in both our helm chart and docker compose while testing, this has a few limitations. First, we can add some dependencies: auto installing packages using docker compose docker compose works for us, but it will make the output from compose less human readable. We can find an easy", "Adds warning about the installation of a dependency: While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compile output is quite intuitive for us, but it still makes the output of docker compose less human readable. We can find an easy", "Adds warning about the installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while compiling, it has a few issues since we can add some dependencies: using docker compose works for us, but it will make the output of docker compose less human readable. We are implementing an easy", "Adds warning about dynamic installation of packages in docker compose. While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a better design since we can add some dependencies: The current docker compose works for us, but it will make the output of docker compose too readable. We can find an easy", "Adds warning about using dynamic installation of containers While we are installing packages dynamically in our helm chart and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works for us, but it will make the helm chart and docker compose very human readable. We can find an easy", "Adds warning about using dynamic ally installed packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works with dynamically installed packages, but it will make the output of docker compose less consistent and user friendly: We can make it easy"], "perturbed_original": ["Adds warning about using dynamic ally installed packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this is inherently insecure in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment back to production) and we need more explanation s and explicit warning against this.", "Adds warning against dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this is inherently dangerous when used in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). More about it and explicit warning against this.", "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this method is inherently insecure in production environments (it opens up one potential attack where removing dependency of a package could potentially bring the Airflow deployment down). Added explanation about this danger and suggested mitigations against this.", "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this method is inherently insecure in production environments (it allows for an attack where removing dependency of a dependency migh break Airflow ). Added explanation for and explicit warning against this.", "Add security warning about using dynamic installation of packages (#16935) While we are building Airflow packages dynamically in our helm chart and docker compose while testing, this method is inherently insecure for production environments (it opens up for an attack where removing dependency of those packages to the Airflow deployment migh bring the Airflow deployment down). Added explanation about it and explicit warning against this.", "Adds warning against dynamic installation of packages (#16935) While we are supporting the installation of packages dynamically in our helm y docker compose while testing, this method is inherently insecure in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Added support for it and explicit warning against this.", "Adds a warning against using dynamic installation of packages. While we are supporting installing packages dynamically in our helm chart and docker compose environments, this method is inherently insecure in Airflow environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Added explanation about it and explicit warning against this.", "Adds warning about using dynamic installation of packages (#16935) While we are supporting packages installed dynamically in our apt and docker compose while testing, the use of dynamic dependency installation is inherently insecure in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Added explanation about it and added security measures against this.", "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our packages and docker compose while testing, this method is inherently insecure in production environments (it opens up for an attack with this attack -- which depending directly on the dependency of a dependency migh bring the Airflow into dependencies). Added explanation s and explicit warning against this.", "Adds warning about using dynamic installation of packages (#16935) While we are supporting packages that install dynamically in our helm chart and docker compose while testing, this method is not recommended in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Adds explanation about it and explicit notification about this."], "original_ll": -4.928926944732666, "sampled_ll": -4.152543067932129, "all_perturbed_sampled_ll": [-4.24129581451416, -4.348077297210693, -4.019106388092041, -3.624878406524658, -4.200350761413574, -4.3457441329956055, -4.27222204208374, -4.183228492736816, -4.259974479675293, -4.322673797607422], "all_perturbed_original_ll": [-5.110772132873535, -4.868830680847168, -4.537708759307861, -5.033511161804199, -4.792523384094238, -4.899231910705566, -4.687045574188232, -4.565215110778809, -5.065360069274902, -4.901669979095459], "perturbed_sampled_ll": -4.1817551612854, "perturbed_original_ll": -4.846186876296997, "perturbed_sampled_ll_std": 0.20716549325254469, "perturbed_original_ll_std": 0.19022279906112283}, {"original": "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "sampled": "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "perturbed_sampled": ["[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In"], "perturbed_original": ["[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)"], "original_ll": -5.7572245597839355, "sampled_ll": -6.185062885284424, "all_perturbed_sampled_ll": [-6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424, -6.185062885284424], "all_perturbed_original_ll": [-5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355, -5.7572245597839355], "perturbed_sampled_ll": -6.185062885284424, "perturbed_original_ll": -5.7572245597839355, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "sampled": "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the", "perturbed_sampled": ["Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the XML filenames and put them in the example folder structure instead of the later , and it gave me a lot of trouble fixing the", "Doc: remove filename references (#20277) Minor : I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the", "Doc: Fix incorrect filename references with typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me a little trouble fixing the", "Doc: Fix incorrect filename references (#20277) Minor typo : I changed the original references in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the", "Doc: Fix incorrect filename references (#20277) Minor correction, I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of problems over the", "Doc: Fix incorrect filename in examples. Minor typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the previous structure instead of the later references, because it gave me a lot more freedom when I was fixing the", "Doc: Fix es typos in later references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me plenty of trouble fixing the", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I actually used the filenames in the example folder structure instead of the later references, because it gave me a lot of extra confusion in the", ": Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure , the later references, because it gave me a lot of trouble fixing the"], "perturbed_original": ["Doc: Fix incorrect naming (#20277) Minor typo corrections. I changed the examples to use the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "Doc: Fix typo in references (#20277) Minor typo corrections. I changed the references in this bug to the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "Doc: Fix incorrect filename references (#20277) Minor typo in the example, I changed the filenames in the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure in the later references to be consistent with the other examples in the documentation.", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references to be more consistent with the other examples in the documentation.", ": incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of incorrect filename references to be consistent with the other examples in the documentation.", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the manual structure instead of the later references to be consistent with the other format in the documentation.", "Fixer of incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the first reference to refer to the folder structure instead of the later references to be consistent with the other examples in the documentation.", "Doc: Fix incorrect filename references (#20277) and add corrections. I changed the initial references in the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the name of the example file to use the correct one instead of the later references to be consistent with the other examples in the documentation."], "original_ll": -3.8832805156707764, "sampled_ll": -4.034200191497803, "all_perturbed_sampled_ll": [-4.233970642089844, -4.324166297912598, -4.074613094329834, -4.3245720863342285, -4.150578022003174, -3.8368818759918213, -4.046411037445068, -4.252027988433838, -3.9531352519989014, -4.605302333831787], "all_perturbed_original_ll": [-3.9775307178497314, -3.8199775218963623, -3.9156486988067627, -3.829148054122925, -3.823951244354248, -4.037293434143066, -4.164641380310059, -3.842864990234375, -4.006811618804932, -3.613788366317749], "perturbed_sampled_ll": -4.180165863037109, "perturbed_original_ll": -3.903165602684021, "perturbed_sampled_ll_std": 0.20728805934846795, "perturbed_original_ll_std": 0.14451209976735918}, {"original": "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "sampled": "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "perturbed_sampled": ["Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]"], "perturbed_original": ["Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)"], "original_ll": -4.893798351287842, "sampled_ll": -4.80837345123291, "all_perturbed_sampled_ll": [-4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291, -4.80837345123291], "all_perturbed_original_ll": [-4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842, -4.893798351287842], "perturbed_sampled_ll": -4.80837345123291, "perturbed_original_ll": -4.893798351287842, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectifies the mistake.", "sampled": "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectified a couple", "perturbed_sampled": ["Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add an issue form :) This was the first couple", "Add issue form template to Chat and Reply Chat (#17917) With so many people reviewing nobody noticed that we forgot to add the issue form :) This rectified a couple", "The form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart support :) This rectified a couple", "Add issue form template for Helm Chat (#17917) With so many people reviewing the charts, we realized that we forgot to add Helm Chart issue form :) This only takes a couple", "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to link to Helm Chart issue form template. This rectified a couple", "Add a new template for Helm chart issue form (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectified a couple", "Add issue form for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm chat as a subject in our issue form :) This rectified a couple", "Add a template for Helm Chat (#17917) With so many people reviewing nobody elses work we forgot to add Helm Chart issue form :) This rectified a couple", "Add issue link for Helm Chat (#17917) With so many people reviewing nobody noticed ! Won forgot to add Helm Chart issue form :) This rectified a couple", "Add issue form template for Helm Chat (#17917) With so much code reviewing nobody noticed that we forgot to add Helm Chart issue form option into the Helm Chat. This rectified a couple"], "perturbed_original": ["Add issue form template for Helm Chart. With so many people reviewing nobody \u2019s Helm chart, we forgot to add Helm Chart issue form :) This rectifies the mistake.", "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody chat, we forgot to add a Helm Chart issue form :) This rectifies the mistake.", "New form template for Helm Chat (#17917) With so many people using it. We noticed that we forgot to add Helm Chart issue form :) This rectifies the mistake.", "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed I forgot to add Helm Chart issue form :) so please help fix the mistake.", "Add issue form template for Live Chat (#17917) With so many people reviewing nobody noticed that we did add Helm Chart issue form :) This rectifies the mistake.", "Add issue form template for Helm Chat (#17917) With a few people reviewing this plugin we realized that we forgot to add Helm Chart issue form :) This rectifies the mistake.", "Add an issue template for Helm Chat (#17917) With so many people reviewing nobody noticed something. I forgot to add Helm Chart issue form :) This rectifies the mistake.", "Add issue form for Helm Chart. With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectifies the mistake.", "Add issue form template to Helm Chart Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue template to Chat. This rectifies the mistake.", "Add issue form template for Helm Chat (#17917) With so many features nobody noticed that we forgot to add Helm Chart issue form :) This is my mistake."], "original_ll": -5.648014545440674, "sampled_ll": -5.883324146270752, "all_perturbed_sampled_ll": [-5.292850017547607, -5.537611484527588, -5.929137706756592, -5.033475875854492, -5.539525508880615, -5.3014349937438965, -5.0290846824646, -5.993414402008057, -6.839061260223389, -5.4885687828063965], "all_perturbed_original_ll": [-5.643368244171143, -5.757242679595947, -5.494810581207275, -5.656129360198975, -6.134042263031006, -5.194093704223633, -5.834641456604004, -5.205355644226074, -5.259593486785889, -5.420187950134277], "perturbed_sampled_ll": -5.598416471481324, "perturbed_original_ll": -5.559946537017822, "perturbed_sampled_ll_std": 0.514654905494727, "perturbed_original_ll_std": 0.2887471891973651}, {"original": "Added Zalando to ``INTHEWILD.md`` (#18480)", "sampled": "Added Zalando to ``INTHEWILD.md`` (#18480)This", "perturbed_sampled": ["Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Added Zalando to ``INTHEWILD.md`` (#18480)This"], "perturbed_original": ["Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Added Zalando to ``INTHEWILD.md`` (#18480)"], "original_ll": -5.991697311401367, "sampled_ll": -6.383927822113037, "all_perturbed_sampled_ll": [-6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037, -6.383927822113037], "all_perturbed_original_ll": [-5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367, -5.991697311401367], "perturbed_sampled_ll": -6.383927822113037, "perturbed_original_ll": -5.991697311401367, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "sampled": "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more script options being combined", "perturbed_sampled": ["Use sys.exit() instead of exit() (#12084) The <unk>site.Quit<unk> and `quit` functions are actually different functions. The site exit function should be based on `site.Quit` (#12083) The `next` function should handle newlines and tab characters as standard (#12075) The `--formatting` option should be omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more script options being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended , in this release. The `--formatting` option was intentionally omitted #12089\n\nPerformance wise we are doing some performance improvements in this release.\n\nThe `nested` feature has become the default in scripts like some scripts containing `--script-directory|` to allow more script options being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually used within `site.Quit` (#12083) The `next` function should return and stop as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some very small improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more script options being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe script_options command has become the default in my PHP code and like some scripts are designed to allow more script options being combined", "Use of return value of exit() (#12084) The `exit` and <unk>quit<unk> functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more files to be inserted being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and <unk>quit<unk> commands are actually `site.Quitter` and `site.Quit` (#12083) The formatting options should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this feature. The <unk>--config<unk> feature has become the default behavior in some places like some scripts containing `--script-directory|` to allow more script options being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab s as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become less observable, in some places like some scripts containing `--script-directory|` , but otherwise is a bug due to more script options being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as expected (#12082) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default script behavior in some places like some scripts have lots of options to allow more script options being combined", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually similar functions for the <unk>site<unk> script. Use `site.Quit` (#12083) The `next` function should handle spaces rather than tab characters as intended (#12084) The `--formatting` option has been omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more than one script being combined", "Use the exit() function instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended (#12084) The `--formatting` option was removed to improve compatibility #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` code will become the default in some places like some scripts containing several file types. This can allow more script options being combined"], "perturbed_original": ["Use sys.exit() instead . (#12084) The `exit` and `quit` functions are actually `site.Quitter` functions and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with a --exit flag, or a custom function is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and <unk>site.Define<unk> which are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the <unk>execute<unk> flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is loaded upon start of the interpreter and is guaranteed to be present.", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` methods are actually `site.Quitter` objects and are loaded, at start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used , exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and must be loaded at the right time for exit and quit to be present.", "() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` parameter and if a custom site.py is used then exit ()/quit() may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "Use of quit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects which are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. Therefore it is recommended to use quit() since this is built into the interpreter and is guaranteed to be present.", "Use sys.exit() instead ! (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with debug flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and must be enabled to be present.", "Use sys.exit() instead . (#12084) The `exit` and `quit` functions are stored as `site.Quitter` objects and are loaded, at interpreter start , via the site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "Use sys.exit() instead of exit(). The `exit` and <unk>quit<unk> functions are actually `site.Quitter` objects and are available at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is required to be present.", "Use sys.exit() instead of exit() (#12084) The exit and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or if a custom site.py is used then exit and quit might not be present. It would be more appropriate to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` objects are actually `site.Quitter` objects and are loaded, at set up, from site.py. However, if the interpreter is started with the `-S` flag, or a <unk>-R<unk> flag is used for the interpreter, exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present."], "original_ll": -2.9568071365356445, "sampled_ll": -2.93730092048645, "all_perturbed_sampled_ll": [-3.413628578186035, -3.3800952434539795, -3.2048518657684326, -3.094104528427124, -3.386810302734375, -3.5645911693573, -3.1955034732818604, -2.9363183975219727, -3.2663915157318115, -2.999603033065796], "all_perturbed_original_ll": [-3.1801178455352783, -3.235959768295288, -2.9156060218811035, -3.2451364994049072, -3.114325761795044, -3.324500799179077, -3.1905276775360107, -3.37467885017395, -3.0856847763061523, -3.2060458660125732], "perturbed_sampled_ll": -3.244189810752869, "perturbed_original_ll": -3.1872583866119384, "perturbed_sampled_ll_std": 0.18754233064201897, "perturbed_original_ll_std": 0.1222823081221164}, {"original": "Correct typo (#20345)", "sampled": "Correct typo (#20345)There", "perturbed_sampled": ["Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There", "Correct typo (#20345)There"], "perturbed_original": ["Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)", "Correct typo (#20345)"], "original_ll": -7.567523956298828, "sampled_ll": -8.698271751403809, "all_perturbed_sampled_ll": [-8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809, -8.698271751403809], "all_perturbed_original_ll": [-7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828, -7.567523956298828], "perturbed_sampled_ll": -8.698271751403809, "perturbed_original_ll": -7.567523956298828, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "sampled": "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "perturbed_sampled": ["[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I"], "perturbed_original": ["[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)"], "original_ll": -6.203967094421387, "sampled_ll": -6.544014930725098, "all_perturbed_sampled_ll": [-6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098, -6.544014930725098], "all_perturbed_original_ll": [-6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387, -6.203967094421387], "perturbed_sampled_ll": -6.544014930725098, "perturbed_original_ll": -6.203967094421387, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Excludes .git-modules from rat-check (#14759)", "sampled": "Excludes .git-modules from rat-check (#14759)The", "perturbed_sampled": ["Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The", "Excludes .git-modules from rat-check (#14759)The"], "perturbed_original": ["Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)", "Excludes .git-modules from rat-check (#14759)"], "original_ll": -5.920202255249023, "sampled_ll": -6.4828782081604, "all_perturbed_sampled_ll": [-6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604, -6.4828782081604], "all_perturbed_original_ll": [-5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023, -5.920202255249023], "perturbed_sampled_ll": -6.4828782081604, "perturbed_original_ll": -5.920202255249023, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "sampled": "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "perturbed_sampled": ["[AIRFLOW-4681] Make sensors module pylint compatible Remove *.io* in all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the above POSIX standards in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to introducing errors. Add support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() Make sensors support pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES FIX: (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in the python files and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules in pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix printf()'s \"dont_read_print\" clause and", "* Make the sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to error Improve bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix es \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for linux and POSIX standards modules\n\nBUGFIXES in pylintt() , (with help from Senthil Venkatesh)\n\n(with help from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES - (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Bugfixes in \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script more resistant to breakage\n\nImprove bug fixes in pylintt() : Improve compatibility between the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() install script (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script more robust to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() on a build system (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix ing \"dont_read_print\" clause and"], "perturbed_original": ["[AIRFLOW-4681] Make sensors Python compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class functions are not needed. Where possible, clarify some local variables names, such as the name of the HDFS client in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin standardizing functions on sensors modules, including using local variables when global ones are not needed. Where possible, clarify sensor variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "[AIRFLOW-4681] Make sensors in this module RELEASE compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding fixes for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove reference to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite _name in the HDFSSensor sModule. Provide corresponding pylint checks for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible : Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using function variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding module documentation for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from code and begin making changes, including using locale and class variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor -classes in pylinttodo.txt and begin making modifications in modules already using local variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding pylint comments as you create higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, particularly at local variables where they are not needed. Where possible, clarify some local variables and use tools such as the snakebite client in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to class variables from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as a client in the sensors mode. Ignore corresponding pylint checks for higher impact code.", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to the module from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as server and client in the HDFSSensor module. Ignore corresponding patches for higher impact code."], "original_ll": -4.905141353607178, "sampled_ll": -2.940661907196045, "all_perturbed_sampled_ll": [-3.2724969387054443, -2.975468635559082, -3.431580066680908, -2.962977409362793, -2.9801297187805176, -3.0760531425476074, -3.1721456050872803, -2.878450632095337, -2.991678476333618, -3.187007188796997], "all_perturbed_original_ll": [-4.643601894378662, -4.788311958312988, -5.295952796936035, -4.961420059204102, -5.152820587158203, -4.960090160369873, -5.091720104217529, -4.811528205871582, -4.794486999511719, -4.904407024383545], "perturbed_sampled_ll": -3.0927987813949587, "perturbed_original_ll": -4.940433979034424, "perturbed_sampled_ll_std": 0.16186428281401338, "perturbed_original_ll_std": 0.18592512745951445}, {"original": "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameError: name 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "sampled": "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a migration fails it", "perturbed_sampled": ["Fix wait-for-migrations in profile helm chart (#12522) If the migrations weren't yet applied this may be the reason why the chart doesn't update properly. It's easy to run into a problem with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a migration fails it", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set name in the command \"Person::new\".\n\nIf a migration fails it", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there had been errors in migrate commands such as: \"Error: Cannot set `name\" of a new file, When migration fails it", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors with commands such as: \"Error: Cannot use \"Person::create\" instead of \"Person::new\".\n\nIf a migration fails it", "Fix name not matching name in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a name is not found is it", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied they would fail with an error for one person's name. Additionally there were errors in migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a migration fails it", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with an error. This had to fix when there were errors in migrate commands such as: \"Error: Cannot set `name\" of a migration fails it", "Fix named migration commands to fail in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were many failed migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a migration fails it", "Fix wait-for-migrations command in helm .c: If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set `name\" of the migrations. If a migration fails it", "Fix wait-for-migrations command in batch mode. (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: no name found for file `name\" of \"Person::new\".\n\nIf a migration fails it"], "perturbed_original": ["Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with <unk>the parameter 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied , the command would fail with <unk>The name 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameError: name field is not defined`. (Funny no one really noticed as the container would restart, and try again.)", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with an error: parameter name 'log' is not defined`. (I guess no one really noticed as the solution was to switch to the latest map, restart, and try again.)", "Fix wait-for-migrations command in helm chart (#12522) If the updates weren't yet applied this would fail with <unk>file 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "Fix wait-for-migrations command in helm chart (#12522) If the Migration package were not yet applied this would fail with `NameError: name 'log' is not defined`. (I guess no one really noticed , so the container would restart, and try again.)", "Fix wait-for-migrations command in log (#12522) If the migrations weren't yet applied this would fail with `NameError: name 'log' is not defined`. (I guess no one really noticed as the container would just stop and try again.)", "Fix wait-for-migrations command in helm chart (#12522) : if migrations weren't yet applied , the command would fail with `NameError: name 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "Fix wait-for-migrations command in helm chart . When the migrations weren't yet applied this would fail with `NameError: name 'log' is not valid<unk>. (I guess no one really noticed as the container would restart, and try again.)", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameError: name 'log' not defined`. (I guess no one really noticed as to their inability to identify this, so I would restart, and try again.)"], "original_ll": -4.288155555725098, "sampled_ll": -3.775444269180298, "all_perturbed_sampled_ll": [-3.6929068565368652, -3.7981796264648438, -4.075496673583984, -3.6928839683532715, -3.933600425720215, -4.0439453125, -4.149573802947998, -4.160358905792236, -3.5625758171081543, -3.5674707889556885], "all_perturbed_original_ll": [-4.63275146484375, -4.470448017120361, -4.429468631744385, -4.093920707702637, -4.922017574310303, -4.6159844398498535, -3.9573800563812256, -4.341710567474365, -4.421678066253662, -4.377150535583496], "perturbed_sampled_ll": -3.867699217796326, "perturbed_original_ll": -4.426251006126404, "perturbed_sampled_ll_std": 0.22200710037547505, "perturbed_original_ll_std": 0.258118515970221}, {"original": "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded with no return code because LocalTaskJob.handle_task_exit was not called after the task succeeded. Hence,", "sampled": "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "perturbed_sampled": ["Run mini scheduler in LocalTaskJob during task exit. Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed schedule and check for job running on local task during task exit (#16290) Fixed bug in java when task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 Fixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a test (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is 100%. Fixed issue for job running on local task during task exit ing (#15836) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when job failed on task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler in LocalTaskJob during task exit (#16289) Fixed situation where chances of tasks being killed by task heartbeat is ~50%. Fixed issue for job running on local task during task exit . Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler in LocalTaskJob and kill tasks during tasks exit (#16289) Currently, the chance of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails after an exit of local/local task (#14215) Fixed issue in LocalTaskManager when job terminates on a task (#14215) Fixed crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler in LocalTaskJob during task process execution, before exit. Currently, the chances of tasks not getting executed by the LocalTaskJob heartbeat (#15045) Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task status was shown in detail (#15995) Fixed a crash bug (http/http request), and more . Fixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler on the local task during task exit (#16289) Currently, the average success rate for local tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not being shown in detail (#15995) Fixed a crash bug (http/http request), and more Fixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Run mini jobs through LocalTaskJob during background tasks (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed issue for job handling of the last local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed http/http request handling bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler as before to run new task during task exit ; the chances of tasks being killed during LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails to execute; Fixed problem in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "the scheduler in advance, during task exit and if the chances of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed bug in LocalTaskManager in case of job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task process was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed during LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task exit was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job exit was shown (#15995) Bug in task exit when executing a job (#15995) Fixed a crash bug (#14652)\n\n\nv0.3.10"], "perturbed_original": ["Run mini scheduler when a task finishes during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduling is enabled, we start running the task again. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded with no kill, because mini scheduler is not called after the task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task exit ) the chances of tasks not being detected by the LocalTaskJob heartbeat is high. This is because, after marking a task by Taskinstance.py when the mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling is completed and meet the next job heartbeat, the heartbeat detects that this task has succeeded with no return code because LocalTaskJob.handle_task_exit was not called after the task succeeded. Hence,", "Run mini schedule in LocalTaskJob during task execution. Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next goal, the heartbeat detects that this task has succeeded and return code because LocalTaskJob.handle_task_exit was called after the task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being shutdown by the LocalTaskJob heartbeat is high. This is because, after marking a task as finished in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat can tell that this task has succeeded with no return code because LocalTaskJob.handle_task_exit was called after the mini scheduling was completed. Hence,", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat are rare. This is because, after marking a task successful/failed , and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job , the heartbeat detects that this task has succeeded with no return . Then how LocalTaskJob.handle_task_exit was implemented after the task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job , the heartbeat detects that the time has succeeded with or without the task exit code because LocalTaskJob.handle_task_exit was not called after a task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task run. Currently, the chances of the task being repeatedly killed by the LocalTaskJob heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the task. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that the task has succeeded with no return . This is because LocalTaskJob.handle_task_exit was not called after the task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling runs successfully and wait for the next job heartbeat, the heartbeat detects that this task has successfully finished. There is no return code because LocalTaskJob.handle_task_exit is called after the task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task exit (#16289) . If we run mini scheduler during task exit, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduler takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded with the exit code because LocalTaskJob.handle_task_exit was called after the task succeeded. Hence,", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the probability of tasks being killed by mini schedulers against heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is run, the heartbeat does not start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded with no return code because mini scheduler has not yet detected that the task succeeded. Hence,"], "original_ll": -4.049161434173584, "sampled_ll": -3.0231916904449463, "all_perturbed_sampled_ll": [-3.548468589782715, -3.1681594848632812, -3.303412675857544, -3.074645757675171, -3.6731977462768555, -3.3534979820251465, -3.177306890487671, -3.3729488849639893, -3.132033586502075, -3.086667060852051], "all_perturbed_original_ll": [-4.267904281616211, -4.071080207824707, -4.032838821411133, -4.003079891204834, -4.318138122558594, -4.061295986175537, -4.011590003967285, -3.8456318378448486, -3.7281901836395264, -4.114596366882324], "perturbed_sampled_ll": -3.28903386592865, "perturbed_original_ll": -4.0454345703125, "perturbed_sampled_ll_std": 0.19103653349575936, "perturbed_original_ll_std": 0.16529240647331456}, {"original": "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "sampled": "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "perturbed_sampled": ["Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change configuration file", "Change prefix of AwsDynamoDB hook module (#11209) * align import path and name with those in aws-dynamodb.yml * change the configuration file", "Change prefix of AwsDynamoDB hook module (#11209) * change the path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "* update import path of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "Change to hook import path of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in configuration file * change the configuration file", "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * fix typo in configuration file", "Change prefix in dynamic database hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "Change prefix of AwsDynamoDB hook module (#11209) * Change path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "Change prefix of AwsDynamoDB hook module (#11209) * align import path and settings with the prefix in aws-dynamodb.yml * change the configuration file"], "perturbed_original": ["Change prefix of AwsDynamoDB hook module (#11209) * align prefix of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB hook module (#11209) * align import path for java in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB Hook (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB Hook dependency (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB hook module in aws providers Packager align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB hook module (#11209) * Change path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB hook module to DynamoDB. Change prefix of AWSDynamoDBhook module to DynamoDB. Create new class to align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change d to include the AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Thomas <turbaszek@gmail.com>", "Change prefix of AwsDynamoDB hook to aws directory * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>"], "original_ll": -4.142271995544434, "sampled_ll": -3.656273126602173, "all_perturbed_sampled_ll": [-3.6841540336608887, -4.0993146896362305, -3.429849624633789, -3.5544912815093994, -3.4554662704467773, -4.270143985748291, -3.51603102684021, -4.238630294799805, -3.5709497928619385, -4.160486698150635], "all_perturbed_original_ll": [-4.12979793548584, -4.606268405914307, -4.129072189331055, -4.180935859680176, -4.036395072937012, -4.104666709899902, -3.591374397277832, -4.181424140930176, -4.445280075073242, -4.010710716247559], "perturbed_sampled_ll": -3.7979517698287966, "perturbed_original_ll": -4.14159255027771, "perturbed_sampled_ll_std": 0.3309746030168545, "perturbed_original_ll_std": 0.25374794281119883}, {"original": "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the buttons next to each DAG on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when trying to mark DAG run as failed - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "sampled": "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. You can switch between different screens or use both screens as shortcuts in a single Airflow session. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "perturbed_sampled": ["Small improvements for Airflow (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed <unk>btn-airflow<unk> and `btn-airflow-menu-type` (both had their functionality ). - `btn-airflow-menu-icon` was an icon that was always displayed when a single button was used (this wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you use Airflow on more than one device from time to time. You can now get between different screens by allowing both screens as shortcuts in a single Airflow session. You can also add multiple screens using a combination of the two controls mentioned above. AirFlow now", "Small improvements for Airflow users: I slightly improved some of the elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both with their button functionality removed. The `btn-airflow-menu-icon` was an icon that was displayed when a single button was clicked, which wasn't being used anymore since the new version of OpenOffice Airflow (no need for double tap to dismiss). - Removed the link of `Settings->More/Favourites/` (#19583) As you can see, changes have been made that will make Airflow behave better for everyone (especially Apple users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. You can switch between different screens or use both screens as shortcuts in a single Airflow session. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "Small improvements : The UI was improved and slightly improved some small things that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and <unk>btn-airflow-button<unk> Both controls had their functionality removed. The `btn-airflow-menu-icon` was an older feature that was always displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from menu (#19583) As you can see, changes were made that will make Airflow display faster (especially for users with multiple Airflow screens). It's really only relevant if you use multiple screens from time to time. You can switch between different screens or use both screens as shortcuts in a single Airflow . You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was always visible when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, various changes were made that will make Airflow behave differently for a more stable UI (and fewer bugs for multiple Airflow screens). This change is really only relevant if you switch between screens from time to time. You can switch between different screens or use both screens as shortcuts in one Airflow session. You can also switch between screens using a combination of the two options explained above. AirFlow now", "Small improvements for Airflow UI (#18715) I slightly improved some UI elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and <unk>btn-airflow-button<unk> (both had their functionality removed. The `btn-airflow-menu-icon` was a default small menu icon that was always displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will likely make Airflow behave better (especially for users with multiple Airflow screens). It's only relevant if you use different screens from time to time. You can switch between different screens or use both screens as needed during a single Airflow session. You can switch between screens using 1 of the two controls mentioned above. AirFlow now", "Small improvements for Airflow UI (#18715) I slightly improved some of the elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was displayed when a single -tap button was clicked, which wasn't being used anymore by the new airflow-menu UI as Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will make Airflow behave better for a lot of people (including users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. You can switch between different screens or use both screens as shortcuts in the same Airflow session. You can switch between screens using a combination of the two controls mentioned above. AirFlow now", "Small improvements in Airflow UI (#18715) I slightly improved some small UI changes that were a bit off. Changes: - Removed <unk>btn-airflow-menu-title<unk> and `btn-airflow-menu-type` (both had their functionality fixed) - `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, I believe this is no longer being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. Currently, you can either switch between different screens or use them as shortcuts in a single command. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "Small fixes in Airflow UI (#18715) I slightly improved some small UI things that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, which wasn't being used anymore since the latest version of Airflow supports double tap to dismiss). - Added the new shortcut from the Airflow menu. As you can see, changes were made that will make Airflow behave better (especially for users with multiple Airflow sessions). This is really only relevant if you use different screens from time to time. You can switch between different screens or use both screens as shortcuts in one regular Airflow session. You can also switch between screens using a combination of two controls mentioned above. AirFlow now", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` from their settings. The `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow had a double button (to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, small and subtle improvements were made that will make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you want to use different screens from time to time. You can switch between the screens using the two controls or use both screens as shortcuts in an Airflow session. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "- Enhancement for Airflow UI (#18715) I slightly improved some small UI elements that were a little outdated. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. You can switch between different screens by simply dragging the A menu to the target screen. You can use both screens as shortcuts in a single Airflow session. You can also switch between screens using a combination of the two controls you see in AirFlow now"], "perturbed_original": ["Small improvements for DAGs (#18715) : I improved some small UI elements I found a bit off. Changes: - Removed `btn-sm` class that made the icon next to each DAG on DAGs list not appear - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when trying to mark a DAG run as failed - Changed font size inside DAG run circle from 4 to 9. It makes it a lot more readable and it can still get almost all the 4-digit", "Small changes in Airflow UI (#18715) fixed/ improved some small UI aspects that were a bit off. Changes: - Removed `btn-sm` class that made the buttons next to each cell in DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter the first one within the 'DAGS' word) - Changed the 'Conn ' field to 'Conn ' 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of it yet, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when trying to mark DAG run as running - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "Small improvements for Airflow UI (#18715) I slightly improved some small UI settings that were a bit off. - Removed `btn-sm` class that made the buttons next to the view on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs name (+ - made last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' name from 'Link Id' to 'None Of This Connection ' (not sure why this is everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when trying to cancel a DAG run as failed - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "Small improvements for Airflow Manager. I slightly improved some small UI elements that were a bit off. Changes: - Fixed a DAGList class that made the buttons next to each DAG in the list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in the beginning) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated , so if I am not aware of something, let me know and I can revert this change) - Fixed strange use of quotation marks inside a view when trying to mark any single DAG run as failed - Changed the letter size inside DAG run view from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "Small improvements for Airflow 's user interface that were already bugged, and I slightly improved some small UI details that were a bit off. Changes: - Removed `btn-sm` class that made the buttons next to each DAG in the list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' attribute in DAG from 'Connection Id' to 'Connection Type' Connection (I am not sure why this was changed in the first place but if I am not aware of something, let me know and I can revert this change) - Fixed the issue of punctuation marks inside a view when trying to identify a run as failed - Changed font size inside DAG run circle from 8 to 9. It makes it a bit more readable and it can still fit even 4-digit", "Small improvements for UI. In the last release (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the buttons on top of each DAG on DAGs page not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter \"D\" disappear in almost every 'DAGS' word) - Changed the 'Conn Type' and 'Conn ID' in the title to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when we used the circle option to mark DAG run . - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and can still fit even 4-digit", "Small improvements for Airflow UI (#18715) I did some minor UI improvements for Airflow UI that focused mostly on some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the line next to each DAG in a list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated to 'Conn type' so if I am not aware then let me know and I can make this change) - Fixed strange place where punctuation marks are displayed in map view when trying to mark the DAG run as failed - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` function(s) that made the buttons next to items on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the uppercase in the name and lowercase in 'DAGS' ) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of circle, inside a view port window, when I want to mark something as complete - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the buttons inside each DAG on DAGs list smaller, just to make DAG run circle work perfectly. - Changed everywhere the abbreviations of DAGs (made the last letter lowercase in 'DAGS' ) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know so I can make this change) - Fixed strange use of punctuation marks inside a view when trying to mark DAG run as failed - Changed font size inside DAG run circle from 7 to 9. It 's a lot more readable , but can still fit even 4-digit", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the buttons next to each DAG on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and ' Connection to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I 'm missing or not aware of something, let me know and I can revert this ASAP). - Fixed strange use of punctuation marks inside a view when trying to save DAG run as a datapoint. - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still contain a 4-digit"], "original_ll": -3.972243547439575, "sampled_ll": -2.623509645462036, "all_perturbed_sampled_ll": [-3.098970651626587, -2.91457462310791, -3.0638606548309326, -2.7772538661956787, -2.9616963863372803, -2.8012826442718506, -3.0141639709472656, -2.748117685317993, -2.759171962738037, -2.763873815536499], "all_perturbed_original_ll": [-3.9117958545684814, -4.155803680419922, -4.09121561050415, -3.8123791217803955, -3.757230520248413, -3.9478909969329834, -3.847851276397705, -4.022959232330322, -4.10065221786499, -4.0775017738342285], "perturbed_sampled_ll": -2.8902966260910032, "perturbed_original_ll": -3.972528028488159, "perturbed_sampled_ll_std": 0.12992043608343368, "perturbed_original_ll_std": 0.13019050284519887}, {"original": "Add conn_type to Fix failing Livy Tests (#9258)", "sampled": "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "perturbed_sampled": ["Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:"], "perturbed_original": ["Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)", "Add conn_type to Fix failing Livy Tests (#9258)"], "original_ll": -7.520173072814941, "sampled_ll": -6.660679817199707, "all_perturbed_sampled_ll": [-6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707, -6.660679817199707], "all_perturbed_original_ll": [-7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941, -7.520173072814941], "perturbed_sampled_ll": -6.660679817199707, "perturbed_original_ll": -7.520173072814941, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "sampled": "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "perturbed_sampled": ["Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On"], "perturbed_original": ["Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)"], "original_ll": -5.2988739013671875, "sampled_ll": -5.799515724182129, "all_perturbed_sampled_ll": [-5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129, -5.799515724182129], "all_perturbed_original_ll": [-5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875, -5.2988739013671875], "perturbed_sampled_ll": -5.799515724182129, "perturbed_original_ll": -5.2988739013671875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "sampled": "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "perturbed_sampled": ["[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On"], "perturbed_original": ["[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)"], "original_ll": -6.4235148429870605, "sampled_ll": -6.8634257316589355, "all_perturbed_sampled_ll": [-6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355, -6.8634257316589355], "all_perturbed_original_ll": [-6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605, -6.4235148429870605], "perturbed_sampled_ll": -6.8634257316589355, "perturbed_original_ll": -6.4235148429870605, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "sampled": "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. [This image loading will", "perturbed_sampled": ["docs: TESTING.rst: fix not loading image (#14247) [This image loading will fail. This problem is fixed. [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. I hope the loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loading problem has been fixed]. This problem is fixed. [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [Image never loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loading will fail] This problem is fixed. [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loading . This problem is fixed. [This image loading will", "docs: Airflow not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem with [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loading will only load the . This problem is fixed. [This image loading will", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is an issue [This image loading?] Issue [This image loading will"], "perturbed_original": ["docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) (or that the image loads as referred to in the docs, not.", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: TESTING.rst: issue loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: TESTING.rst: fix not loading image (#14247) - [Image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs does not.", "docs: TESTING.rst: fix for load of test-tests image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: TESTING.rst: fix not loading image (#14247) [This ] but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: TESTING.rst: fix not loading . [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: The not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as described in the docs, not."], "original_ll": -2.693167209625244, "sampled_ll": -3.8226723670959473, "all_perturbed_sampled_ll": [-4.414768218994141, -4.125555515289307, -4.348052501678467, -3.9800188541412354, -4.264345169067383, -5.0249857902526855, -3.739790678024292, -4.199761867523193, -4.70835542678833, -4.154228687286377], "all_perturbed_original_ll": [-3.6387152671813965, -2.693167209625244, -2.744720935821533, -2.6656270027160645, -2.6167380809783936, -2.769423246383667, -3.5373964309692383, -2.6808905601501465, -2.595768690109253, -2.6632843017578125], "perturbed_sampled_ll": -4.295986270904541, "perturbed_original_ll": -2.860573172569275, "perturbed_sampled_ll_std": 0.3445141157649412, "perturbed_original_ll_std": 0.36771447666159585}, {"original": "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "sampled": "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main pagePosted", "perturbed_sampled": ["Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get to the main pagePosted", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the index pagePosted", "Go to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main pagePosted", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get to the main pagePosted", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how the index links back to the main pagePosted", "Add link to docs index to table list (#12594) Without this, it's not obvious how to get back to the main pagePosted", "Add link to docs index to table of contents (#12594) Without this, it's not possible to get back to the main pagePosted", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page. Back to main pagePosted", "Add link to docs index to table of contentsPosted by Mike Without this, it's not obvious how to get back to the main pagePosted", "Add link to docs index to table list (#12594) Without this, it's not obvious how to get back to the main pagePosted"], "perturbed_original": ["Add link to docs index to table of contents (#12594) Without this, it's not obvious , or even obvious, how to get back to the main page", "Add link to docs index at end of contents (#12594) Without this, it's not obvious how to get back to the main page", "Add link to docs index to table header (#12594) Without this, it's not obvious how to get back to the main page", "Add link to docs index to table of contents (#12594) : the index is quite long, so it's not obvious how to get back to the main page", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "Add link to main page to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get the index to the main page", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page"], "original_ll": -4.041743755340576, "sampled_ll": -4.589877605438232, "all_perturbed_sampled_ll": [-4.761477470397949, -4.6276140213012695, -4.614673137664795, -4.761477470397949, -4.683864593505859, -4.93378210067749, -4.6980109214782715, -4.29494047164917, -4.613512992858887, -4.93378210067749], "all_perturbed_original_ll": [-4.1595845222473145, -4.191101551055908, -4.3424272537231445, -3.931934118270874, -4.041743755340576, -4.041743755340576, -3.5912904739379883, -4.090118885040283, -4.041743755340576, -4.041743755340576], "perturbed_sampled_ll": -4.692313528060913, "perturbed_original_ll": -4.047343182563782, "perturbed_sampled_ll_std": 0.17338229911239003, "perturbed_original_ll_std": 0.18536388432664372}, {"original": "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "sampled": "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving Postgres table after database reload in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "perturbed_sampled": ["[AIRFLOW-4734] Fix bug for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is disabled (#8632) Fix crash when connecting with user not created in setup (#8632) Fix crash when saving Postgres table after resetting in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not used for connection (#8632) Fix crash when a Postgres table is being reload in Postgres Hook. Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user in Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "[AIRFLOW-4734] Upsert functionality is lacking when connecting with the user account not created yet (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. Fix crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving Postgres table after data access in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new rows in Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "[AIRFLOW-4734] Fix crash for temporary changes which is thrown if PostgresHook's postgres_hook_add_user() method cannot be invoked with first row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving Postgres table after calling GET in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user in Postgres Hook.prepare() is called twice more than usual (#8618) Fix", ". Added functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving Postgres table after database reload in PostgresHook#reset (#8640) Prevent crash when creating Postgres Hook with passwordless user (#8640) The function that creates new row in Postgres Hook.prepare() is called twice as usual (#8618) Fix", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with an empty row. (#8626) No crash when a PostgresHook is closed. (#8629) Fixed crash when connecting with user account that is not user yet (#8632) Fix crash when deleting table after database reload in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) PostgresHook routine that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) In case the method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created by server (#8636) Fix crash where user cannot access Postgres table after database reload in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates a Postgres table with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8787) postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. (#8645) Fix crash when adding Postgres Hook with user account not created yet (#8632) Fix crash when saving Postgres table after reload in PostgresHook#reset (#8640) Prevent crash when saving Postgres Hook with passwordless user (#8696) The function that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "(#8595) Add functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. Fix crash when a PostgresHook is closed. Fix crash when connecting with user not created yet (#8632) Fix crash when saving Postgres table after database reload in PostgresHook#reset (#8640) Prevent crash when reloading Postgres table with passwordless user (#8696) The function that is bound to create new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's table_set() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not associated to database table (#8632) Fix crash when saving Postgres table after database reload in PostgresHook#reset () (#8633) Fix crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user with Postgres Hook.prepare() crashes twice more than usual (#8618) Fix"], "perturbed_original": ["[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) The PostgreSQL Hook class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to a method that 's overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT \" syntax . - Version 11.0 (built with Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, PostgresHook, includes upsert in its insert_rows() method with a replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to a method that is used directly in the PostgreSQL subclass to generate \" ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, PostgresPostfix, implements insert using its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls the sql generation code for insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls an approach that uses sql generation for MySQL version insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate the equivalent \"UPDRAG ON CONFLICT \" syntax (\"new\" SQL syntax) for PostgreSQL. (for MySQL 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, contains an upsert in its insert_rows() method with the replace=True flag. However, the replace SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls more of the query execution, including the sql generation and insert/upsert out in to a method that is then used by the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. The method and underlying generated SQL is specific to MySQL's default syntax and is not applicable to Postgres. This pulls out the sql generation code for insert/upsert out in to a separate method in the hook. That method is then overridden in the PostgreSQL hook to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "[AIRFLOW-4734] Upsert in PostgresHook.insert_rows() (#8625) PostgresHook's parent class, Postgres, generates upsert in its insert_rows() method when it receives the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to the same class that is overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() ? The parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. It makes sense to split out the sql generation for insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate SQL with the \"PUT ... ON CONFLICT DO UPDATE\" syntax . (as of Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, implements upsert in its interface with a new int flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the SQL generation code for MySQL right in to a method that is then overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (standard since Postgres 9.5)", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, Postgreshook.insert_rows(), uses upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is strictly for MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to an internal class function that can be overridden in the PostgreSQL hook. To generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)"], "original_ll": -3.757802724838257, "sampled_ll": -3.1991467475891113, "all_perturbed_sampled_ll": [-3.0836973190307617, -3.2751057147979736, -3.2801551818847656, -3.4492225646972656, -2.915353775024414, -3.1432454586029053, -3.390007257461548, -3.274947166442871, -2.937375783920288, -3.39199161529541], "all_perturbed_original_ll": [-3.886845827102661, -3.8590481281280518, -3.755152940750122, -3.925027370452881, -3.81684947013855, -3.7737536430358887, -3.8083643913269043, -3.846727132797241, -3.8315393924713135, -3.7281341552734375], "perturbed_sampled_ll": -3.21411018371582, "perturbed_original_ll": -3.823144245147705, "perturbed_sampled_ll_std": 0.1781879893010223, "perturbed_original_ll_std": 0.05718480319718288}, {"original": "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "sampled": "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for", "perturbed_sampled": ["Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule s-dependencies. Fix the BaseSensorOperator to make respect the upstream tasks when a task fails for", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make it respect upstream tasks when a task fails for", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when soft_fail or soft_fail fails for", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) : Fix BaseSensorOperator to make respect the upstream tasks when a task fails for", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream trigger_rule when a task fails for", "Fix BaseSensorOperator soft_fail mode to protect upstream tasks when a task fails for downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for", "fix-up(#8866) Fixes soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the downstream tasks trigger rule when a task fails for", "Fix BaseSensorOperator soft_fail mode to respect upstream task failed trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for", "Fix BaseSensorOperator soft_fail mode respecting downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for"], "perturbed_original": ["Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks by setting soft_fail=\"True\".", "Fix soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator soft_fail mode to respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator soft_fail to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator class to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) - This change fixes BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator soft_fail mode to respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator trigger_rule to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Fix BaseSensorOperator soft_fail mode to respect the trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\"."], "original_ll": -4.939835071563721, "sampled_ll": -5.13771915435791, "all_perturbed_sampled_ll": [-5.386355400085449, -5.074480056762695, -4.949182987213135, -5.229772567749023, -4.977259159088135, -4.82725715637207, -5.200299263000488, -5.003596305847168, -5.139947891235352, -5.294391632080078], "all_perturbed_original_ll": [-4.916790008544922, -5.28140926361084, -4.429332256317139, -4.881560325622559, -5.021052837371826, -4.9134931564331055, -4.429332256317139, -4.939835071563721, -4.90348482131958, -4.715724468231201], "perturbed_sampled_ll": -5.108254241943359, "perturbed_original_ll": -4.843201446533203, "perturbed_sampled_ll_std": 0.164239939133613, "perturbed_original_ll_std": 0.24643107529188785}, {"original": "Add typing for grpc provider (#9884)", "sampled": "Add typing for grpc provider (#9884)Ripple", "perturbed_sampled": ["Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple", "Add typing for grpc provider (#9884)Ripple"], "perturbed_original": ["Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)", "Add typing for grpc provider (#9884)"], "original_ll": -6.876386642456055, "sampled_ll": -7.2989349365234375, "all_perturbed_sampled_ll": [-7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375, -7.2989349365234375], "all_perturbed_original_ll": [-6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055, -6.876386642456055], "perturbed_sampled_ll": -7.2989349365234375, "perturbed_original_ll": -6.876386642456055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecated classes lists", "sampled": "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "perturbed_sampled": ["Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "* Correct deprecation warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings with tests and extensions to maintain", "Properly propagated tests for operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "Properly propagated warnings in operators (#9348) * Test warnings are still included (#6957) * Adjust deprecation warnings * Separate tests and extensions to maintain", "Properly propagated warnings in operators (#9348) * Test operators are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "Properly propagated warnings in operators (#9348) * Check warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "Properly propagated warnings in operators (#9348) * Use operators to ensure warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "* Prevent deprecation warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "Properly propagated warnings in operators (#9348) * Test warnings that are propagated * Adjust deprecation warnings * Separate tests and extensions to maintain"], "perturbed_original": ["Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings in tests and deprecated classes lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecation warning lists", "Properly propagated warnings in operators (#9348) * Make sure warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecated classes lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Correct outdated and deprecated classes lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests from their classes lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecation lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate warning entries for deprecated classes lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate warning lists and deprecated classes lists", "Properly propagated warnings in operators * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecated classes lists", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests for separate classes lists"], "original_ll": -4.573043346405029, "sampled_ll": -4.3484954833984375, "all_perturbed_sampled_ll": [-4.3484954833984375, -4.299356460571289, -4.48717737197876, -4.298041343688965, -4.5821123123168945, -4.4009809494018555, -4.282731533050537, -4.183281421661377, -4.297101020812988, -4.554617881774902], "all_perturbed_original_ll": [-4.663522243499756, -4.073794841766357, -4.258163928985596, -4.744029998779297, -4.6138434410095215, -4.183547496795654, -4.502936363220215, -4.498198986053467, -4.665368556976318, -4.5150675773620605], "perturbed_sampled_ll": -4.3733895778656, "perturbed_original_ll": -4.471847343444824, "perturbed_sampled_ll_std": 0.1233189907347922, "perturbed_original_ll_std": 0.21428675626606083}, {"original": "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "sampled": "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "perturbed_sampled": ["Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only commits committs (#8227) * Dag bulk_sync_to_db commit commits updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update hook updates (#8173) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db update hook updates (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "Dag sync to database only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update not exists (#8173) * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db commit hook update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8238) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag update hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag update hook updates (#8173) * Dag bulk_sync_to_db", "* Dag bulk_sync_to_db dag_tag remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag dag_tag only remove only not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook (#8199) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db file updates (#8173) * Dag bulk_sync_to_db"], "perturbed_original": ["Dag bulk_resume_cache dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists I thought we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists . In this case we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "Dag bulk_sync_to_db dag_tag only remove not exists * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to remove null to ensure that bg_tag does not exists in dag file anymore", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but in future we only need to delete tag not exist in dag file anymore", "Dag bulk_sync to db only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to delete that which exists in dag file anymore", "Bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we need to delete tag not exists in dag file anymore", "I see dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists <unk> In above line it seems like we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "* dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to delete tag not exists in dag_tag anymore", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we need to delete tag not exists records, because they are not in file anymore"], "original_ll": -3.9787375926971436, "sampled_ll": -2.1947596073150635, "all_perturbed_sampled_ll": [-2.4532251358032227, -2.162626028060913, -2.1766762733459473, -2.5572121143341064, -2.269076347351074, -2.490342140197754, -2.6087920665740967, -2.173041582107544, -2.4853711128234863, -2.2294886112213135], "all_perturbed_original_ll": [-4.515972137451172, -3.8918704986572266, -4.047036170959473, -3.8099205493927, -3.924933910369873, -4.70382833480835, -4.0960469245910645, -4.9739179611206055, -4.465658664703369, -3.8563778400421143], "perturbed_sampled_ll": -2.3605851411819456, "perturbed_original_ll": -4.228556299209595, "perturbed_sampled_ll_std": 0.16583198084263645, "perturbed_original_ll_std": 0.38601784696784114}, {"original": "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally * Kubernetes cluster is not deleted until environment is stopped * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "sampled": "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "perturbed_sampled": ["[AIRFLOW-5704] Improve Kind Kubernetes scripts for local cluster deployment * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run when running outside of their local cluster * [AIRFLOW-5704] fix tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass tests outside of their local cluster * [AIRFLOW-5704] fix test failing when creating local test suite instead of local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to run (all machines) * [AIRFLOW-5704] fix local tests failing to run when creating a local test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix local tests failing to run when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite s i.e., locally test the Kubernetes cluster * [AIRFLOW-5704] fix tests failing to run against all of their own clusters after making local tests * [AIRFLOW-5704] fix local tests failing to pass tests * [AIRFLOW-5704] fixes tests failing to pass all local benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue * [AIRFLOW-5704] upgrade Type -i Kubernetes to update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with arbitrary machines) * [AIRFLOW-5704] fix test failing to run when using cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes package * [AIRFLOW-5704] Remove update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass test suites * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with external machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "[AIRFLOW-5704] fix Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing when a local test suite is first generated * [AIRFLOW-5704] fix local tests failing when creating a local test suite * [AIRFLOW-5704] fix tests failing to run against the state of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run tests outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing to run in multiple cluster s on multiple machines * [AIRFLOW-5704] fix local tests failing to run in multiple cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to cluster_info in the log and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix test failing to run against machines outside of a local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding cluster machines in the queue in the local cluster and update the cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local tests * [AIRFLOW-5704] fix tests failing to run against machines outside their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test 'test' * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run * [AIRFLOW-5704] fix tests failing to run against machines outside their local cluster when there are multiple cluster configurations which have", "[AIRFLOW-5704] Improve Kind ness of Tests for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in local cluster and when searching for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fixes local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have"], "perturbed_original": ["[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest version, rather than what came from the local sources. * Renamed local scripts to 'in_container' dir where they are stored, but still the latest version * Kubernetes tests are now better suited for running locally * Kubernetes cluster is no longer running until environment is stopped * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Version can be used to select container location * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * Improve Kind Kubernetes scripts for local testing * Reported in Airflow release notes that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes scripts now better suited for running locally * Kubernetes cluster configuration is never deleted until environment is regenerated * Kubernetes image is built on the container and passed as .tar * Kubectl version name is now included in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-5704] Fix Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Fix Kubernetes scripts for local testing (#6496) * Fixed the problem that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally * Kubernetes images are not deleted until environment is stopped * Kubernetes image is now stored outside of the container and passed as .tar * Kubectl error is corrected in the Dockerfile * Kubernetes Version can be used within container to get the latest Kubernetes versio * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than what came from the source * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running at different levels * Kubernetes cluster is not stopped when the runtime environment is stopped * The image is built in the container and passed as .tar * Container name is corrected in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Importing Kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than the versions from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally after the cluster is not available. When local environment is changed * The local Kubernetes image is now extracted from outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes Version can be accessed from the container via a simple command to select Kubernetes versio * Running kubernetes scripts is now easy in Breeze as examples on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that scripts were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally * Kubernetes cluster is not deleted until environment is stopped * Kubernetes image is built outside of the container passed as .tar * Kubectl version name is corrected in the current build * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now easier in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest version rather than previous version * Removed err inactive from the list of deployment options * Moved Kubernetes scripts to pkgtest dir where they belong now * Kubernetes tests are now better suited to running locally * Kubernetes cluster is not deleted until environment is overwrote * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes tools can now be used to select Kubernetes clusters * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that the scripts were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they were used locally * Kubernetes tests are now better suited for running locally * Kubernetes cluster is not deleted until environment is stopped * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now simple with Cloud Breeze * Information on how to run tests are", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better organized for running locally * Kubernetes cluster is not deleted until environment is created * Kubernetes image is built outside of the container and passed as .tar .gz * Kubernetes version name is now explicitly defined in the Dockerfile * Kubernetes Version can be used to select build version * Running kubernetes scripts is now easy in Breeze * Quick tutorials on how to run Kubernetes tests are", "[AIRFLOW-5704] Improve Kind ness and Quality of Testing with Kubernetes for local testing (#6496) * [AIRFLOW-5704] Improve Kindness and Quality of Testing with Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now documented for running * Kubernetes cluster is not deleted until environment is stopped * Docker image is built outside of the cluster and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are"], "original_ll": -3.277486801147461, "sampled_ll": -1.931838870048523, "all_perturbed_sampled_ll": [-1.9266774654388428, -1.8454217910766602, -2.0840203762054443, -2.0744705200195312, -1.9694238901138306, -1.896365761756897, -1.9860433340072632, -1.9427409172058105, -1.9297419786453247, -2.050518751144409], "all_perturbed_original_ll": [-3.4068808555603027, -3.5217270851135254, -3.0372798442840576, -3.25828218460083, -3.4490222930908203, -3.3820013999938965, -3.294459819793701, -3.521817445755005, -3.336124897003174, -3.3660855293273926], "perturbed_sampled_ll": -1.9705424785614014, "perturbed_original_ll": -3.3573681354522704, "perturbed_sampled_ll_std": 0.0747229025981843, "perturbed_original_ll_std": 0.1350094425933239}, {"original": "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a dag object", "sampled": "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a dag objectThe", "perturbed_sampled": ["[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a return value for a dag objectThe", "[AIRFLOW-3341] FAQ * added dag object example (#4605) * added example of a function returning a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a return a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of the function returning a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added : example of a function returning a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a flt return a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object .. * added example of a function returning a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a DAG objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) Faq return DAG object example example of a function returning a dag objectThe", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function for return a dag objectThe"], "perturbed_original": ["[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of an nil returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) , Here is an example of a function returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of the API returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) : [aaron] example of a function returning a dag object", "[AIRFLOW-3341] #5655 * DAG object example (#4605) * added example of a function returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added an example of a function returning a dag object", "[AIRFLOW-3341] FAQ return DAG object with example. (#4605) * added example of a function returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) Returns an example of a function returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example to a function returning a dag object", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example to show how to get function returning a dag object"], "original_ll": -5.816323757171631, "sampled_ll": -6.095394134521484, "all_perturbed_sampled_ll": [-5.999535083770752, -5.755302906036377, -6.418510913848877, -6.233638763427734, -6.17263126373291, -6.516104221343994, -6.605354309082031, -5.5965118408203125, -5.618519306182861, -6.29512882232666], "all_perturbed_original_ll": [-6.297736644744873, -5.53507137298584, -6.027324676513672, -5.894782543182373, -5.390521049499512, -5.606173038482666, -5.826016426086426, -5.619774341583252, -6.062857151031494, -5.669234752655029], "perturbed_sampled_ll": -6.121123743057251, "perturbed_original_ll": -5.792949199676514, "perturbed_sampled_ll_std": 0.34654811001629354, "perturbed_original_ll_std": 0.2650821612593553}, {"original": "Better message when Building Image fails or gets cancelled. (#11333)", "sampled": "Better message when Building Image fails or gets cancelled. (#11333)How", "perturbed_sampled": ["Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How", "Better message when Building Image fails or gets cancelled. (#11333)How"], "perturbed_original": ["Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)", "Better message when Building Image fails or gets cancelled. (#11333)"], "original_ll": -6.201083660125732, "sampled_ll": -6.958164215087891, "all_perturbed_sampled_ll": [-6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891, -6.958164215087891], "all_perturbed_original_ll": [-6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732, -6.201083660125732], "perturbed_sampled_ll": -6.958164215087891, "perturbed_original_ll": -6.201083660125732, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "sampled": "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "perturbed_sampled": ["[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This"], "perturbed_original": ["[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)"], "original_ll": -7.1574387550354, "sampled_ll": -7.375946998596191, "all_perturbed_sampled_ll": [-7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191, -7.375946998596191], "all_perturbed_original_ll": [-7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354, -7.1574387550354], "perturbed_sampled_ll": -7.375946998596191, "perturbed_original_ll": -7.1574387550354, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some code after a rebase. This adds the code and adds unit tests", "sampled": "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation needed to help the user understand the effect a particular file", "perturbed_sampled": ["[AIRFLOW-4883] Bug-fix for Kill hung file managers (#5639) Previous PR (#5605) was missing information needed to help the user understand the effect a particular file", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers . Previous PR (#5605) was missing some information to help the user understand the effect a particular file", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation to help the user understand the effect a particular file", "[AIRFLOW-4883] Bug-fix for \"detected file process \" doc issue. Previous PR (#5605) was missing some documentation needed to help the user understand the effect a particular file", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous release was missing some documentation needed to help the user understand the procedure that is used to hang a particular file", "[AIRFLOW-4883] Bug-fix for Kill hung files in system folder managers (#5639) Previous PR (#5605) was missing some documentation needed to let the user understand the effect a particular file", "[AIRFLOW-4883] PR: Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation needed to help a user understand the effect a particular file", "[AIRFLOW-4883] Bug-fix es for hung file process managers (#5639) Previous PR (#5605) was missing some documentation needed to help the user remember what change was made to effect a particular file", "[AIRFLOW-4883] Bug-fix for inline file process managers (#5639) Previous PR (#5605) Also added some documentation needed to help the user understand the effect a particular file", "[AIRFLOW-4883] [AIRFLOW-4958] [RFC5436] Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation needed to make the user understand the effect a particular file"], "perturbed_original": ["[AIRFLOW-4883] Bug-fix for Kill hung file process . Previous PR (#5605) was missing some code after a rebase. This adds the original solution, which also adds unit tests", "[AIRFLOW-4883] Bug-fix for hung file process managers (#5639) Previous PR (#5605) was missing some of the code and needed a rebase. This adds the code and adds unit tests", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing function tests after a rebase. This PR fixes code and adds unit tests", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some code after a rebase. This PR fixes the code but also adds unit tests", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) Remove some code after a rebase. This adds the code and the unit tests", "[AIRFLOW-4883] Bug-fix ing for hung file process managers (#5639) Previous PR (#5605) was missing some tests and a rebase. This adds the code and adds unit tests", "[AIRFLOW-4883] Bug-fix for Kill -Bot process managers (#5639) Previous PR (#5605) was missing some tests so required a rebase. This adds the code and adds unit tests", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some code - so rebase. This adds refactoring, fixes bug and adds unit tests", "Fix issue for Kill hung file process managers (#5639) Previous PR (#5605) was missing some code after a change. This PR adds the code and adds unit tests", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous version was missing some code after a small bug. This adds the code and adds unit tests"], "original_ll": -5.241806983947754, "sampled_ll": -5.1595635414123535, "all_perturbed_sampled_ll": [-5.182625770568848, -5.390768051147461, -5.164880752563477, -4.9222187995910645, -4.930253028869629, -5.205637454986572, -5.3711652755737305, -5.119427680969238, -4.904646873474121, -4.679994583129883], "all_perturbed_original_ll": [-5.378845691680908, -4.697017192840576, -5.239368915557861, -4.968320846557617, -5.426975250244141, -5.348637104034424, -5.191740989685059, -5.082215785980225, -5.120849132537842, -5.4409685134887695], "perturbed_sampled_ll": -5.087161827087402, "perturbed_original_ll": -5.189493942260742, "perturbed_sampled_ll_std": 0.21307024658017545, "perturbed_original_ll_std": 0.22158105163716957}, {"original": "Remove locks for upgrades in mssql (#17213) Closes: #17088", "sampled": "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "perturbed_sampled": ["Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "Remove locks for upgrades in mssql (#17213) Closes: #17088The"], "perturbed_original": ["Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "Remove locks for upgrades in mssql (#17213) Closes: #17088"], "original_ll": -4.8284101486206055, "sampled_ll": -5.301568984985352, "all_perturbed_sampled_ll": [-5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352, -5.301568984985352], "all_perturbed_original_ll": [-4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055, -4.8284101486206055], "perturbed_sampled_ll": -5.301568984985352, "perturbed_original_ll": -4.8284101486206055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "sampled": "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "perturbed_sampled": ["[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This"], "perturbed_original": ["[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)"], "original_ll": -5.327218055725098, "sampled_ll": -5.688559055328369, "all_perturbed_sampled_ll": [-5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369, -5.688559055328369], "all_perturbed_original_ll": [-5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098, -5.327218055725098], "perturbed_sampled_ll": -5.688559055328369, "perturbed_original_ll": -5.327218055725098, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not working on Windows. This fixes the problem by turning the error into warning, and directing the users to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also in our quick-start docker-compose.", "sampled": "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "perturbed_sampled": ["Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had some broken behaviour for non-standard inputs. This update adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about your running containers. For example: docker image .exe --format=command --type=command=node <docker-compose.yml> will set --extra-debug to the options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of additional warn s that should prevent that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug as one of the options for the docker-compose.yml . The --extra-debug option is useful when you want to see how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation (#18164) The quick-start documentation for docker-compose had a bit broken for non-standard inputs. This update adds a lot of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker -compose --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "[x86] docker-compose warnings and documentation (#18164) The recently updated docker-compose add-on had a bit broken behaviour for non-standard inputs. This update adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node , set --extra-debug and add to --extra-info for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation (#18164) : Before this release the initial updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of options to help me about that: Use --extra-debug instead of --extra-info to get additional information about your running containers. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set the --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated package does have a bit broken behaviour for containers. This update adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example, running docker-compose image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info to optional in the docker-compose.yml . The --extra-debug option is useful when you want to see how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation The recently updated docker-compose had a bit broken behaviour for non-standard inputs. Docker-compose 2.2 adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about your containers running. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to know more about how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of warnings to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about what is running . For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set only --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to get additional information about how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation for advanced users The most recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of warnings and helpers to warn you about that: [x86] Use the full text of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> This update creates the --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a small set of documentation options to warn you about that: [x86] Use any of the optional options available in the docker-compose-config to get additional information about running containers. For example: [x64] --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by"], "perturbed_original": ["Improves quick-start docker-compose warnings for non-linux users. (#18164) The recently introduced quick-start docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not provided on Linux but on Windows. This fixes the problem by turning the prompt into a warning, and directing the users to the right instructions per operating system. (#17959) Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also in our quick-start docker-compose.", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken for Windows and non-Linux users. It expected the host to be created always, but the instructions to create them were not working on Windows. This fixes the issue by turning the error into warning, and directing the users to the right instructions per operating system. Also the support was added for PUT signals, which will allow to handle signals properly also in our quick-start docker-compose.", "Improves quick-start code and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the <unk>bin<unk> file to be created always, but the instructions to create them were not working . This fixes the problem by turning the error into warning, and directing the users to the right instructions given by the system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also during quick-start docker-compose.", "Improves quick-start docker-compose warnings and instructions: The recently updated docker-compose had a bit broken behaviour for non-Linux users. It meant that there needs an .env file to be created always, but the instructions to create them were not working on Windows. This fixes the problem by turning the error into warning, and directing them to the right instructions per operating system. Also the option to change notifications was added for worker to allow to handle signals received later. Adds some more support in debug output. Added the workaround in our quick-start docker-compose.", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the worker to be created always, but the instructions to create them were not working on Windows. This code fixed this problem by turning the error into warning, and directing the user to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to set it up properly also in the update docker-compose.", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for some time. It expected the .env file to be created instead of using the instructions it had set, but some of them were not working on Windows. This refixed the problem by turning the error into warning, and pointed users to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also in our quick-start docker-compose.", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions needed to create them were not working on Windows. This fixes the problem by turning the error into an error message, directing you to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added to the docker-env config to allow to handle signals properly also in our quick-start docker-compose.", "Improves quick-start docker-compose UI and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not working on Windows. This fixes the problem by turning the error into warning, and directing the user to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` change was a bit harder for worker to allow these signals properly also in our quick-start docker-compose.", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It asked the .env file to be created always, but the instructions to create them were not working on Windows. This commit fixes the problem by turning the error into warning, and direct the users to the right instructions per operating system. Also the new output interface was added for worker to allow to handle signals properly also for quick-start docker-compose.", "Improves quick-start docker-compose warnings and error handling for non-Linux users. The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not working . This fixes the issue by turning the error into warning, which refers the users to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to be able to handle signals properly also in our quick-start docker-compose."], "original_ll": -3.794034957885742, "sampled_ll": -2.4158170223236084, "all_perturbed_sampled_ll": [-2.7200100421905518, -2.622623920440674, -2.368548631668091, -2.6292037963867188, -2.5150389671325684, -2.488996982574463, -2.3016927242279053, -2.5141947269439697, -2.648144245147705, -2.6355764865875244], "all_perturbed_original_ll": [-3.58241868019104, -3.5641303062438965, -4.119047164916992, -3.8133625984191895, -3.810433864593506, -3.9460089206695557, -3.6672117710113525, -3.868002414703369, -3.663797378540039, -3.683358669281006], "perturbed_sampled_ll": -2.544403052330017, "perturbed_original_ll": -3.7717771768569945, "perturbed_sampled_ll_std": 0.12565827514000968, "perturbed_original_ll_std": 0.16517305619023162}, {"original": "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "sampled": "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "perturbed_sampled": ["Use the same command on Apache Infrarot. Update for Apache Airflow (#14276). Update for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the new version for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Docker repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow upstack repo (#1573) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow . Apache Airflow Dockerflow repository (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Use the correct link to Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://upstack.apache.org/apache/apache/apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow Dockerhub repo https://dh.apache.org/docker/apairflow. Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hordes.apache.org Use the correct link for Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Use the correct link for Apache Airflow to open it. Apache Airflow Dockerflow repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/"], "perturbed_original": ["Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires CI build, but https://hub.docker.com/r/apache/airflow does not", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth to use if it does not", "Use the correct credentials. Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth to work... but does not", "Use the correct link for Apache Airflow Dockerhub repositories. For example, https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "Use the correct repo. Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "Use the correct link for Apache Airflow Dockerhub repo . https://hub.docker.com requires auth while https://hub.docker.com/r/apache/airflow does not", "Use the correct link for Apache Airflow to use (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "Use the correct link for Apache Airflow Dockerhub and avoid any confusion, for example, https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "Use the following public keys to sign for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not"], "original_ll": -3.3570971488952637, "sampled_ll": -2.6647961139678955, "all_perturbed_sampled_ll": [-3.023782253265381, -2.594594717025757, -2.66209077835083, -2.8090567588806152, -2.583425283432007, -2.660419225692749, -3.2775144577026367, -3.217409610748291, -3.3413493633270264, -2.631659746170044], "all_perturbed_original_ll": [-3.291307210922241, -4.0577802658081055, -3.285470962524414, -4.101362228393555, -2.820977210998535, -3.2698497772216797, -3.6719095706939697, -3.192476511001587, -2.870293617248535, -3.261387586593628], "perturbed_sampled_ll": -2.8801302194595335, "perturbed_original_ll": -3.382281494140625, "perturbed_sampled_ll_std": 0.2893437574664681, "perturbed_original_ll_std": 0.41417206070650986}, {"original": "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and attach to it.", "sampled": "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "perturbed_sampled": ["Add reattach flag to ECSOperator (#10643) ..so that whenever the server restarts, it does not leave rogue ECS drivers exposed. We still want to do the following, however. #10644 Update the documentation about \"Air", "the flag s (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "Add reattach flag to ECSOperator (#10643) ..so that whenever the ECSOperator restarts, it does not leave the external drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "Add reattach flag to ECSOperator (#10643) so that whenever the Airflow server restarts, it does not leave rogue ECS drivers installed. Airflow should still expect the following, however. #10644 Update the documentation about \"Air", "Add reattach flag s (#10643) ..so that whenever the server restarts, it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server is reconnected does not leave rogue ECS drivers exposed. We still expect the following, however. Read the documentation about \"Air", "Add reattach flag to ECSOperator (#10643) ..so that when the Airflow server restarts, it does not leave rogue ECS drivers exposed. We add the following, however. #10644 Update the documentation about \"Air", "a flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers exposed. We still expect the results to be #10644 Update the documentation about \"Air", "Add reattach support for ECSOperator (#10643) ..so that whenever the Airflow API connects if it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "Add update to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update s notes about \"Air"], "perturbed_original": ["Add reattach flag to ECSOperator (#10643) ..so that whenever the ECS Operator restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and reconnect to it.", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave any idle Active Tasks. Instead the operator will seek for any running Task and re attach to it.", "Add reattach flag for Task Attachment (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any Task in Airflow and attach to it.", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow operator reattaches, it does not leave rogue ECS Tasks. Instead the operator will seek for a clean Task instance and attach to it.", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS instances on the table ... it will seek for any running instance and attach to it.", "Add reattach flag to Airflow Service (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead , Airflow services will seek for any running instance and attach to it.", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead it will seek for any running instance and restart it.", "Add reattach flag to ECSOperator (#10643) ..so that whenever ecs server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for the new instance and attach to it.", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server logs out the operator does not leave the EC Tasks. Instead the operator will seek for any running instance and attach to it.", "Add reattach flag to ECSOperator (#10643) ..so that when the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any existing task and attach to it."], "original_ll": -5.026678085327148, "sampled_ll": -4.861417293548584, "all_perturbed_sampled_ll": [-4.551736831665039, -5.168227672576904, -4.573604106903076, -4.592848300933838, -5.146722316741943, -5.213467121124268, -4.862510681152344, -5.150620937347412, -5.243740558624268, -5.148125648498535], "all_perturbed_original_ll": [-4.578775405883789, -5.035303592681885, -4.9817023277282715, -4.821596622467041, -4.944694995880127, -4.788342475891113, -5.061907768249512, -4.713385581970215, -5.1135382652282715, -4.84515905380249], "perturbed_sampled_ll": -4.9651604175567625, "perturbed_original_ll": -4.8884406089782715, "perturbed_sampled_ll_std": 0.27472253526384166, "perturbed_original_ll_std": 0.16033985739488893}, {"original": "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "sampled": "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "perturbed_sampled": ["Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New"], "perturbed_original": ["Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Add aws_conn_id to DynamoDBToS3Operator (#20363)"], "original_ll": -5.383778095245361, "sampled_ll": -5.835010051727295, "all_perturbed_sampled_ll": [-5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295, -5.835010051727295], "all_perturbed_original_ll": [-5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361, -5.383778095245361], "perturbed_sampled_ll": -5.835010051727295, "perturbed_original_ll": -5.383778095245361, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't need to compile Java code.", "sampled": "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using jdt-unstable the packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "perturbed_sampled": ["Use the JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using jdt-unstable the packages used to create the binary are renamed.\n\nThe subdirectory is now called .\n\nversion.\n\nWhen running the JIT compiler, new version values will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version JRE/JRE-<version>-<br> can be found here .\n\n. When installing this branch, the packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "Use Debian's provided JRE from Buster (#8919) Installing packages (not even the JRE) from Sid is starting to break things. The new version of Debian's JRE can be found here .\n\n. When using jdt-unstable the packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT the version flags will be passed with each JIT command. The default is , but a value of 1 is better. See the -f option when compiling.\n\noption is passed only when building the binary. The default is , but a value of 1 is better. See the -f option when", "Use Debian's provided JRE from jdt-unstable. Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of jdt-unstable can be found here .\n\n. When using jdt-unstable the packages used to build the binary are renamed.\n\nThe -D package is now called .\n\nversion.\n\nWhen running the JIT command, version flags will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "Use Debian's provided JRE s. (#8919) Installing the JDK (not even the old code) : Sid is starting to fix many things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using jdt-unstable the packages used by the binary are renamed.\n\nThe /boot directory is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "Remove this provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> / can be found here . Note that using jdt-unstable / is used to build the compiler for this release. Also, the JIT partition has been renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of Suse can be found here .\n\n. When using JIT, the executable packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen using the JIT compiler, these flags will be passed with each JIT command. The default is , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "Use Debian's provided versions. Buster (#8919) Installing anything except JIT (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using jdt-unstable the packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new er version values will be passed with each JIT command. is an optional option. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption . The default is 0, but a value of 1 is better. See the -f option when", "Use Debian's provided JRE from Buster (#8919) because the supplied JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using JIT compiler, JIT-compiled packages used to build the binary are renamed.\n\nThe /boot partition directory will be called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default value is , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default value is , but a value of 1 is better. See the -f option when", "Use the JRE . (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here. When using the JIT compiler, the packages used to build the binary are now grouped by version. This partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when"], "perturbed_original": ["Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the latest) on Sid is starting to break on Buster : > Some versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI scenario to: 1. Depend on something from Sid (unstable, packages change/get updated) when we are using Buster (unstable, security fixes). 2. Installed the JRE, not the JDK. We are now unable to compile Java code.", "Use Debian's provided JRE from Buster (#8919) because the JDK (not only the JRE) from Sid is going to break on Buster as the JRE and JDK packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, never updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't need to upgrade this code.", "Use the JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of Sid conflict: > The following packages have unmet dependencies on libgcc-8-dev : Depends: libmpx2 [(>= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, packages are not missing) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't need to compile Java code.", "Use Debian's provided JRE from Buster (#8919) Installing the JFile (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: The following packages have unmet dependencies: > Depends : libmpx2 (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. 3. Removed need to compile Java code.", "Use Debian's provided JRE from Buster . Install the JDK (not even the JRE) from Sid . Packages start to break on Buster as the versions of packages get changed. The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, packages change/get updated) when instead using Buster (stable, only install things from Sid) 2. Installed the JRE, not the JDK. We don't need any of these packages to compile Java code.", "Use Debian's provided JRE from Buster (#8919) The JDK (not JRE JRE) from Sid is starting to break on Buster as the versions of packages conflict: The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (>= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not to use JDK from Sid (unstable, packages change/get updated) when we are using Buster (no release, no security fixes). 2. Installed the JRE, not the JDK. We don't need to compile Java code.", "Use Debian's provided JRE from Buster! > Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev > Depends: > gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed We updated our CI docker images to: 1. Not install something from Sid (unstable, packages ) when we were using Buster (stable, only security fixes). 2. Installed the JRE, not Sid. We don't need to restart the code.", "Use Debian's provided JRE . Re: Java (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev : Depends: libgcc-8 (= 8.4.0-4) but > 9.2.0-0 is missing to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is missing to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't use Sid to compile Java code.", "Use Debian's build from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages change- The following packages have two dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > libmpx2 : Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. We don't want to install something from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We only need to compile Java code.", "Use the JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as some of packages conflict: > some packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed this changes the docker images for Buster a bit because of dependency conflicts. 1. Not install something from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't need to compile Java code."], "original_ll": -3.1274375915527344, "sampled_ll": -2.6779944896698, "all_perturbed_sampled_ll": [-2.702619791030884, -2.748960018157959, -2.9628400802612305, -2.5333566665649414, -2.883854389190674, -2.892775058746338, -2.792711019515991, -3.1047847270965576, -2.759289026260376, -2.635310411453247], "all_perturbed_original_ll": [-3.1852433681488037, -3.026761531829834, -3.148963212966919, -3.3117401599884033, -3.072962522506714, -3.1176819801330566, -3.1537723541259766, -3.1847097873687744, -3.0006656646728516, -3.2196762561798096], "perturbed_sampled_ll": -2.80165011882782, "perturbed_original_ll": -3.142217683792114, "perturbed_sampled_ll_std": 0.15717569408371102, "perturbed_original_ll_std": 0.08794252474460457}, {"original": "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "sampled": "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "perturbed_sampled": ["[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS"], "perturbed_original": ["[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)"], "original_ll": -6.312582492828369, "sampled_ll": -6.2323899269104, "all_perturbed_sampled_ll": [-6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104, -6.2323899269104], "all_perturbed_original_ll": [-6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369, -6.312582492828369], "perturbed_sampled_ll": -6.2323899269104, "perturbed_original_ll": -6.312582492828369, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve typing in airflow/models/pool.py (#9835)", "sampled": "Improve typing in airflow/models/pool.py (#9835)As", "perturbed_sampled": ["Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As", "Improve typing in airflow/models/pool.py (#9835)As"], "perturbed_original": ["Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)", "Improve typing in airflow/models/pool.py (#9835)"], "original_ll": -6.227518081665039, "sampled_ll": -6.864846229553223, "all_perturbed_sampled_ll": [-6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223, -6.864846229553223], "all_perturbed_original_ll": [-6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039, -6.227518081665039], "perturbed_sampled_ll": -6.864846229553223, "perturbed_original_ll": -6.227518081665039, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "perturbed_sampled": ["[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker execution being terminated with failure * [AIRFLOW-6086] Fix error handling with get_class() * [AIRFLOW-6086] Set parent of worker", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug in which it wasn't possible to remove sub-classes (class and member) with the method get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6092] Set parent of worker", "[AIRFLOW-6062] Executor would only delete workers in its own class * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) from worker * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Fixed fix for changing parent of worker", "that would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error message when creating new worker class for set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "[AIRFLOW-6062] Executor would only delete workers in the named namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class name and namespace) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't stopped in the namespace when run with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * Fix error handling with exception handling * [AIRFLOW-6086] Corrected status of worker", "[AIRFLOW-6062] Executor would only show an instance of an object with name in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being stopped with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set up of the worker", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove workers (in type and object) with get_class_with_name() * [AIRFLOW-6086] Fixed a bug where a worker would keep on being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "[AIRFLOW-6062] Executor would only delete workers in their namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with set_child_with_name() * [AIRFLOW-6086] Fixed crash for instances where an object wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed bug where it wasn't possible to remove sub-classes (class and object) from the awl.rb * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with the end flag. * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker"], "perturbed_original": ["[AIRFLOW-6062] Executor would only delete pods in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * static resize tarball * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete workers in its own namespace * fix for version #7143 * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests for up stream static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * Adds patch to fix that Executor would only delete pods in its own namespace * add a clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * update PR * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Fixes: * new version would only delete workers in its own namespace (#7123) * new version would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR patches * add tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6062] Executor would only exist in its own namespace . [AIRFLOW-6062] Executor would only exist in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -2.0847816467285156, "sampled_ll": -2.649420976638794, "all_perturbed_sampled_ll": [-2.8662214279174805, -2.7384464740753174, -2.693122386932373, -2.722430944442749, -2.736924648284912, -3.0444393157958984, -2.7203211784362793, -2.663691520690918, -2.7481911182403564, -3.0424041748046875], "all_perturbed_original_ll": [-2.0212719440460205, -2.1728928089141846, -2.07853627204895, -2.100980043411255, -2.2034289836883545, -2.003941059112549, -1.9356533288955688, -2.0124435424804688, -2.0739104747772217, -1.9560809135437012], "perturbed_sampled_ll": -2.7976193189620973, "perturbed_original_ll": -2.0559139370918276, "perturbed_sampled_ll_std": 0.1325363948627842, "perturbed_original_ll_std": 0.08278376301024963}, {"original": "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "sampled": "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "perturbed_sampled": ["[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari"], "perturbed_original": ["[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)"], "original_ll": -5.82694673538208, "sampled_ll": -6.352202892303467, "all_perturbed_sampled_ll": [-6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467, -6.352202892303467], "all_perturbed_original_ll": [-5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208, -5.82694673538208], "perturbed_sampled_ll": -6.352202892303467, "perturbed_original_ll": -5.82694673538208, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster.", "sampled": "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume the redshift cluster.", "perturbed_sampled": ["Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator to your schema. These operators provide the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to temporarily freeze and resume the redshift cluster.", "Add the RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) : provide the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators add the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide additional operations to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator To (#19665) These operators provide the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator . These operators provide the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume the redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators give the ability to pause and resume the redshift cluster."], "perturbed_original": ["Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause or resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These provide the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators add the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) : provide the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator operators to clusters. These operators provide the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators give you the ability to pause and resume a redshift cluster.", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster.", "Add the RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster."], "original_ll": -3.6216113567352295, "sampled_ll": -3.6062259674072266, "all_perturbed_sampled_ll": [-3.1998817920684814, -3.9174821376800537, -4.350191116333008, -3.7749366760253906, -3.725046157836914, -3.7519900798797607, -4.285757064819336, -3.141733407974243, -3.6062259674072266, -3.7149415016174316], "all_perturbed_original_ll": [-3.6301708221435547, -3.6216113567352295, -3.701451539993286, -3.6216113567352295, -3.7390825748443604, -3.787177801132202, -3.34820294380188, -3.543731927871704, -3.6216113567352295, -4.390271186828613], "perturbed_sampled_ll": -3.7468185901641844, "perturbed_original_ll": -3.700492286682129, "perturbed_sampled_ll_std": 0.3702675943533817, "perturbed_original_ll_std": 0.2563382029495819}, {"original": "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "sampled": "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "perturbed_sampled": ["[AIRFLOW-5850] Capture d by XStream in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The Server is configured to be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Showing logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) ? * The client connects to the Server using docker swarm. The Client can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task logs in the browser. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture pending exception in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connections will be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "[AIRFLOW-5850] Capture task logs . #6551 (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be * [AIRFLOW-5850] The client connects to the Server using docker swarm. The"], "perturbed_original": ["[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Use the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested objects from the docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: get the blocks in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmExamples * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the nested blocks in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove the mock in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove mock from a command in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in docker swarm * [AIRFLOW-5850] Fix the mock in the docker swarm tests * Bug me: Remove nested blocks in docker swarm operator", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator"], "original_ll": -3.2144367694854736, "sampled_ll": -3.1164932250976562, "all_perturbed_sampled_ll": [-3.2474868297576904, -3.573328971862793, -2.9989514350891113, -3.6623780727386475, -3.096543788909912, -3.025770425796509, -3.762963056564331, -2.763653039932251, -3.1415677070617676, -3.216919422149658], "all_perturbed_original_ll": [-3.2395851612091064, -3.6322813034057617, -3.2046737670898438, -3.2144367694854736, -3.0576953887939453, -3.0564541816711426, -3.136718988418579, -3.2159361839294434, -3.9322526454925537, -3.2144367694854736], "perturbed_sampled_ll": -3.2489562749862673, "perturbed_original_ll": -3.2904471158981323, "perturbed_sampled_ll_std": 0.30396745939166814, "perturbed_original_ll_std": 0.2623688961730123}, {"original": "Fix typo in docker-stack documentation (#16221)", "sampled": "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "perturbed_sampled": ["Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed"], "perturbed_original": ["Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)", "Fix typo in docker-stack documentation (#16221)"], "original_ll": -5.430027961730957, "sampled_ll": -4.810083866119385, "all_perturbed_sampled_ll": [-4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385, -4.810083866119385], "all_perturbed_original_ll": [-5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957, -5.430027961730957], "perturbed_sampled_ll": -4.810083866119385, "perturbed_original_ll": -5.430027961730957, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "sampled": "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "perturbed_sampled": ["[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue \" [EN] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] \"AirFlow SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "Messages \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue \" [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue \" [INFO] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue as part of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "; \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue support of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] Revert \"Queue threading used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of a Reliable Queue\" [SEL] [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]"], "perturbed_original": ["[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Multiprocessing should be used instead of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue , instead of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used with SynchronizedQueue used in multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of \" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used for multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" (#5191) This patch is 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" (#5191) This is 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of SQ\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue for use in the early stages of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63."], "original_ll": -4.546453952789307, "sampled_ll": -3.7181692123413086, "all_perturbed_sampled_ll": [-3.805042266845703, -3.8936562538146973, -3.5305347442626953, -3.745082139968872, -3.717181444168091, -3.680147647857666, -3.633681297302246, -3.785982608795166, -3.6798934936523438, -3.36165452003479], "all_perturbed_original_ll": [-4.5693464279174805, -4.119050979614258, -4.574930191040039, -4.4720540046691895, -4.769906520843506, -4.578031539916992, -4.912984848022461, -4.917258262634277, -4.692971229553223, -4.492265224456787], "perturbed_sampled_ll": -3.683285641670227, "perturbed_original_ll": -4.6098799228668215, "perturbed_sampled_ll_std": 0.142778230003935, "perturbed_original_ll_std": 0.22282004773499883}, {"original": "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "sampled": "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "perturbed_sampled": ["Fix occasional cleartask failures . The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. Changing CLEARTOOL to default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) Cleartask tests occasionally failed due to not consistent sequence after task clearing was performed. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The test sequence was changed to prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to insufficient information about the sequence in which task clearing happened. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was finished. A change to the default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is shorter than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) Cleartask tests occasionally failed due to not consistent sequence in which task clearing was done. A more consistent default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed because of the not consistent sequence in which task clearing was performed. The default should prevent this from failing. Fix (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to difficulties in consistent sequence in which task clearing was performed. The test manager updated the sequence to prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix for cleanup failures (#18859) The clearing of tasks has occasionally failed due to not consistent sequence in which task clearing was performed. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not finding the tasks in which clearing was performed. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\""], "perturbed_original": ["Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent return of the order in which task had been performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not always return tasks with expected ordering and sometimes the tasks were returned in different order than expected.", "Fix occasional cleartask failures . Some cleartask tests occasionally failed due to not consistent query order following which task clearing was performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not have ordering and sometimes the query returned in different order than were expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which a cleartask was performed. The query did not have ordering and sometimes the tasks were returned in order or delayed further than expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which cleartask testing was performed. The cleantask tests did not have ordering and sometimes the tasks were returned in different order than expected.", "Fix occasional cleartask failures . Cleartask and cleartask tests occasionally failed due to not consistent sequence on which task clearing was performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not always follow the expected sequence for task ordering and sometimes the tasks were cleared in different order than expected.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not have ordering and sometimes the tasks were cleared in different order than expected."], "original_ll": -4.659791469573975, "sampled_ll": -4.204808235168457, "all_perturbed_sampled_ll": [-4.4806623458862305, -4.426743984222412, -4.08473539352417, -4.03287935256958, -4.3484907150268555, -4.281770706176758, -4.206188201904297, -4.215198516845703, -4.0846662521362305, -4.096540451049805], "all_perturbed_original_ll": [-4.494734287261963, -4.495472431182861, -4.720515727996826, -4.809906482696533, -4.536491394042969, -4.659791469573975, -4.262472629547119, -4.607588768005371, -4.358668804168701, -4.610660552978516], "perturbed_sampled_ll": -4.225787591934204, "perturbed_original_ll": -4.555630254745483, "perturbed_sampled_ll_std": 0.14756707267056132, "perturbed_original_ll_std": 0.15519342244259762}, {"original": "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "sampled": "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "perturbed_sampled": ["Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The"], "perturbed_original": ["Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add error check for config_file parameter in GKEStartPodOperator (#17700)"], "original_ll": -6.057900905609131, "sampled_ll": -6.470308303833008, "all_perturbed_sampled_ll": [-6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008, -6.470308303833008], "all_perturbed_original_ll": [-6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131, -6.057900905609131], "perturbed_sampled_ll": -6.470308303833008, "perturbed_original_ll": -6.057900905609131, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "sampled": "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "perturbed_sampled": ["Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>"], "perturbed_original": ["Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>"], "original_ll": -5.106704235076904, "sampled_ll": -4.6911468505859375, "all_perturbed_sampled_ll": [-4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375, -4.6911468505859375], "all_perturbed_original_ll": [-5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904, -5.106704235076904], "perturbed_sampled_ll": -4.6911468505859375, "perturbed_original_ll": -5.106704235076904, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "sampled": "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "perturbed_sampled": ["Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The"], "perturbed_original": ["Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)"], "original_ll": -4.794984340667725, "sampled_ll": -5.170685768127441, "all_perturbed_sampled_ll": [-5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441, -5.170685768127441], "all_perturbed_original_ll": [-4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725, -4.794984340667725], "perturbed_sampled_ll": -5.170685768127441, "perturbed_original_ll": -4.794984340667725, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add some basic metrics to the Triggerer (#18214)", "sampled": "Add some basic metrics to the Triggerer (#18214)In", "perturbed_sampled": ["Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In", "Add some basic metrics to the Triggerer (#18214)In"], "perturbed_original": ["Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)", "Add some basic metrics to the Triggerer (#18214)"], "original_ll": -5.445986747741699, "sampled_ll": -5.9782538414001465, "all_perturbed_sampled_ll": [-5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465, -5.9782538414001465], "all_perturbed_original_ll": [-5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699, -5.445986747741699], "perturbed_sampled_ll": -5.9782538414001465, "perturbed_original_ll": -5.445986747741699, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the config, while it should be int like the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was forgotten. I think we can safely remove it because: * There", "sampled": "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used with XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "perturbed_sampled": ["[AIRFLOW-6740] Remove Undocumented, broken bug that allows you to remove PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing global variables for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used with XFree86-EFI-32 . This creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was the primary error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (#6936) Fix the unused global values for the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow that option to be used with XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add global values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the new -E option to be used with XFree86-EFI-32 (#6935), which causes bugs\n\n(X11) Fix the case where XFree86-EFI-32 uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Fix the unused default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E mode of XFree86/QNX to be used with this test which creates many bugs\n\n(X11) Fix the issues when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, faulty PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing line for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E flag to be used with XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE . (SAP) Fix unused global variables for FUNC_STARTUP, FUNCTIONS , && (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used for XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove the dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default s for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used on the same port as XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused start variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used with XFree86-EFI-32 because using the right value creates many bugs\n\n(X11) Fix an issue when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE S and for the unused parameter from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP . (RHEL) Modify to allow the -E option to be used with XFree86-EFI-32 (#6935), which causes bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously deprecated because it would create a duplicate in missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the same XFree86_FMI to be used with XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the bug where XFree86-CORE/QNX uses"], "perturbed_original": ["* Undocumented, deprecated, and untested. (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is not deprecated by werkzeug. The value is acquired as string from the config, while it should be int like the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was not. I think we can safely remove it because: * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This API has been deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it 's broken. The value is acquired as string from the config, while it should be int . The other APIs are useless. Those were fixed in #6901, but `num_proxies` was forgotten. I think we can safely remove it because: * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is no longer supported by werkzeug, since it is broken. * However, it is also broken. The value is acquired as string from the config, but should be int like the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was forgotten. I think we can safely remove it because: * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The parameter is acquired as string from the config, while it should be int like the other variables. Those were fixed in #6901, but `num_proxies` still breaks, which I think we can safely ignore because: * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the proxy list name, it should be int like the values in `x_*` attributes. Those were fixed in the core werkzeug. But the `num_proxies` was forgotten. I think we can safely remove it. * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is documented in werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as a long in the parameter, where it should be int like the other `x_*` attributes. Those were fixed in #6901, but maybe one day, they may be forgotten. I think we can safely remove it because: * There", "[AIRFLOW-6740] Warning: This parameter is deprecated, see (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the script, while it should be int like the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was forgotten. I think you can safely remove it because: * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also a problematic one. * The value is acquired as string from the config, while it should be int like the above attributes. This was fixed in #6901, but was forgotten. I think we can safely remove it because: * There", ": * Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 . If the fix is not updated to L119, the parameter is also broken. The value is acquired as string from the config, while it should be int like the other `x_*` attributes. Those were checked off in #6901, but `num_proxies` was forgotten. I think we should remove it because: * There", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated , see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the config, while it should be assigned to it using the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was added as dependency in #7485. I think we can safely remove it because: * There"], "original_ll": -3.320007801055908, "sampled_ll": -2.675156831741333, "all_perturbed_sampled_ll": [-2.731111764907837, -2.9757792949676514, -2.6002049446105957, -2.7789855003356934, -2.7194502353668213, -3.030338764190674, -2.7641055583953857, -2.7471702098846436, -3.250485897064209, -3.075063705444336], "all_perturbed_original_ll": [-2.9856836795806885, -3.3561651706695557, -3.9735546112060547, -3.3571619987487793, -3.348914861679077, -3.4438891410827637, -3.2853269577026367, -3.3526437282562256, -3.261230707168579, -3.370177745819092], "perturbed_sampled_ll": -2.867269587516785, "perturbed_original_ll": -3.373474860191345, "perturbed_sampled_ll_std": 0.19312504830302332, "perturbed_original_ll_std": 0.23182286691335804}, {"original": "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary rather than a NoneType since the argument should be a dictionary __Why__ * Building out type annotations is good for the code base * The query parameter is accessed with an index at one point, which means that it cannot be a None type, but should rather be defaulted to an empty dictionary if not provided * Remove useless return", "sampled": "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the code (Ezra), add support for a .getMethod that gets an instance and is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "perturbed_sampled": ["Add type annotations to ZendeskHook, update unit tests * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for the Zendesk class: update the code (Ezra), add s a .getMethod that gets an instance and is then processed of methods : type/getInstance() , method/getAllMethodsExpr() . (#11003) * Support for Zendesk, on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook __getMethod; upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the code (Ezra), add support for a hook that gets an instance and is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#10888) * Support for .getMethod() on Zendesk class::getInstance() . (#10888) * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using class instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the tests for Z", "* Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the code (Ezra), add support for a .getMethod that gets an instance , which is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Addtype annotations to ZendeskHook, upgrade the tests for ZDesk. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook, upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: :getInstance()! Fix error code (Ezra), add support for a .getMethod that gets an instance of the method and is then passed to methods like get method/getAllMethodsExpr() . * Support for .getMethod() on Zendesk class::getInstance() ! code (Ezra) * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of types . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "Add type annotations to ZendeskHook, update unit tests * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() in Zendesk Get the code (Ezra), add support for a .getMethod that gets an instance and then an instance can be passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Add type annotations to ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update unit test (#10032) (Ezra), add support for a .getMethod that gets a method instance and is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the test for ZDE .getMethod() . * Support for ZendeskHook __getMethod() using a type annotation instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) * Support for .getMethod() on Zendesk class: update the unit tests to add support for a .getMethod that gets an instance and is then passed to methods like getInstance() or get method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations and upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations , add/ upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) * Support for .getMethod() on Zendesk class: :getInstance()* In the code (Ezra), add support for a .getMethod that gets an instance and is then used to get all methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Refactor ZendeskHook __getMethod() using ZendeskExpr types, add unit tests .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the tests, add support for a method that gets an instance and then is passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() . * Support for ZendeskHook __getMethod() using ZendeskExpr () for the sake of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). * - Support for .getMethod() on Zendesk Hooks * Updated the code (Ezra), add support for a .getMethod that gets an instance and is then passed to methods like a method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . * Add type annotations to ZendeskHook; upgrade the test for Z"], "perturbed_original": ["Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update the unit test to call an empty dictionary rather than calling the same method since the argument should be a dictionary __Why__ * Building out type annotations is good for the code base * A single query parameter is accessed with a None type at one point, which means that it cannot be a None type, but should rather be defaulted to not being a dictionary if not provided * Use of return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ is the correct type annotation for the ZendeskHook query method in case of error! * Change one unit test to call an empty dictionary rather than a NoneType since the argument should be a dictionary by default * Building out type annotations is good for the code base * The query parameter was modified to be a number with an index at one point, which means that it cannot be a None type, but should rather be defaulted to a dictionary if not provided * Remove useless return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary . * Remove the callback to a NoneType since the argument should be an Array __Why__ * Building out type annotations is good for the code base * The query parameter is accessed with an index at the query parameter, which means that it cannot be a None type, but should rather be defaulted to an empty dictionary if not available * Remove useless return", "Add type annotations to ZendeskHook, and update one unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one unit test to return an empty dictionary rather than None, since the answer MUST be a dictionary __Why__ * Building out type annotations is good for the code * The query string will be accessed with an index at one point, which means that it cannot be a None type, but should rather be defaulted to an empty string if not provided * Remove useless return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to call each method * Update one unit test to call an empty dictionary rather than a None type, when the argument should be a dictionary __Why__ * The lack of type annotations is not good for the code base * The query parameter should be replaced with an index at one point, which means that it should not be a None type, but should rather be defaulted to an empty dictionary in cases where an index is not provided * Remove useless return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to methods and methods at the constructor level __What__ * Add correct type annotations to ZendeskHook and each method on it * Move at least one unit test to call an empty dictionary method that returns a NoneType since the argument should be a dictionary __Why__ * Building out type annotations is good for the code * The query should be accessed with an index at one point, which means that it cannot be a None type, but should be defaulted to an existing dictionary if not provided * Remove useless return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one method to call an empty dictionary rather than a NoneType since the argument should be a dictionary * Building out type annotations in ZendeskHook for the entire code base * The query parameter is accessed with an index at one point, which means it cannot be a None type, but should rather be called an empty dictionary if necessary * Remove useless return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook and Update Unit Test * Add correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary rather than NoneType since query parameter type should be a dictionary __Why__ * Building out type annotations is good for the code base * The query parameter is accessed with an exception that is disabled at one point, which means that it cannot be a None type, but should rather be defaulted to an empty dictionary if not provided * Remove useless return", "Add type annotation to ZendeskHook, update unit tests of each method * Add type annotations to ZendeskHook __What__ * Add the correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary rather than a NoneType since the argument cannot be a dictionary __Why__ * Building out type annotations is good for the code base * The query parameter is accessed with an index at one point, which means that it cannot be a NoneType, but should rather be defaulted to an empty dictionary if not provided * Remove useless return", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook * Add correct type annotations to ZendeskHook * Get the return type for each method in one unit test * Correct ReturnType will always call an empty dictionary rather than a NoneType since the argument should be a dictionary __Why__ * Building out type annotations is good for the code * The query parameter is accessed with an index at one location. This means that it cannot be of another type, but should rather be defaulted to an empty dictionary if not provided * Remove useless return"], "original_ll": -3.707293748855591, "sampled_ll": -2.2390048503875732, "all_perturbed_sampled_ll": [-2.6656298637390137, -2.2078897953033447, -2.3494110107421875, -2.4595272541046143, -2.379761219024658, -2.3538925647735596, -2.563357353210449, -2.4425292015075684, -2.3382794857025146, -2.6036229133605957], "all_perturbed_original_ll": [-3.7039175033569336, -3.6922388076782227, -3.7943968772888184, -3.612567663192749, -3.5296525955200195, -3.930576801300049, -3.5812673568725586, -3.7437937259674072, -3.56693172454834, -3.6855766773223877], "perturbed_sampled_ll": -2.4363900661468505, "perturbed_original_ll": -3.6840919733047484, "perturbed_sampled_ll_std": 0.1329012792753194, "perturbed_original_ll_std": 0.11411158680195543}, {"original": "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce amount of times the pod object", "sampled": "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "perturbed_sampled": ["[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 secure connections. (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP over HTTP 2.0 secure connections instead of HTTP for use when they are insecure (#2627) - Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) - Fixed bug for older version of netcat (#4365, not supported on Windows, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - Fixed <unk>net/<unk>inet6.h<unk> implementation for HTTP 1.1 by default. (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and supported SSL/TLS (#3384) Fixed crash on Windows - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) - Fixed crash for many modern versions of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "[AIRFLOW-5873] KubernetesPodOperator : Fixed warning related to connection test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a crash after installing `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Updated <unk>pg_encoding<unk> to use HTTP 2.0 and use `ProxyPass` (#3384) - Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) - Fixed crash for older version of HTTP (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "includes bug fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2 as default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - <unk>net/<unk>inet6.h<unk> implementation changed default to use HTTP 2 and does not use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed crash on Windows before allowing netcat connection. - Fixed missing data in `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed bug in <unk>http_get()<unk>. - Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on certain OSes. - Fixed issue with netcat connection. - Fixed missing object `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "[AIRFLOW-5873] KubernetesPodOperator fixes crash. (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` and <unk>net/<unk>inet6.h<unk> to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed (#3369) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing argument in `http_get(). (#2789) - Fixed crash for older version of TCP (#4365, not working on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "with fixes and test (#6524) - `security_context` was missing . (#3090) - Fixed a bug in the proxy pass (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed errors encountered with netcat connection. - Fixed missing documentation for `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not testing on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on kernel/netcat connection (#5228) - Fixed crash on kernel on netcat connection. (#2098) Fixed <unk>http_get()<unk> missing `http://` before `http_get(). (#2789) - Fixed crash for older version of netcat (was not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) Fixed bug #3147", "[AIRFLOW-5873] on lvm-test and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed some bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and prevent <unk>net/<unk>inest6.h<unk> by default. (#3384) Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before user. - Fixed crash for ARM versions of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", ". - Minor fixes and test (#6524) - `security_context` renamed security_pass <unk>password<unk> into `proxy`. (#3369) Fixed a bug in netcat. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed crash with netcat connection. - Fixed warning before `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, KDE, OpenBSD etc..) - Fixed bug #3147"], "perturbed_original": ["[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - <unk>pod.py<unk> was missing from docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator` - <unk>base_command<unk> and `resources` could not be passed to each other. They should only go to their respective pods, so the two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values . - Reduce amount of times the pod object", "and fixes and test (#6524) - Initial values missing from docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False and default to `BaseOperator` . - kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two classes use different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce amount of arguments given to a pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was introduced into the docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` option to False in contradiction to <unk>xcom_push<unk> - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should be passed to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they accept arbitrary values in `setattr` - Reduce the number of times the pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - <unk>in_cluster.py<unk> code for `in_cluster` erroneously defaults to TRUE in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to <unk>PodErator<unk> when it should only go to `PodGenerator`. The two have different syntax. (both <unk>build-test<unk> and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept values in `setattr` - Reduce amount of arguments for the pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing in the default.py of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` was set to False in the config and should be passed to `default_args.py`, also default `do_xcom_push` was set to False in the version of `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce stack size and how many times the pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, instead `do_xcom_push` was also False in contradiction to `BaseOperator` - ebv's pod kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept all methods in `setattr` - Reduce amount of callbacks on pod object", "- Add some fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - <unk>default_args.py<unk> and `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also <unk>define_in_cluster<unk> was overwritten to False in contradiction to the latter - Data for `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - Some pod classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce amount of times the pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes a few things (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - <unk>Default_args.py<unk> method `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to create contradiction . - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to <unk>operator_resource<unk>, because the two have different syntax. (both in <unk>v1-10-development<unk> and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce amount of times the pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes and test fixes. - kwarg `security_context` was missing from the `KubernetesPodOperator` - `KubernetesPodOperator` kwarg option <unk>hostname<unk> defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to <unk>target<unk>. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have names, so they would accept arbitrary references to `setattr` - Reduce amount of times the pod object", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - kwarg `in_cluster` erroneously defaults to False in contradiction to `default_args.py`, also default `do_xcom_push` in that pod to False in contradiction to `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to <unk>container<unk> because these two have different syntax. (tested in both `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not support <unk>resource_name<unk>, so they would accept arbitrary values in `setattr` - Reduce amount of times the pod object"], "original_ll": -3.4834775924682617, "sampled_ll": -3.0729053020477295, "all_perturbed_sampled_ll": [-3.228456735610962, -3.202831506729126, -3.324303150177002, -3.1640543937683105, -3.3388466835021973, -3.1402432918548584, -3.119271755218506, -3.2213382720947266, -3.346766233444214, -3.330897569656372], "all_perturbed_original_ll": [-3.6840438842773438, -3.793680191040039, -3.4627344608306885, -3.710062026977539, -3.3567991256713867, -3.8064939975738525, -3.938206195831299, -3.7580251693725586, -3.7000088691711426, -3.8085222244262695], "perturbed_sampled_ll": -3.2417009592056276, "perturbed_original_ll": -3.7018576145172117, "perturbed_sampled_ll_std": 0.08286591643572395, "perturbed_original_ll_std": 0.1632195594635991}, {"original": "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "sampled": "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "perturbed_sampled": ["Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In"], "perturbed_original": ["Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Add Parquet data type to BaseSQLToGCSOperator (#13359)"], "original_ll": -6.176843643188477, "sampled_ll": -6.630948066711426, "all_perturbed_sampled_ll": [-6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426, -6.630948066711426], "all_perturbed_original_ll": [-6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477, -6.176843643188477], "perturbed_sampled_ll": -6.630948066711426, "perturbed_original_ll": -6.176843643188477, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we reserve the right to delete it at any point (for instance once we add better methods of doing what the OP wanted with these hooks.)", "sampled": "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,", "perturbed_sampled": ["Mark passing pre/post execute on operators as experimental. (#18140) My primary concern here is that by allowing someone to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available standard and we are all that can keep building applications with async/await in Elm,", "Mark passing of callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" should be incredibly easy to write without needing to care about the actual implementation at all in order to make them useful. Finally, both of the above concerns are addressing by the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building for async/await in Elm,", "Mark passing pre/post execute with operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" how any Operator can be written, the Operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that are required when building applications with async/await in Elm,", "Mark passing pre/post execute callbacks to operators as experimental. My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all in order to make the change. However the above concerns are mitigated for the now available standard ; and that is the best! Operators are all that matter when building applications and in Elm,", "Mark passing pre/post execute callbacks to Operator as experimental. (#18140) My concern here is that by being able to arbitrarily \"change\" if an Operator can be written, then both it and its implementation become incredibly easy to write without needing to care about the actual implementation at all in order to be useful. However the above concerns are mitigated for the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be invoked, that that operator and its \"superclass\" are incredibly easy to modify without needing to care about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available experimental version of the Operator (#17139, below).\n\nPipe\n\npipes\n\nPipes are all that matter in applications with async/await in Elm,", "Including pre/post execute callbacks to operators as experimental. (#18140) My main concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to use without needing to worry about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available standard (see #43847). Using it for testing purposes is all that matter when building applications with async/await in Elm,", "Mark passing pre/post methods) to be experimental. (#18140) My primary concern here is that by being able to verify if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all . Thus, a lot of work will need to be done to make them useful. However the above concerns are mitigated for other areas in the available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to modify without needing to alter the class at all in order to make them useful. However the above concerns are mitigated for the now available callback feature (see examples below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with operators. Also check out the post-it comments for Elm,", "Mark passing pre/post execute callbacks to a Pipe is experimental. (#18140) My primary concern here is that by providing the ability to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the pipe arguments at all in order to make them useful. However the potential benefits of this proposed change are mitigated for the now available experimental pipe Pipes API (outlined below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,"], "perturbed_original": ["Mark passing pre/post execute callbacks to an operator as experimental. (#18140) My primary concern here is that by being able to do pre- and post execute calls against what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental , I reserve the right to delete it at any point (unless and once we add better methods and frameworks for doing what the OP wanted with these hooks.)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what the operator does will greatly increase the \"accidental complexity\" of both Airflow (for us ) and of the DAG (for our users). By marking this as experimental we reserve the right to remove it at any point (for instance once we add better methods of doing what the OP wanted with these hooks.)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is, that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental \" complexity of both Airflow (for us as developers) and of the operator (for our users). By marking this as experimental we reserve the right to delete it at any point (for instance once we add better methods of doing what the operators are doing with these hooks.)", "the pre/post execute callbacks should be marked as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental, I reserve the right to delete it at any point (for instance, when we add better methods of doing what the OP wanted with these hooks.)", "Mark passing pre/post commands of type \"handler\" to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental destruction\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we would reserve the right to delete it at any point (for instance once we figure out alternate methods of doing what the operator wants with these hooks.)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the program itself (for our users). By marking these features as experimental we reserve the right to delete it at any point (for instance until we add better methods of doing what the OP wanted with these hooks.)", "Mark passing pre/post _hooks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we reserve the right to delete it at some point (for instance once we find better methods of doing what the OP wanted with these hooks.)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that not being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we reserve the right to delete it at any point (but probably should once we add better methods of doing what the OP wanted with these hooks.)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG (for our users). By marking this features as experimental I reserve the right to remark it at any point (for instance to add better mechanisms for doing what the OP wanted with these hooks.)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both the code itself (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we reserve the right to delete or change any feature under any circumstances. (For instance once we add better methods of doing what the OP wanted with these hooks.)"], "original_ll": -4.022066593170166, "sampled_ll": -3.567488670349121, "all_perturbed_sampled_ll": [-4.032139301300049, -3.623297691345215, -3.6117541790008545, -3.904709815979004, -3.6276800632476807, -3.688422441482544, -3.8053765296936035, -3.591451406478882, -3.797835111618042, -3.6620290279388428], "all_perturbed_original_ll": [-4.070798873901367, -4.060118675231934, -4.08007287979126, -3.7047431468963623, -4.021406173706055, -3.999788284301758, -4.026571273803711, -4.053991317749023, -4.146182537078857, -3.8406412601470947], "perturbed_sampled_ll": -3.7344695568084716, "perturbed_original_ll": -4.0004314422607425, "perturbed_sampled_ll_std": 0.1390238889186507, "perturbed_original_ll_std": 0.12371623006309454}, {"original": "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "sampled": "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "perturbed_sampled": ["[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I"], "perturbed_original": ["[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)"], "original_ll": -5.51270866394043, "sampled_ll": -5.905052185058594, "all_perturbed_sampled_ll": [-5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594, -5.905052185058594], "all_perturbed_original_ll": [-5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043, -5.51270866394043], "perturbed_sampled_ll": -5.905052185058594, "perturbed_original_ll": -5.51270866394043, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "sampled": "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "perturbed_sampled": ["[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This"], "perturbed_original": ["[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)"], "original_ll": -6.514249324798584, "sampled_ll": -6.799709320068359, "all_perturbed_sampled_ll": [-6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359, -6.799709320068359], "all_perturbed_original_ll": [-6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584, -6.514249324798584], "perturbed_sampled_ll": -6.799709320068359, "perturbed_original_ll": -6.514249324798584, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix formatting in AWS Connections docs (#8223)", "sampled": "Fix formatting in AWS Connections docs (#8223)\"", "perturbed_sampled": ["Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\"", "Fix formatting in AWS Connections docs (#8223)\""], "perturbed_original": ["Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)", "Fix formatting in AWS Connections docs (#8223)"], "original_ll": -6.179692268371582, "sampled_ll": -6.933568000793457, "all_perturbed_sampled_ll": [-6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457, -6.933568000793457], "all_perturbed_original_ll": [-6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582, -6.179692268371582], "perturbed_sampled_ll": -6.933568000793457, "perturbed_original_ll": -6.179692268371582, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "sampled": "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "perturbed_sampled": ["[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker . 1-setup-env.sh should only run in docker", "sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker . 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker [AIRFLOW-XXX] 1-setup-env.sh should only run in docker"], "perturbed_original": ["[AIRFLOW-XXX] 1-setup-env.sh should only run in docker. (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1.setup.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker"], "original_ll": -3.1218481063842773, "sampled_ll": -3.1218481063842773, "all_perturbed_sampled_ll": [-3.1218481063842773, -3.62496018409729, -4.642158031463623, -3.1218481063842773, -3.1218481063842773, -3.62496018409729, -3.1218481063842773, -3.1218481063842773, -2.972933292388916, -2.972933292388916], "all_perturbed_original_ll": [-3.1246495246887207, -3.1218481063842773, -3.474472761154175, -3.1218481063842773, -3.1218481063842773, -3.5623762607574463, -3.1218481063842773, -3.1218481063842773, -3.1218481063842773, -4.642158031463623], "perturbed_sampled_ll": -3.3447185516357423, "perturbed_original_ll": -3.353474521636963, "perturbed_sampled_ll_std": 0.4863357568352197, "perturbed_original_ll_std": 0.45752063950801247}, {"original": "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "sampled": "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "perturbed_sampled": ["[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One"], "perturbed_original": ["[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)"], "original_ll": -6.870513439178467, "sampled_ll": -7.3190226554870605, "all_perturbed_sampled_ll": [-7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605, -7.3190226554870605], "all_perturbed_original_ll": [-6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467, -6.870513439178467], "perturbed_sampled_ll": -7.3190226554870605, "perturbed_original_ll": -6.870513439178467, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove unnecessary messages in CI (#7951)", "sampled": "Remove unnecessary messages in CI (#7951)Description:", "perturbed_sampled": ["Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:", "Remove unnecessary messages in CI (#7951)Description:"], "perturbed_original": ["Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)", "Remove unnecessary messages in CI (#7951)"], "original_ll": -6.142459869384766, "sampled_ll": -6.510763645172119, "all_perturbed_sampled_ll": [-6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119, -6.510763645172119], "all_perturbed_original_ll": [-6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766, -6.142459869384766], "perturbed_sampled_ll": -6.510763645172119, "perturbed_original_ll": -6.142459869384766, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "sampled": "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes", "perturbed_sampled": ["Clarifies version args for installing 1.10 in Docker images. This change clarifies that AIRFLOW_VERSION should be passed . This change adds the version of FLOW_VERSION as an argument. This removes", "Clarifies version of FLOW for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed as an argument instead of passing the version of FLOW_VERSION as an argument. This removes", "clarify args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument from the command. removes", "Pass args for Airflow in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes", "Clarifies version args for installing 1.10 in Docker files. This change clarifies that AIRFLOW_VERSION should be passed to install the version of FLOW_VERSION as an argument. This removes", "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version number as an argument. This removes", "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be set with the version of FLOW_VERSION as configured. (#12878) This removes", "Clarifies version args for installing 1.10 in Docker (#12875) This should clarify that AIRFLOW_VERSION should be passed together with the version of AIRFLOW in the command as an argument. This removes", "Removes args for installing 1.10 in Docker (#12875) . This also clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes", "Clarifies version args for installing FLOW in Docker (#12875) This change clarifies that the version of AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes"], "perturbed_original": ["Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION and AIRFLOW_VERSION are passed together with version when the Docker image is build. Fixes #8612", "Clarifies version args for installing 1.10 or 2.0 (#12875) . This patch clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "fix the args for installing 1.10 in version 1.4. This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "Clarifies instructions for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the AIRFLOW Docker image is build. Fixes #8612", "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be replaced with AIRFLOW_INSTALL_VERSION when the Docker image is installed. Fixes #8612", "Update the args for installing 1.10 in Docker image. The following change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "Clarifies AIRFLOW_VERSION for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed to AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "Clarifies version args for installing 1.10 in Dockerfile. This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker file is build. Fixes #8612", "Clarifies that AIRFLOW_VERSION_VERSION should be passed for installing an AIRFLOW image on Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be compared manually with AIRFLOW_INSTALL_VERSION when the Docker image is first released (see previous revision #8612"], "original_ll": -3.8051023483276367, "sampled_ll": -3.8938870429992676, "all_perturbed_sampled_ll": [-3.9246466159820557, -3.4303884506225586, -4.132014274597168, -3.9190514087677, -3.8858084678649902, -4.149923801422119, -3.906320810317993, -3.8892951011657715, -4.226877689361572, -3.6250193119049072], "all_perturbed_original_ll": [-4.070969581604004, -3.840155601501465, -3.7593798637390137, -3.54758358001709, -3.528026819229126, -3.782585382461548, -3.350555181503296, -3.6073575019836426, -3.304413318634033, -4.037534713745117], "perturbed_sampled_ll": -3.9089345932006836, "perturbed_original_ll": -3.6828561544418337, "perturbed_sampled_ll_std": 0.2279954011500397, "perturbed_original_ll_std": 0.24871420976119346}, {"original": "Submodules are needed to update constrains (#15242)", "sampled": "Submodules are needed to update constrains (#15242)I", "perturbed_sampled": ["Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I", "Submodules are needed to update constrains (#15242)I"], "perturbed_original": ["Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)", "Submodules are needed to update constrains (#15242)"], "original_ll": -5.360805511474609, "sampled_ll": -6.136443614959717, "all_perturbed_sampled_ll": [-6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717, -6.136443614959717], "all_perturbed_original_ll": [-5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609, -5.360805511474609], "perturbed_sampled_ll": -6.136443614959717, "perturbed_original_ll": -5.360805511474609, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixing MyPy issues in testa/jobs (#19998)", "sampled": "Fixing MyPy issues in testa/jobs (#19998)As", "perturbed_sampled": ["Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As", "Fixing MyPy issues in testa/jobs (#19998)As"], "perturbed_original": ["Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)", "Fixing MyPy issues in testa/jobs (#19998)"], "original_ll": -6.631772041320801, "sampled_ll": -7.173069953918457, "all_perturbed_sampled_ll": [-7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457, -7.173069953918457], "all_perturbed_original_ll": [-6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801, -6.631772041320801], "perturbed_sampled_ll": -7.173069953918457, "perturbed_original_ll": -6.631772041320801, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Use stable API versions where available (#17211)", "sampled": "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "perturbed_sampled": ["Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Chart: Use stable API versions where available (#17211)\"\n\nFix:"], "perturbed_original": ["Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)", "Chart: Use stable API versions where available (#17211)"], "original_ll": -5.557555675506592, "sampled_ll": -5.0435004234313965, "all_perturbed_sampled_ll": [-5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965, -5.0435004234313965], "all_perturbed_original_ll": [-5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592, -5.557555675506592], "perturbed_sampled_ll": -5.0435004234313965, "perturbed_original_ll": -5.557555675506592, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "sampled": "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "perturbed_sampled": ["Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`"], "perturbed_original": ["Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`"], "original_ll": -4.603448390960693, "sampled_ll": -4.851093769073486, "all_perturbed_sampled_ll": [-4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486, -4.851093769073486], "all_perturbed_original_ll": [-4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693, -4.603448390960693], "perturbed_sampled_ll": -4.851093769073486, "perturbed_original_ll": -4.603448390960693, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add schema as DbApiHook instance attribute (#16521)", "sampled": "Add schema as DbApiHook instance attribute (#16521)One", "perturbed_sampled": ["Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One", "Add schema as DbApiHook instance attribute (#16521)One"], "perturbed_original": ["Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)", "Add schema as DbApiHook instance attribute (#16521)"], "original_ll": -5.966734409332275, "sampled_ll": -6.6604838371276855, "all_perturbed_sampled_ll": [-6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855, -6.6604838371276855], "all_perturbed_original_ll": [-5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275, -5.966734409332275], "perturbed_sampled_ll": -6.6604838371276855, "perturbed_original_ll": -5.966734409332275, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "sampled": "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "perturbed_sampled": ["Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If"], "perturbed_original": ["Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891"], "original_ll": -6.390114784240723, "sampled_ll": -6.725689888000488, "all_perturbed_sampled_ll": [-6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488, -6.725689888000488], "all_perturbed_original_ll": [-6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723, -6.390114784240723], "perturbed_sampled_ll": -6.725689888000488, "perturbed_original_ll": -6.390114784240723, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "sampled": "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation", "perturbed_sampled": ["Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain -text metadata instead of nested files as described in the documentation", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as shown in the documentation", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files , which has been ignored in the documentation", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain file metadata instead of nested files as described in the documentation", "Docs: Better metadata documentation for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation", "Docs: Better description for the metadata file: In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata , instead of nested files as described in the documentation", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text instead of embedded files instead of nested files as described in the documentation", "Docs: Update of the documentation for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation", "Docs: Better description for `pod_template_file` (#16861) In particular, the `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation"], "perturbed_original": ["Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, it's the only way to configure workers, so we can remove the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to create new templates that get uploaded to workers, so we can remove the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In this case, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure , we can remove the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "Docs: Better description for this. In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "New description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure pod templates, so we can remove the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can 't use the \"other fields\" language.", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language."], "original_ll": -3.8986904621124268, "sampled_ll": -3.60378098487854, "all_perturbed_sampled_ll": [-3.7993459701538086, -3.6322150230407715, -3.8107659816741943, -3.6672778129577637, -3.623756170272827, -4.175543308258057, -3.6945552825927734, -3.474771499633789, -3.598536968231201, -3.274631977081299], "all_perturbed_original_ll": [-4.43127965927124, -3.8091349601745605, -3.5559146404266357, -4.045015811920166, -3.8986904621124268, -4.547728538513184, -4.044344425201416, -3.7204341888427734, -4.016205310821533, -3.8986904621124268], "perturbed_sampled_ll": -3.6751399993896485, "perturbed_original_ll": -3.996743845939636, "perturbed_sampled_ll_std": 0.22261375888915622, "perturbed_original_ll_std": 0.28678976484444335}, {"original": "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "sampled": "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "perturbed_sampled": ["Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment , DB_URL with DB_URLThis\n\nThe", "Fix error message in p_name (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "Fix filename error in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, which references DB_URLThis\n\nThe", "to a failed message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "Fix error message in production entrypoint.sh (#8396) Fix non-existent index variable, replace with DB_URLThis\n\nThe", "Fix error message in production database Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "Fix incorrect URLs in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variables in DB_TABLE with DB_URLThis\n\nThe", "Fix error message in production entrypoint.sh (#8396) Fix invalid environment variable, replace with DB_URLThis\n\nThe"], "perturbed_original": ["Fix error message in production entrypoint.sh (#8396) Fix non-existent database_URL variable, replace with DB_URL", "Fix error in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "Fix error message in case of bad error header. (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "Fix non-existent entrypoint environment variable non-existent in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "Fix error message in production entrypoint.sh : DB_URL contains the non-existent BACKEND environment variable, replace with DB_URL", "Fix error message in production entrypoint.sh (#8396) Fix non-existent DB_TIME variable, replace with DB_URL", "Fix error message displayed when calling entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "Fix error message in production entrypoint.sh (#8396) Fix SQL environment variable, replace with DB_URL", "Fix error message in production entrypoint.sh (#8396) Fix non-existent database_url variable, replace with DB_URL", "Fix error message in production entrypoint.sh (#8396) Fix non-existent DB_URL variable, replace with DB_URL"], "original_ll": -4.723080158233643, "sampled_ll": -4.782496452331543, "all_perturbed_sampled_ll": [-5.046059608459473, -4.899448871612549, -5.113826274871826, -4.9151530265808105, -5.215552806854248, -4.969646453857422, -4.976794242858887, -5.050538539886475, -4.886351108551025, -5.076560020446777], "all_perturbed_original_ll": [-4.60056734085083, -4.755735397338867, -4.534318923950195, -4.344315528869629, -4.324879169464111, -4.692612648010254, -4.597515106201172, -5.157014846801758, -4.461296081542969, -4.352172374725342], "perturbed_sampled_ll": -5.01499309539795, "perturbed_original_ll": -4.5820427417755125, "perturbed_sampled_ll_std": 0.09969655277281056, "perturbed_original_ll_std": 0.2378897547656101}, {"original": "Add ME-Br to who uses Airflow list (#9770)", "sampled": "Add ME-Br to who uses Airflow list (#9770)As", "perturbed_sampled": ["Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As", "Add ME-Br to who uses Airflow list (#9770)As"], "perturbed_original": ["Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)", "Add ME-Br to who uses Airflow list (#9770)"], "original_ll": -7.349530220031738, "sampled_ll": -7.802426338195801, "all_perturbed_sampled_ll": [-7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801, -7.802426338195801], "all_perturbed_original_ll": [-7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738, -7.349530220031738], "perturbed_sampled_ll": -7.802426338195801, "perturbed_original_ll": -7.349530220031738, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "sampled": "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "perturbed_sampled": ["[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless"], "perturbed_original": ["[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)"], "original_ll": -5.665342330932617, "sampled_ll": -5.958355903625488, "all_perturbed_sampled_ll": [-5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488, -5.958355903625488], "all_perturbed_original_ll": [-5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617, -5.665342330932617], "perturbed_sampled_ll": -5.958355903625488, "perturbed_original_ll": -5.665342330932617, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a version, so we don't have to edit a file and then remember to delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits or periods, but it's copy-pasteable this way.", "sampled": "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "perturbed_sampled": ["(#13021). As part of the test process after snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or for whatever reason the maintainer forgot to add support for a version), this suffix is used to add a new test to the build to resolve it (which is typically", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a prefix to a release build. If in the path a build fails due to a bug (or the upstream maintainer forgot to add support for a build), the suffix is used to add a new test to the build to fix it (which is typically", "Simplify release builds using PyPI snapshots (#13020) Setuptools now has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or the upstream maintainer forgot to add tests for a version), this suffix is used to add a new test to the build to fix it (which is typically", "deployment process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release , if in the future a build fails because of a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "Simplify build for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future the build fails due to a bug (e.g. the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or the dev team forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "Simplify release process for PyPI tests. Setuptools has a built in mechanism for adding a `suffix` to a fixed release build. If in any case a build fails due to a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "Simplify release process for PyPI snapshots : PyPI has a built in mechanism of adding a `suffix` to a release when testing features in the snapshot. If the test build fails due to a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "Simplify release process for PyPI snapshots (#13020) Setuptools has a nice mechanism for adding the suffix build to a release build. If at some point in the future a build fails due to a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "Simplify release process for PyPI release builds! Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or the upstream maintainer forgot to add support for a new feature), this suffix is used to add a new test to the build to try to fix the mistake. It is typically"], "perturbed_original": ["Simplify release process for PyPI snapshots (#13020) Setuptools has a built -in check for adding a `suffix` to a version, so we don't have to edit a file and then remember to delete it! It's a little messy to get the suffix like this -- using sed to strip out all the digits or periods, but it's copy-pasteable this way.", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a suffix to a version, so we don't have to edit a previous version then go ahead and delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any quotes or periods, but it's copy-pasteable this way.", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a Python configuration file, so we don't have to edit a file and then remember about it! It would be a little bit messy to get the suffix like this after first using sed to strip out any leading digits or periods, but it's copy-pasteable this way.", "Simplify release process for PyPI . Setuptools has a built in mechanism for adding a `suffix` to a version, so you no longer have to add a file and remember to delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits or periods, but it's copy-pasteable this way.", "Simplify release notification for PyPI snapshots (#13020) and include a built in mechanism for adding a `suffix` to it, so we don't have to edit a file and then remember to delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits , but it's copy-pasteable this way.", "Simplify release process for PyPI snapshots (#13020) : The release path has a built in mechanism for adding a `suffix` to a version, so we don't have to edit a version -- first we add it to PyPI and then we delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits -- but it's copy-pasteable this way.", "Simplify release process for PyPI snapshots (#13020) Setuptools has a pretty nifty mechanism for adding a `suffix` to a version, so you don't have to edit a file and then remember to delete it! It's a bit messy to get the suffix like this -- using sed to strip out any leading digits or periods, but it's copy-pasteable this way.", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for attaching a version `suffix` to a version, so we don't have to edit an entire version and then remember to link to it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits or periods, but it is easy, and copy-pasteable this way.", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a file. So we don't have to edit a file and then remember to apply a <unk>suffix<unk>. It's a little bit strange to get the suffix like this -- using sed to strip out any leading digits or periods, but it's the only way.", "Simplify Suffixing for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a file. The best part about this is that we don't have to edit a file and then go back and delete it! It's a little bit messy to add the suffix like this -- using sed to strip out any leading digits or periods, but it's copy-pasteable this way."], "original_ll": -3.695136547088623, "sampled_ll": -3.137489080429077, "all_perturbed_sampled_ll": [-3.3281335830688477, -3.347108840942383, -3.1653711795806885, -3.410961627960205, -3.048225164413452, -3.186356544494629, -3.0984232425689697, -3.056408166885376, -3.213296890258789, -2.830066680908203], "all_perturbed_original_ll": [-3.9128782749176025, -3.7521209716796875, -3.664754867553711, -3.521057605743408, -3.828617811203003, -3.614382743835449, -3.6609644889831543, -3.827634334564209, -3.806304454803467, -3.3748419284820557], "perturbed_sampled_ll": -3.1684351921081544, "perturbed_original_ll": -3.6963557481765745, "perturbed_sampled_ll_std": 0.1627837472058967, "perturbed_original_ll_std": 0.15488031143652992}, {"original": "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "sampled": "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "perturbed_sampled": ["[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This"], "perturbed_original": ["[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)"], "original_ll": -5.537713050842285, "sampled_ll": -5.894845008850098, "all_perturbed_sampled_ll": [-5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098, -5.894845008850098], "all_perturbed_original_ll": [-5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285, -5.537713050842285], "perturbed_sampled_ll": -5.894845008850098, "perturbed_original_ll": -5.537713050842285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depenedncies of airflow, all the provider packages released at the time of the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies needed for core airflow installation. This allows to install/upgrade airflow without also forcing the provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the provider 'extras' for apache airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are now transitive only", "sampled": "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the core constraint generation, and the custom provider generation for generic services (#14151).\n\n* The generated providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class generator. It is based on \"create provider\", which has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "perturbed_sampled": ["Implements the separate constraints for core and providers (#14227) There are two types of constraints : * new default constraints that contain all depots in the service (#14151 docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators docs gives examples of the generation process. It covers the core constraint generation, and custom provider generation for generic services (#14151).\n\n* The generated providers docs include all the docs for all provider classes, including generic providers (#14152).\n\n* The current generator docs also provides references for the new provider generator. It is based on \"create provider\", which has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a custom provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for core and providers . There are two types of constraints now: * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* generator for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the core constraint generation, and custom constraints for generic services (#14151).\n\n* The generated providers docs now include the docs for provider classes, including their usage (#14152).\n\n* Improved handling for the new provider class generator. It is now \"create provider\", which has many of the same features as \"run provider\" (#14147).\n\n* Support for \"run provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints in the code now: * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The generated provider documentation (#14151) gives examples of the output. It provides the default core constraint generation, and the custom constraint generation for generic services (#14151).\n\n* The generated providers docs now include the docs for new provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class \"run provider\", which is based on \"create provider\", which has many of the same properties as \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator * Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for core providers (#14227) There are two types of constraints : * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the new generators. It covers the core providers generator and the custom provider generation for generic services (#14151).\n\n* The generated providers docs now contains docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class generator. It is based on \"create provider\", which has many of the features of \"create provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for core and specialized providers. There are two types of constraints now: * default constraints that contain all depots in the network (see #14151) * custom constraints from provider objects #14152\n\n* support for generic provider generation (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the generic provider generation, and the custom provider generation for generic provider generation. * The generated providers docs now include the docs for all provider classes, including generic provider generation (#14152).\n\n* Improved handling for the new provider class generator. It is now \"create provider\", which has many of the same implementation details as a generic provider generator of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for depots and providers (#14227) There are a range of constraints: * default constraints contain all depots in the network (see docs) * custom constraints from provider objects * constraints for generic providers (similar to the run-provider generator). * The new generators documentation (#14151) gives examples of the generation process. It covers the core provider generation process and the custom provider generation for generic services (#14151).\n\n* The generated providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new generic provider class generator. It is based on \"create provider\", which has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints : * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the core generation, and specific provider classes and generic services (#14151).\n\n* The generated providers docs now include the generator examples for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class generator. It is based on \"create provider\", which has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a core provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of provider constraint for all providers (#14227) There are two types of constraints now: * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The provided generators documentation now provides full examples of the provider generation process. It covers the core constraint generation, and the custom provider generation for generic services (#14151).\n\n* The generated providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling of \"create provider\" * Implemented new provider class generator. It is an extension of \"create provider\", and shares many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"generate providers\",", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints: * core constraints, * default constraints that contain all depots in the network (see docs) * custom constraints from provider generators * support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers both core constraint generation, and custom provider generation for generic services (#14151).\n\n* The generated providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class , creator, which is based on \"create provider\", which has all the features of create generator (#14147).\n\n* Support for \"create provider\", a generic core provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Implements generation of separate constraints for core and generic providers (#14227) There are three types of constraints available now: * default constraints that contain all depots in the network * generic constraints from specific depots * custom constraints from provider objects * custom constraints for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the core constraint generation, and the custom provider generation for generic services (#14151).\n\n* The new providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling of the new provider generator. It is based on \"create provider\", which has many of the features of \"create provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\","], "perturbed_original": ["Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depenedncies of airflow, all the provider packages released at the time of the relese of Airflow as well as all transitive dependencies. Following those constraints, you can be sure that the service is repeatable * no-providers constraints - containing only dependencies needed for core airflow installation. This allows to keep core airflow dependencies without also forcing the provider's to be installed at specific version of Airflow. This allows for flexible management of Core and Provider packages separately. Documentation about it has been added. Also the 'extras' of airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are transitive only", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints - containing all the dependencies needed for core airflow, all the provider packages and all the packages in the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is complete. * no-providers constraints - containing only the dependencies needed for core airflow installation. This allows to install/upgrade airflow without also forcing the provider packages to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the provider 'extras' for apache airflow do not keep direct access to all the packages , and the provider dependencies are now transitive only", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depenedncies of airflow, all the provider packages released at the time of the relese ved version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies for the core airflow installation. This allows to install/upgrade airflow without having to depend on the provider's to be installed at the same time. Separate constraint generation is performed separately for providers and core of Airflow. This supports flexible management of Airflow and Provider packages separately. Documentation about it has been updated because the provider 'extras' for apache airflow do not keep direct dependencies from the provider. They keep only packages needed by the provider. These packages are now transitive only", "Implements how to build separate constraints by airflow and providers (#14227) There are two types of constraints now: * default constraints that contain all core depenedncies of airflow, all the provider packages released at the time of the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies needed by airflow installation. This allows to install/upgrade airflow without also using specific provider's to be installed at specific version of Airflow. Is nice for flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the requirements for apache airflow do not need to add new critical dependencies to the packages needed by the provider. Those dependencies are now transitive only", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints : default constraints that contain all depenedncies of airflow, all the provider packages released at the time of installation of that version, as well as all transitive packages. By checking those constraints, you can be sure Airflow's installation is not broken. The second kind of constraints - no-providers constraints - containing only the dependencies of core airflow installation. This allows to install/upgrade airflow by forcing the provider's to be installed at specific version of Airflow. This allows more flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the provider 'extras' for apache airflow do not keep direct dependencies on the packages needed by the provider. Those are now transitive only", "Implements generation of separate constraints for core and providers. There are two types of constraints now: * default constraints that contain all depenedncies of all the provider packages at the time of the relese en version, as well as all transitive dependencies. Using those constraints, you can be sure Airflow's installation is repeatable * provider constraints - containing only the dependencies needed at current core airflow installation. This allows to install/upgrade airflow without also forcing the provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages . New provider documentation about it has been added. Also the provider 'extras' for apache airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are now transitive only", "Implements generation of separate constraints for AIRFLOW and providers (#14227) There are two types of constraints now: * default constraints that contain all the depenedncies of airflow, all the provider packages released at the time of the relese ve/update of Airflow version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies needed for core airflow . This allows to install/upgrade airflow without also requiring provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation for this change has been added. The new provider 'extras' for apache airflow do not keep direct dependencies to the packages needed by Airflow. Those dependencies are now transitive only", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints - contain all depenedncies of airflow, all the airflow packages currently released at the time of the update to that version, as well as all transitive dependencies. Following those constraints, you can be sure your installation is repeatable * no-providers constraints - containing only the dependencies needed for core airflow installation. This allows to install/upgrade airflow modules on different versions, while also forcing the provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation about these constraints has not been added. Also the provider 'extras' for apache airflow do not keep any dependencies to the packages needed by the provider. Those dependencies are used only", "Implements generation of separate dependencies for Airflow core and providers (#14227) There are two types of constraints * default constraints that contain all depenedncies of airflow, all the provider packages released at the version released by the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * minimal constraints - containing only the dependencies needed for core airflow installation. This allows to install/upgrade airflow without also forcing the provider's packages to be installed at specific version . This allows for flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the system's constraints for apache airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are stored for core airflow installation only", "Implements generation of separate constraints for airflow and providers (#14227) There are two types of constraints available: * default -providers constraints - contain all depenedncies of airflow, all the provider packages released at the time of the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies needed for core airflow installation. This allows installing airflow without also checking for provider's to dos at specific versions of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation about the constraints has also been added. Also the provider 's dependencies of apache airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are now transitive only"], "original_ll": -3.9686787128448486, "sampled_ll": -2.6518170833587646, "all_perturbed_sampled_ll": [-2.861625909805298, -2.8075966835021973, -2.767378807067871, -2.7055816650390625, -2.7120962142944336, -2.8890976905822754, -2.773848295211792, -2.7137088775634766, -2.91194224357605, -2.7055795192718506], "all_perturbed_original_ll": [-4.0165605545043945, -3.6240663528442383, -3.8437297344207764, -4.102789878845215, -3.8829522132873535, -3.9543139934539795, -3.9647347927093506, -3.879002332687378, -4.052644729614258, -4.105517387390137], "perturbed_sampled_ll": -2.7848455905914307, "perturbed_original_ll": -3.942631196975708, "perturbed_sampled_ll_std": 0.07535230711821177, "perturbed_original_ll_std": 0.13756873592089877}, {"original": "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - this have been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "sampled": "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - that is, if you use locks to do a transaction and then try to start a transaction later on. If you use mutexes, you would be stuck. (Issue #19857, #19867)\n\nIn some places, we added MSSQL replication support in MySQL 5, and there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance when running on", "perturbed_sampled": ["Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that result in temporary deadlocks - that is, if you use locks in a transaction and then try to start a transaction later on. But when you use mutexes, you'll still be stuck. (Issue #19857, #19867)\n\nIn some places, we added MSSQL replication in MySQL 5, and there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the issue, I can find it, but I think it'll help us when looking for ways to improve performance when running on", "Workaround occasional deadlocks with MSSQL (#19856) It would be good to have a mechanism to retry operations that could result in temporary deadlocks - that is, if you use locks to do a transaction and then try to start a transaction later on. If you use mutexes, you would be stuck. (Issue #19857, #19867)\n\nIn some places, we have an issue with the MSSQL replication in MySQL 5, and there needs to be a way to tell it where to look for transactions to replicate it to. (If that is the case, I think it'll help us when looking for ways to improve performance .) This needs to be worked on", "Workaround occasional deadlocks with MSSQL (#19856) Note that MSSQL does not have a mechanism to retry operations that could result in temporary deadlocks - that is, if you use locks or the state of a transaction and then try to lock a transaction later on. If you use mutexes, you would be stuck. Share transactions (MSSQL #19867)\n\nIn some places, we added replication support in MySQL - even if there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance when running on", "Workaround occasional deadlocks with MSSQL (#19856) We need a mechanism to retry operations that could result in temporary deadlocks - that is, if you use locks to stop a transaction and then try to start a transaction later on. If you use mutexes, you would have trouble. MSSQL Replication (Issue #19857, #19867)\n\nIn addition, we added MSSQL replication support in MySQL 5, and there needs to be a way to tell MySQL where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve the performance of MySQL 5 running on", "Workaround occasional deadlocks with MSSQL (#19856) We already have a way to prevent transactions that result in temporary deadlocks - that is, if you use locks to do a transaction and tables to start a transaction . If you use mutexes, you would want to use locks for both. MSSQL replication requirements (Issue #19857, #19867)\n\nIn some places, we added MSSQL replication support in MySQL 5, and there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance when running on", "Workaround occasional deadlocks with MSSQL (#19856) We already have an attempt to retry operations that could result in temporary deadlocks - that is, if you spawn locks to do a transaction and then try to start a transaction later when you use mutexes, you would be stuck. (Issue #19857, #19867)\n\nIn some places, we added MSSQL replication support in an effort to be faster and there needs to be a way to tell the cluster to look for transactions to replicate it to. If this is the case, I think it's worth considering by us when looking for ways to improve performance when running on", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry transactions that could result in temporary deadlocks - that is, if you use locks to do a transaction and then try to go into a transaction later on. If you use mutexes, you would be able to do that. #19857, #19867)\n\nIn some places, there may be MSSQL replication support in MySQL . But there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it would help us when looking for ways to improve performance when running on", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to correct problems that could result in temporary deadlocks - that is, if you use locks to do a transaction, and then try to start a transaction later on. If you use mutexes, you would be more vulnerable. Replication (#19857, #19857, #19867)\n\nIn some places, we added MSSQL replication support in Java 5, and there needs to be a way to tell it where to look for and replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance further on", "Workaround s with MSSQL (#19856) We already have a handle for retry operations that could result in failure - that is, if you use locks to do a transaction and then try to do another transaction later on. If you use mutexes, you would be stuck. MSSQL replication support (ticket: #19867)\n\nIn some places, we added MSSQL replication support in MySQL in Q6 so maybe there might be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance when running on", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could cause some temporary deadlocks - that is, if you use locks to do a transaction and then try to start a transaction later on. If you 're in lock-state you would be stuck. (Issue #119105) In some places, we added lock support in MySQL 5, and there needs to be a way to tell MySQL where to look for transactions to replicate it to. (If this is the case, I think it'll help us in searching for ways to improve performance when running on"], "perturbed_original": ["Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in deadlocks - this have been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', ' Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was terminated while running on lock resources with the table and has been designated as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds comments to the DBAPIError errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism for dealing with operations that could result in temporary deadlocks - which has been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a much stronger error message than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 ][Microsoft SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been chosen as deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This workaround uses DBAPIError to represent an error from the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that result in temporary deadlocks - this have been used widely for handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError . OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been disabled by the deadlock victim. Rerun the transaction (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors and this will be handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in deadlocks - this have been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL. In there we get a similar error than OperationalError: `sqlalchemy.exc.DBAPIError: Error '[40001] [Microsoft][ODBC Driver 17 ][SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction. (1205) ' . This PR adds DBAPIError to the list of errors that can be handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks : (#19856) We already have a mechanism to retry operations /scripts that result in temporary deadlocks - this have been helpful with handling occasional issues encountered with ODBC driver - however similar problems occur in MSSQL and there we get a DBAPIError : OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was unable to lock resources with another process and has been chosen as the deadlock victim. Rerun the process with (dm:exec_retries)('40001', (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have mechanism to retry operations that could result in temporary deadlocks - this have been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: : ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server]: Database (Process ID 55) was deadlocked on lock resources in the process and has been set as the deadlock victim. Rerun the program, and call SQLRetry(Process ID (1205) (SQLExecDirectW)'); This fix adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to detect deadlocks that could result in temporary executions in SQL Server or MySQL (issues like this have nothing to do with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Borrowing error 17 for SQL Server][SQL Server]Transaction (Process ) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds that to the list of errors that are handled by `run_with_db_retries` to handle occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could cause temporary deadlocks - this have been used before for handling MySQL deadlocks - however similar problems occur occasionally when dealing with MSSQL and there is a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) <unk> - [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been replaced as the deadlock victim. Rerun the transaction. (1205) This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - and this has been helpful to help mitigate MySQL deadlocks - however sometimes deadlocks occur with MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been chosen as the victim. Rerun the transaction. (1205) <unk> This PR adds 'DaladbcIrretry Error' to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Workaround occasional deadlocks in MSSQL (#19856) We already have a mechanism to retry the transaction that could result in temporary deadlocks - this worked very helpful with handling MySQL deadlocks - there can be occasional problems however in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 ][Database Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been locked to the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks."], "original_ll": -4.178400039672852, "sampled_ll": -3.0038838386535645, "all_perturbed_sampled_ll": [-3.051311492919922, -2.9851584434509277, -3.2407898902893066, -2.926513671875, -3.1919052600860596, -3.2644870281219482, -3.065722703933716, -3.118940591812134, -3.2705881595611572, -3.506263494491577], "all_perturbed_original_ll": [-4.457903861999512, -4.256223201751709, -4.235906600952148, -4.21250581741333, -4.304572582244873, -4.376108169555664, -4.233490467071533, -4.079967021942139, -4.205631732940674, -4.270015716552734], "perturbed_sampled_ll": -3.1621680736541746, "perturbed_original_ll": -4.2632325172424315, "perturbed_sampled_ll_std": 0.16062887184507607, "perturbed_original_ll_std": 0.0967215457230024}, {"original": "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures when executing long-running jobs", "sampled": "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures. You might want to", "perturbed_sampled": ["Databricks /Date expiration time check (#20036) There was a logical error in the check of expiration time that could lead to some cache failures. You might want to", "Databricks hook: fix expiration time. (#20036) There was a logical error in the check of expiration time that could lead to failures. You might want to", "Databricks hook: Expiration time check (#20036) There was a logical error with the check of expiration time that could lead to authentication failures. You might want to", "Databricks hook: fix expiration time check (#20036) There was a logical error in the calculation of expiration time that could lead to a system crash. You might want to", "Databricks hook: invalid expiration time check (#20036) There was a logical error in the check of expiration time that may have contributed to authentication failures. You might want to", ": fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures. You might want to", "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expired time that could lead to authentication issues. We might want to", "Databricks hook: fix the expiration time check (#20036) There was a logical error in the check of expiration time that could have produced authentication failures. You might want to", ", and fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures. You might want to", "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration times that could lead to authentication failures. You 'll need to"], "perturbed_original": ["Databricks hook: fix expiration time check : There was a logical error in the check of expiration time on Databricks that could lead to authentication failures when executing long-running jobs", "Databricks hook: fix expiration time check (#20036) There was a logical error in the hook check of expiration time that could lead to bad behavior when executing long-running jobs", "Databricks hook: fix expiration time check (#20036) There \u2019s a logical error in the check of expiration time that might lead to authentication failures when executing long-running jobs", "Databricks hook: fix expiration time check (#20036) There was a bug in the check of expiration time that could possibly lead to authentication failures when executing long-running jobs", "Databricks : expiration time check (#20036) There was a failure in the check of expiration time that could lead to authentication failures when executing long-running jobs", "Databricks hook: fix expiration time check (#20036) There was a logical error before the check of expiration time that could lead to authentication failures in long-running jobs", "Databricks hook: Expire time check (#20036) There was a logical error in the calculation of expiration time that could lead to authentication failures when executing long-running jobs", "to fix expiration time check (#20036) There was a logical error in the check of expiration time that could cause authentication failures when executing long-running jobs", "Databricks hook: fix expiration time check (#20036) There were logical errors in the check of expiration time that could lead to authentication failures when executing long-running jobs", "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to failures in long-running jobs"], "original_ll": -3.969269275665283, "sampled_ll": -3.750504732131958, "all_perturbed_sampled_ll": [-4.198747158050537, -3.8907008171081543, -3.785825490951538, -3.644418716430664, -4.046666622161865, -3.7187039852142334, -4.07899284362793, -4.061470031738281, -3.9180006980895996, -3.9779415130615234], "all_perturbed_original_ll": [-3.8339037895202637, -4.00725793838501, -4.689431190490723, -3.8080174922943115, -3.9484968185424805, -4.148340702056885, -4.080516338348389, -4.213494300842285, -4.148464679718018, -3.8985249996185303], "perturbed_sampled_ll": -3.9321467876434326, "perturbed_original_ll": -4.0776448249816895, "perturbed_sampled_ll_std": 0.1662074148641975, "perturbed_original_ll_std": 0.2426077627928905}, {"original": "Add Pinterest to Airflow users list (#19117)", "sampled": "Add Pinterest to Airflow users list (#19117)If", "perturbed_sampled": ["Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If", "Add Pinterest to Airflow users list (#19117)If"], "perturbed_original": ["Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)", "Add Pinterest to Airflow users list (#19117)"], "original_ll": -8.04257583618164, "sampled_ll": -8.27450942993164, "all_perturbed_sampled_ll": [-8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164, -8.27450942993164], "all_perturbed_original_ll": [-8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164, -8.04257583618164], "perturbed_sampled_ll": -8.27450942993164, "perturbed_original_ll": -8.04257583618164, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in celery_executor error log * Add test for airflow/configuration.py::as_dict", "sampled": "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "perturbed_sampled": ["Prevent mixed env vars from crashing processes with mixed names (#14380) * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed arguments without crashing * Include traceback mechanisms if not handled by tracing mechanism *", "Prevent mixed case env vars from crashing processes like worker * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "Prevent mixed case env vars from crashing processes like -wt * Handle misformed environment var without crashing * Include traceback in exception if not handled by tracing mechanism *", "Prevent mixed case env vars from crashing processes like worker (#14380) * Allow misformed env vars without crashing * Include traceback in exception if not handled using catch and hold mechanism *", "Prevent mixed case env vars for concurrent processes crash (#14380) * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "Prevent mixed env vars from crashing processes . (#14380) * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "Prevent mixed case env vars from crashing processes like worker (#14380) * Allow mixed case env vars without crashing * Include traceback in exception s handled by tracing mechanism *", "Prevent mixed case env vars from being handled like worker (#14380) * Handle misformed env vars without tracing mechanism * Include traceback in exception if not handled by tracing mechanism *", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crash * Include traceback in exception if not handled with crash mechanism *"], "perturbed_original": ["Prevent mixed env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing (#14379) * Include traceback in celery_executor error log * Add test for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like worker * Handle misformed env vars without crashing * Include traceback in celery_executor error log * Fix env_mode for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing * Fix env vars not like worker * Handle misformed env vars without crashing * Include traceback in celery_executor error log * Add test for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like swarm * Handle misformed env vars without crashing * Include traceback in the output log * Add test for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Write traceback in celery_executor error log * Added support for airflow/configuration.py::as_dict", "Prevent mixed env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in celery_executor error log * Add trap warning for all airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in celery_executor .py * Add test for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env s like driver that are crashing * Include traceback in celery_executor error log * Fix callout usage for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env without crashing * Include traceback s in error log * Add test for airflow/configuration.py::as_dict", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle mixed case env vars without crashing * Include traceback in error log * Add test for airflow/configuration.py::as_dict"], "original_ll": -4.966541767120361, "sampled_ll": -4.844755172729492, "all_perturbed_sampled_ll": [-4.63587760925293, -5.3869147300720215, -4.923811435699463, -5.385757923126221, -5.111875534057617, -4.847282886505127, -4.660635471343994, -5.010406494140625, -4.615832805633545, -4.991371154785156], "all_perturbed_original_ll": [-4.712242126464844, -5.14035177230835, -4.769775867462158, -5.077864646911621, -5.036457061767578, -5.165721416473389, -4.975595474243164, -5.425656318664551, -5.306670665740967, -4.800200462341309], "perturbed_sampled_ll": -4.95697660446167, "perturbed_original_ll": -5.041053581237793, "perturbed_sampled_ll_std": 0.26779955026860186, "perturbed_original_ll_std": 0.22119078712512913}, {"original": "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "sampled": "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "perturbed_sampled": ["[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * + * [AIRFLOW-5068] Fix incorrect warning when using some query parameters using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name (#5249) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add validation for the bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using bigquery hook by using invalid_query_parameter() in query_config() * AIRFLOW-5487 Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect default values of queries for using some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name generated with missing query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5051] Fix crash when using data_model_by_column", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 ] Fix incorrect warning , possibly caused by some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crashes when accessing nested tables with hql query (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add validation for src_fmt_configs in HQL (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when testing large queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5470] Fix wrong column name for HQL query in the root table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add support for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 * Fix bug with incorrect warning when using some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong warning if you use column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5814) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * [AIRFLOW-5209] * [AIRFLOW-5068] Fix incorrect warning when passing valid arguments in some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables in bigquery queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add validation for query_value parameter in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries from invalid_query_parameter() in query_config() * AIRFLOW-5410 * [AIRFLOW-5090] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested table from multiple queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "[AIRFLOW-5049] Add option to enable src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries with invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix error when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Fix incorrect warning when accessing data_model_by_column", "[AIRFLOW-5049] Add validation for a bigquery that contains any parameter * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for some fields in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Add validation for a largequery that should be used explicitly when accessing data_model_by_column"], "perturbed_original": ["[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds validation for config arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook * Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * #5671 - Add validation for src_fmt_configs in bigquery hook Add common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise , they would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be safely ignored which is non-desireable. * AIRFLOW-5049 - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * # [AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be ignored which is non-desireable. * [AIRFLOW-5049] Update the validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) \u2022 Change - Add validation for src_fmt_configs in bigquery hook Adds validation for src_fmt_configs arguments in bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. \u2022 Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for src_fmt_configs arguments in the bigquery hook. Otherwise src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds new method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for src_fmt_config arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * AIRFLOW-5049 - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. The wrong src_fmt_configs arguments are silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds the new validation method for the src_fmt_configs arguments in the bigquery hook. The src_ftm_configs", "Update - Add validation for src_fmt_configs in bigquery hook (#5671) * * Add validation for src_fmt_configs in bigquery hook Adds validation for configuration arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs"], "original_ll": -2.717329978942871, "sampled_ll": -2.891718626022339, "all_perturbed_sampled_ll": [-3.012007236480713, -2.9250857830047607, -3.012063503265381, -3.255235433578491, -2.9365270137786865, -3.0766921043395996, -2.9235293865203857, -2.753941535949707, -2.8868987560272217, -2.963900327682495], "all_perturbed_original_ll": [-2.6644530296325684, -2.805460214614868, -2.806480884552002, -2.6915667057037354, -2.596639394760132, -2.9180822372436523, -2.674595594406128, -2.7084014415740967, -2.452662467956543, -3.091057538986206], "perturbed_sampled_ll": -2.9745881080627443, "perturbed_original_ll": -2.7409399509429933, "perturbed_sampled_ll_std": 0.12449394556237915, "perturbed_original_ll_std": 0.16736040328601628}, {"original": "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, not just when the file itself changes.", "sampled": "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it much easier to keep up-to-date", "perturbed_sampled": ["Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it much easier to keep the chart schema up-to-date", "Also check chart schema when the schema itself changes (#15902) - We want our schema pre-commit hooks to also run when the schema itself changes, as it makes it much easier to keep up-to-date", "Also check chart schema when the schema itself commits. This changes our schema pre-commit hooks to also run when the schema itself commits, making it much easier to keep up-to-date", "Also check chart schema when the schema itself changes (#15902) This changes the pre-commit hooks to also run when the schema changes, making it much easier to keep up-to-date", "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it easier for us to keep up-to-date", "Also check if your schema hooks run when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it much easier to keep up-to-date", "Also check chart schema when the schema itself changes (#15902) This change allows chart schema pre-commit hooks to run when the schema itself changes, making it much easier to keep up-to-date", "Also check chart schema when the schema itself changes! This changes our schema pre-commit hooks to also run when the schema itself changes, making it easier to keep up-to-date", "our chart schema when the schema itself changes (#15902) This changes our chart hooks to also run when the schema itself changes, making it much easier to keep up-to-date", "runs when chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to only run when the schema itself changes, making it much easier to keep up-to-date"], "perturbed_original": ["Also check chart schema when the schema itself changes. This changes our schema pre-commit hooks to also run when the schema itself changes, not just when the schema itself changes.", "Also check chart schema when the schema itself changes. (#15902) This changes our schema pre-commit hooks to also check when the schema itself changes, not just when the file itself changes.", "Also check chart to run when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, not just when the file itself changes.", "Also check chart schema when the schema itself changes, not just when graph schema itself changes. (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, not just when just the chart schema itself changes.", "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit s to also run when the schema itself changes, and not when the file itself changes.", "Also check chart schema when the schema itself changes (#15902) This changes the pre-commit hooks to also run when the schema itself changes, not just when the current schema changes.", "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit script to also run when the schema itself changes, and when the file itself changes.", "Also check chart schema when the schema itself changes (#15902) This allows schema pre-commit hooks to also run when the schema itself changes, not just when the file itself changes.", "Also check chart schema when the schema itself changes (#15902) This changes the pre-commit hooks to also run when the schema actually changes, not just when the file itself changes.", "Also check chart ing is for not just the file, but when the schema itself changes (#15902) This changes our schema pre-commit hooks to run when the schema itself changes, not just when the file itself changes."], "original_ll": -4.254692077636719, "sampled_ll": -3.8770689964294434, "all_perturbed_sampled_ll": [-3.7932019233703613, -3.8855605125427246, -3.9205174446105957, -3.798543930053711, -3.798063278198242, -3.5030465126037598, -3.793088912963867, -3.9084362983703613, -3.696361541748047, -3.679502010345459], "all_perturbed_original_ll": [-3.959625720977783, -4.189500331878662, -4.215615749359131, -3.9928348064422607, -4.61817741394043, -4.063722133636475, -4.485867977142334, -4.214293003082275, -4.202502727508545, -4.583056926727295], "perturbed_sampled_ll": -3.777632236480713, "perturbed_original_ll": -4.252519679069519, "perturbed_sampled_ll_std": 0.11922341856157552, "perturbed_original_ll_std": 0.22261532037173928}, {"original": "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "sampled": "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "perturbed_sampled": ["Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "Allow to define custom XCom class closes: * Allow to define custom XCom class closes: #8059A6", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class; closes: #8059A6", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6"], "perturbed_original": ["Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class opens: #8469 Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "Allow to define custom XCom class (#8560) allows: #9047 Allow to define custom XCom class closes: #8059"], "original_ll": -4.427076816558838, "sampled_ll": -4.584950923919678, "all_perturbed_sampled_ll": [-4.584950923919678, -4.584950923919678, -4.947638988494873, -4.496926784515381, -4.584950923919678, -4.584950923919678, -4.548282146453857, -4.584950923919678, -4.947638988494873, -4.584950923919678], "all_perturbed_original_ll": [-4.427076816558838, -4.427076816558838, -4.427076816558838, -4.268922805786133, -4.427076816558838, -4.427076816558838, -4.80714750289917, -4.427076816558838, -4.427076816558838, -4.39580774307251], "perturbed_sampled_ll": -4.645019245147705, "perturbed_original_ll": -4.446141576766967, "perturbed_sampled_ll_std": 0.15365418333965797, "perturbed_original_ll_std": 0.12915476060814113}, {"original": "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "sampled": "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "perturbed_sampled": ["[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If"], "perturbed_original": ["[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)"], "original_ll": -5.876304626464844, "sampled_ll": -6.286564350128174, "all_perturbed_sampled_ll": [-6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174, -6.286564350128174], "all_perturbed_original_ll": [-5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844, -5.876304626464844], "perturbed_sampled_ll": -6.286564350128174, "perturbed_original_ll": -5.876304626464844, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "sampled": "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "perturbed_sampled": ["[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In"], "perturbed_original": ["[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)"], "original_ll": -5.826963901519775, "sampled_ll": -6.150223731994629, "all_perturbed_sampled_ll": [-6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629, -6.150223731994629], "all_perturbed_original_ll": [-5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775, -5.826963901519775], "perturbed_sampled_ll": -6.150223731994629, "perturbed_original_ll": -5.826963901519775, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a", "sampled": "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default behaviour) and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated after this branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "perturbed_sampled": ["Gracefully handle missing start_date s for projects. Issue for DagRun (#14452) closes: #14384 It fixes two issues: (1) the syntax for missing start_date and end_date is not used in the running projects (the default behaviour) and (2) it is possible to use incorrect code when using (2). Fixes #14354 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"set\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated before the branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "Gracefully handle missing start_date and end_date in DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default behaviour) and (2) it can lead to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14495. (partial). Fixed bug #14501 (code in a get/set/delete hook might overlap with objects created or updated before the current branch was merged). Fixes #14358 (partially). (partial), code in a hook might overlap with objects", "Gracefully handle missing start_date and end_date for DagRun . Fixes #14384 (partial). (partial) fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (expected behaviour) and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14356 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a hook might cause issues with objects created or updated after the project was merged). Fixes #14502 (partial).\n\n(code in a hook might cause issues with objects", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 -11452, fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default is missing); (2) it leads to incorrect callbacks for cases not using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when the above library's default value is not clear), caused by providing the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by providing the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects not updated after this branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "Gracefully handle missing start_date property for DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the behaviour for missing start_date and end_date is not applied to DagRun projects (the default behaviour) and (2) it leads to incorrect code when using (2). Fixes #14384 (partial). Fixed bug #14355 (when using a library's default library), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created before or after this branch was merged). Fixes #14358 (partial).\n\n(code in a get/set/delete hook might cause issues with objects", "Fixed issue about missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default behaviour) and (2) it leads to incorrect code when using the default constructor. Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14358 Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or modified after the hook was merged). Fixes #14502 (partial).\n\n(code in a hook might cause issues with objects", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is only applied to running projects (the default behaviour) and (2) it leads to incorrect configuration file access when using (2). Fixes #14393 . Fixed bug #14355 (when using a library's newer method), which was caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14377. Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated after this PR was merged). Fixes #14502 (partial).\n\n(code in a hook might cause issues with objects", "and end_date for DagRun. Fixes missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two bugs: (1) the fix for missing start_date and end_date is not applied to the start_date (the last day of the run), and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated after this branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "Gracefully handle missing start_date and start_time for DagRun (#14452) closes: #14384 This PR fixes 2 issues: (1) the fix for missing start_date and start_time is not applied to running projects (the default behaviour) and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14351. Fixed bug #14358 (code in a hook might cause issues with objects created ..) (for libraries created after this branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "Gracefully handle missing start_date and extension when modifying the DagRun (#14452) project parameters. (partial). This PR fixes two issues: (1) the fix for missing start_date and extension methods is not applied to running projects (the default behaviour) and (2) it leads to incorrect code when using (2). Fixes #14403. Fixed bug #14355 (when using a library's default methods, caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in the get/set/delete hook might cause issues with objects created or updated after this branch was merged). Fixes #14501, Code in a hook might cause issues with objects"], "perturbed_original": ["Gracefully Clear start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues. A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitioned a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date . An issue with the code which would clear the start_date and ending_date for a DagRun, if the scheduler was used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun had already been in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a", "Gracefully handle missing start_date and end_date for DagRun (#14452) - This patch fixes a combination of two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed or success state into a running state (or any other state). In some situations where the scheduler would determine the DagRun was running, but not in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a", "Gracefully removing start_date and end_date for DagRun (#14452) closes: #14384 . This patch fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if that form was used to transition a Task from a failed state back into a running state (or succeeding state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the task instances had been updated), then this would lead to a", "Gracefully handle non-empty start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when it transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form is used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the start and end dates weren't cleared), then this would raise a", "Gracefully handle missing start_date and end_date . PR (#14452) closes: This PR fixes two issues: 1) An issue that would be thrown by _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have either a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or from a successfully finished state back into a running state). In the event where the scheduler would determine the DagRun should've been in the failed or success state, because the start_date and end_date instances weren't cleared), then this would lead to a", "Gracefully handle missing start_date and end_date for DagRun (#14452) - This PR fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler would transition a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date of any DagRun, if the form was used to transition a DagRun from a success or failed state into a running state (or any other state). In the event where the scheduler would determine the DagRun should've been in either a running or success state (e.g. because the task instances weren't cleared), then this would lead to a", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be raised from the scheduler when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date and end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for the DagRun if the form was used to transition a DagRun from a failed state to a running state (or to a progress state). In the event where the scheduler transitioned after the DagRun should've been in the failed or success state (e.g. because the start_date and end_date weren't cleared), then this would lead to a", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two possible issues: 1) A TypeError that would be raised from when the scheduler transitions a DagRun -based instance from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date fields on a DagRun, if the form was used to transition a DagRun-based instance from a failed state back into a running state (or other state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the instances weren't cleared), then this would lead to a", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be triggered after calling _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state, where the DagRun did have a start_date or end_date set. 2) An issue with the form that would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or a failing state or success state). In cases where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitioned the DagRun from a running state into a success or failed state, and the DagRun did not have a start_date or an end_date specified; and 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler transitioned the DagRun incorrectly while it should have been in the failing state or success state (e.g. because the task instances weren't visible), this would lead to a"], "original_ll": -3.084679126739502, "sampled_ll": -2.28708553314209, "all_perturbed_sampled_ll": [-2.567445993423462, -2.586458921432495, -2.297912836074829, -2.601163148880005, -2.49470591545105, -2.3897082805633545, -2.7372841835021973, -2.2176413536071777, -2.5384111404418945, -2.969564437866211], "all_perturbed_original_ll": [-3.127760171890259, -2.9663772583007812, -3.119039535522461, -3.06061053276062, -3.060276985168457, -3.008720874786377, -2.8503129482269287, -3.082465648651123, -3.1058006286621094, -3.1170895099639893], "perturbed_sampled_ll": -2.5400296211242677, "perturbed_original_ll": -3.0498454093933107, "perturbed_sampled_ll_std": 0.204269983753001, "perturbed_original_ll_std": 0.08262653402313953}, {"original": "Remove unnecessary string concatenations in AirflowException messages (#18817)", "sampled": "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "perturbed_sampled": ["Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Remove unnecessary string concatenations in AirflowException messages (#18817)For"], "perturbed_original": ["Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Remove unnecessary string concatenations in AirflowException messages (#18817)"], "original_ll": -5.398134708404541, "sampled_ll": -6.124137878417969, "all_perturbed_sampled_ll": [-6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969, -6.124137878417969], "all_perturbed_original_ll": [-5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541, -5.398134708404541], "perturbed_sampled_ll": -6.124137878417969, "perturbed_original_ll": -5.398134708404541, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "sampled": "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "perturbed_sampled": ["Fixes a couple of bad behavior in tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test suite failures for this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.1. Upgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Upgrade to V8 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Upgrade to Version 2.1.4-rc2-1\n\nUpgrade to V8 2.2.3. Upgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views (#14341) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures due to new animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests (#14599) reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations . This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 496ae49f07d03876 which caused a number of failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. GitHub Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Features a few test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which fixes a few test failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit #15112. There were issues that caused a few test failures with this animation , see #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to"], "perturbed_original": ["Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. I am not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in Git repositories back in 2016. I am not sure what happened here but this should be resolved for Master.", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should be on Master.", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change if we detect a cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happened here, made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "Fixes failing test_views tests. This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 . We are not sure what happened here but this should fix the Master.", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 . We are not sure what happened ded. but this should fix the Master.", "Fixes failing test_views tests . this reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure why that commit change happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "Fixes failing test_views tests and reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in this repository. I am not sure what happened here but this should fix the Master.", "against test_views tests (#14599) This reverts commit #114480 but not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master."], "original_ll": -4.303369045257568, "sampled_ll": -3.03253436088562, "all_perturbed_sampled_ll": [-2.99794602394104, -3.1740217208862305, -3.200279951095581, -3.309290647506714, -3.0428431034088135, -3.1944732666015625, -3.0871059894561768, -3.2791693210601807, -3.1595399379730225, -3.0393474102020264], "all_perturbed_original_ll": [-4.277860164642334, -4.191335201263428, -4.304385662078857, -4.363253593444824, -4.254263401031494, -4.216451644897461, -4.446557998657227, -4.342594146728516, -4.143388271331787, -4.176246643066406], "perturbed_sampled_ll": -3.1484017372131348, "perturbed_original_ll": -4.271633672714233, "perturbed_sampled_ll_std": 0.0989996677650124, "perturbed_original_ll_std": 0.08971052112941509}, {"original": "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used an uppercase E. Also made the exception a bit clearer when running on kubernetes.", "sampled": "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they log the return code with a space before the space. This results in different exit", "perturbed_sampled": ["the submit operator for Spark 4 (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they log the return code with a space before the space. This results in different exit", "Update Spark submit operator for Spark 3 and Spark 2. In spark 3 they log the exit code with a space after the e, in spark 2 they log the return code with a space before the r. This results in different exit", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the output with a lowercase e, in spark 2 they log the return code with a space and spark 3 with a space. This results in different exit", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a space followed by a return code in spark 2 they log the return code with a space past the space. This results in different exit", "Update Spark submit operator for Spark 3 / 3. In spark 3 they log the exit code with a lowercase e, in spark 4 they log the return code with a space before the space. This results in incorrect exit", "the data submit operator for the spark support (#8730) In spark 3 they log the exit code with a space before the space and in spark 2 they log the return code with a space before the space. This results in different exit", "Update d operator for Spark 3 support (#8730) In spark 3 they log the return code with a lowercase space before the space and in spark 2 they log the return code with a space before the space. This results in different exit", "Update Spark submit operator for Spark 2 (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they log the return code with a space before the e. This results in an exit", "Update Spark submit operator for Spark 3 support ? Note in spark 3 we log the exit code with a lowercase e, in spark 2 we log the return code with a space before the space. This results in different exit", "Update Spark 2 plugin for Spark 3 support (#8730) In spark 3 they log the exit code with a space in spark 2 they log the return code with a colon before the space. This results in different exit"], "perturbed_original": ["Update Spark submit operator for Spark 2 and Spark 3 to support (#8730) In spark 3 they replaced the exit code with a lowercase E. In spark 2 they used an uppercase E. Also made the exception a bit clearer when running on kubernetes.", "Update Spark 2 for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used an uppercase e. Update Spark2 for Spark 3 support They made the exception a bit clearer when running on kubernetes.", "Update Spark submit operator for Spark 4. (#8730) In spark 3 they log the spark source code with a lowercase e, in spark 2 they used an uppercase E. Also made the exception rules clearer when running on kubernetes.", "Update Spark submit operator and spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used a uppercase E. Also made the exception a bit clearer when running on kubernetes.", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the data with a lowercase e, in spark 2 they log the data with an uppercase E. Also made this a bit clearer when running on kubernetes.", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 the commas get interpreted in Spark 3 log the code with a lowercase e, in spark 2 they used an uppercase E. Also made the exception a bit clearer when running on kubernetes.", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they replaced the Spark exit code with a lowercase N, in spark 2 they used an uppercase E. Also made the syntax a bit clearer when running on kubernetes.", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used an uppercase E. Also , Make the error exception a bit clearer to avoid some confusion on kubernetes.", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the message with a lowercase e, but in spark 2 they used an uppercase E. Also made the exception a bit bit easier to handle in Spark 3 running on kubernetes.", "Update Spark submit operator for kubernetes support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used an x Also made the exception code clearer when running on kubernetes."], "original_ll": -4.301545143127441, "sampled_ll": -4.08167028427124, "all_perturbed_sampled_ll": [-4.103590965270996, -3.6709706783294678, -4.339546203613281, -4.190616607666016, -4.124435901641846, -3.941899061203003, -3.9499285221099854, -4.0531463623046875, -4.179807662963867, -4.239597320556641], "all_perturbed_original_ll": [-4.02084493637085, -3.878856658935547, -4.547970294952393, -4.374070167541504, -3.900792121887207, -4.464496612548828, -4.208984851837158, -4.494144916534424, -4.121815204620361, -4.793032169342041], "perturbed_sampled_ll": -4.079353928565979, "perturbed_original_ll": -4.280500793457032, "perturbed_sampled_ll_std": 0.17932557303178093, "perturbed_original_ll_std": 0.2873491503767321}, {"original": "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring exceptions here we let the failure be exposed where its broken, not in the next test that happens to run.", "sampled": "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "perturbed_sampled": ["Fix broken MSSQL test (#17797) This broken test was causing the next test on the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was working as expected: [TestDriverTest] ] [TestDriverTest #run] [ExceptionHandler]", "Fix broken MSSQL test (#17797) This broken test was causing the test to use the db to fail. Also, by not ignoring the crash, my test wasn't failing; in fact it was running as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "The MSSQL test (#17797) This broken test was causing the next test for the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "by the MSSQL test, it was broken. This broken test was causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "Fix broken MSSQL test. This broken test , when ignored, caused the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "Fix broken MSSQL test (#17797) This broken test was causing the test to use the db to fail. Also, by fixing the test at the command line, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "Fix broken MS SQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: Exception: <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not breaking the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> ; [ExceptionHandler]", "Fix broken MSSQL test: This broken test was causing the test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing because I was ignoring the test. [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "the MSSQL test (#17797) This broken the test causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing again, so why was it failing? [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]"], "perturbed_original": ["Fix broken MSSQL test (#17797) This broken test was going to cause the next test to use the db to fail. Also, instead of ignoring exceptions here we let the failure be exposed where its broken, not in the next test for the database to run.", "Fix broken MSSQL test (#17797) This problem was causing the next test to use the db connection to the problem SQL. Also, by not ignoring exceptions here we let the failure be exposed where its broken, not in the next Test it happens to run.", "Fix broken MSSQL test . I believe that the broken test was causing the test to use the db to fail. Also, by not ignoring exceptions here we let the failure be exposed where it would have been not in the next test that happens to run.", "Fix broken test here (#17797) This broken test was causing the next test to use the db to fail. Instead of not ignoring exceptions here we let the failure be exposed where its likely to show up in the next test that happens to run.", "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by passing the exceptions here we let the failure report occur where its broken, not in the next test that happens to run.", "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the DB and fail. Also, by not ignoring exceptions here we let the test be exposed where its broken, not just to the next test that happens to run.", "Fix broken MSSQL test (#17797) This broken test was causing the next test that tested the db to fail. So by not ignoring exceptions here we let the failure be exposed where its broken, not just to the next test that happens to run.", "Fix broken MSSQL test (#17797) The broken MSSQL test was causing the next test running on the db to fail. Also, by not ignoring exceptions here we only cause the failure be exposed where its broken, not in the next test that happens to run.", "Fix broken test (#17797) This broken test was causing a test to use the db to fail. Also, by not ignoring exceptions in this failing test, the previous test let the failure be exposed where its broken, not in the next test that happens to run.", "database and MSSQL . This broken test was causing the next test to use MS SQL to fail. Also, by not ignoring exceptions here we let the failure be exposed where its broken, not in the next test that happens to run."], "original_ll": -4.434438705444336, "sampled_ll": -3.2608354091644287, "all_perturbed_sampled_ll": [-3.6267380714416504, -3.3880887031555176, -3.2180445194244385, -3.110334634780884, -3.142397403717041, -3.281393527984619, -4.029016017913818, -3.3845903873443604, -3.014984369277954, -3.4034173488616943], "all_perturbed_original_ll": [-4.337409496307373, -4.726256370544434, -4.1822991371154785, -4.34549617767334, -4.421884059906006, -4.224625110626221, -4.241457462310791, -4.078309059143066, -4.356151580810547, -4.399192810058594], "perturbed_sampled_ll": -3.3599004983901977, "perturbed_original_ll": -4.331308126449585, "perturbed_sampled_ll_std": 0.27875797143805703, "perturbed_original_ll_std": 0.165927328946062}, {"original": "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "sampled": "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "perturbed_sampled": ["Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For"], "perturbed_original": ["Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)"], "original_ll": -6.469966411590576, "sampled_ll": -6.97290563583374, "all_perturbed_sampled_ll": [-6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374, -6.97290563583374], "all_perturbed_original_ll": [-6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576, -6.469966411590576], "perturbed_sampled_ll": -6.97290563583374, "perturbed_original_ll": -6.469966411590576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "sampled": "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "perturbed_sampled": ["[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by means of add directives :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , >> // @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we need to include some optional parts by adding @include directive . @include : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude explicitly in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"main.c\" : #include \"prog.c\" <unk> \"main.c\" : #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by adding @include : : #include \"myfuncode.cc\" , #include : : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst to explicitly include some optional parts by adding @include :\n\n{ - #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"main.c\" , @include \"myfuncode.cc\" , #include \"prog.c\" @include #include \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we added some directives in tutorial.rst where we explicitly include some functions by adding @include :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts in the tutorial.rst. @include directives : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in your code. Recently we hard code in tutorial.rst where we explicitly include some optional parts by using include directives. <unk> :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst - we have an example in tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include"], "perturbed_original": ["[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard coded because something such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Can we use exampleinclude directive that is a better way", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as if you change to shift in tutorial.py or change to shift in tutorial.rst. Use sphinx is a better way", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as `set_upstream` is a shift in tutorial.py but still in rsync. Surely sphinx is a better way", "Use sphinx of exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard code to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "Fix exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "Remove exampleinclude directives in tutorial.rst (#6868) Recently we changed exampleinclude directives in tutorial.rst which is hard to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently some of code in tutorial.rst which are difficult to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "[AIRFLOW-6316] New hard coded directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to use such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "[AIRFLOW-6316] Use Sphinx in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as `set_upstream` etc. We use to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard -coding such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is only way"], "original_ll": -4.7604193687438965, "sampled_ll": -3.431135654449463, "all_perturbed_sampled_ll": [-4.077353000640869, -3.6513750553131104, -3.5201642513275146, -4.0913777351379395, -3.6709554195404053, -3.6108241081237793, -3.2568697929382324, -3.636995315551758, -3.7560737133026123, -3.133946180343628], "all_perturbed_original_ll": [-4.602783203125, -4.499493598937988, -5.07183313369751, -4.500198841094971, -4.534996032714844, -4.087081432342529, -4.605074405670166, -4.564629077911377, -4.392092227935791, -5.198458671569824], "perturbed_sampled_ll": -3.6405934572219847, "perturbed_original_ll": -4.6056640625, "perturbed_sampled_ll_std": 0.28769372833667034, "perturbed_original_ll_std": 0.30204991176995244}, {"original": "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "sampled": "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default role, though it's still better because it improves concurrency in the core. (This is really hard to evaluate, since they don't really know to", "perturbed_sampled": ["to simplify role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default role, though it's a good thing because it improves concurrency in the core. (This is really hard to read to my guys since they don't really know to", "Faster default role syncing during webserver start . It makes use of bigger queries instead of many queries when syncing the default role, though it's still better because it improves performance of the core. (This is really hard to evaluate, since they don't really know to", "Faster default role syncing during Syncing process. (#15017) This makes a handful of bigger queries instead of many queries when syncing the default role, though it's still better because it only has one of those queries in the core. (This is tricky for people to evaluate, since they don't really know to", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries . This is the default behavior now, however it's still better because it improves concurrency in the core. (This is really hard for beginners since they don't really know to", "Faster default role and webserver start (#15017) makes use of a handful of bigger queries instead of many queries when syncing the default role, though it's still better because it improves efficiency for the core. (This is really hard to evaluate, since they don't really know to", "Faster default role syncing during webserver start (#15017) : the benefit is only a handful of bigger queries instead of many queries when you choose a default role, but it's still better because it improves concurrency in the core. (This is really hard to evaluate, since they don't really know to", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the roles, though it's still better because it improves concurrency in the core. And it's really hard to use these as they don't really know to", "Faster default role syncing during webserver start (#15017) This makes a handful of statements instead of queries when syncing the default role, though it's still better because it improves concurrency in the process. This is really hard to evaluate, since they don't really know to", "Faster default role syncing during webserver start (#15017) This makes a handful of queries faster instead of many queries when syncing the default role, though it's still better than many. This also improves concurrency in the core. Thanks, is really hard to evaluate, since they don't really know to", "Faster default role syncing during webserver start (#15017) This makes sure that they use a single set of queries instead of many queries when syncing the default role, though it's not so important because it improves concurrency in the core. (This is really hard to evaluate, since they don't really know to"], "perturbed_original": ["Faster default role syncing during webserver start up: makes a handful of bigger queries instead of multiple smaller queries when syncing the default Airflow roles. On my local version with 5k DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "Faster default role syncing at start (#15017) This makes a handful of queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "Faster default role syncing during webserver configuration. This makes a handful of queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second in startup time (which made tests faster too).", "Faster default role syncing during webserver start (#15017) This makes a handful of queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to an immediate reduction of 4-6 minutes in startup time (bonus, makes tests faster too).", "Faster default role syncing during webserver start (#15017) This makes a handful of queries instead of many queries when syncing the default roles. On my machine with 5k DAGs, this led to a decrease of <unk> 1 second in startup time (bonus, makes tests faster too).", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger tests faster than running one of many queries when syncing the default Airflow roles. On testing with 5k DAGs, this led to a reduction of 1 second in startup (and makes tests faster too).", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries more scalable, decreasing the time of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this resulted a reduction of 1 second in startup time (and makes other tests faster too).", "Faster default role syncing during webserver start (#15017) adds a handful of bigger JS to the outputs of many queries when syncing the default Airflow roles. On my machine running on multiple DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default roles. On a machine with 5k DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "Faster role syncing during tests; (#15017) This makes a handful of bigger queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second test time (bonus, makes tests faster too)."], "original_ll": -4.243828296661377, "sampled_ll": -3.9926559925079346, "all_perturbed_sampled_ll": [-4.211739540100098, -3.993569850921631, -4.018751621246338, -4.190281391143799, -4.264878749847412, -4.086319923400879, -4.176085472106934, -3.9461045265197754, -4.112504005432129, -3.7613983154296875], "all_perturbed_original_ll": [-4.30172061920166, -4.149094104766846, -3.993927478790283, -4.1585259437561035, -4.259230613708496, -4.47297477722168, -4.255712985992432, -4.511914253234863, -4.120694637298584, -4.4785261154174805], "perturbed_sampled_ll": -4.076163339614868, "perturbed_original_ll": -4.270232152938843, "perturbed_sampled_ll_std": 0.14301597103979788, "perturbed_original_ll_std": 0.1643420193123414}, {"original": "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "sampled": "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "perturbed_sampled": ["Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects references to dist with an `@` URL but still", "Don't reference sphinx airflow theme via sphinx web browser in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an invalid content. but still", "Don't reference sphinx airflow theme via `@` anymore, it violates certain Python build requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects all dist with an `@` URL but still", "Don't reference sphinx file via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "Don't reference sphinx airflow theme via `@` URL in requirements. It rejects uploading a dist with an `@` URL but still", "Don't reference sphinx /git via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "Don't reference sphinx airflow theme in URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "Don't reference sphinx airflow theme /app URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still"], "perturbed_original": ["Don't reference sphinx airflow theme via `@` URL in requirements. Sphinx rejects uploading a dist with an `@` in the requirements.", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist /airflow theme from code containing `@` in the requirements.", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist file via `@` in the requirements.", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading sphinx theme with an `@` in the requirements.", "Don't upload airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "uploading sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "Don't reference sphinx _dist via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "uploading hyderas sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "Don't reference sphinx airflow theme /sphinx URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "Don't reference sphinx airflow theme via `@` URL in dist. PyPI rejects uploading a dist with an `@` in the requirements."], "original_ll": -5.419686317443848, "sampled_ll": -5.618743896484375, "all_perturbed_sampled_ll": [-5.564445972442627, -5.911214351654053, -6.182334899902344, -5.494503021240234, -5.646717071533203, -5.268867015838623, -5.86021089553833, -5.418024063110352, -6.301289081573486, -6.441191673278809], "all_perturbed_original_ll": [-5.441112041473389, -5.5575103759765625, -5.317743301391602, -5.051941871643066, -5.52628755569458, -5.537498950958252, -5.22947359085083, -5.742138385772705, -5.902049541473389, -5.337798118591309], "perturbed_sampled_ll": -5.808879804611206, "perturbed_original_ll": -5.464355373382569, "perturbed_sampled_ll_std": 0.3773752216087227, "perturbed_original_ll_std": 0.2345014836759864}, {"original": "fix tests (#11368)", "sampled": "fix tests (#11368)When", "perturbed_sampled": ["fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When", "fix tests (#11368)When"], "perturbed_original": ["fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)", "fix tests (#11368)"], "original_ll": -6.143937110900879, "sampled_ll": -7.5693182945251465, "all_perturbed_sampled_ll": [-7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465, -7.5693182945251465], "all_perturbed_original_ll": [-6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879, -6.143937110900879], "perturbed_sampled_ll": -7.5693182945251465, "perturbed_original_ll": -6.143937110900879, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update node installation cmd (#10744)", "sampled": "Update node installation cmd (#10744)When", "perturbed_sampled": ["Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When", "Update node installation cmd (#10744)When"], "perturbed_original": ["Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)", "Update node installation cmd (#10744)"], "original_ll": -7.4543538093566895, "sampled_ll": -8.300008773803711, "all_perturbed_sampled_ll": [-8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711, -8.300008773803711], "all_perturbed_original_ll": [-7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895, -7.4543538093566895], "perturbed_sampled_ll": -8.300008773803711, "perturbed_original_ll": -7.4543538093566895, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner", "sampled": "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "perturbed_sampled": ["Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. Add UI improvements to icons - Added", "Swap s error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "- Fixed import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. I fixed them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This was fixed to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "Swap dag s in dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close icons. [fixed] - Added", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to align with the expand/close buttons. (#18209) [fixed] - Added"], "perturbed_original": ["Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract icons in the dag error banner", "Swap dag error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the rest of the icons and whole dag error banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole window banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This was to be consistent with the expand/contract the whole dag error banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be in the same order as the expand/contract the whole dag error banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons - This swaps them to be consistent with the expand/contract the whole dag error banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons are just the default values. This swaps them to be consistent with the expand/contract the whole dag error banner", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. I don't think there is a way to change them to be consistent with the expand/contract the whole dag error banner", "Swap dag import error dropdown icons . The Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner"], "original_ll": -5.944174766540527, "sampled_ll": -4.573419094085693, "all_perturbed_sampled_ll": [-5.000829696655273, -4.325081825256348, -4.573419094085693, -4.3434038162231445, -4.4470977783203125, -4.376029968261719, -4.563589572906494, -4.952391147613525, -4.573419094085693, -4.643925666809082], "all_perturbed_original_ll": [-5.591085910797119, -5.843509197235107, -5.485189437866211, -5.702845573425293, -5.917616367340088, -5.603195667266846, -6.206526279449463, -5.707099437713623, -5.029283046722412, -6.294229030609131], "perturbed_sampled_ll": -4.579918766021729, "perturbed_original_ll": -5.73805799484253, "perturbed_sampled_ll_std": 0.22370582210239806, "perturbed_original_ll_std": 0.3435755145361178}, {"original": "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "sampled": "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "perturbed_sampled": ["[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If"], "perturbed_original": ["[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "[AIRLFOW-XXX] Display other integrations in single table (#6133)"], "original_ll": -6.349506378173828, "sampled_ll": -6.7463698387146, "all_perturbed_sampled_ll": [-6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146, -6.7463698387146], "all_perturbed_original_ll": [-6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828, -6.349506378173828], "perturbed_sampled_ll": -6.7463698387146, "perturbed_original_ll": -6.349506378173828, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg in example DAG", "sampled": "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "perturbed_sampled": ["Replacing non-attribute template_fields for DBSMSP (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS ES(no template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BSP_CAS (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS CALE (Replaceing attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator with non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BSP_CAS) * Replacing", "no-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing template_fields for BigQueryToMsSqlOperator); * Replacing"], "perturbed_original": ["Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * New source_project_dataset_table arg in example DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields in source_project_dataset_table * Updating source_project_dataset_table arg in example DAG", "Replacing parameter template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg in example DAG", "* Replacing attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg in example DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table to use example DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg s for MSI BIG DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table s with example DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg in example DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) Replaces source_project_dataset_table arg in example DAG", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) // the source_project_dataset_table arg in example DAG"], "original_ll": -3.9587743282318115, "sampled_ll": -3.110168695449829, "all_perturbed_sampled_ll": [-3.812176465988159, -3.6670241355895996, -3.3329005241394043, -3.3111557960510254, -3.110168695449829, -3.64996337890625, -2.8796091079711914, -3.189455270767212, -3.292880058288574, -3.3308119773864746], "all_perturbed_original_ll": [-3.661855459213257, -3.989304304122925, -4.223495006561279, -4.09412145614624, -3.779676675796509, -4.2447896003723145, -3.9098262786865234, -3.9587743282318115, -3.7552690505981445, -3.899568796157837], "perturbed_sampled_ll": -3.357614541053772, "perturbed_original_ll": -3.9516680955886843, "perturbed_sampled_ll_std": 0.26711476799838124, "perturbed_original_ll_std": 0.18389673759094644}, {"original": "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a DockerOperator that can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random return [random.random() for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as this would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash). To work with these requirements, we use base64 encoding to store a jinja generated python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "sampled": "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the code above, the output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "perturbed_sampled": ["Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder Add the ability to run @task.docker on a python function and turn it into a docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This fixes a few problems ; as you can see in the code above, the output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Add a event decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a docker event in a process builder , which I could do with 'file.sh'. This solves a few problems. As you can see in the code above, the output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs are fixed \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix image configurations (#16060) This has been a pain in", "Add a Docker Taskflow decorator (#15330) that would provides the ability to run @task.docker on a python function and turn it into a Docker event in a workflow. builder\n\non a python function and turn it into a docker event in a workflow builder. Replace task.docker.run('my_task'), as previously added, with that function. This change solves a few problems as you can see in the code above, the output of a function is now added and processed directly into the workflow before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our taskflow instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task flow instead. dockerfile.sh \u2013 This has been a pain in", "Script Executable. Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a docker event in a process . Add the ability to run @task.docker on a python function and turn it into a docker event in a process . And replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few of the bugs that you can see in the code \u2013 A output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Add a Docker Taskflow decorator (#15330) Add the ability to put an @task.docker on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a Docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the code above, the output file gets loaded and processed directly into a task's process builder before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an an an image, the workflow now allows our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Add a Docker event to a process builder (#15330) Add the ability to run @task.docker .run on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the Dockerfile.sh output, the output file gets emptied before they are created and processed directly into a directory before a task can launch . Workflow update flow and configs \u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder Add the ability to run @fon.docker on a python function and turn it into a docker event in a process builder Replace the dockerfile decorator, I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the code above, the output file gets added and processed directly into a directory before we can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Add a Docker Taskflow decorator that offers the ability to run @task.docker on a python function and turn it into a Docker event. Run a process builder\n\non a python function and turn it into a docker event . Use process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the console above, the output file gets added and processed directly into a directory before a task can launch . Specify the flow and image configs \u2013 In the case of my app not being able to start if a client cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a client cannot connect to an an image, the workflow now allows changing our image to use our task instead. Documentation for importing image files \u2013 This has been a pain in", "Add a Docker Taskflow decorator (#15330) Add the ability to run on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a Docker event in a process builder Replace task.docker.run('my_task'), which requires a file to be added, with 'file.sh'. This solves a nagging issue here\u2014as you see in the code , the output file gets added and processed directly into a directory before a task is run on an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Add a Docker event builder (#15330) Add the ability to run builder on a python function and turn it into a Docker event in a task Add the ability to run builder\n\non a python function and turn it into a docker event in a process builder Replace the docker executable I previously added, with 'file.sh'. This solves a few old issues. As you can see in the code above, the output file gets added , directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in"], "perturbed_original": ["Add a Docker Taskflow Operator based on the AirFlow worker. Add the ability to run @task.docker .toast. Get a python function and turn it into a DockerOperator that can run a function remotely. ( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random return [random.random() for i d]. ``` One notable aspect of this architecture is that we designed and build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as that would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 ). To work with these requirements, we use a filesystem to store a jinja generated python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a container and turn it into a DockerOperator that can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random return [random.random() for i in range(10000000)] ``` One notable aspect of the architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the user machine and the container as this would break if the user runs the airflow worker on a docker container. We could not assume that users would have specialized system libraries on their images (this implementation only requires python 3 and bash). To work with these requirements, we use base64 encoding to store a link in all python file and inputs are generated at the same time (based on output output by the PythonVirtualEnvOperator). Once the container starts, it uses these", "Add a Docker Taskflow decorator (#15330) Add the ability to run the worker inside a container. To get a python function and bring it into a container that can run that python function , run the following: @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', default=true) f(): import random return [random.random() for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as that would break if the user runs the worker on a volume. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash). To work with these requirements, we use base64 encoding to store a jinja generated python file and inputs (which are generated using the same functions as the PythonVirtualEnvOperator). When a container starts, it uses these", "The Docker Taskflow decorator (#15330) adds the ability to run @task.docker on a python image and turn it into a DockerOperator that can run that image remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random _files array [value for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as this would break if the user runs the airflow worker on a docker image, and we could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash). To work with these requirements, we use base64 authentication to store a jinja generated python file and then call the file in some other form (these strings are generated using the same functions as in the PythonVirtualEnvOperator). Once the container image is created, it uses these", "Add a Docker Operator (#15330) Add the ability to run @task.docker on a python function and turn it into a DockerOperator that can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, , api_version='auto', ) def f(): : return [random.random() for i in range(10000000)] ! What's notable about this architecture is that we had to build it to make as few assumptions about the system and image setups as possible. We could not share a volume between the worker and the docker container, as this way we would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only supports Python 3 and bash). To work with these requirements, we wrote an adio encoding to store a jinja generated python file and inputs to the job (these would be generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "Add a Docker Taskflow decorator (#15330) Add an architecture to run taskflow scripts that take a python function and turn it into a docker icon. <unk> So you can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", #150333 ) def f(): import random return [y i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and container as the worker would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and 3. To work with these requirements, we use base64 encoding to store a jinja generated python file and inputs are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, we use these", "Add a Docker Taskflow decorator to the ability to run @task.docker on a python function and turn it into a DockerOperator that can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", <unk>) <unk> def f(): import random return [random.random() for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups . The only assumptions were that there will be docker images. We could not use a volume between the worker and the container as this would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (the Docker Taskflow decorator only requires python and bash). To prevent these assumptions happening, we use space to store a jinja data file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a task that can run that python function via python taskflow. execute @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random return [random.random() for i ] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions for setups as possible. For example we could not share a volume between the image and the container as this would break if the user runs the airflow worker on a different volume. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash). To deal with this we use base64 encoding to store a shared python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container boots, it uses these", "Add a Docker Taskflow decorator (#15330) Add a decorator to run @task.docker on a python function and turn it into a DockerOperator that can run that function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, python=3.8.8, api_version='auto', ) def f(): import random func i [test for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups and configuration as possible. We could not share a volume between the worker and the container as this would break if the worker runs the airflow s as a docker container. We could not assume that users would have any specialized system libraries on their images (this , in particular, requires python 3 and bash). To work with these requirements, we use base64 encoding to store file names when generated , and a simple version of our API for displaying all commands and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "Add a Docker Taskflow decorator (#15330) Add the ability to call @task.docker on a python function and turn it into a DockerOperator that can run a python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import . <unk> <unk> [random.random() for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about the user as possible. We could not share a volume between the worker and the container as this would break docker if the user runs the airflow worker on a docker container. We could not assume that users would wish to switch to specialized system libraries and images (this implementation uses python 3 and bash). To comply with these requirements, we use base64 encoding to store a static python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these"], "original_ll": -3.4138376712799072, "sampled_ll": -2.61106014251709, "all_perturbed_sampled_ll": [-2.577392816543579, -2.7619855403900146, -2.918623924255371, -2.7124292850494385, -2.639249086380005, -2.745194435119629, -2.719146490097046, -3.0114834308624268, -2.7186968326568604, -2.8218624591827393], "all_perturbed_original_ll": [-3.784468650817871, -3.4858407974243164, -3.4649767875671387, -3.5744311809539795, -3.6655704975128174, -3.8206562995910645, -3.50693678855896, -3.5884275436401367, -3.700929880142212, -3.5637800693511963], "perturbed_sampled_ll": -2.7626064300537108, "perturbed_original_ll": -3.6156018495559694, "perturbed_sampled_ll_std": 0.12081048876703104, "perturbed_original_ll_std": 0.11695895084158991}, {"original": "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "sampled": "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "perturbed_sampled": ["Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew"], "perturbed_original": ["Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472"], "original_ll": -4.88201379776001, "sampled_ll": -4.845544338226318, "all_perturbed_sampled_ll": [-4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318, -4.845544338226318], "all_perturbed_original_ll": [-4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001, -4.88201379776001], "perturbed_sampled_ll": -4.845544338226318, "perturbed_original_ll": -4.88201379776001, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "sampled": "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "perturbed_sampled": ["Unify command names in CLI (#10720) * Unify command names in CLI * Message coding fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI and CLIA * Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI A (#10721) * Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "Unify command names in CLI (#10720) * Unify command names in CLI * Unify command names in CLIA"], "perturbed_original": ["Unify command names in CLI (feature) * Fixup! (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI * fixup! Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI (#10720) Fixup! Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI * fixup! * Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI * fixup! Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI * Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI"], "original_ll": -4.076107978820801, "sampled_ll": -4.379867076873779, "all_perturbed_sampled_ll": [-4.754877090454102, -4.379867076873779, -3.5974321365356445, -4.379867076873779, -4.379867076873779, -3.624450922012329, -4.379867076873779, -4.379867076873779, -4.379867076873779, -3.72187876701355], "all_perturbed_original_ll": [-3.755143404006958, -3.113196611404419, -3.898963212966919, -4.076107978820801, -3.3129796981811523, -3.113196611404419, -3.775672197341919, -4.076107978820801, -4.076107978820801, -4.076107978820801], "perturbed_sampled_ll": -4.19778413772583, "perturbed_original_ll": -3.727358365058899, "perturbed_sampled_ll_std": 0.37747810026520423, "perturbed_original_ll_std": 0.380152420729939}, {"original": "[AIRFLOW-4000] Return response when no file (#4822)", "sampled": "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "perturbed_sampled": ["[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara"], "perturbed_original": ["[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)", "[AIRFLOW-4000] Return response when no file (#4822)"], "original_ll": -6.41796350479126, "sampled_ll": -6.605205059051514, "all_perturbed_sampled_ll": [-6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514, -6.605205059051514], "all_perturbed_original_ll": [-6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126, -6.41796350479126], "perturbed_sampled_ll": -6.605205059051514, "perturbed_original_ll": -6.41796350479126, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "sampled": "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "perturbed_sampled": ["Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #17024 SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: remove call(Object#value,", "#17024 Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: if call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Refactor SQL/BigQuery/DruID Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,"], "perturbed_original": ["Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This release refactors three SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() method for a database kwarg *CheckOperators for a consistent check pattern use _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create class to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators to reduce duplicated code PR #10271 PR #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how BigQuery implements the generic SQL operators retrieve DB hook with the method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with default __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 to #27424 refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to remove duplicates with the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicates replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid duplicated code use class", "Refactor SQL/BigQuery/Qubole/Druid Check Operators closes: #10271 related: #9844 PR #1753 PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how some of the generic operators retrieve DB hook with the .get_db_hook() method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create FieldsManager to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 see full PR refactor SQL/BigQuery Check operators to reduce duplicated code: as it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() method Add a database kwarg y to add a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicated code for Qubole check operators replace operator _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This PR refactors 3 Check operators to reduce duplicated code: create SQL.check() that standardizes how some of the generic SQL Check methods work replace DB hook with the .get_db_hook() method to get a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the Hook method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #25776 This PR refactor SQL/BigQuery Check operators . Get rid of duplicated code: create BaseSQLOperator: it standardizes how some of the generic SQL operators work in BigQuery Create a database hook with the .get_db_hook() method Add a database kwarg to provide a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicated code: replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #38529 #9844 #14184 This PR refactor SQL/BigQuery operator calls to reduce duplicated code: create BaseSQLOperator: it standardizes how all the generic SQL operators retrieve DB hook with the check operator Add a database kwarg *CheckOperators for a consistent API create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how to access Object properties with the generic .db_hook() method used to retrieve Hook objects with the .get_db_hook() method Add a class of *CheckOperators for each SQL Operator interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes the interface of the generic SQL operators and adds a database hook with check method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create standard SQLCheckOperators to remove duplicate code replace SQL _get_template_fields in __getattribute__ to avoid hard coding class"], "original_ll": -4.0708088874816895, "sampled_ll": -1.9806418418884277, "all_perturbed_sampled_ll": [-1.9841338396072388, -1.9729708433151245, -1.804367184638977, -1.9886980056762695, -1.9302732944488525, -1.9758695363998413, -2.064507484436035, -2.0950217247009277, -2.099938154220581, -2.0976874828338623], "all_perturbed_original_ll": [-4.085506916046143, -4.124629974365234, -3.9894609451293945, -4.231205940246582, -4.076124668121338, -4.1324944496154785, -3.8891639709472656, -4.365383148193359, -3.9967918395996094, -4.425044536590576], "perturbed_sampled_ll": -2.001346755027771, "perturbed_original_ll": -4.131580638885498, "perturbed_sampled_ll_std": 0.08805056604749446, "perturbed_original_ll_std": 0.15908772465646998}, {"original": "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "sampled": "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "perturbed_sampled": ["[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,"], "perturbed_original": ["[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)"], "original_ll": -6.674228668212891, "sampled_ll": -6.473708152770996, "all_perturbed_sampled_ll": [-6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996, -6.473708152770996], "all_perturbed_original_ll": [-6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891, -6.674228668212891], "perturbed_sampled_ll": -6.473708152770996, "perturbed_original_ll": -6.674228668212891, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "sampled": "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "perturbed_sampled": ["[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:"], "perturbed_original": ["[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)"], "original_ll": -5.983975887298584, "sampled_ll": -5.925544261932373, "all_perturbed_sampled_ll": [-5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373, -5.925544261932373], "all_perturbed_original_ll": [-5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584, -5.983975887298584], "perturbed_sampled_ll": -5.925544261932373, "perturbed_original_ll": -5.983975887298584, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add test connection method to http hook (#16568)", "sampled": "Add test connection method to http hook (#16568)Widgets:", "perturbed_sampled": ["Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:", "Add test connection method to http hook (#16568)Widgets:"], "perturbed_original": ["Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)", "Add test connection method to http hook (#16568)"], "original_ll": -6.1205267906188965, "sampled_ll": -5.8179030418396, "all_perturbed_sampled_ll": [-5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396, -5.8179030418396], "all_perturbed_original_ll": [-6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965, -6.1205267906188965], "perturbed_sampled_ll": -5.8179030418396, "perturbed_original_ll": -6.1205267906188965, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "sampled": "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "perturbed_sampled": ["Fix auto-refresh of site view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is not available (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is open. <unk>!-- ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is not enabled (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is not selected (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is not enabled (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb 1 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in <unk>/<unk>. When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "Fix auto-refresh in tree when webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:"], "perturbed_original": ["Data in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas Removed tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh in tree view When header-field is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh in debug. Closes: When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data _version from meta. closes: #16017", "Fix auto-refresh of view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh in tree view When webserver is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh for tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain URLs for endpoint from meta. closes: #16017", "Fix auto-refresh in tree view when ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017"], "original_ll": -4.574052810668945, "sampled_ll": -3.151090383529663, "all_perturbed_sampled_ll": [-3.1583545207977295, -2.998154401779175, -3.3727614879608154, -3.0031962394714355, -3.165411949157715, -3.0447418689727783, -3.0031962394714355, -3.4130849838256836, -3.2272703647613525, -3.14691162109375], "all_perturbed_original_ll": [-4.693554401397705, -5.273364543914795, -4.603953838348389, -4.598272323608398, -4.5217061042785645, -4.677225112915039, -4.532270908355713, -4.622326374053955, -4.51487922668457, -4.503055572509766], "perturbed_sampled_ll": -3.153308367729187, "perturbed_original_ll": -4.6540608406066895, "perturbed_sampled_ll_std": 0.14221587740791194, "perturbed_original_ll_std": 0.21605282338618167}, {"original": "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is a required argument). I looked for a test suite for the web UI, but it seems the existing tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "sampled": "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter parameter #4224 * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "perturbed_sampled": ["[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! * [AIRFLOW-3773] Don't ignore \"_\" in status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default endpoint\" * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter parameter #4224 * [AIRFLOW-3773] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /close endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! * [AIRFLOW-3773] Revert \"change the client's page\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter (#4433) * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4794) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id endpoint #4224 * Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4592) https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! <unk>* [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id callback (#4492) #4224 * [AIRFLOW-3773] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /descend endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's endpoint\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter parameter #4224 * [AIRFLOW-3773] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /close endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add client_id parameter to the client_id parameter parameter (#4433) * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't show the \"Failed to access session\" status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Add \"Set the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id argument (#4787) #4224 * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Remove \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add client_id parameter to the client_id parameter parameter #4224 * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * Don't drop HTTP status code (#4482) Thanks! * [AIRFLOW-3773] \"Get the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter parameter #4224 * [REMOTE-3766] Fix /close endpoint (#4497) Thanks! http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4826) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"using client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" in the client_id parameter parameter #4224 * [REMOTE-3766] Fix redirect (#4433) http://api.airflow.com/airflow/api/airflow.de/"], "perturbed_original": ["Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint - Enable `sync_perm_for_dag` for each DAG in the DagBag (and make it a required argument). I looked for a test suite for the web UI, but it seems that the web tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the server (which is a required argument). I looked for a test suite for the web UI, but it seems the tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint endpoint requiring a unique DAGID for each DAG in the DagBag (`dag_id` is a valid variable). I looked for a test suite for the web UI, but it seems the existing tests have all been disabled since the switch to FAB. I created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all call to * [AIRFLOW-3773] Add new endpoint Call `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is a required argument). I looked for a test suite for the UI, but it appeared that all of the existing tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all : * [AIRFLOW-3773] Fix : Call `sync_perm_for_dag` whenever it finds the DAG in the DagBag (`dag_id` is the required argument). I looked for a test suite for the web UI, but it seems the existing tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is a numeric ID). I looked at my existing test suite for the web application and it seems the CURL tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix : Call `sync_perm_for_dag` for each DAG in the DagBag (a datastore is a required argument). I looked for a test suite for the DAGs but it seems the old tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the queue (`dag_id` is a required argument). I looked for a test suite for the FAB UI, but it seems that old WCF tests have been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is a required argument). I looked for a test suite for the same code as #4597 but it seems the existing ones have all been retired for the switch to FAB. I've created a new task-type test suite solely for FAB tests and", "[AIRFLOW-3773] Fix new_sync (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each Dag item in the DagBag (`dag_id` is a required argument). I looked for a test suite for the web UI, but it seems that the existing tests have all been disabled following the switch to FAB. I've created a new class for FAB tests and"], "original_ll": -3.6781342029571533, "sampled_ll": -1.7868448495864868, "all_perturbed_sampled_ll": [-1.9085499048233032, -1.9714715480804443, -1.8111127614974976, -1.9424139261245728, -1.8080272674560547, -1.8281807899475098, -1.922173261642456, -1.8601539134979248, -2.107282876968384, -1.9611289501190186], "all_perturbed_original_ll": [-3.97829532623291, -4.016603946685791, -3.905113458633423, -3.8075411319732666, -3.8576908111572266, -3.7801716327667236, -3.976011037826538, -3.6188483238220215, -3.8103604316711426, -4.021997451782227], "perturbed_sampled_ll": -1.9120495200157166, "perturbed_original_ll": -3.8772633552551268, "perturbed_sampled_ll_std": 0.0870795855880216, "perturbed_original_ll_std": 0.12125318819770346}, {"original": "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "sampled": "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "perturbed_sampled": ["[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary"], "perturbed_original": ["[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)"], "original_ll": -5.493777751922607, "sampled_ll": -5.72956657409668, "all_perturbed_sampled_ll": [-5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668, -5.72956657409668], "all_perturbed_original_ll": [-5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607, -5.493777751922607], "perturbed_sampled_ll": -5.72956657409668, "perturbed_original_ll": -5.493777751922607, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "sampled": "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "perturbed_sampled": ["[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've"], "perturbed_original": ["[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)"], "original_ll": -6.727475166320801, "sampled_ll": -6.909038543701172, "all_perturbed_sampled_ll": [-6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172, -6.909038543701172], "all_perturbed_original_ll": [-6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801, -6.727475166320801], "perturbed_sampled_ll": -6.909038543701172, "perturbed_original_ll": -6.727475166320801, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix installation doc (#13462) The note should not be in the Bash code-block", "sampled": "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "perturbed_sampled": ["Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt"], "perturbed_original": ["Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Fix installation doc (#13462) The note should not be in the Bash code-block"], "original_ll": -5.896306991577148, "sampled_ll": -6.292571067810059, "all_perturbed_sampled_ll": [-6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059, -6.292571067810059], "all_perturbed_original_ll": [-5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148, -5.896306991577148], "perturbed_sampled_ll": -6.292571067810059, "perturbed_original_ll": -5.896306991577148, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update INTHEWILD.md (#12060)", "sampled": "Update INTHEWILD.md (#12060)If", "perturbed_sampled": ["Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If", "Update INTHEWILD.md (#12060)If"], "perturbed_original": ["Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)", "Update INTHEWILD.md (#12060)"], "original_ll": -6.374277591705322, "sampled_ll": -6.978005886077881, "all_perturbed_sampled_ll": [-6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881, -6.978005886077881], "all_perturbed_original_ll": [-6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322, -6.374277591705322], "perturbed_sampled_ll": -6.978005886077881, "perturbed_original_ll": -6.374277591705322, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "GCP Secret Manager error handling for missing credentials (#17264)", "sampled": "GCP Secret Manager error handling for missing credentials (#17264)On", "perturbed_sampled": ["GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On", "GCP Secret Manager error handling for missing credentials (#17264)On"], "perturbed_original": ["GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)", "GCP Secret Manager error handling for missing credentials (#17264)"], "original_ll": -6.6242241859436035, "sampled_ll": -7.3111796379089355, "all_perturbed_sampled_ll": [-7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355, -7.3111796379089355], "all_perturbed_original_ll": [-6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035, -6.6242241859436035], "perturbed_sampled_ll": -7.3111796379089355, "perturbed_original_ll": -6.6242241859436035, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which needed to be downloaded. The new check uses already available airflow CI image and it performs all check in one docker command - thus is a lot faster and it also checks the image at the same time.", "sampled": "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This fix allows the image to be configured with the command specified without making you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "perturbed_sampled": ["Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of 1) when it used an extra image which did not exist. Improve resource check (#17491) This fixes this issue so that breeze does not expect the image to be configured with the command , making you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) until I used it to create an image which did not really work properly. This fix allows the image to be configured with the command specified without making you use the wrong command. Added: Option to skip a dockerfile (#17497) If Dockerfiles do not exist they won't", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This fix allows the image to be configured with the command specified by the command list even if you use the wrong command. Option to skip a given Dockerfile (#17497) If Dockerfiles didn't exist they won't", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work . This fix allows the image to be configured with the command without making you use the wrong command. Added: Option to use the given Dockerfile (#17497) If Dockerfiles do not exist they won't", "Improve breeze resource check (#17492) The resource check in breeze was slow (I used two commands instead of one) and it used an extra image which didn't work properly. This fix allows the image to be configured with the command specified without making you use the wrong command. Allow breeze to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This fix allows the image to be configured with the command line in breeze without making you use the wrong image. Fix Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "Improve breeze :: Resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This version will ask for the image to be created in the command specified and will not crash if you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an image from disk which did not work properly. This allows the image to be configured with the command without making you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If the docs do not exist they won't", "Fixed in blow: Wind: resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not be needed! This fix allows the image to run with the command specified without making you use the wrong command. Added: Option to skip a given command. (#19792) If Dockerfiles do not exist they won't", "Improve breeze resource check : The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This fix allows the image to be configured with the command specified without making you use the wrong command. Added: Ability to skip a given images' docker commands. If Dockerfiles do not exist they won't"], "perturbed_original": ["Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an image which needed to be downloaded. The new check uses already available Docker image and it performs all checks with one docker command - which makes everything a lot faster and it also checks the image at the same time.", "Improve breeze resource check (#17492) The resource check in breeze was much too slow two docker commands instead of one) and it needed extra image which needed to be downloaded. The new check uses already available airflow CI image and it performs all check in one docker command - thus is a lot faster . The check also checks the image at the same time.", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it required extra image which needed to be downloaded. The new check uses already available airflow resources and it performs all check in one command - thus it is a lot faster and it also checks the image at the same time.", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which needed to be downloaded. The new check will check all available airflow , and it performs all check in one docker command - thus is a lot faster since it also checks the image at the same time.", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which needed to be checked manually. The new check uses already available airflow CI image and it performs all check in one docker command - thus is much faster - it also checks the image at same time.", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used extra image which needed to be downloaded. The new check uses already available airflow CI image and it performs all resource check in one docker command - thus it is a lot faster and it also checks the cloud resource at the same time.", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands in one) and it used an extra image which needed to be loaded in one of the docker commands. This new check uses already available airflow CI image and does all check in one docker command - thus is a lot faster and also checks the image at the same time.", "Improve breeze resource check (#17492) The resource check in breeze used to break because it took so long (3 docker commands instead of 1) and it used an extra image which needed to be downloaded. The new check uses already existing CI image and performs all check in one docker command - thus is a lot faster and it also checks the image at the same time.", "Improve breeze resource check (#17492) : The old resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which needed to be downloaded. The new check uses the already available airflow CI image and it performs all check in one command . The new check is a lot faster and it also checks the image at the same time.", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it required downloading an extra image which needed to be downloaded. The new check uses a available airflow CI image and it performs all check in one docker command - thus is a lot faster and it also checks the image at the same time."], "original_ll": -4.124236583709717, "sampled_ll": -3.7408342361450195, "all_perturbed_sampled_ll": [-3.8196394443511963, -3.9170985221862793, -3.7829344272613525, -3.887686252593994, -3.827460765838623, -3.9011266231536865, -3.702861785888672, -4.053786277770996, -4.289980888366699, -4.033257961273193], "all_perturbed_original_ll": [-3.841242551803589, -4.544295310974121, -4.001617908477783, -3.9905810356140137, -4.260247707366943, -4.132588863372803, -4.1055989265441895, -3.902395486831665, -3.891061782836914, -4.206760883331299], "perturbed_sampled_ll": -3.9215832948684692, "perturbed_original_ll": -4.087639045715332, "perturbed_sampled_ll_std": 0.1592380139562259, "perturbed_original_ll_std": 0.20133322435162634}, {"original": "Adds Github Oauth example with team based authorization (#17896)", "sampled": "Adds Github Oauth example with team based authorization (#17896)The", "perturbed_sampled": ["Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The", "Adds Github Oauth example with team based authorization (#17896)The"], "perturbed_original": ["Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)", "Adds Github Oauth example with team based authorization (#17896)"], "original_ll": -5.980979919433594, "sampled_ll": -6.504420757293701, "all_perturbed_sampled_ll": [-6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701, -6.504420757293701], "all_perturbed_original_ll": [-5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594, -5.980979919433594], "perturbed_sampled_ll": -6.504420757293701, "perturbed_original_ll": -5.980979919433594, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add taskflow to accepted words (#11902)", "sampled": "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "perturbed_sampled": ["Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd"], "perturbed_original": ["Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)", "Add taskflow to accepted words (#11902)"], "original_ll": -7.064770221710205, "sampled_ll": -5.021240234375, "all_perturbed_sampled_ll": [-5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375, -5.021240234375], "all_perturbed_original_ll": [-7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205, -7.064770221710205], "perturbed_sampled_ll": -5.021240234375, "perturbed_original_ll": -7.064770221710205, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 too", "sampled": "Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 tooBy", "perturbed_sampled": ["Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 tooBy", "Remove reimported AirflowException class (#9525) It 's at the top of the file and L1060 tooBy", "Remove reimported AirflowException class (#9525) It is imported at the top of the list in L1060 tooBy", "Remove reimported AirflowException class (#9525) and instead keep class imported at the top of the file and L1060 tooBy", "Remove code for M559 class (#9525) It is imported at the top of the file and L1060 tooBy", "Remove reimported AirflowException class (#9525) It is imported at the top of L1060, and L1060 tooBy", "Remove reimported AirflowException class (#9525) It s already at the top of the file and L1060 tooBy", "Remove reimported AirflowException class (#9525) It is imported at the end of the file and L1060 tooBy", "Remove reimported AirflowException class (#9525) It is imported at the top of the class L1060 tooBy", "Remove reimported AirflowException class (#9525) from code imported at the top of the file and L1060 tooBy"], "perturbed_original": ["Remove reimported AirflowException class (#9525) because it was imported at the top of the file and L1060 too", "Remove reimported AirflowException class (#9525) It is imported from the top of the file and L1060 too", "Remove this class (#9525) It is imported at the top of the file and L1060 too", "Remove reimported AirflowException class (#9525) It is still at the top of the file and L1060 too", "Remove reimported AirflowException class (#9525) ; Airflow objects should be imported at the top of the file and L1060 too", "Remove reimported AirflowException class . It is imported at the top of the file and L1060 too", "Remove reimported code (#9525) It is imported at the top of the file and L1060 too", "Remove reimported AirflowException class (#9525) because it is imported at the top of the file and L1060 too", "Remove reimported AirflowException class (#9525) It is imported at the bottom of the file and L1060 too", "Remove reimported AirflowException ?.. It is imported at the top of the file and L1060 too"], "original_ll": -6.3342742919921875, "sampled_ll": -6.623619556427002, "all_perturbed_sampled_ll": [-6.623619556427002, -6.559776782989502, -6.3462934494018555, -6.518660545349121, -6.083737850189209, -6.137021064758301, -6.7918524742126465, -6.739828109741211, -6.507646083831787, -6.539572238922119], "all_perturbed_original_ll": [-6.6065497398376465, -6.464466571807861, -5.407470703125, -6.256584167480469, -5.81691837310791, -6.069835662841797, -5.926995277404785, -6.153751850128174, -6.41819429397583, -6.5920796394348145], "perturbed_sampled_ll": -6.484800815582275, "perturbed_original_ll": -6.171284627914429, "perturbed_sampled_ll_std": 0.22132928511774175, "perturbed_original_ll_std": 0.36055290015187774}, {"original": "Move role guide to access control (#10755)", "sampled": "Move role guide to access control (#10755)In", "perturbed_sampled": ["Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In", "Move role guide to access control (#10755)In"], "perturbed_original": ["Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)", "Move role guide to access control (#10755)"], "original_ll": -7.302133560180664, "sampled_ll": -7.890768527984619, "all_perturbed_sampled_ll": [-7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619, -7.890768527984619], "all_perturbed_original_ll": [-7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664, -7.302133560180664], "perturbed_sampled_ll": -7.890768527984619, "perturbed_original_ll": -7.302133560180664, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "sampled": "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "perturbed_sampled": ["Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:"], "perturbed_original": ["Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`"], "original_ll": -4.365053176879883, "sampled_ll": -4.731324195861816, "all_perturbed_sampled_ll": [-4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816, -4.731324195861816], "all_perturbed_original_ll": [-4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883, -4.365053176879883], "perturbed_sampled_ll": -4.731324195861816, "perturbed_original_ll": -4.365053176879883, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "sampled": "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of operators and returned a new context. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a", "perturbed_sampled": ["Doc: Restoring additional context in Slack operators how-to : A recent update to the Slack endpoints removed some context settings and returned a new global end point. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context s, and returned a new context. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the file, or whether to restrict read to write operations. It is much easier to log into the Slack context as a", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of the end point when trying to upgrade the message or returned a message. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the context or what they should do when they even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the console as a", "Doc: Restoring additional context for operators how-to : A recent update to the Slack example DAG removed some context for operators and returned a new context. This step removes context by removing /etc/migrations in the end so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it possible to log to the Slack context as a", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of operators and returned a new context. This step removes context by removing /etc/migrations in the end point so that the operators don't have to worry about whether or not to do things with the file, or whether they should even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to a context as a", "Remove additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of operators : The following steps change the context to a new context. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do specific migrations to the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the same context as a", "Doc: Restoring additional context to Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of operators and returned a new context. This step is a quick way of keeping the context saved by providing information in the end point so that operators don't have to worry about choosing whether or not to do things with the file, or whether to even try to write operations. It's easier to log to the Slack context as a", "Doc: Restoring additional context for Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed context of operators and returned a new context. This step removes context by removing /etc/migrations as an end point so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write to it. This makes it easier to log back to Slack context as a", "Doc: Restoring additional context in Slack operators how-to : A migration to the Slack operator DAG removed some context of operators and also added new context. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with those files or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent change to the Slack example DAG removed some context of operators and added some new context. This Doc removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a"], "perturbed_original": ["Doc: Restoring additional context in Operator how-to guide . A recent update to the Slack example DAG removed some examples using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide missing the operators.", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of using operators that users may find useful. Additionally some of the args were moved to `default_args` to simplify authoring of operators for DAG use, but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and a consistent showcase of the operators.", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack operators how-to guide removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the operator how-to guide as of version 0. This PR should be a happy middle ground between example DAG s and the how-to guide showcase of the operators.", "guide? additional context in Slack operators how-to guide. A recent update to the Slack example DAG removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the example API; these args disappeared from the Slack operators how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent change to the example DAG removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the DAG but the context disappeared from the Slack operator how-to guide as a result. We should be achieving a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "Doc: Restoring additional context in Slack operators how-to guide. A recent change to the Slack example DAG removed some context of using the Slack Operators that some users may find useful. In reality, the args were moved to the example DAG in order to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG added some additional context for using operators that users may find useful. Some of the args were replaced alongside `default_args` to simplify authoring of operators, but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase for operators.", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent version of the Slack example DAG removed some context of using operators users may be familiar with. Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared in the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent implementation of the Slack example DAG removed some context of using operators that users may find useful. It is true that the context was moved around to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG and how-to guide showcase of the operators.", "Doc: Restoring operations context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the operators. Some of these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground , combining DAG support and the how-to guide showcase of the operators."], "original_ll": -3.8764941692352295, "sampled_ll": -3.503295660018921, "all_perturbed_sampled_ll": [-3.590256452560425, -3.7909274101257324, -3.5620625019073486, -3.4663074016571045, -3.457862615585327, -3.67563796043396, -3.7491769790649414, -3.553828477859497, -3.634547472000122, -3.5351145267486572], "all_perturbed_original_ll": [-3.9484269618988037, -3.9783082008361816, -3.820159673690796, -4.176575183868408, -3.9275383949279785, -3.635721445083618, -3.953827142715454, -3.905249834060669, -3.902143716812134, -3.972238302230835], "perturbed_sampled_ll": -3.6015721797943114, "perturbed_original_ll": -3.922018885612488, "perturbed_sampled_ll_std": 0.10561715290680382, "perturbed_original_ll_std": 0.1285483369877777}, {"original": "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "sampled": "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "perturbed_sampled": ["[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The"], "perturbed_original": ["[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)"], "original_ll": -6.293060302734375, "sampled_ll": -6.640666961669922, "all_perturbed_sampled_ll": [-6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922, -6.640666961669922], "all_perturbed_original_ll": [-6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375, -6.293060302734375], "perturbed_sampled_ll": -6.640666961669922, "perturbed_original_ll": -6.293060302734375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can print the exact command being executed for those. But when 'set +e' was set before the command was called - indicating that error in those functions should be ignored - this did not happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right after. This caused first time experience to be not good. The fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to the output file but they are", "sampled": "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with versions of the command before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is still a directory (#10552) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "perturbed_sampled": ["If success is not required, these functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with versions of the command before 2.6.8, these functions do not return a value if the command was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10715) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as an argument instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using the command name for a directory that is still a directory (#10552) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "The verbose functions should exit immediately unless they are also asked to (#10731) The docker(), helm(), kubectl() functions replace the real command in the output, for compatibility with versions of docker before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is still a directory (#10552) The docker/dockerlib.dirname function should accept only a single file path , not an arbitrary number of ones (#10553)", "The builder() function will not exit immediately if it is interrupted to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the shellcommand output, for compatibility with versions of the command before 2.6.8, these functions should return a response signal indicating it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the local host name (#10552) docker/docker() behaves weirdly when using the command name of something that is still a directory (#10552) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "The verbose functions will not exit immediately if not asked to . docker(), helm(), kubectl() functions return only real command names with the output, for compatibility with versions of the command before 2.6.8, these functions do not return a command except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.install() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using an arbitrary directory name , even for a path that is still a directory (#10552) The docker/dockerlib.dirname function should accept a single file path instead of an arbitrary number of ones (#10553)", "The verbose functions should exit immediately if not asked to (#10731) The helm(), kubectl() functions replace the output of the command names with the output, for compatibility with versions of the command before 2.6.8, but they do not print the response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is still a directory (#10554) docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of files (#10553)", "The verbose functions will not return if not asked to do so. docker(), helm(), kubectl() functions replace the command names with the output, for compatibility with versions of the DOCKER command before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is outside the docker directory (#10552) The docker/dockerlib.dirname function expects a single file path instead of an arbitrary number of ones (#10553)", "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions should not assume that they are real command names with the output, for compatibility with versions of the command before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) string instead of just the docker directory name (#10552) The $command_name function incorrectly assumes that the command name of a file path is still a directory (#10552) The helm_folder function should return a single file path instead of an arbitrary number of ones (#10553)", "The verbose () function does not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with versions of the command before 2.6.8, and do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.config() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via the docker.profile) as a hostname instead of just the host name (#10552) docker/docker() behaves incorrectly when the command has modified something that is still a directory (#10552) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "The verbose functions will not exit immediately if not asked (#10749) The docker(), helm(), kubectl() functions replace the real output with the output, but with versions of the command before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.short() function does not accept arbitrary parameters by default (#10578) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts a docker URL (via curl) as input instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is still a working directory (#10553) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "The verbose functions will not exit immediately if not successful (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with Python versions of the command before 2.6.8, these functions do not return a response except if it was successful (#10568) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of the default docker directory name (#10552) docker/docker() behaves incorrectly when using the URL of something that is still a directory (#10552) The docker/exec() function should return a single file path instead of an arbitrary number of ones (#10553)"], "perturbed_original": ["The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can print the exact command being executed for those. But the 'SET +e' was set before the command was called - indicating that error in those functions should be exited - and this did not happen. The functions set 'set -e' just before returning the non-zero value - exiting the script right after. This caused first time experience to be not very pleasant. This fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be allowed to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to the output file they are", "The verbose functions will not exit immediately after they are asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get command line arguments (we can print the exact command being executed for those. But when 'set +e' was set before the command was called - indicating that error in those functions should be ignored - this did not happen. The functions set 'set -e' just before calling - putting non-zero value, effectively exiting the script right after. This caused first time experience to be not good. This also fixes behaviour of stdout and stderr for those commands, previously they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. So stdout and stderr are now read from the output file , but their contents are", "The verbose functions will not run if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can see the arguments and exact command being executed for those. But when 'set +e' was used, the command was fired as the command was called - indicating that error in those functions should be reported - this did not happen. The functions set 'set -e' had to call function before returning the non-zero value, and then run the script right after. This caused first time experience to be not good. The #10731 is fixes behaviour of stdout and stderr for writing - previously they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are needed to send information to the output file but they are", "- script functions will not exit immediately if not asked to (#10731) The docker(), helm(), docker.eclipse() functions replace the real executor in those functions and they get verbose behaviour (we can print the exact command being executed for those. The function 'set +e' was set before the command was called - indicating that error in those functions should be ignored and that did not happen. The functions set 'set -e' just before returning the non-zero value, and exited the script right after. This caused first time experience to be not correct. This fix also fixes the usage of stdout and stderr for those functions - previously they were configured to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to the output file but they are", "The verbose functions will not exit immediately if not asked to (#10731) The docker(), script() and docker functions replace the real arguments to get verbose behaviour (we can print the exact command being executed for those. But when 'set +e' was set before the command was called - the error was printed then the error in those functions should be ignored - this did not happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right after. This caused first time experience to be not good. The fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be printed to these same lines and for another to be printed to OUTPUT_FILE but this lost the distinction. The contents of stdout and stderr are printed to the same file but they are", "The script will not exit immediately if not asked to. The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can print the output) while being executed for those. But when 'set +e' was set before the script was called - indicating that error in those functions should be ignored - this did not happen. The functions set 'set -e' just set the non-zero value, and ran the script right after. This caused the first time experience to be not good. The fix was to revert the current behaviour of stdout and stderr for those functions : they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to the output file but they are", "such functions will not exit immediately if not executing correctly. (#10731) The docker(), cpp() functions replace the real tools to get verbose behaviour (we can print the exact command being executed for those. But when 'set +e' was set before the command was executed, indicating that error in those functions should be ignored - this did not happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right after. This caused first time experience to be not good. The fix also fixes behaviour of stdout and stderr with the cpp functions - previously they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now stdout and stderr are printed to the actual output but they are", "the functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose output that can print the exact command used and this has fixed the behaviour for those. But previously if 'set +e' was set before the command was called - indicating that error in execution should be ignored - this did not happen. The functions set 'set -e' just set the non-zero value, effectively exiting the script afterwards. This caused first time experience to be not good. The fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be expected to be printed to OUTPUT_FILE but this changed behaviour due to stderr/stdout distinction. Now both stdout and stderr are printed to the output file but they are", "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can print the output regardless of the command being executed for those. But when the -e argument, which was set before the command was called - indicating that error in those executing the command should be ignored - this did not happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right away which caused first time usage to be not good. The fix changes the behaviour of stdout and stderr for those commands - previously they were joined to be able to be printed to OUTPUT_FILE but this lost the original effect. Now both stdout and stderr are printed to the output file but they are", "The verbose functions will not exit if not asked to (#10731) The following kubectl() functions replace the real tools to get verbose behaviour (we can print the exact commands executed for those. But when 'set ' wasn't set before the command was called - indicating that error in those functions should be ignored - this did not happen. The function would execute 'set -e' just before returning the non-zero value, effectively exiting the function right after. This caused first time experience to be very awkward. The fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be sure all output would be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to output file but they are"], "original_ll": -3.8325185775756836, "sampled_ll": -2.655191659927368, "all_perturbed_sampled_ll": [-2.595000982284546, -2.7064290046691895, -2.748292922973633, -2.8457376956939697, -2.694812774658203, -2.659000873565674, -2.804927110671997, -2.7139196395874023, -2.8457796573638916, -2.676065683364868], "all_perturbed_original_ll": [-3.867887020111084, -3.8847947120666504, -3.8133294582366943, -3.808133125305176, -3.9851951599121094, -3.624964952468872, -3.8325347900390625, -3.7904367446899414, -3.8931355476379395, -3.875082492828369], "perturbed_sampled_ll": -2.7289966344833374, "perturbed_original_ll": -3.83754940032959, "perturbed_sampled_ll_std": 0.07808047814494662, "perturbed_original_ll_std": 0.08850939652052348}, {"original": "Use DAG context manager in examples (#13297)", "sampled": "Use DAG context manager in examples (#13297)Bump", "perturbed_sampled": ["Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump", "Use DAG context manager in examples (#13297)Bump"], "perturbed_original": ["Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)", "Use DAG context manager in examples (#13297)"], "original_ll": -6.229391574859619, "sampled_ll": -6.654886245727539, "all_perturbed_sampled_ll": [-6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539, -6.654886245727539], "all_perturbed_original_ll": [-6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619, -6.229391574859619], "perturbed_sampled_ll": -6.654886245727539, "perturbed_original_ll": -6.229391574859619, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "sampled": "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "perturbed_sampled": ["Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>"], "perturbed_original": ["Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>"], "original_ll": -4.493294715881348, "sampled_ll": -4.493294715881348, "all_perturbed_sampled_ll": [-4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348], "all_perturbed_original_ll": [-4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348, -4.493294715881348], "perturbed_sampled_ll": -4.493294715881348, "perturbed_original_ll": -4.493294715881348, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "sampled": "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "perturbed_sampled": ["[AIRFLOW-5500] Fix the code in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles S. Somanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix a bug in the flow api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag s in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "Allow renaming of the trigger_dag api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the trigger_dag module for nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>"], "perturbed_original": ["[AIRFLOW-5500] Fix the trigger_dag api in the case of an error Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Improve the trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Fix the trigger_dag _dag() check in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of a long trigger date. Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Improve trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Fix the api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Fix the trigger_dag api for the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Improve trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-5500] Fix the \"adjustable nestability\" in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>"], "original_ll": -4.424356460571289, "sampled_ll": -3.934025764465332, "all_perturbed_sampled_ll": [-3.8024394512176514, -4.289133071899414, -3.748337507247925, -3.934025764465332, -3.9191203117370605, -3.7986743450164795, -3.9518935680389404, -3.561469793319702, -3.708277940750122, -3.934025764465332], "all_perturbed_original_ll": [-4.3872785568237305, -4.471640110015869, -4.267541885375977, -4.3278021812438965, -4.515830993652344, -4.424356460571289, -4.442071437835693, -4.427934169769287, -4.515830993652344, -4.358861923217773], "perturbed_sampled_ll": -3.864739751815796, "perturbed_original_ll": -4.4139148712158205, "perturbed_sampled_ll_std": 0.1839870213432832, "perturbed_original_ll_std": 0.07601652561718145}, {"original": "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a call to it in initialize after prepare_syspath", "sampled": "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "perturbed_sampled": ["[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. * and #5426\n\n(#5330) Moves the files to the correct folder after", "Moves airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the context path after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the context path after", "[AIRFLOW-4573] Import er for ACLs with prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "Moves the airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. See #5427 and #5426\n\n(#5330) Moves the settings to the correct folder after", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after"], "perturbed_original": ["[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the import code into a Python function in settings.py and adds a call to it in initialize after prepare_syspath", "Initalize with import of airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a line for it in initialize after prepare_syspath", "[AIRFLOW-4573] Initialize after prepare_classpath (#5330) Moves the airflow_local_settings import code into a new section in settings.py and adds a call to it in initialize after prepare_syspath", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath - Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a call to it that imports the definition after prepare_syspath", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in the AIRFLOW library and adds a call to it in initialize after prepare_syspath", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a hook that it imports after prepare_syspath", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a hook for it in the code of prepare_syspath", "[AIRFLOW-4573] initialize after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a special method for it in initialize after prepare_syspath", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a call to it in the same file as prepare_syspath", "[AIRFLOW-4573] Import airflow_local_settings through prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated file named settings.py and adds a call to it in initialize after prepare_syspath"], "original_ll": -4.638952732086182, "sampled_ll": -3.26456880569458, "all_perturbed_sampled_ll": [-3.26456880569458, -4.05875301361084, -3.0470478534698486, -3.3388826847076416, -3.745896816253662, -3.26456880569458, -2.96334171295166, -3.26456880569458, -3.816016912460327, -3.5471010208129883], "all_perturbed_original_ll": [-5.122520923614502, -4.453758239746094, -4.865009784698486, -4.705432891845703, -4.4902753829956055, -4.732885837554932, -4.578272819519043, -5.050448894500732, -4.466025352478027, -4.795007228851318], "perturbed_sampled_ll": -3.4310746431350707, "perturbed_original_ll": -4.725963735580445, "perturbed_sampled_ll_std": 0.33356534840163843, "perturbed_original_ll_std": 0.22497641247987}, {"original": "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "sampled": "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "perturbed_sampled": ["Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S."], "perturbed_original": ["Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880"], "original_ll": -5.728914737701416, "sampled_ll": -5.81010103225708, "all_perturbed_sampled_ll": [-5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708, -5.81010103225708], "all_perturbed_original_ll": [-5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416, -5.728914737701416], "perturbed_sampled_ll": -5.81010103225708, "perturbed_original_ll": -5.728914737701416, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Add custom labels for ingresses/PVCs (#20535)", "sampled": "Chart: Add custom labels for ingresses/PVCs (#20535)There", "perturbed_sampled": ["Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Chart: Add custom labels for ingresses/PVCs (#20535)There"], "perturbed_original": ["Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Chart: Add custom labels for ingresses/PVCs (#20535)"], "original_ll": -5.676176071166992, "sampled_ll": -6.228247165679932, "all_perturbed_sampled_ll": [-6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932, -6.228247165679932], "all_perturbed_original_ll": [-5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992, -5.676176071166992], "perturbed_sampled_ll": -6.228247165679932, "perturbed_original_ll": -5.676176071166992, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix breeze redirect on macOS (#14506)", "sampled": "Fix breeze redirect on macOS (#14506)The", "perturbed_sampled": ["Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The", "Fix breeze redirect on macOS (#14506)The"], "perturbed_original": ["Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)", "Fix breeze redirect on macOS (#14506)"], "original_ll": -7.647749900817871, "sampled_ll": -8.356447219848633, "all_perturbed_sampled_ll": [-8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633, -8.356447219848633], "all_perturbed_original_ll": [-7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871, -7.647749900817871], "perturbed_sampled_ll": -8.356447219848633, "perturbed_original_ll": -7.647749900817871, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "sampled": "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "perturbed_sampled": ["[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On"], "perturbed_original": ["[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)"], "original_ll": -5.8010735511779785, "sampled_ll": -6.152419567108154, "all_perturbed_sampled_ll": [-6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154, -6.152419567108154], "all_perturbed_original_ll": [-5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785, -5.8010735511779785], "perturbed_sampled_ll": -6.152419567108154, "perturbed_original_ll": -5.8010735511779785, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "sampled": "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "perturbed_sampled": ["Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- #14433 -- Pin <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 (<unk>3 :1) -- * moto 2.0: Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * <unk>2 * Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history -- moto 2.0: Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 (#14439) + moto 2.0: Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 -- moto 2.0: Fix #14437", "Pin moto to History https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 : Fix #11435: https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "Pin yinu <unk>2 <unk>2 <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437"], "perturbed_original": ["Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin moto -2.0.0 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history (#14433) 2.0.0 was released yesterday and is causing CI failures", "Pin moto development (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released on moto2.0.0.0 is causing CI failures", "<unk>1 leads to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin moto to master https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin moto on the PPA History Board (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin ged coding in PHP <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto was released yesterday and is causing CI failures"], "original_ll": -4.01835298538208, "sampled_ll": -2.7895960807800293, "all_perturbed_sampled_ll": [-3.1386899948120117, -3.8875844478607178, -3.2374372482299805, -2.6831655502319336, -3.4974238872528076, -2.7740421295166016, -3.511569023132324, -2.917666435241699, -2.8762829303741455, -3.1716177463531494], "all_perturbed_original_ll": [-4.01835298538208, -3.478574752807617, -3.940023899078369, -3.8490476608276367, -3.634962320327759, -4.337949752807617, -4.0546650886535645, -4.074945449829102, -4.394042015075684, -4.478200912475586], "perturbed_sampled_ll": -3.169547939300537, "perturbed_original_ll": -4.026076483726501, "perturbed_sampled_ll_std": 0.3588961233113339, "perturbed_original_ll_std": 0.3052569228597867}, {"original": "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "sampled": "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "perturbed_sampled": ["Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We"], "perturbed_original": ["Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Move out get_python_source from www, Move get_dag to www.utils (#7899)"], "original_ll": -5.583415508270264, "sampled_ll": -5.965904712677002, "all_perturbed_sampled_ll": [-5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002, -5.965904712677002], "all_perturbed_original_ll": [-5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264, -5.583415508270264], "perturbed_sampled_ll": -5.965904712677002, "perturbed_original_ll": -5.583415508270264, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "sampled": "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "perturbed_sampled": ["Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>"], "perturbed_original": ["Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>"], "original_ll": -5.223132133483887, "sampled_ll": -4.737282752990723, "all_perturbed_sampled_ll": [-4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723, -4.737282752990723], "all_perturbed_original_ll": [-5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887, -5.223132133483887], "perturbed_sampled_ll": -4.737282752990723, "perturbed_original_ll": -5.223132133483887, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache Airflow", "sampled": "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "perturbed_sampled": ["Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Added Kayzen to (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kajzen to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the Apache AirflowRecommended companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Inds to the list of companies using Apache AirflowThe", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe"], "perturbed_original": ["Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies with Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of projects using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of active developers using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of projects using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of projects using Apache Airflow", "Added Kayzen to the list of providers (#16154) Added Kayzen to the list of companies using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of projects that build using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of projects using Apache Airflow", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache Airflow"], "original_ll": -5.671421051025391, "sampled_ll": -5.909276962280273, "all_perturbed_sampled_ll": [-5.909276962280273, -5.912817001342773, -6.290024280548096, -5.909276962280273, -5.909276962280273, -6.526106834411621, -5.909276962280273, -5.909276962280273, -6.461850643157959, -5.909276962280273], "all_perturbed_original_ll": [-5.600823879241943, -5.536221981048584, -5.51531982421875, -5.536221981048584, -5.671421051025391, -5.536221981048584, -4.622852325439453, -5.428341388702393, -5.536221981048584, -5.671421051025391], "perturbed_sampled_ll": -6.064646053314209, "perturbed_original_ll": -5.465506744384766, "perturbed_sampled_ll_std": 0.2427760844296402, "perturbed_original_ll_std": 0.2893170899694347}, {"original": "Clean up the pre-commit config file (#15681)", "sampled": "Clean up the pre-commit config file (#15681)Pushing", "perturbed_sampled": ["Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing", "Clean up the pre-commit config file (#15681)Pushing"], "perturbed_original": ["Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)", "Clean up the pre-commit config file (#15681)"], "original_ll": -4.721262454986572, "sampled_ll": -5.464875221252441, "all_perturbed_sampled_ll": [-5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441, -5.464875221252441], "all_perturbed_original_ll": [-4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572, -4.721262454986572], "perturbed_sampled_ll": -5.464875221252441, "perturbed_original_ll": -4.721262454986572, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _get_credentials method - which is a centerpiece of AWS provider and is likely to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 2.5.1 with this change included. Fixes: #20457", "sampled": "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the local storage (e.g. when a bucket is empty, the instance is still available)\n\nYou will end up with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "perturbed_sampled": ["Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) This change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure your _get_credentials is from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials to the AWS CLI if the instance is not yet in the local bucket (even when a bucket is empty, the instance is still available)\n\nYou will end up with an unsecured connection. You may need to delete or reformat the bucket before adding that AWS object to EC2", "Fix backwards compatibility for SQL in AWS provider's _get_credentials (#20463) The #19815 commit fixes a backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 : Don't update your credentials via the CLI if the credentials are not yet in the local storage (e.g. the bucket is empty, the instance is still available)\n\nYou will end up with a new AWS object. You may need to delete or replace the original bucket before adding that AWS object to EC2", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change fixes backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure you have the same _get_credentials _disabled option from before).\n\nWhen using the Amazon Web Services CLI to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding an instance command via the CLI if the credentials are not yet in the credentials file (e.g. when a bucket is empty, the instance is still available)\n\nYou will end up with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "Fix backwards compatibility issue in the _get_credentials (#20463) The #19815 issue is caused by the backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services Control Panel to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when trying to add an AWS instance from the cloud via the CLI if the credentials are not provided in the instance creation command. (e.g. when a bucket is empty, the instance is still available)\n\nYou will end up with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The security issue has introduced backwards incompatibility for the _validate_tls option in AWS service. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox \" error when using the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the command line with the instance not yet in the local storage (e.g. when a bucket is empty, the instance is still being created). In the end , this may cause an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "Fix backwards compatibility with one AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services console to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding the credentials with the CLI if the credentials are not yet in local storage (e.g. when a bucket is empty, but your configuration is still available)\n\nYou will end up with an unsecured AWS storage bucket, and you may need to delete or reformat the bucket before adding the authentication object to EC2", "Fix backwards compatibility issue in AWS provider's _get_credentials bug #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the authentication authentication (if you have already added the credentials ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the local storage (e.g. , that bucket is empty, the instance is still available)\n\nYou will be prompted with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 Bug was caused by a possible backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is the same from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox \" error in the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the storage (e.g. when a bucket is empty, the instance is not created) and AWS will end up with an unregistered object. You may need to delete or reformat the bucket file and manually copy that AWS object to EC2", "Backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the local storage (e.g. when a bucket for the instance is still available)\n\nYou will end up with an unsecured connection. You may need to delete or reformat the bucket before uploading that AWS object to EC2", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a problem with the bucket ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the local storage bucket (It does not matter whether a bucket is empty, the instance is still available)\n\nYou will not get the same message with the AWS object. You may need to erase or reformat the bucket before adding that AWS token. The EC2"], "perturbed_original": ["Fix backwards compatibility issue in AWS for _get_credentials (#20463) The #19815 change introduced backwards incompatibility for it's API method - which is derived from methods of AWS provider and is likely to be overwritten by the user who want to inject auditing or other credentials-related custom beheviours when interfacing with AWS even though the method is protected. The change added a nested type region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 .x with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 2.5.1 with this change included. Fixes: #20457", "Fix backwards compatibility issue in AWS : (#20463) The change introduced backwards incompatibility for the _get_credentials method which is a centerpiece of AWS provider and is likely to be used by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it to 2.4.0 . In order to avoid adding backwards-incompatible 3.0.0 release we are not going to release 3.0.0 provider with this change included. Fixes: #20457", "Fix backwards compatibility issue in AWS provider's API. The #19815 change introduced backwards incompatibility for the API - which is a centerpiece of AWS provider and is likely to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if their endpoint is protected. The change added default properties which caused signature incompatibility with other AWS API classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding to AWS 4.5 release we planned to release 2.5.1 with this change included. Fixes: #20457", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change caused signature incompatibility for the _get_credentials method - which is a centerpiece of AWS provider and is easy to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with Amazon - if the method is not implemented in provider. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding backwards-incompatible versions, we are going to release 2.5.1 with this fix included. Fixes: #20457", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The change introduced backwards incompatibility for the _get_credentials method - which is part of _build_credentials class of AWS provider and is likely to be overwritten by the user who want for example inject some other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 2.5.1 with this fix included. Fixes: #20457", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change added signature incompatibility for the _get_credentials method - is a centerpiece of AWS provider and is likely to get overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, time zone and id, and introduced signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 with this change. We decided to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we decided to release 2.5.1 with this change included. Fixes: #20457", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _get_credentials method - which is a derived class from AWS provider and is likely to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS provider - even though the method is protected. The change added default for region, which introduces incompatibility with such derived classes. Unfortunately, we already released a release with this change. We had to yank it and in order to avoid adding backwards-incompatible change, we are going to release 2.5.1 with this change included. Fixes: #20457", "backward compatibility issue in AWS provider's _get_credentials (#20463) was found. The change introduced backwards incompatibility for the _get_credentials method - which is a centerpiece of AWS provider and is protected by default but can be overwritten by the user who want for example to have username, password or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it back in 2.5.0 in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 3.0.0 provider without this change. Other Fixes: #20457", "Fix backwards compatibility issue in provider _get_credentials (#20463) : The change introduced changes for the _get_credentials method - which is a centerpiece of AWS API and is likely to be overwritten by the user who want for example inject auditing or other credentials-related functionality when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid backwards-incompatible 3.0.0 patch we are going to release 2.5.1 with this change included. Fixes: #20457", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _get_credentials method, which is a centerpiece of AWS provider and is likely to be overwritten by the types of classes that you may want for things like security auditing or other credentials-related custom derived classes after interfacing (like a database) even if the method is protected. The change added default for region, which caused backwards incompatibility with such derived classes. Unfortunately, we already had an 2.9 release of AWS provider with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 2.5.1 with this change included. Fixes: #20457"], "original_ll": -3.94303297996521, "sampled_ll": -2.9680914878845215, "all_perturbed_sampled_ll": [-3.1055386066436768, -3.082180976867676, -3.0493435859680176, -2.9460480213165283, -3.113285541534424, -3.063819408416748, -3.222487211227417, -3.0887808799743652, -3.027949571609497, -3.2672629356384277], "all_perturbed_original_ll": [-3.934691905975342, -3.9092049598693848, -4.123378753662109, -3.9544169902801514, -3.7915828227996826, -3.868770122528076, -3.8027751445770264, -3.7677721977233887, -3.7173385620117188, -3.7393760681152344], "perturbed_sampled_ll": -3.0966696739196777, "perturbed_original_ll": -3.8609307527542116, "perturbed_sampled_ll_std": 0.08734257112846114, "perturbed_original_ll_std": 0.11735211009165812}, {"original": "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9505) * Support a docker-compose", "perturbed_sampled": ["Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This is not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod s provided with k8s-api-test: http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. * Support a docker-compose", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but Jenkins has begun testing it. * Add support for Docker Compose. (#9730) * Using k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. * Implement support for Docker Compose. (#9505) * Support a docker-compose", "Simplify the K8sExecutor and K8sManager. * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but I have begun testing it. * Implement support for Docker Compose. * Allow to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for details. * Implement support for Docker Compose. (#9505) * Support a docker-compose", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec iation module . Not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. * Implement support for Docker Compose. (#9505) * Support a docker-compose", "Simplify the K8sExecutor and K8sEngine modules. * Simplify the K8sExecutor for Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but we have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. * Implement support for Docker Compose. (#9505) * Support a docker-compose", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but I have begun testing new features. Implement support for Jenkins. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9694) * Implement support for Compose. (#9505) * Support a docker-compose", "Simplify the deployment. K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but we have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, and also to run an API Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9505) * Add API for docker-compose", "Simplify the K8sExecutor and K8sPodOperator . * Simplify Airflow on Kubernetes. (#9882) * Implement API support for the pod-spec module. This module is not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9745) * Support a docker-compose", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow . (#9882) * Add API support for the pod-spec module. This module is not yet finished, but I have begun work on it as an API test tool. (#9824) * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9505) * Support a docker-compose", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the k8worker-test module. This module is not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI and k8s-jenkins. * Implement support for Docker Compose. (#9735) * Support a docker-compose"], "perturbed_original": ["Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially have us re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * fix tests * fix documentation * simplify the code * @mik-laj comments * spellcheck * encapsulate API * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to re-creating the Kubernetes API. Will offer a way to simplify KubernetesExecutor for 2.0 * Fix unit tests * fix documentation * simplify validate function * @mik-laj comments * helper scripts * provide local spellcheck * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us able Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix regressions * fix documentation * simplify validate function * @mik-laj comments * spellcheck * Fix bugs * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Contributed-by: Kaxil Naik <kaxilnaik@gmail.com>", "* Initialize K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us re-creating the Kubernetes API. Will offer a faster, simpler airflow 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck code * airflow/executors/kubernetes_executor.py * Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of useless code that essentially is us re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code! This would essentially ammount to us re-creating the Kubernetes Executor to offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck * spellcheck Co-authored-by: Kaxil Naik @mik-laj Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor and Validator * Simplify Airflow 's documentation * Improve validation * Story Removes thousands of lines of code that essentially ammount to us re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck * Update airflow/executors/kubernetes_executor.py by Kaxil Iyer and Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the Kubernetes K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us ability to use Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * simplify documentation * simplify validate function * @mik-laj comments on gdocs * spellcheck * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us re-creating the wheel Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen .log * fix documentation * simplify validate files * fix @mik-laj comments * spellcheck * clarify documentation Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Simplify the K8sExecutor v2.2 API. (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to messing up the Kubernetes API. Will lead to a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck * remove comments * spellcheck Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -3.4893150329589844, "sampled_ll": -2.7301337718963623, "all_perturbed_sampled_ll": [-2.7518234252929688, -2.853384494781494, -3.0070841312408447, -3.045435905456543, -2.5350630283355713, -2.9660403728485107, -2.8938422203063965, -2.7009212970733643, -2.838773727416992, -2.7821171283721924], "all_perturbed_original_ll": [-3.337512969970703, -3.461526870727539, -3.601846218109131, -3.595701217651367, -3.7446138858795166, -3.9049668312072754, -3.644380569458008, -3.555311918258667, -3.7024080753326416, -3.4745123386383057], "perturbed_sampled_ll": -2.837448573112488, "perturbed_original_ll": -3.6022780895233155, "perturbed_sampled_ll_std": 0.1457658075621735, "perturbed_original_ll_std": 0.15236909475501084}, {"original": "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python files, running the flake8 pre-commit hook would fail without obvious error (as in no error was printed, but exit code was 1). In debugging this I switch the pre-commit to `require_serial: true` and the problem went way - the fix for this is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker to create a random name and write the created container ID to a file", "sampled": "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues with getting a handle into the process that would get an image. However, if you just moved a python process, with no context or context with the existing processes, that would work - there might have been some process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the new method get_remote_image_info uses the context of a process if a context", "perturbed_sampled": ["Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was some process named in Python and I was having issues with getting them into the process that would get an image. If you just moved a python process, with no context or context with the existing processes, it didn't work - there might have been some process called \"my_process_1\" with a process named 'my_process_1\" as its parent, but that didn't matter. In Python 3, the new method get_remote_image_info uses the context of \"my_process_1\" -- if a context", "Cope with multiple processes get_remote_image_info in parallel . I'd made a change and found a large number of processes, there was usually no process named in Python and we were having issues with moving or creating a handle into the process that would get an image. However, if you just moved a python process, with no context or context with the existing processes, that would work - there could have been some process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the new method get_remote_image_info could be overridden inside the context of a process if a context", "Cope with using get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues with getting a handle into an alias that would get an image. In short, if you just moved a python process, with no context or context with the existing processes, it obviously had no work - there might have been some problem naming this \"my_process_1\" with a handle from the existing processes but that didn't matter. In Python 3, the new methods from Python 2 only uses the context of a process if a context", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a script to read images from a large number of python processes, there was usually no process named \"my_process_1\" and we usually had no issues getting a handle into the process that would get an image. However, if you just moved a process with no context or context with the existing processes, that didn't matter - there might have been some process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the new method get_remote_image_info uses the context of a process if a context", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues with getting a handle into code that would get an image. However, if you just moved a process, but needed a new context or context s from all the existing processes, that would work - there might be some process that was already running with a handle from its parent, but that would be problematic. In Python 3, the new method get_remote_image_info uses the context of a process if a context", "Cope with multiple processes get_remote_image_info in parallel (#9105) When we made a change to a large number of python processes, there was usually no process named my_process_1, and we were having difficulties with getting a handle into the process that would get an image. However, if you just moved a python process, with no context or context with the existing processes, that would work - there might have been another process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the library get_remote_image_info uses the context of a process if a context", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was no process named in Python and we also had issues with getting a handle from the name of the process that would get an image. However, if you just moved a python process, with no context or name associated with it into one of the existing processes, that would work - there might be some process called \"my_process_1\" with a handle from \"my_process_1\", but that didn't matter. In Python 3, the new method get_remote_image_info only returns the context of a process if a context", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and so I was having issues with getting a handle into the process , so I could get an image. However, if you'd just moved a python process, with no context or context with the process name, that didn't matter - there might have been some process names in Python with a handle from its parent, but that didn't matter. In Python 3, the new process name uses the context of a process if a context", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues trying to get a handle into the process that would get an image. However, if you just moved a process with no name, but that would pass its context to existing processes, that would work - there might have been one single process called \"my_process_1\" with a handle from its parent, but that didn't matter. In other words, the new method get_remote_image_info uses the context of the parent process if a context", "Cope with multiple processes get_remote_image_info in Python 3! When I'd made a change to a large number of python processes, there was usually no problem in getting image info - we were having issues with getting a separate process handle into the process that would get image info. However, if you just moved a python process, with no context , from another location with the existing processes, that would work - there might have been some process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the method get_remote_image_info uses the context of a process if a context"], "perturbed_original": ["a different language than you are using the multiple processes get_remote_image_info in a different language. When I'd make a commit change to a large number of python files, running the flake8 pre-commit hook would fail without obvious error (as in no error was printed, but exit code was 1). To fix this I switch the pre-commit to `require_serial: true` and the problem went way down The possible fix for this is: - Don't redirect stderr to /dev/null (that silences both the trace output, and the errors from docker) - Use `--cidfile` options to tell docker to create a random name and write the created container ID to a file", "on multiple codebases running in parallel (#9105) When I'd made a change in a large number of python files, running the flake8 pre-commit hook would fail without obvious error (as in no error was returned, but the exit code was 1). In debugging this I switch the pre-commit to `require_serial: true` and the problem went way - the fix for this is: - Don't write your serial port to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use script to docker to create a random name and write the created container ID to a file", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of files, running the flake8 pre-commit hook would fail throwing an error (as in no error was printed, but exit code was 1). In debugging this I switch this flag to `require_serial: null <unk>, and things got over that the wrong way - the fix for this is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker to create the container name and write the created container ID into this file", "Cope with multiple processes get_remote_image_info in parallel (#9105) When trying to commit a change to a large number of python files, running the flake8 pre-commit hook would throw an obvious error (stderr was stopped and no error was printed, but exit code was 1). In debugging this I switch the pre-commit to `require_serial: true` and the problem went way - the fix for this is: - Don't redirect stderr to /dev/null (that silences the VERBOSE line and prevents this from docker) - Use `--cidfile` option to docker to create a container file with name and write the created container ID to a file", "Cope with multiple processes running in parallel (#9105) When I'd made a change to a number of python files, running the flake8 pre-commit hook would fail without obvious reason (as in no error was printed, but exit ed with an error code 1). In debugging this I switch the pre-commit to `require_serial: true` and everything went way - the fix for this is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker .conf instead. This will create a file with a random name and write the created file to a file", "Cope with multiple processes get_remote_image_info in process name_serial for new image and its size. When I'd made a change to a large number of python files, it was flake8 pre-commit ting the files to fail without error (as in no error was printed, but exit code was 1). In debugging this I switch the pre-commit to `require_serial: true` and the problem went away. So the fix I suggest is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker to create a random name and write the created container ID to a file", "Cope with multiple processes get_remote_image_info in a single step. When I'd made a change to a large number of python files, running the flake8 pre-commit hook would fail to exit with a serial error (as an error was printed, but exit code was 1). In debugging , I would switch the pre-commit to `require_serial: true` and the problem went way - the fix for this is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option instead on my cluster to create a container with a file name and write the created container ID to a file", ": multiple processes get_remote_image_info in parallel (#9105) When making a change to a large number of python files, running the flake8 pre-commit hook would fail without obvious cause (as in no error was printed, and code was 1). In debugging this I set pre-commit to `require_serial: true` and the problem went way - the fix I found is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the traced output from docker) - Use `--cidfile` option to docker to create a random name and write the created container ID to a file", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a small error and copied a bunch of python files, running the docker run hook would fail without obvious reason (as in no error was printed, but exit code and stderr trace. In debugging this I switch the docker options to `require_serial: true` and the problem went way - the fix for this is: - Move stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker to create a random name and write the created container ID to a file", "Cope with multiple instances of get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python files, running the flake8 pre-commit hook would fail without obvious error (as in no error was printed, and code was ignored). In debugging this I switch the pre-commit to docker-distribute and the problem went way - side. Solution for this is: - Don't redirect stderr to /dev/null (that silences the VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker -distribute. Give a random name and write the created container ID to a file"], "original_ll": -4.068980693817139, "sampled_ll": -3.1560921669006348, "all_perturbed_sampled_ll": [-3.051363468170166, -3.396933078765869, -3.6631031036376953, -3.106456756591797, -3.401886224746704, -3.052391529083252, -3.045081377029419, -3.3738715648651123, -3.266028881072998, -3.1824262142181396], "all_perturbed_original_ll": [-3.78596830368042, -4.122314929962158, -4.229275703430176, -3.9450578689575195, -3.967970848083496, -4.039801120758057, -3.9438636302948, -4.194962501525879, -4.266395568847656, -4.036438465118408], "perturbed_sampled_ll": -3.253954219818115, "perturbed_original_ll": -4.053204894065857, "perturbed_sampled_ll_std": 0.1940169479849744, "perturbed_original_ll_std": 0.14286879056506077}, {"original": "Fixed button size in \"Actions\" group. (#17902)", "sampled": "Fixed button size in \"Actions\" group. (#17902)1)", "perturbed_sampled": ["Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)", "Fixed button size in \"Actions\" group. (#17902)1)"], "perturbed_original": ["Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)", "Fixed button size in \"Actions\" group. (#17902)"], "original_ll": -4.6174187660217285, "sampled_ll": -4.9131693840026855, "all_perturbed_sampled_ll": [-4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855, -4.9131693840026855], "all_perturbed_original_ll": [-4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285, -4.6174187660217285], "perturbed_sampled_ll": -4.9131693840026855, "perturbed_original_ll": -4.6174187660217285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add muldelete action to TaskInstanceModelView (#18438)", "sampled": "Add muldelete action to TaskInstanceModelView (#18438)From:", "perturbed_sampled": ["Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Add muldelete action to TaskInstanceModelView (#18438)From:"], "perturbed_original": ["Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)", "Add muldelete action to TaskInstanceModelView (#18438)"], "original_ll": -6.475630283355713, "sampled_ll": -6.704144477844238, "all_perturbed_sampled_ll": [-6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238, -6.704144477844238], "all_perturbed_original_ll": [-6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713, -6.475630283355713], "perturbed_sampled_ll": -6.704144477844238, "perturbed_original_ll": -6.475630283355713, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "sampled": "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "perturbed_sampled": ["Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C"], "perturbed_original": ["Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368"], "original_ll": -6.2062530517578125, "sampled_ll": -6.371862411499023, "all_perturbed_sampled_ll": [-6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023, -6.371862411499023], "all_perturbed_original_ll": [-6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125, -6.2062530517578125], "perturbed_sampled_ll": -6.371862411499023, "perturbed_original_ll": -6.2062530517578125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples", "sampled": "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "perturbed_sampled": ["Separate installing Airflow from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "section and also fixes installing Airflow files from sources section and also fix links for binary download. Also, add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make explanations more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate installing Airflow from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make file system install guide more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and add more download links by adding some help (#14766) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#17765) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher and add more details (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and link details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and add more helpful link (#14443) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make"], "perturbed_original": ["Separate Installing from sources section and add more details (#18171) , separate installing Airflow from sources section and also fixes links to source, besides removes `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` option but we don<unk>t anymore. And I have added a bit more details on verifying integrity. And add more details with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also separated files for binary source, it had `-bin` suffix which we don't use anymore. And also added section on how to install this file from source. And add more details with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary source, but they have the `-bin` suffix which we don't use . I have added section on verifying integrity. And add more details with examples", ". Separate installing Airflow from sources section and add more details (#18171) This PR separate installing Airflow from sources and also fixes links for binary files which had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section , also fixes links for binary source, it had `-bin` suffix which is not use anymore. And I have added section on verifying integrity. And more details with examples", "Separate Airflow installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` suffix which is not use anymore. And I have added section on installation. And add more details with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and removes old download links for binary source, it had `-bin` suffix which we don't use anymore. And I have added help for verifying integrity. And add configuration documentation with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing from sources section and add more details. After add updated links for binary source, it had links for raw sources which we don't use anymore. And I have added section on verifying integrity. And add more details with examples", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links that are installing from source, it had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details and examples"], "original_ll": -4.831807613372803, "sampled_ll": -3.2652435302734375, "all_perturbed_sampled_ll": [-3.116877317428589, -3.1100986003875732, -3.2652435302734375, -3.166799545288086, -3.341136932373047, -3.467496395111084, -3.2342584133148193, -3.3093068599700928, -3.2894515991210938, -3.2652435302734375], "all_perturbed_original_ll": [-4.813450336456299, -5.039078712463379, -4.450250148773193, -4.83502197265625, -4.70868444442749, -5.136953830718994, -4.998553276062012, -4.919393062591553, -4.3935980796813965, -4.785827159881592], "perturbed_sampled_ll": -3.256591272354126, "perturbed_original_ll": -4.808081102371216, "perturbed_sampled_ll_std": 0.1025567159202665, "perturbed_original_ll_std": 0.2282883941294085}, {"original": "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "sampled": "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' no longer exists.", "perturbed_sampled": ["Removes unnecessary function call (#15956) No need for this call, as if no perms are passed `sync_resource_permissions' no longer exists.", "Removes unnecessary function call (#15956) No need to make this call, as resources and perms are passed `sync_resource_permissions' no longer exists.", "Removes unnecessary function call (#15956) No need to make the call as if no perms are passed `sync_resource_permissions' no longer exists.", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are present the function no longer exists.", "Removes the call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' no longer exists.", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed this method no longer exists.", "Removes this call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' no longer exists.", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' always exists.", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are to be set, `sync_resource_permissions' no longer exists.", "Removes unnecessary function call (#15956) No need to make this call, as if no arguments have been passed `sync_resource_permissions' no longer exists."], "perturbed_original": ["Removes unnecessary function call (#15956) as I'd not really need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed they get handled through circuits anyways.", "Removes unnecessary function calls. No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function calls. No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function call (#15956) No need to make this call, as if no permissions were passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function call (#15956) No need to make this call, as once perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function call . No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function call . There is no need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary arguments. (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Removes unnecessary function call (#15956) No need to make this call, provided no perms are passed `sync_resource_permissions` short circuits anyways."], "original_ll": -4.979821681976318, "sampled_ll": -4.241580963134766, "all_perturbed_sampled_ll": [-4.318856716156006, -4.501797676086426, -4.443696975708008, -4.045012950897217, -4.2119364738464355, -4.214919567108154, -4.168785095214844, -4.574530601501465, -3.9965944290161133, -4.032745361328125], "all_perturbed_original_ll": [-4.975205898284912, -5.069675922393799, -4.76303768157959, -4.76303768157959, -4.946800231933594, -5.063949108123779, -4.935596942901611, -4.587121963500977, -4.937252044677734, -5.212902545928955], "perturbed_sampled_ll": -4.2508875846862795, "perturbed_original_ll": -4.925458002090454, "perturbed_sampled_ll_std": 0.19330883321003703, "perturbed_original_ll_std": 0.17117453032486415}, {"original": "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm Chart tests in case default branch is different than main.", "sampled": "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (which should already match branch #1609), so we do not include the chart when testing for new", "perturbed_sampled": ["Disable Helm tests when branch is in airflow version (#17457) We are preparing Helm chart from main branch only and will never run it from airflow version branches (which should already match branch #1609), so we do not include the need to enable Helm testing for new", "Disable the chart when branch is not main ! Is this because we always are preparing Helm chart from main branch only and we never run test from airflow version branches (which should already match branch #1609), so we do not include the chart when testing for new", "for airflow tests when branch is not main . But we are preparing Helm chart from main branch only and we never run it from airflow version branches (which should already match branch #1609), so we do not include the chart in test for new", "Disable Helm Chart when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (which should already match branch #1609), so we do not include the chart when we create new", "Disable Helm tests when branch is not main (#17457) We always run the Helm chart from main branch only and we never run Helm chart from different airflow version branches (which should already match branch version). Also, we do not include the chart when testing for new", "Disable Helm tests if the version is not main branch! We are preparing Helm chart from main branch only and we never run tests on new airflow version branches (which should already match branch #1609), so we do not include the chart when testing for new", "Disable Helm tests when branch test for new main (#17457) We are preparing Helm tests from main branch only and we never run it from airflow version branches (which we never match branch #1609), so we do not include the chart when testing for new", "that should only happen in some tests when new release is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (which should already match branch #1609), so why not include the chart when testing for new", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart for main branch only and we need to keep it from airflow version branches (which should already match branch #1609), so we do not include the chart yet for new", "Disable Helm tests for branch, which is not main (#17457) We are preparing Helm chart from main branch only and we never run it from new branches (which should already match branch #1609), so we will not include the chart when testing for new"], "perturbed_original": ["Disable Helm tests when branch is different than main?, (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) so this disables Helm Chart in case default branch is different than main.", "Disable Helm Chart tests in case default branch is not main (#17457) We are preparing Helm chart from main branch only and we never run tests on airflow version branches (similarly as providers) This feature disable Helm Chart tests in case default branch is different than main.", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm test when source branch is not main; the problem occurred in airflow.h so this change is necessary if source branch is not main.", "Disable airflow tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow tests (similarly as providers) This change disables airflow tests in case default branch is different than main.", "Helm Chart run tests when default branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm Chart tests when default branch is different than main.", "chart tests when branch is not the default one. We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm chart tests in case default branch is different than main.", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version (except as providers) This change disables all Helm Chart tests in case default branch is different than main.", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch and we never run it with version branches (similarly as providers) This change disable Helm Chart tests in case default branch is different than main.", "Disable Helm tests when branch is not main (#17457) We run Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm Chart testing in case default branch is not main.", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run those airflow version branches (similarly as providers) to get helm chart. This bug disables Helm Chart tests in case default branch is other than main."], "original_ll": -5.295029163360596, "sampled_ll": -4.991035461425781, "all_perturbed_sampled_ll": [-4.847375869750977, -5.306380271911621, -5.452065944671631, -4.911708831787109, -4.687283515930176, -4.891233921051025, -5.035858154296875, -5.116967678070068, -4.9097900390625, -4.718095302581787], "all_perturbed_original_ll": [-5.30210542678833, -4.92542839050293, -4.611850738525391, -5.327737808227539, -4.8767313957214355, -5.075375080108643, -5.2804951667785645, -5.293382167816162, -5.0531325340271, -5.287371635437012], "perturbed_sampled_ll": -4.987675952911377, "perturbed_original_ll": -5.103361034393311, "perturbed_sampled_ll_std": 0.23237902729401874, "perturbed_original_ll_std": 0.22783726849687386}, {"original": "[AIRFLOW-XXX] Fix doc error (#5179)", "sampled": "[AIRFLOW-XXX] Fix doc error (#5179)Here", "perturbed_sampled": ["[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "[AIRFLOW-XXX] Fix doc error (#5179)Here"], "perturbed_original": ["[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)", "[AIRFLOW-XXX] Fix doc error (#5179)"], "original_ll": -6.116067886352539, "sampled_ll": -6.878108978271484, "all_perturbed_sampled_ll": [-6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484, -6.878108978271484], "all_perturbed_original_ll": [-6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539, -6.116067886352539], "perturbed_sampled_ll": -6.878108978271484, "perturbed_original_ll": -6.116067886352539, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "sampled": "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "perturbed_sampled": ["Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov"], "perturbed_original": ["Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>"], "original_ll": -3.375816822052002, "sampled_ll": -3.2358322143554688, "all_perturbed_sampled_ll": [-3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688, -3.2358322143554688], "all_perturbed_original_ll": [-3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002, -3.375816822052002], "perturbed_sampled_ll": -3.2358322143554688, "perturbed_original_ll": -3.375816822052002, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be in one place. Documentation for Celery does not describe the inspect ping command, but hopefully, this will be added soon.", "sampled": "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of these monitoring settings under the Advanced tab for the respective", "perturbed_sampled": ["Add docs about airflow: (#14533) Part of: #11161 To have a full description of the Airflow system from all Airflow components, you will also need to enable airflow: :app. Please ensure you have enabled Airflow::App.App settings in the app settings for the appropriate package and running app. If the device is not working, check the status in the current monitoring settings under the Advanced tab for the respective", "Add features for Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of Celery components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the advanced app settings for the appropriate package and running app. If this is not working, check the status of these monitoring settings under the Advanced app settings of the respective", "Add docs about Celery monitoring (#14533) Part of: Airflow Monitoring: To have a full description of monitoring of all Airflow components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the app settings for the associated package and running app. If anything is missing, please make sure to check the status of these monitoring settings under the Advanced tab for the respective", "Add docs about Airflow monitoring; (#14533) Part of: #11161 To ensure a full description of the monitoring of all Airflow sensors you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of these monitoring settings under the app settings for the respective", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a clear status of the monitoring of all Airflow components, you also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the settings tab for the appropriate package and software versions. If the device is not working, check the status of these monitoring settings under the Advanced section of the respective", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow applications, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure that you have enabled all monitoring settings in the app settings for the Celery package and running on your device. If monitoring for the device is not working, check the status of these monitoring settings under the Advanced tab for the respective", "Add docs about package monitoring (#14533) Part of: #11161 For a full description of the monitoring of all Airflow components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure that you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of these monitoring settings under the Advanced tab for the respective", "Add docs about Celery monitoring (#14533) Part of: To ensure you have a full description for monitoring of all Airflow components, you will also need to enable airflow: (default) and to ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is connected to an active WiFi network, check the status of these monitoring settings under the monitoring tab for the respective", "Add docs : Enable ACM monitoring (#14533) Part of: #11161 To have a list of the monitoring of all Airflow components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of these monitoring settings under the Advanced Settings tab for the respective", "Add -on Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, you will also need to enable airflow: monitor Please ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of the settings under the app settings for the respective"], "perturbed_original": ["Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow tools, we should add information about HTTP and CLI checks . Thanks to this, we will not have to search for information one by one, but everything will be presented in one place. Documentation for Celery monitoring and HTTP checks now only describe the inspect ping command, but it will be added soon.", "Add docs about Celery monitoring (#14533) Part 2. - To have a full description of the monitoring of all Airflow components, add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be in one place. Documentation for Celery does not describe the inspect ping command, but this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: To have a full description of the monitoring of all Airflow components, I add information of IPC and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be in one place. Documentation for Celery still does not describe the inspect ping , hopefully, this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 As this is not a full description of the monitoring of all Airflow components, I add information about HTTP and CLI interface for Celery. Thanks to this, we will not have to look for information one by one, but everything will be in one place. Documentation for Celery does not describe the inspect process, but hopefully, this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full documentation in Airflow for the monitoring of all Airflow components, I add the description of HTTP and CLI checks for Celery. With this, we will not have to search for information one by one, but everything will be in one place. Documentation for Celery does not describe the inspect ing/monitoring process, but hopefully, this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search these one by one, everything will be in one place. Documentation for Celery will also describe the inspect ping command, and this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, I add information about the console and CLI checks for Celery. Thanks to this, we will not have to search for each component one by one, everything will be in one place. Also, the console doc for Celery does not describe the inspect ping command, but hopefully, this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full documentation about the monitoring of all components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to download all the information one by one, but everything will be in one place. Documentation for Celery does not include the inspect ping command, but hopefully, this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, I add information about monitoring and data for Celery. Thanks to this, we will not be forced to search for information one by one, but everything will be in one place. The documentation for Celery does not describe the inspect module itself, but hopefully, this will be added soon.", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to look for information one by one, but also all the information be in one place. Documentation for Celery does not describe the ping , so hopefully, this will be added soon."], "original_ll": -3.8315184116363525, "sampled_ll": -3.775820016860962, "all_perturbed_sampled_ll": [-4.066613674163818, -3.6122758388519287, -3.675135612487793, -3.7916457653045654, -3.88140606880188, -3.613955020904541, -3.772934913635254, -4.081031322479248, -3.9310920238494873, -4.085701942443848], "all_perturbed_original_ll": [-3.9011168479919434, -3.7232892513275146, -4.021740436553955, -3.7391357421875, -3.7593653202056885, -4.017306804656982, -3.8111495971679688, -3.625307321548462, -3.6479218006134033, -3.919025182723999], "perturbed_sampled_ll": -3.8511792182922364, "perturbed_original_ll": -3.8165358304977417, "perturbed_sampled_ll_std": 0.1776379803470794, "perturbed_original_ll_std": 0.13529898037840823}, {"original": "Added notification to solve \"docker-credential-service-error\" (#18524)", "sampled": "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "perturbed_sampled": ["Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested"], "perturbed_original": ["Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)", "Added notification to solve \"docker-credential-service-error\" (#18524)"], "original_ll": -4.5467000007629395, "sampled_ll": -4.871146202087402, "all_perturbed_sampled_ll": [-4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402, -4.871146202087402], "all_perturbed_original_ll": [-4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395, -4.5467000007629395], "perturbed_sampled_ll": -4.871146202087402, "perturbed_original_ll": -4.5467000007629395, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "sampled": "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "perturbed_sampled": ["[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What"], "perturbed_original": ["[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)"], "original_ll": -5.137643337249756, "sampled_ll": -5.546127796173096, "all_perturbed_sampled_ll": [-5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096, -5.546127796173096], "all_perturbed_original_ll": [-5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756, -5.137643337249756], "perturbed_sampled_ll": -5.546127796173096, "perturbed_original_ll": -5.137643337249756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Remove profiling link (#4602)", "sampled": "[AIRFLOW-XXX] Remove profiling link (#4602)One", "perturbed_sampled": ["[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "[AIRFLOW-XXX] Remove profiling link (#4602)One"], "perturbed_original": ["[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "[AIRFLOW-XXX] Remove profiling link (#4602)"], "original_ll": -6.615202903747559, "sampled_ll": -7.196170806884766, "all_perturbed_sampled_ll": [-7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766, -7.196170806884766], "all_perturbed_original_ll": [-6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559, -6.615202903747559], "perturbed_sampled_ll": -7.196170806884766, "perturbed_original_ll": -6.615202903747559, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but then it causes a lot of extra strain in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the subsequent merge cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but this happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "sampled": "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "perturbed_sampled": ["Enables duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are set. Enables duplicate cancelling on push/schedule (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We 'd removed duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe 'd removed duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all parameters are set properly. Disables back duplicate cancellation on push/schedule (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at the moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at the moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Re: duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same. We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule in the future. We disabled duplicate cancelling on push/schedule in #11372) but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all of the parameters are clear. Enables back duplicate cancelling on push (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all of the parameters are"], "perturbed_original": ["Enables back duplicate cancelling on push/schedule . If we have disabled duplicate cancelling on push/schedule in the config, then it causes a lot of extra strain in case several commits are merged in quick succession. The master merges are always full builds , which take a lot of time, but if we merge PRs quickly, the subsequent merge cancels the master one. This has the negative side that we might not know who broke the master build, which actually happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling on push/schedule . I disabled duplicate cancelling on push/schedule in #11397 but then it causes a lot of extra strain in case several commits are merged in quick succession. PR merges are always full builds and take a lot of effort, but if we merge PRs quickly, the subsequent merge s might not even match the previous ones. This has the negative consequence that we might merge PRs from people who broke the master build, but this happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling on push/schedule . We disabled duplicate cancelling on push/schedule in #11397 but then it causes a lot of extra strain in case several commits are merged in parallel. The master merges are always full builds and take a lot of time, so if we merge PRs quickly, the subsequent merge cancels the prior merge. This has the negative consequence that we might not know who completed the master build, but this happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling on push/schedule . I disabled duplicate cancelling on push/schedule in #11397 but then realized it is often a lot of extra strain in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the master build just cancels the previous ones. This has the negative consequence that the PR might not be included, we broke the master build, but this happens rarely enough to offset much of the pain at expense of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling in push/schedule (#11471) We disabled duplicate cancelling on push/schedule in the past, but then it causes a lot of extra strain in case several commits are merged in quick succession. First, master merges are always full builds and take a lot of time, so if we merge PRs quickly, the latest commit cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but this happens rarely enough to suffer not at expense of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling in push/schedule (#11471) We disabled duplicate cancelling back in Push. But then it causes a lot of extra strain in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs in quick succession the subsequent merge cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but this happens rarely enough to suffer the pain at expense of less strained queue in GitHub Actions.", "Enables back duplicate cancelling on push/schedule (#11471) Enables back duplicate cancelling on push/schedule , but then it will create a lot of extra strain in case several commits are merged in quick succession. Obviously, master merges are always full builds and take a lot of time, but if we merge PRs quickly, the subsequent merge cancels the previous ones. This has the negative consequence that we might only be the ones who broke the master build, but this happens rarely enough to suffer the pain at expense of a less strained queue in GitHub Actions.", "Enables PR cancelling on push/schedule (#11471) We disabled cancelling on push/schedule in #11397 but then it causes a lot of extra strain in case several PRs are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the next PR cancels the previous ones. This has the negative consequence that we might not know who is which PR for the merged master build, but this happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling on push to schedule. We disabled duplicate cancelling on push/schedule in #11397 but then it causes a lot of extra work in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the next PR cancels the previous ones. We face the fact that we might not know who broke the master build, but this happens rarely enough to suffer the pain of duplicate cancelling when we have many PRs of much less strained queue in GitHub Actions.", "Enables back duplicate cancelling on push/hash. We disabled duplicate cancelling on push/schedule in #11397 because it causes a lot of strain in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge several commits quickly, the subsequent merge cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but this happens rarely enough to justify the pain at expense of a less strained queue in GitHub Actions."], "original_ll": -4.034751892089844, "sampled_ll": -1.8665119409561157, "all_perturbed_sampled_ll": [-1.792556643486023, -1.9412622451782227, -1.8665119409561157, -1.9240527153015137, -1.8249361515045166, -1.8665119409561157, -1.948654055595398, -1.8665119409561157, -2.1546401977539062, -1.9514076709747314], "all_perturbed_original_ll": [-3.981126546859741, -4.087655067443848, -4.107502460479736, -4.0414958000183105, -4.02256441116333, -4.097561836242676, -3.954106092453003, -3.9379160404205322, -3.989642858505249, -3.938369035720825], "perturbed_sampled_ll": -1.913704550266266, "perturbed_original_ll": -4.015794014930725, "perturbed_sampled_ll_std": 0.09528775450660733, "perturbed_original_ll_std": 0.06222344325185747}, {"original": "Disable experimental REST API by default (#12337)", "sampled": "Disable experimental REST API by default (#12337)In", "perturbed_sampled": ["Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In", "Disable experimental REST API by default (#12337)In"], "perturbed_original": ["Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)", "Disable experimental REST API by default (#12337)"], "original_ll": -5.420007705688477, "sampled_ll": -6.299905300140381, "all_perturbed_sampled_ll": [-6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381, -6.299905300140381], "all_perturbed_original_ll": [-5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477, -5.420007705688477], "perturbed_sampled_ll": -6.299905300140381, "perturbed_original_ll": -5.420007705688477, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds a forgotten word in a README.md (#12066)", "sampled": "Adds a forgotten word in a README.md (#12066)When", "perturbed_sampled": ["Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When", "Adds a forgotten word in a README.md (#12066)When"], "perturbed_original": ["Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)", "Adds a forgotten word in a README.md (#12066)"], "original_ll": -4.451131820678711, "sampled_ll": -5.095714092254639, "all_perturbed_sampled_ll": [-5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639, -5.095714092254639], "all_perturbed_original_ll": [-4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711, -4.451131820678711], "perturbed_sampled_ll": -5.095714092254639, "perturbed_original_ll": -4.451131820678711, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove flask-admin based Plugins (#11515)", "sampled": "Remove flask-admin based Plugins (#11515)It", "perturbed_sampled": ["Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It", "Remove flask-admin based Plugins (#11515)It"], "perturbed_original": ["Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)", "Remove flask-admin based Plugins (#11515)"], "original_ll": -6.669112205505371, "sampled_ll": -7.409968852996826, "all_perturbed_sampled_ll": [-7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826, -7.409968852996826], "all_perturbed_original_ll": [-6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371, -6.669112205505371], "perturbed_sampled_ll": -7.409968852996826, "perturbed_original_ll": -6.669112205505371, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix mypy apache kylin operators (#20595)", "sampled": "Fix mypy apache kylin operators (#20595)The", "perturbed_sampled": ["Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The", "Fix mypy apache kylin operators (#20595)The"], "perturbed_original": ["Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)", "Fix mypy apache kylin operators (#20595)"], "original_ll": -6.794467449188232, "sampled_ll": -7.321258068084717, "all_perturbed_sampled_ll": [-7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717, -7.321258068084717], "all_perturbed_original_ll": [-6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232, -6.794467449188232], "perturbed_sampled_ll": -7.321258068084717, "perturbed_original_ll": -6.794467449188232, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number it has the scheduler generates the following warning: `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be an action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific log notice for this case.", "sampled": "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number of slots defined, if the number of slots required exceeds the available pool you could find that, from our perspective, this indicates that something is wrong. This kind of event has to happen at least once every 15 seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be able to run much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", "perturbed_sampled": ["Add specific warning when task requires for more slots than pool defined with (#20178) In cases where task requests more pool slots than the total number of slots in pool and the number of slots required exceeds the available pool you could find that, from our benchmark tests, this warning indicates that something is wrong. This kind of event usually has to happen at least once every few seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be able to run much faster now and a single instance of this code could be enough to improve the performance of your task.\n\n\nTask.addProperties() for instance of Task object now returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", "run. We now provide a warning when Task uses more slots than pool defined with (#20178) In cases where task uses more pool slots than the total pool of slots defined, if the number of slots required for the task exceeds the total capacity of the available pool you could find that, from our perspective, it is possible that something is wrong. This kind of event has to happen at least every 15 seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be fine. Your task will run much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects that are of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", ". Bug warning when Task asks for more pool slots than the available pool defined with (#20178) In cases where task asks for more pool slots than the available number of slots defined, if the number of slots is smaller than the available pool you could find that, from our perspective, this indicates that something is wrong. This kind of event has to happen at least once every 15 seconds, based on our benchmarks. Since most of the tasks were able to be executed and not all of them were waiting for this event, things should be able to run much faster now and a single instance of this code could be executed to improve the behavior of the task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects of the same object type , instead of \"Object reference object,\" if you had object type of Object instance in", "Add specific warning when Task asks more pool slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number of slots defined, when number of slots required exceeds the available pool you could find a warning. From our perspective, this indicates that something is wrong. This kind of situation has to happen at least once every 15 seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be able to run much faster now and using specific instance of tasks could be enough to improve the behavior of that task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns a list of currently available pools and pool names for objects of the same object type (instead of just \"A new object,\" if you had object type of Object instance in", "Add specific warning when Task asks for more slots than pool defined with (#20178) . In cases where task asks for more pool slots than the total number of slots defined, if the number of slots required exceeds the available pool you are aware that, from our perspective, this indicates that something is wrong. This kind of event has to happen at least once every minute based on our benchmarks. Since we were using an event-based system, so not all tasks were waiting for this event, things should be able to run much faster now and a single instance of events could be used to improve the behavior of the task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects of the same object type (instead of \"Object reference object,\" if the object type of Object instance in", "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number of slots defined, if the number of slots exceeds the available pool you could find that, from our perspective, this indicates that something is wrong. This kind of event has to happen at least every 15 seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not the usual list() and were waiting for this event, things should be able to run much faster . Also, a small version of this code could help you to improve the behavior of your task.\n\n\nTask.addProperties() for instance of Object returns proper object for all instances of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", "Add specific warning when Task asks for more slots than pool slots? (#20178) In cases where task asks for more pool slots than its total number of slots defined, if the number of slots required exceeds the available pool you could find a warning in our perspective, this indicates that something is wrong. This kind of event has to happen at least once every 2 seconds, based on benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should look like they should have to run much faster now and a single instance of this code could be used to improve the performance of your task.\n\n\nTask.addProperties() in the case of tasks.getPoolLists() now returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had to create a new class of Object instance in", "Add specific warning when task asks for more than pool slots (#20178) In case your task asks for more pool slots than the total number of slots defined, if the number of slots required exceeds the available pool you could find that, from our perspective, this indicates that something is going on and that your task should be stopped. This kind of event has to happen at least once every 15 seconds, based on our experience. If you were using tasks.getPoolLists() and not all tasks were passing this event, you would be able to run much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of Object returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had added this for instance of Object instance in", "Add specific warning when Task asks for more slots than pool defined with Task. In cases where task asks for more pool slots than the total number of slots , you could see that the number of slots required exceeds the available pool you could find . From our perspective, we think that something is so complex that this kind of operation has to happen at least once every 15 seconds, or until things are close to our benchmarks. If you were using this code, as not all tasks were waiting for this event, things should be able to happen much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects that have the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", "[0:24] Change to warning when Task asks for more slots than the pool it has been set with (#20178) In case your task asks for more pool slots than the total number of slots defined, task.getPoolList() shows that the number of slots required exceeds the available pool you could use and from our perspective, this indicates something is wrong. This kind of event has to happen at least once every 15 seconds, according to our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be able to run much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for using tasks.getPoolLists() now returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in"], "perturbed_original": ["Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where the Task asks for more slots than the total number it has the scheduler generates the following warning: `[2021-12-09 19:37:51,949] > - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> which requires 4 slots but there are 2 open slot in the pool . https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this case is very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be two action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add s specific notice in this case.", "Add specific warning when Task asks for more slots than pool defined limits. In cases where task asks for more pool slots than the total number it has the scheduler will give the following warning: `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO : We are executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 3 slots, but there are 2 open slots in the pool my_pool. ` Note this message is very confusing as the problem is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task execute successfully there must be an action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. Need to add specific warning for this case.", "Add specific warning when Task asks for more slots than pool defined with a given name. In cases where task asks for more slots than the total that pool has the scheduler generates the event `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: 4 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the issue is not with a specific task but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be 2 action from the user: 1. User to change the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific notice for this case.", "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number it has the scheduler generates the following message: [22/12/2011 19:37:51,949] {scheduler_job.py:407} INFO : Issue executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are only 4 slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the issue is not with task execution but with number of slots in pool. Waiting for slots will not resolve the problem. To make the task run there must be an action taken by the user: 1. User to increase the total number of slots in the Pool 2. User need to change the TaskInstance to requests less slots. This PR add specific warning for this case.", "Add specific warning when task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number it has it generates the following warning: `[2021-12-09 T19:00:45+00:0] INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it is with 8 active slots but there are 2 open slots in the pool > https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the request is not with open slots but with less than the total slots of the pool. Waiting for slots will not resolve the issue. To make the task run there must be an action from the user: 1. User to increase the total number of slots in the pool. 2. User need to change the task code to requests less slots. This PR add s a more clear notice for this case.", "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number it has the scheduler job send the following warning: `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task <unk> since it requires 4 slots but there are only 2 open slots in the defined pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the issue is not with open slots but with the total slots of the pool. Changing the pool slots will not solve the problem. For the task run there must be an action from the Task User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific log notice for this case.", "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task request more pool slots than the total number it has in pool it generates the following warning: `[2021-12-09 T19:08:00+00:0] INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [item] because it needs slots but there are 2 open slots in the pool my_pool. my_pool#5 However this message can be very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task return the requested number of open slots there must be an option available to the user: 1. User to increase the number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific log notice for this case.", "Add specific warning when Task asks for slots less than pool defined with (#20178) In cases where task wants more pool slots than the total number it has the scheduler generates the following warning: `[2021-12-09 19:37:51,949] {scheduler_job.py:407} : Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the pool contains slots for a task, not with open slots but with the total slots of the pool. Waiting for the message to fix the issue will not resolve . To make the message clearer there must be an action from the user: 1. User to increase the number of slots in the Pool 2. User need to change the task code to requests less slots. We need to add specific log notice for this case.", "generates an error warning when Task asks for more slots than defined by pool definition. In cases where task asks for more pool slots than the total number it has the scheduler generates the message `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 slots available in the pool my_pool. My_pool is 4 . However this message is very confusing as task is not doing anything with slots but with the total slots of the pool. Asking for more slots will not resolve the problem. To make the task run there must be an action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific error for this case.", "adding warning when Task request more slots than pool defined with (#20178) - In the case where task asks for more pool slots than the total number it has the scheduler generates the following warning: `[2021-12-09 T19:08:00+00:0] INFO : Problem with executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 slots available in the pool my_pool. . However this message is very confusing because the issue is not with open slots but with the total number of slots in the pool. Waiting for slots will not resolve the problem. To make this task run there must be an action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific log notice for this case."], "original_ll": -3.83475661277771, "sampled_ll": -3.1840834617614746, "all_perturbed_sampled_ll": [-3.3463008403778076, -3.1670007705688477, -3.3569562435150146, -3.3657732009887695, -3.3691115379333496, -3.4662413597106934, -3.376253366470337, -3.2122881412506104, -3.529862403869629, -3.2270188331604004], "all_perturbed_original_ll": [-3.894578456878662, -3.6475894451141357, -3.7017152309417725, -3.8030524253845215, -3.7217235565185547, -3.9212963581085205, -3.745741128921509, -3.900312900543213, -3.666522264480591, -3.71055006980896], "perturbed_sampled_ll": -3.341680669784546, "perturbed_original_ll": -3.771308183670044, "perturbed_sampled_ll_std": 0.10682388496915747, "perturbed_original_ll_std": 0.09657917929024576}, {"original": "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all the CLI command already turn the literal `DAGS_FOLDER` in to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR brings back this behaviour", "sampled": "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, this functionality is still missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "perturbed_sampled": ["Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in older versions of Docker, using ipac but it is also required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, this functionality is missing in newer releases. This is now resolved by adding a hook: \"libcsh_runhooks_exec.csh\" In Docker", "Support DAGS folder being in different location on scheduler and runners . Although there has been some vestigial support for this in older versions of Docker, it was not provided in latest releases and it is also required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, this functionality is still missing from current releases. This situation is resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "\u2013 folder being in different location on scheduler and runners (#16860) ? There has been some vestigial of this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or Ubuntu if you want to use other dependencies such as Mota etc. However, this functionality has been almost missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "Support DAGS folder being in the correct location on the external runners based on the same name. There has been some vestigial support for this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros if you need to use other dependencies such as Mota etc. However, this functionality is still missing in newer releases. This is now supported as a hook by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "Support DAGS folder in different location on scheduler and runners . There has been some vestigial support for this concept in older versions of Docker, using libcsh and it is not required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies for Mota etc. This functionality is still missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in earlier versions of Docker, using libcsh and it is also expected to be implemented in FreeBSD or FreeBSD-based distros if you don't want to use other dependencies such as Mota etc. However, this functionality is currently missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial implementation of this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros that want to use other dependencies such as Mota etc. Nonetheless, this functionality is still missing in newer releases. This is being addressed by adding this comment to the file system. In Docker", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been vestigial support for this concept in older versions of Docker, using libcsh and it is also possible to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, this functionality is still missing in newer and newer versions and is now implemented via adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "Support DAGS folder s to use a different location on scheduler and on the host for storage. There has been previously some partial support for this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, it is still missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "etc) /usr/local/bin/csh/boot folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in older versions of the libcsh and it is also required to be implemented in FreeBSD or Linux if you want to use other software such as Mota etc. However, this functionality is still missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker"], "perturbed_original": ["Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all the CLI command already turn the literal path to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 this was fully broken and the scheduler only accepts the full paths of the DAGS files. This patch brings back this behaviour", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all the CLI command already turn the `DAGS_FOLDER` in to the real value of the DAGS folder when loading dags), but sometime around 6.2, the implementation got fully broken and the scheduler only ever passed this value to DAG files. This PR brings support for this broken behaviour", "Support DAGS folder being in different location on scheduler and command line. There has been some vestigial support for this concept in R2 for a while (all the CLI command passed the literal `DAGS_FOLDER` as the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAGS folder. This PR brings back this behaviour", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all dag loader command already turn the literal `DAGS_FOLDER` in the scheduler and runner to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got removed and Airflow only ever passed full paths to DAG files. This PR brings back this behaviour", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been good in-place support for this concept in Airflow for a long time (all the schedulers and runners already turn the literal `DAGS_FOLDER` in to the real path to the DAGS folder when loading dags), but around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR brings back this behaviour", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all the CLI command already turn the literal `DAGS_FOLDER` in to the real <unk>DAGS_FOLDER<unk> for referencing the DAGS folder when loading dags), but sometime in the past it got broken and the scheduler only ever passed that to DAG files. This PR brings back this behaviour", "Support DAGS folder being in the path when starting an RTOS on scheduler and CLI commands. There has been some support for this concept in Airflow for a while (all the CLI command already turn the literal `DAGS_FOLDER` in to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken , as the scheduler only ever passed full paths to DAG files. This PR fixes this behaviour", "Support DAGS folder s and different paths in scheduler and runners (#16860) There has been some vestigial support for this in Airflow for a while (all the CLI packages did was turn the literal `DAGS_FOLDER` in to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to folders. This PR brings back this behaviour", "). The bug about the DAG folder being in different location on scheduler and runners (#16860) had been some vestigial support for this concept in JavaScript for a while (all the CLI command already turn the literal `DAGS_FOLDER` to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR fixes this behaviour", "Support DAGS folder being in different location on scheduler and CLI commands. There has been some vestigial support for this concept in Airflow for a while (all the CLI command options pass the literal `DAGS_FOLDER` in to the value of the Dags folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR brings back this behaviour"], "original_ll": -3.847554922103882, "sampled_ll": -3.453601837158203, "all_perturbed_sampled_ll": [-3.736832618713379, -3.4307265281677246, -3.7556095123291016, -3.4978208541870117, -3.5902063846588135, -3.4997096061706543, -3.809725284576416, -3.66059947013855, -3.413287878036499, -3.366335391998291], "all_perturbed_original_ll": [-3.8540115356445312, -3.9901857376098633, -3.525487184524536, -3.839329957962036, -3.7059953212738037, -3.9011409282684326, -3.761955499649048, -3.9804468154907227, -4.023989200592041, -3.732858419418335], "perturbed_sampled_ll": -3.576085352897644, "perturbed_original_ll": -3.831540060043335, "perturbed_sampled_ll_std": 0.14952728941718874, "perturbed_original_ll_std": 0.14621839706311104}, {"original": "Fix Experimental API Client (#9849)", "sampled": "Fix Experimental API Client (#9849)In", "perturbed_sampled": ["Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In", "Fix Experimental API Client (#9849)In"], "perturbed_original": ["Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)", "Fix Experimental API Client (#9849)"], "original_ll": -6.725271701812744, "sampled_ll": -7.660573959350586, "all_perturbed_sampled_ll": [-7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586, -7.660573959350586], "all_perturbed_original_ll": [-6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744, -6.725271701812744], "perturbed_sampled_ll": -7.660573959350586, "perturbed_original_ll": -6.725271701812744, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this we throw an error, that is captured and showing using the existing import_error mechanism for DAGs. This almost certainly happens because a user has done \"something interesting\".", "sampled": "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, the number of rows was reduced. Now, we just add a line when creating. Also, the column name and ID used to include the query string, is now used to keep the query consistent if the query is a sequence -- i.e., there is no need to include the", "perturbed_sampled": ["Show all errors in the UI. (#12866) The old behaviour led to \"bad\" data being written in the DB -- for example, the number of rows was reduced. Now, we just add a line when creating. (#12850) A column for query ID used to include the query string, is now removed. This should keep the query consistent if the query is a sequence -- i.e., there is no need to include the", "Show DAG serialization errors in the UI. (#12866) Sometimes behaviour led to \"bad\" data being written in the DB -- for example, the number of rows was reduced. As a result, just add a line when creating. Also, the name and ID used to include the query string, is now used to keep the query consistent if there is a sequence of queries -- there is no need to include the", "Show DAG serialization errors in the UI. (#12866) The previous behaviour was to remove the \"bad\" data lines from the DAGs saved in the DB -- for example, the number of lines added in the DB reduced. Now, we just add a line when creating. Also, the column name and ID used to include the query string, are no longer used to keep the query consistent if the query is a sequence -- i.e., there is no need to include the", "Show DAG s in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, an incorrect number of rows . Now, we just add a check before creating. Also, the column name and ID , which used to include the query string, is now used to keep the query consistent if the query is a sequence -- i.e., there is no need to include the", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to less data being written in the DB -- for example, the number of stored procedures on rows was reduced. Now, we can also write a line when need be. Here, the column name and ID used to include the query string, is now used to keep the query line as the query is a sequence -- i.e., there is no need to include the", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, the number of possible rows was reduced. Now, we just add a line when creating. Also, the column name and ID used , used by the query string, is now used to keep the query sequential even if the query is a sequence -- i.e., there is no need to enter an ID in the", "Show DAG serialization errors in the UI. (#12866) - Unsafe behaviour led to \"bad\" data being written in small volumes -- for example, the number of rows was reduced. Now, we use a similar behaviour for larger databases we are creating. Also, the column name and ID used to include the query string, is now used to keep the query consistent if the query string has sequence -- i.e., there is no need to include the", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DAG -- for example, the number of rows was reduced. Now, we just add a line when necessary. The field ID variable, which contains the column name and ID used to create the SQL query string, is now used to keep the query consistent if the query contains a sequence -- i.e., there is no need to include the", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, the number of rows under-defined. Now, we just add a line when creating. The new behavior, where the DB is populated with column name and ID used to include the query string, is now used to keep the query consistent if the query is compiled -- there is no need to include the", "Show DAG serialization errors in the UI. (#12866) The previous behaviour caused \"bad\" data to be deleted in the DB -- for example, the number of rows was reduced. Now, we just add a row to an arbitrary table we're creating. Also, the column name and ID used to include the query value are now used to keep the query consistent if the query is a sequence -- i.e., there is no column name to include the"], "perturbed_original": ["Show DAG serialization errors in the UI. (#12866) The wrong idea led to \"bad\" data being written to the DB -- for example: (db: { \"tasks\": [ \"serialization_failed\" ], it should be a list of dictionaries. It clearly isn't.) Instead of doing this we throw an error, that is captured and showing using the existing import_error , when adding DAGs. This almost certainly happens because a user has done \"something interesting\".", "Show DAG serialization errors in the UI. (#12866) The previous fix seems to be due to \"bad\" data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a dictionary of dictionaries. It clearly isn't.) By doing this we throw an error, that is captured and showing using the existing import_error mechanism for DAG. The default behaviour almost certainly happens when a user has done \"something interesting\".", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, \"dag\": { \"tasks\": [ \"serialization_failed\" ] (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this, throw an error, that is captured and handled by the existing import_error mechanism for DAGs. This almost certainly happens if the user has done \"something interesting\".", ", we should also not report serialization errors in the UI. (#12866) The previous behaviour led to json errors being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this we try to provide a more clear error, that is captured automatically by using the existing import_error mechanism for DAGs. This almost certainly happens because a DAG has done \"something interesting\".", "Show DAG serialization errors in the UI. (#12866) The previous patch to #29742 has no use due to \"bad\" data being written in the DB . For example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this as an error, that is worth showing using the existing import_error mechanism for DAGs. This also happens because a user has done \"something interesting\".", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to incorrect json being written in the UI as for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this as an error, that should be exported and showing using the existing import_error mechanism for DAGs. This certainly happens because a user has done \"something interesting\".", "to display serialization errors in the UI. (#12866) The previous behaviour led to more data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], as if this should be a list of tasks. It clearly isn't.) Instead of doing this we throw an error, that is captured and showing using the existing import_error s_json DAGs. This almost certainly happens because a user has done \"something interesting\".", "Show DAG serialization errors in DB. (#12866) The previous error was actually due to \"bad\" data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ]>> (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of this we throw an error, which is captured and showing using the existing import_error mechanism for DAGs. This almost certainly happens because a user has done \"something interesting\".", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being inserted in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list as well. It clearly isn't.) Instead of \"bad\" data, we now provide an error, that is captured and showing using the existing import_error mechanism for DAGs. This almost certainly happens because a user has done \"something interesting\".", "Show DAG serialization errors in the UI. The previous methods have caused errors due to \"bad\" data being passed to the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], which should be a list of dictionaries. (It isn't.) Instead of doing this we throw an error, that is captured and showing using the existing import_error mechanism for DAGs. This almost certainly happens because a user has done \"something interesting\"."], "original_ll": -3.8079771995544434, "sampled_ll": -3.432356834411621, "all_perturbed_sampled_ll": [-3.632918357849121, -3.6998708248138428, -3.4191677570343018, -3.4812541007995605, -3.7240095138549805, -3.5876822471618652, -3.7633538246154785, -3.436683177947998, -3.758665084838867, -3.56791353225708], "all_perturbed_original_ll": [-4.165932655334473, -3.9236531257629395, -3.7126717567443848, -3.817854404449463, -3.907766819000244, -3.8574788570404053, -4.224273204803467, -3.923046588897705, -3.7926464080810547, -3.7540760040283203], "perturbed_sampled_ll": -3.6071518421173097, "perturbed_original_ll": -3.907939982414246, "perturbed_sampled_ll_std": 0.12346590365402069, "perturbed_original_ll_std": 0.1590157910087388}, {"original": "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "sampled": "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "perturbed_sampled": ["Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Update other tests *", "Update example on docs/howto/connection/index.rst (#10236) * Update example on docs/howto/connection/index.rst (#10261) * Various other tests *", "Update example on (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10236) * Various other tests *", "Update example on docs/howto/connection/index.rst (#10246) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on the connection * Various other tests *", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example in documentation (#10261) * Various other tests *", "Update example on index.rst * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *"], "perturbed_original": ["Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * fixup! Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * fixup! Update example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Fix example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on this fixup! Upddate example on docs/howto/connection/index.rst", "Update example on the connection (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example to avoid duplicate * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10135) * Upddate example on docs/howto/connection/index.rst"], "original_ll": -2.716885805130005, "sampled_ll": -2.9966976642608643, "all_perturbed_sampled_ll": [-3.0513100624084473, -2.674370527267456, -4.524030685424805, -2.946826219558716, -2.976579189300537, -4.836416244506836, -2.9966976642608643, -4.2029619216918945, -4.2985148429870605, -2.9966976642608643], "all_perturbed_original_ll": [-2.716885805130005, -2.437161922454834, -2.4892139434814453, -2.716885805130005, -2.8079614639282227, -3.6094608306884766, -3.6311371326446533, -3.76669979095459, -2.716885805130005, -2.144428014755249], "perturbed_sampled_ll": -3.5504405021667482, "perturbed_original_ll": -2.9036720514297487, "perturbed_sampled_ll_std": 0.768802311625484, "perturbed_original_ll_std": 0.534026841777648}, {"original": "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "sampled": "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "perturbed_sampled": ["Add support for defining source location for get current context (#9631) Support for getting current context at any code location that is not in the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the user parameter (#9510) Fixed", "Add support for call to get current context (#9631) Support for getting current context at any code location , including under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "Add function to get current context (#9631) Support for getting context at any code location (#9630) Support for calling commands under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "Add function to get current context (#9536) Added function for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added environment attribute for the get() method (#9510) Fixed", "Add function to get current execution location (#9517) Support for getting current execution location from any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "Add function to get current context with attributes for getting context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "Add function to get current context (#9522) Added facility for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for an updated context method (#9510) Fixed", "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter function to return the current context (#9510) Fixed", "Add function to get current context (#9631) Support s getting the current context at any code location that runs within the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed"], "perturbed_original": ["Add function to get current context (#9631) Support for getting current context at any time. This will make it possible to halt execute of BaseOperator that is outside the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context (#9631) Support for getting current context at any code location that is under the scope of the currentC function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context (#9631) Support for getting current context at any code location , under the scope of BaseOperator.execute function. This proposal is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context that runs outside current scope of BaseOperator.execute function. Add function for getting current context at any code location that runs outside the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "// to get current context (#9631) Support for getting current context at any process that runs under the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context (#9631) Support for getting current context at any code location that is within the scope of current context This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context (#9631) Support getting current context at any code location that runs under the scope of BaseOperator.execute function. This functionality is part of BaseOperator 5.0 Beta 2 Jonathan Shir <jonathan.shir@databand.ai>", "context. Use to get current context for any code location. 2. \u2013 Support for getting current context at any code location that runs under the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context (#9631) Support for getting current context from code location that runs under the scope of BaseOperator.execute function. This functionality is part of a legacy solution. -- Jonathan Shir <jonathan.shir@databand.ai>", "Add function to get current context (#9631) Support for getting current context context at any code location that runs under the scope of BaseOperator.execute function. This work was approved and is now part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>"], "original_ll": -4.226373195648193, "sampled_ll": -3.6902377605438232, "all_perturbed_sampled_ll": [-3.740328073501587, -3.9571526050567627, -3.9147186279296875, -3.496262550354004, -3.675755262374878, -3.5351645946502686, -4.080622673034668, -3.83131742477417, -3.5715951919555664, -3.8570668697357178], "all_perturbed_original_ll": [-3.8533775806427, -3.971201181411743, -4.410182476043701, -3.63191556930542, -4.440483093261719, -4.041489601135254, -4.290728569030762, -4.202023506164551, -4.472578048706055, -4.322545528411865], "perturbed_sampled_ll": -3.765998387336731, "perturbed_original_ll": -4.163652515411377, "perturbed_sampled_ll_std": 0.1851602499976172, "perturbed_original_ll_std": 0.2658365719650761}, {"original": "Adding missing word to welcome message (#16726)", "sampled": "Adding missing word to welcome message (#16726)Dry", "perturbed_sampled": ["Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry", "Adding missing word to welcome message (#16726)Dry"], "perturbed_original": ["Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)", "Adding missing word to welcome message (#16726)"], "original_ll": -6.654922962188721, "sampled_ll": -7.028921127319336, "all_perturbed_sampled_ll": [-7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336, -7.028921127319336], "all_perturbed_original_ll": [-6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721, -6.654922962188721], "perturbed_sampled_ll": -7.028921127319336, "perturbed_original_ll": -6.654922962188721, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.", "sampled": "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.The", "perturbed_sampled": ["Fix broken master (isort fix) (#11954) Static fix failing because of a Bad merge to Master.The", "Fix broken master (isort fix) (#11954) : The merges are failing because of a Bad merge to Master.The", "Fix broken master static check (#11954) Static checks are failing because of a Bad merge to Master.The", "Fix broken master (isort fix) (#11954) The correct steps are failing because of a Bad merge to Master.The", "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad Isort Master.The", "Fix broken master (isort ) Static checks are failing because of a Bad merge to Master.The", "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad Isort Master.The", "Fix broken master (isort fix) (#11954) - all Sort attempts are failing because of a Bad merge to Master.The", "Fix broken master (isort fix) (#11954) Static checks are failing because of a wrong set of properties for binding to Master.The", "Fix broken master (isort fix) (#11954) , where Isorts are failing because of a Bad merge to Master.The"], "perturbed_original": ["Fix broken master (isort fix) (#11954) Static checks are failing due to a Bad merge to Master.", "Fix broken master (isort fix) (#11954) Static checks are failing because of Bad merge to Master.", "Fix broken master (isort fix) (#11954) Static checks are failing , causing a Bad merge to Master.", "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.", "Fix broken master (isort fix) (#11954) Static fixing failing because of a Bad merge to Master.", "Fix broken master (isort s) merging as Static checks are failing because of a Bad merge to Master.", "Fix broken master (isort ers). Static checks are failing because of a Bad merge to Master.", "Fix broken master (isort fix) (#11954) Static checks are failing because of Bad merge to Master.", "Fix broken master (isort fix) a bunch of isort checks are failing because of a Bad merge to Master.", "Fix broken master (isort ). Static checks are failing because of a Bad merge to Master."], "original_ll": -6.013061046600342, "sampled_ll": -6.174251079559326, "all_perturbed_sampled_ll": [-6.55993127822876, -5.508954048156738, -6.051336765289307, -5.9516706466674805, -6.072782039642334, -6.643019676208496, -6.072782039642334, -6.037980079650879, -5.480352878570557, -5.894981384277344], "all_perturbed_original_ll": [-5.9651994705200195, -6.226444721221924, -6.5198588371276855, -6.013061046600342, -6.5826191902160645, -6.4076690673828125, -6.579212188720703, -6.226444721221924, -5.711813449859619, -6.503755569458008], "perturbed_sampled_ll": -6.027379083633423, "perturbed_original_ll": -6.27360782623291, "perturbed_sampled_ll_std": 0.3547812695601572, "perturbed_original_ll_std": 0.28346498662496167}, {"original": "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has a set of provider packages that are needed by the extra and they will be installed automatically if this extra is specified. For now we do not add any version specificatiion, until we agree the process in #11425 and then we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "sampled": "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has to specify when it's installed. So you could now have two different versions at all times: On the one hand, airflow will install for the next day and install for the next day and night, then you can specify in which way. The other thing is for the next package install, I could not get the packages install", "perturbed_sampled": ["Adds automated installation of dependent packages (#11526) since most package names are dependent! When airflow is installed, this one triggers installation of dependent packages. Each extra has to specify when it's going to be installed, so you could now have two different versions at all times: On the one hand, airflow will install for the next day and install for the next day and night, then you can specify in which way. The other thing is for the next package name: Airflow could not get the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, it triggers installation of dependent packages. Each extra has a set method to trigger installation when in the past. So we now have two different versions at all times: On the one hand, airflow will install for the next day and night, on the other hand it will continue to install for the next day and night, then you can specify in which way. The other thing is for the next package install, I could not get the packages install", "Adds automated installation of dependent packages (#11526) When you are specifying when airflow is activated, each one triggers installation of dependent packages. Each extra has to specify when it's installed. So you could now have two different versions at all times. On the one hand, airflow will launch for the next day and install for the next day , and then you can specify in which way. The other thing is for the next package install, I could not get the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has to specify when it's installed. So you could now have two different versions of times: On the one hand, airflow will install for the next day and night. If packages install for the next day and night, then you can specify in which days. On the other hand, for some reason during the package install, I could not get the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed each one triggers installation of dependent packages. Each extra has to specify when it's installed. So you could have two different versions at all times: On the one hand, Airflow install for the next day and then install for the next day and night, then you can select if the autoinstallation runs both which way. The other thing is for the automatic install, I could not get the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when they should be installed, this one triggers installation of dependent packages. Each extra has a time/space for when it's installed. So you could now have two different install schedules for all things at all times: on one hand, airflow will install for the next day and install for the next day and night, then you can specify which way. The other thing is for the next package install, I could not get dependency packages install", "Adds automated installation of dependent packages (#11526) : While other updates are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has to specify when it's installed. This is nice, as it means that you could now have two different versions at all times: On the one hand, airflow will install for the next day and install for the next day and installs for the exact day you can specify in the install. The other thing is for each package install, I tried to get the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when things are installed, this one triggers installation of dependent packages. Each extra has to specify when these packages are installed. So you will have two different versions at all times: On the one hand, airflow might just install for the next day and install for the next day and install for the next day, but you can specify in which way. The other thing is for the next package install, I could have the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra can specify when it's installed. So you can have two different versions at all times: On the one hand, airflow will install for one day and install for the next day and night, then you can specify airflow to install the other way. The other thing is for the airflow install, I could not get the packages install", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one bit of magic is needed : Each extra has to specify when it's installed. So you can have two different versions at all times: On the one hand, airflow will install for the next day and night, on the other hand for the next day and night, then you have to specify in which way. The other thing is for the next package install, I could not get the packages install"], "perturbed_original": ["Adds automated installing of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers the automatic installation of dependent packages. Each extra has a set of provider packages which are needed by the extra and they will be installed automatically if this extra is specified. For now we can't add any version specificatiion, until we agree the process to be automated and then we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of dependencies (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has a set of packages that are needed by the extra and they will be installed automatically if this extra is specified. For now we do not add any version specificatiion, until we agree the process in #11425 and #11461 should be used. Also we need to implement an easier way of getting information about cross-package version ing than in #11464", "Adds automated installation of dependent packages. When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra specifies a set of provider packages that are dependends to the package created by the extra and they will be installed automatically after the extra is specified. For now we do not add any version specificatiion, until we agree the process in #11425 and then we should be able to introduce an automated way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has a list of provider packages that are needed by the extra and they are installed automatically if this extra is specified. For now we do not automate version specificatiion, until we make the process of providing provider packages more reliable and then we should be able to implement an automated way of ensuring packages are installed. This is about how packages specify about cross-package version dependencies. Fixes: #11464", "Adds automated installation of packages! (#11526) When extras are specifying when airflow is enabled one triggers installation of dependent packages. Each extra has a set of provider packages that are needed by the extra and they will be installed automatically if this extra is enabled. For now we cant add any version specificatiion, until we agree the process in #11425 and then we will be able to implement an easier way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of dependent packages . Since extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has a set of dependent packages that are needed by the extra and they will be installed automatically if this extra is installed. For now we do not add any version dependency reporting, but we will add this feature after we agree on more details in #11425 and then we should be able to have more automated way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of airflow dependencies. (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra will have a set of provider packages that is installed by the extra and they will be installed when this extra is specified. The installation process is automated if we do not add any version specificatiion, until we simplify the process in #11425 and then we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of airflow dependent packages (#11526) When extras are specifying when airflow is installed, this triggers installation of dependent packages. Each extra has a set of provider packages that are needed by the extra and they will be installed automatically if this extra is specified. For now we do not have automated installation for version specificatiion, until we have finished the process in #11425 and we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of dependencies (#11526) When extras are specifying when airflow is installed, the extra triggers installation of dependent packages. Each extra will specify a set of provider packages that are needed by the extra and they will be installed automatically when the extra is specified. For now we do not add any version specificatiion, until we agree the process in the next release. And then we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of svn packages. Package list: Each extra has a set of packages that are needed by the extra and they will be installed automatically if this is specified. For now we do not add any version specificatiion, until we agree the process in #11425 and #11262. We should be able to implement an easier way of getting information about cross-package version dependencies. Fixes: #11464"], "original_ll": -4.125349998474121, "sampled_ll": -3.723193883895874, "all_perturbed_sampled_ll": [-3.7831978797912598, -3.639646053314209, -3.9158952236175537, -3.5750303268432617, -3.9325735569000244, -3.889214277267456, -3.7753055095672607, -3.502134323120117, -3.7862257957458496, -3.595078945159912], "all_perturbed_original_ll": [-3.9982900619506836, -4.126323223114014, -4.2161126136779785, -4.066742897033691, -4.295987606048584, -3.82978892326355, -4.1766676902771, -4.013424396514893, -3.966871738433838, -4.1547627449035645], "perturbed_sampled_ll": -3.7394301891326904, "perturbed_original_ll": -4.084497189521789, "perturbed_sampled_ll_std": 0.14505268638386043, "perturbed_original_ll_std": 0.12990117629285727}, {"original": "Reorder middleware - ProxyFix and BaseUrl (#8157)", "sampled": "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "perturbed_sampled": ["Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Reorder middleware - ProxyFix and BaseUrl (#8157)The"], "perturbed_original": ["Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Reorder middleware - ProxyFix and BaseUrl (#8157)"], "original_ll": -6.123979568481445, "sampled_ll": -6.623643398284912, "all_perturbed_sampled_ll": [-6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912, -6.623643398284912], "all_perturbed_original_ll": [-6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445, -6.123979568481445], "perturbed_sampled_ll": -6.623643398284912, "perturbed_original_ll": -6.123979568481445, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however it has some unforeseen consequences. The 'provider_packages' script copy all the providers code for backports in order to refactor them to the empty \"airflow\" directory in provider_packages folder. The #10806 change turned that empty folder in 'airflow' package because it was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "sampled": "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which could only be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages were added or removed via explicit commands. If they were added the module-info of the module in question is now visible (the check of whether it was updated). In order to", "perturbed_sampled": ["Moves some scripts to dev (#12082) Change #10806 made airflow works with implicit packages , but not explicit ones got imported. So the \"expose\" option got removed but the scripts, which could only be done via explicit commands, did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages were added or removed via explicit commands. If they were added the module-info of the module in question is updated automatically (the check of modules was updated). In order to", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. So the \"expose\" option got removed , but explicit installation via scripts, which could only be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if a module was added or removed via explicit commands. Only when implicit packages were added the module-info of the module in question was visible (the check of this was updated). Moves provider package scripts to", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. So the \"expose\" option got removed from scripts, which used to be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure whether explicit modules were added or removed via explicit commands. If you added the module-info -set-update() module in airflow, it is now visible (the check of whether it was updated). In order to", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages, however \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which could only be exported using \"pkgutil\" did not work yet. This was removed yesterday. Moves provider packages scripts to dev (#12101)\n\nThe script \"pkgutil\" no longer checks if packages were added or removed via explicit commands. If they were added the module-info of the module in question is now visible (regardless of whether it was updated). In order to", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow unable to find implicit packages when \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which could only be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script no longer makes sure if packages were added through explicit commands or deleted via explicit commands. If they were not then the module-info of the packages' in question is now visible (the script does not remember whether it was updated). In order to", "Moves provider packages scripts to dev / xml. This was change #10806 This script no longer works with implicit packages when \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which could only be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer checks to make sure if packages were added or removed via provider packages check, etc. If they were added the version of the module in question is now visible (the check of whether it was updated). In order to", "Moves provider packages scripts to pref packages. The change #10806 made airflow works with implicit packages when \"airflow\" got imported. So the \"expose\" option got eliminated from the scripts, so the update could only be done using \"pkgutil\" because airflow packages scripts dont work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes us check if packages were added or removed via provider packages. If they were added the module-info of the module in question is now visible (the check of whether it was updated). In order to", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages that got imported. So the \"expose\" option got removed but the scripts, which could only be done using \"embare\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages were added or removed via explicit commands. If they were not imported the module-info of the module in question is now used instead (a check of modules was updated). Added airflow to", "the packages scripts to the module-info. The change #10806 made airflow works with implicit packages which got imported. So the \"expose\" option got removed in packages scripts, which could only be done when explicit packages did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages are added or removed via explicit commands. If they were added the module-info of the module in question is now visible (the check of whether it was updated). In order to", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit commands. \"airflow\" got built, the \"expose\" option got removed but the scripts, which can only be done using \"pkgutil\" did not . This issue is no longer discussed. Relative change (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages were added or removed via explicit commands. If they were added the module-info of the module in question is now visible (the check of whether packages were added is no longer updated). In order to"], "perturbed_original": ["Provider packages scripts to dev (#12082) The change #10806 made the scripts work with implicit packages when \"airflow\" got imported. This is a good change but it has some unforeseen consequences. The 'provider_packages' script copy the \"airflow\" providers code from their directory in order to refactor them to the empty \"airflow\" directory in provider_packages . The #10806 change turned that empty folder in 'airflow' package because it was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "providing packages scripts to development. The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however it has some bugs. The 'provider_packages' script copy all the providers code for backports in order to refactor them to the empty \"airflow\" directory in provider_packages folder. The n turned that empty folder into a package for airflow package because it was located in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however it has some unforeseen consequences. Last days some script copy all the providers and backports in order to refactor them to an empty \"airflow\" directory in provider_packages directory. Unfortunately the #10806 change turned that empty folder into a provider package because it was in the same directory as the provider_packages scripts. Moving the script to dev solves this problem.", "Move packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however it had some unforeseen consequences. The 'provider_packages' script copy all the necessary packages for airflow, in order to refactor them to the empty \"airflow\" directory in dev directory. The #10806 change turned that empty folder in 'airflow' package . This empty folder was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however there were some unforeseen consequences. The 'provider_packages' script copy all the code for backports in order to refactor them to airflow. But airflow still is in \"airflow\" (the \"airflow\" directory in provider_packages ). The #10806 change turned that empty directory into the 'airflow' package , so it was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with provider package when \"airflow\" got imported. This change is good change, however it has some unforeseen consequences. The 'provider_packages' script copy all the providers code for backports and allows to refactor them to the empty \"airflow\" directory in provider_packages folder. The script turned out to have no folder in 'airflow' package because it was in the same directory as dev scripts. Moving the scripts to dev solves this problem.", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with backports when \"airflow\" folder is empty. This is a good change, but has some unforeseen consequences. The 'provider_packages' script copy package providers code for backports in order to refactor them to the empty \"airflow\" directory in provider_packages folder. The #10806 change turned that empty folder in 'airflow' package because it was in the same directory as the provider_packages scripts. Moving the scripts to development solves this problem.", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit ly import files and a directory called \"airflow\" got imported. This is a good thing, but it has some unforeseen consequences. The 'provider_packages' package needed to import all the providers code for backports in order to refactor them to the empty \"airflow\" directory in provider_packages folder. The #10806 change turned that empty folder in 'airflow' package , that was in the same directory as the provider_packages folder. Moving the scripts to dev solves this problem.", "Provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, but the change #10806 has some unforeseen consequences. The 'provider_packages' script copy all the providers code for backports , which has to refactor . This script copy the providers code to the empty \"airflow\" directory in provider_packages folder. The #10806 had to change that empty folder in 'airflow' package because it was in the same folder as the provider_packages scripts. Moving the scripts to dev solves this problem.", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow and its underlying dependencies implicit packages when \"airflow\" got imported. This is a good change, but unfortunately it has some unforeseen consequences. The 'provider_packages' script copy all the providers code and their dependencies to the .deb file in order to refactor them to the empty \"airflow\" directory under that folder. The #10806 change turned that empty folder in a package because it was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem."], "original_ll": -3.9833812713623047, "sampled_ll": -3.8473503589630127, "all_perturbed_sampled_ll": [-3.9396331310272217, -4.019060134887695, -4.135603427886963, -3.6604673862457275, -4.079336166381836, -3.9302031993865967, -4.14152717590332, -4.164376735687256, -3.9287798404693604, -3.918825387954712], "all_perturbed_original_ll": [-3.7850475311279297, -4.1419901847839355, -4.129569053649902, -3.868602991104126, -3.861205816268921, -4.103997230529785, -3.8596067428588867, -3.9511024951934814, -3.809018135070801, -3.9423537254333496], "perturbed_sampled_ll": -3.9917812585830688, "perturbed_original_ll": -3.945249390602112, "perturbed_sampled_ll_std": 0.143833152699545, "perturbed_original_ll_std": 0.12744428017411044}, {"original": "Override project in dataprocSubmitJobOperator (#14981)", "sampled": "Override project in dataprocSubmitJobOperator (#14981)With", "perturbed_sampled": ["Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With", "Override project in dataprocSubmitJobOperator (#14981)With"], "perturbed_original": ["Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)", "Override project in dataprocSubmitJobOperator (#14981)"], "original_ll": -7.058787822723389, "sampled_ll": -7.659811496734619, "all_perturbed_sampled_ll": [-7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619, -7.659811496734619], "all_perturbed_original_ll": [-7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389, -7.058787822723389], "perturbed_sampled_ll": -7.659811496734619, "perturbed_original_ll": -7.058787822723389, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Upload provider distribution artifacts during CI (#19807)", "sampled": "Upload provider distribution artifacts during CI (#19807)We", "perturbed_sampled": ["Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We", "Upload provider distribution artifacts during CI (#19807)We"], "perturbed_original": ["Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)", "Upload provider distribution artifacts during CI (#19807)"], "original_ll": -7.708691120147705, "sampled_ll": -8.38458251953125, "all_perturbed_sampled_ll": [-8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125, -8.38458251953125], "all_perturbed_original_ll": [-7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705, -7.708691120147705], "perturbed_sampled_ll": -8.38458251953125, "perturbed_original_ll": -7.708691120147705, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "sampled": "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by name which is great for data consistency. Also on web browser it will search for the", "perturbed_sampled": ["Add support for arbitrary json in conn uri format (#15100) Currently in airflow web api or in the CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by its form which is great for data consistency. Also on every call to airflow web API it will search for the", "Add support for data in conn uri format (#15100) Currently in airflow web UI and the internal airflow library you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by conn uri format, which is great for searching, but not great for reading. Also on web browser it will search for the", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by name which is arbitrary data . While on web browser it will look for the", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and CLI you can store arbitrary (e.g.: arbitrary json ) data in conn uri format. In that case the conn uri format will also look for the data by name which is great for data consistency. Also on web browser it will search for the", "api to look for data in conn uri format (#15100) ? In airflow web UI and the CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the network name which is great for data consistency. Also on web browser it will search for the", "Add support for arbitrary json data in conn uri format (#15100) Currently in airflow web UI and CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by using airflow. That is convenient for data consistency. Also on web browser it will search for the", "Add support for conn uri data in conn uri format (#15100) Currently in airflow web UI and the dashboard you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by URI which is great for data consistency. Also on the console it will search for the", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can import arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by name which is bad for data consistency. Also on name, it will search for the", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and CLI you can store arbitrary json data in conn uri format. In that case this format will also look for the data by name which is great for data consistency. Also on web browser it will search for the", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI it's not possible to store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will search for the data by default, which is great for data discovery, only on web browser it will search for the"], "perturbed_original": ["Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` subclass of the URI format to handle primitive key-value pairs. This PR adds support for arbitrary json in the conn uri format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) Currently in both the web UI and the CLI you can store arbitrary or anonymous json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Author: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add support for json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. However, currently the URI format can only handle primitive key-value pairs. This branch adds support for arbitrary json in the URI format. Co-authored-by: Daniel Martin <unk>dan.matz@gmail.com>. Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. json) contents in the `extra` field. But the URI format can only support key-value pairs. This PR provides support to store arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. But the URI format can only handle primitive <unk>extra<unk> data. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Sharma <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) : In the airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. But this format can only support key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary amount of json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> and Ashraf Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in a key value field. But the URI format can only handle primitive key-value pairs. This patch adds support for arbitrary json in the conn uri format. Co-authored-by: D. Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow , json data and the URI format can store arbitrary (e.g. nested) json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> , Anand Berlin-Taylor <ash_github@firemirror.com>", "Add support for arbitrary json to the uri format . Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. Unfortunately the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> ; Annette Berlin-Taylor <ash_github@firemirror.com>"], "original_ll": -3.4491465091705322, "sampled_ll": -3.6833443641662598, "all_perturbed_sampled_ll": [-3.7757415771484375, -3.582228899002075, -3.7992396354675293, -3.7515511512756348, -3.94508695602417, -3.8299646377563477, -3.658489227294922, -3.765275239944458, -3.9671518802642822, -3.5876049995422363], "all_perturbed_original_ll": [-3.4517688751220703, -3.5811288356781006, -3.883608818054199, -3.3886075019836426, -3.5932979583740234, -3.3688032627105713, -3.752919912338257, -3.305203914642334, -3.7065720558166504, -3.6552209854125977], "perturbed_sampled_ll": -3.7662334203720094, "perturbed_original_ll": -3.5687132120132445, "perturbed_sampled_ll_std": 0.12457502678320725, "perturbed_original_ll_std": 0.17772274661217619}, {"original": "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "sampled": "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "perturbed_sampled": ["[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The"], "perturbed_original": ["[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)"], "original_ll": -5.797331809997559, "sampled_ll": -6.141626358032227, "all_perturbed_sampled_ll": [-6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227, -6.141626358032227], "all_perturbed_original_ll": [-5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559, -5.797331809997559], "perturbed_sampled_ll": -6.141626358032227, "perturbed_original_ll": -5.797331809997559, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6820] split breeze into functions (#7433)", "sampled": "[AIRFLOW-6820] split breeze into functions (#7433)The", "perturbed_sampled": ["[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "[AIRFLOW-6820] split breeze into functions (#7433)The"], "perturbed_original": ["[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)", "[AIRFLOW-6820] split breeze into functions (#7433)"], "original_ll": -6.755216598510742, "sampled_ll": -7.13143253326416, "all_perturbed_sampled_ll": [-7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416, -7.13143253326416], "all_perturbed_original_ll": [-6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742, -6.755216598510742], "perturbed_sampled_ll": -7.13143253326416, "perturbed_original_ll": -6.755216598510742, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "sampled": "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "perturbed_sampled": ["Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>"], "perturbed_original": ["Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>"], "original_ll": -4.839278221130371, "sampled_ll": -5.155980110168457, "all_perturbed_sampled_ll": [-5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457, -5.155980110168457], "all_perturbed_original_ll": [-4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371, -4.839278221130371], "perturbed_sampled_ll": -5.155980110168457, "perturbed_original_ll": -4.839278221130371, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "sampled": "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "perturbed_sampled": ["[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator"], "perturbed_original": ["[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator"], "original_ll": -3.294480562210083, "sampled_ll": -3.5100462436676025, "all_perturbed_sampled_ll": [-3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025, -3.5100462436676025], "all_perturbed_original_ll": [-3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083, -3.294480562210083], "perturbed_sampled_ll": -3.5100462436676025, "perturbed_original_ll": -3.294480562210083, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "sampled": "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "perturbed_sampled": ["CI: Propogate Exit Code Correctly (#9247) This code has been broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Displaying Recall Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken in my latest version of Ubuntu Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately missed with bug #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This was a regression since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This is unfortunately broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately pushed in #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This was corrected since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since March 2011: Fixed by Ash Berlin-Leischner Fixed", "//Provide Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed"], "perturbed_original": ["CI: Propogate Exit Code Correctly (#9247) This was unfortunately added as #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate Exit Method. (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately fixed from #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Firewall Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate 'propogate' Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate Exit Code Correctly (#9247) This code was not broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: ash <ash_github@firemirror.com>", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 . Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken by the release team. Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 . By Ash Berlin-Taylor <ash_github@firemirror.com>"], "original_ll": -4.909962177276611, "sampled_ll": -5.242663383483887, "all_perturbed_sampled_ll": [-4.919930458068848, -5.433773040771484, -5.187931060791016, -5.163936138153076, -4.965929985046387, -5.253736972808838, -5.295246601104736, -5.132071495056152, -5.598535060882568, -5.289368152618408], "all_perturbed_original_ll": [-4.934793472290039, -5.078086853027344, -4.855003356933594, -5.046813011169434, -4.855667591094971, -4.8715386390686035, -4.873049259185791, -5.209051132202148, -4.66195011138916, -5.306349754333496], "perturbed_sampled_ll": -5.224045896530152, "perturbed_original_ll": -4.969230318069458, "perturbed_sampled_ll_std": 0.1915548186140671, "perturbed_original_ll_std": 0.18148777652416687}, {"original": "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "sampled": "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "perturbed_sampled": ["Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>"], "perturbed_original": ["Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>"], "original_ll": -4.182562828063965, "sampled_ll": -4.182562828063965, "all_perturbed_sampled_ll": [-4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965], "all_perturbed_original_ll": [-4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965, -4.182562828063965], "perturbed_sampled_ll": -4.182562828063965, "perturbed_original_ll": -4.182562828063965, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is set. We are utilising aliases if possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work as expected so we have to run a few functions directly in other to show spinner. Extracted from #10368", "sampled": "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that often occurred after precommits were run during development. The change does not impact all build systems. For", "perturbed_sampled": ["When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to the first commit. For this reason, it is now much simpler for developers to commit from the local machine right after precommits are run. This change takes care of some bugs and issues on both locally and CI builds that often occurred after precommits were run during development. The change does not impact all build systems. For", "When precommits are run, output is silenced (#10390) The output that it produces from pre-commit builds on both CI and locally is now limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This takes care of some of the potential problems with certain CI builds that often occurred after precommits were run on CI development. The change does not impact all build systems. For", "When precommits are run, output is silenced until the commit is made. The output of pre-commit builds on both CI and locally is limited to the first commit. For this reason, it is not recommended to make the first commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that will occur right after precommits are run during development. The change does not impact all build systems. For", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and source coder is now limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that would not compile or publish after precommits were run . The change does not impact all builds. For", "When precommits are run, output is silenced (#10390) The output of pre-commit s run on both CI and locally is now limited to the first commit. For now, it is discouraged to attempt to build the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that may not have worked right after precommits were run during development. The change does not effect all build systems. For", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and development is now limited to the first commit. For this reason, it is important to build the first commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that often occurred after precommits were run during development. It does not impact all build and deployment operations. For", "After precommits are run, output is silenced (#10390) The output of precommits on both CI and locally is now limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain commit errors that often occurred after precommits were run during development. The change does not change all build systems. For", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and production is now limited to the first 3 characters. Because of this , it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain builds systems that often occur when precommits were run during development. The change does not impact all build systems. For", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is silenced between commits and after that to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are committed. The change takes care of some of the issues with certain CI builds that often occurred after precommits were committed during development. This change does not satisfy all build systems. For", "When precommits are run, output is silenced (#10390) The output of CI builds on both Amazon and locally is now limited to the current commit. For this reason, it is not necessary to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that often occurred after precommits were run during development. The change does not affect any other build systems. For"], "perturbed_original": ["When pre-commit builds are run, output is silenced (#10390) The spinner on pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is set. We are utilising aliases if possible but in our own pre-commits they are run in the background on tbl which means that the spinner might not work as expected so we have to run a few functions directly in other to show spinner. Extracted from #10368", "where no functions are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is set. We are using spinner to show build errors if possible but in case of pre-commits they are run inside shell which means that they do not work as expected so we have to run all functions directly in other to show spinner. Extracted from #10368", "When precommits are run, output is silenced (#10390) so running of pre-commit builds on both CI and locally is now silent and only shows output unless verbose variable is set. We are hoping to help with it if possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work as expected so we have to run a few functions directly in other to show spinner. Extracted from #10368", "When precommits are run, output is silenced (#10390) The output of precommits run on local machines and locally is now limited to only show errors, unless verbose variable is set. We are utilising aliases if possible but in case of pre-commits , they are run in non-interactive shell which means that aliases do not work as expected so we have to run a few functions directly in an interactive shell. Fixed the warning show spinner. Extracted from #10368", "When precommits are run, output is silenced (#10390) The output of precommit on both CI and locally is now limited to only show errors, unless verbose variable is set. We are utilising aliases if it is possible but in case of pre-commits they are run in non-interactive shell which is not always an option and our aliases do not work as expected so we had to run a few functions directly in other to show results. This is a follow-up from #10368", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both the server and locally is limited to only show errors, when the spinner environment variable is set. We are utilising aliases if possible but in case of pre-commits they are run in the backend which means that aliases do not work as expected so we have to run the functions directly in other to show spinner. Extracted from #10368", "When precommits are built, the pre-commit command is silenced (#10390) The output of pre-commit builds on both externally and locally is now limited to only show errors, unless verbose variable is set. We are utilising interactive shell as much as possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work properly, so we did run a few functions directly in other to show spinner. Extracted from #10368", "When precommits are run on CI, the output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose output is set. We are utilising aliases if possible but in some pre-commits they are run in non-interactive shell which means that aliases may not work as expected so we 've decided to run a few functions directly in other to show spinner. Extracted from #10368", "When precommits are run, output is silenced (#10390) The output of spinner on both CI and PR is now limited to only show errors, unless verbose variable is set. We are utilising aliases if possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work as expected so had to run a few functions directly in shell to show spinner. Extracted from #10368", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is set. We should continue to use aliases if possible but in case of pre-commits they are run in non-interactive shell which means the aliases do not work as expected . We would have to run these functions directly in other to fix this problem. Extracted from #10368"], "original_ll": -4.0718560218811035, "sampled_ll": -3.147503137588501, "all_perturbed_sampled_ll": [-3.166116714477539, -3.334265947341919, -3.040336847305298, -3.45761775970459, -3.3006043434143066, -3.180407762527466, -3.3029749393463135, -3.433952808380127, -3.513036012649536, -3.1686511039733887], "all_perturbed_original_ll": [-4.1716718673706055, -4.103085517883301, -4.105752468109131, -3.935760259628296, -3.8611764907836914, -4.077422618865967, -3.877546787261963, -4.11436653137207, -4.100131511688232, -3.9407594203948975], "perturbed_sampled_ll": -3.2897964239120485, "perturbed_original_ll": -4.028767347335815, "perturbed_sampled_ll_std": 0.14325425630188354, "perturbed_original_ll_std": 0.10677502863808626}, {"original": "Add note about using dag_run.conf in BashOperator (#9143)", "sampled": "Add note about using dag_run.conf in BashOperator (#9143)For", "perturbed_sampled": ["Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Add note about using dag_run.conf in BashOperator (#9143)For"], "perturbed_original": ["Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)", "Add note about using dag_run.conf in BashOperator (#9143)"], "original_ll": -5.764639377593994, "sampled_ll": -6.227072238922119, "all_perturbed_sampled_ll": [-6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119, -6.227072238922119], "all_perturbed_original_ll": [-5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994, -5.764639377593994], "perturbed_sampled_ll": -6.227072238922119, "perturbed_original_ll": -5.764639377593994, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match", "sampled": "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "perturbed_sampled": ["Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of <unk>self.assertOpen<unk> (#10031)\n\ninstead", "Test exact match of Executor name (#10465) Use this instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of code name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do a general EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an equal version number test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of EOF (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of <unk>self.assertEqual' and do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Test exact match of EOF for you (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "from match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead"], "perturbed_original": ["Test exact match for string name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of <unk>self.matchVersion<unk> to do an exact match of string name instead of partial match", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of executor name instead of partial match", "Test exact match of string name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do test, and use full match of string name instead of partial match", "Test exact match of Executor name (#10465) Use <unk>auto.env<unk> instead of `self.assertIn` to do an exact match of string name instead of partial match", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to check for an exact match of string name instead of partial match", "Test exact match of Executor string name. Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string . Test of partial match", "Test exact match of Executor name using `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match"], "original_ll": -3.916752576828003, "sampled_ll": -2.7977545261383057, "all_perturbed_sampled_ll": [-3.3844423294067383, -3.1896064281463623, -2.7977545261383057, -2.6533091068267822, -2.856856107711792, -2.954209804534912, -2.757847785949707, -3.0955331325531006, -2.8695502281188965, -2.8189265727996826], "all_perturbed_original_ll": [-3.5638070106506348, -4.606484889984131, -3.674778938293457, -3.524826765060425, -3.9355669021606445, -4.98115348815918, -3.757830858230591, -3.726618528366089, -4.231800079345703, -3.848586082458496], "perturbed_sampled_ll": -2.937803602218628, "perturbed_original_ll": -3.985145354270935, "perturbed_sampled_ll_std": 0.21126199598555714, "perturbed_original_ll_std": 0.45434508336855656}, {"original": "Added json_render method to separate filtering from view (#14024)", "sampled": "Added json_render method to separate filtering from view (#14024)I", "perturbed_sampled": ["Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I", "Added json_render method to separate filtering from view (#14024)I"], "perturbed_original": ["Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)", "Added json_render method to separate filtering from view (#14024)"], "original_ll": -5.7230143547058105, "sampled_ll": -6.441814422607422, "all_perturbed_sampled_ll": [-6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422, -6.441814422607422], "all_perturbed_original_ll": [-5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105, -5.7230143547058105], "perturbed_sampled_ll": -6.441814422607422, "perturbed_original_ll": -5.7230143547058105, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "sampled": "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "perturbed_sampled": ["[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The"], "perturbed_original": ["[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)"], "original_ll": -5.527222156524658, "sampled_ll": -5.849558353424072, "all_perturbed_sampled_ll": [-5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072, -5.849558353424072], "all_perturbed_original_ll": [-5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658, -5.527222156524658], "perturbed_sampled_ll": -5.849558353424072, "perturbed_original_ll": -5.527222156524658, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add placement_strategy option (#9444)", "sampled": "Add placement_strategy option (#9444)We", "perturbed_sampled": ["Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We", "Add placement_strategy option (#9444)We"], "perturbed_original": ["Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)", "Add placement_strategy option (#9444)"], "original_ll": -5.775016784667969, "sampled_ll": -6.8082594871521, "all_perturbed_sampled_ll": [-6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521, -6.8082594871521], "all_perturbed_original_ll": [-5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969, -5.775016784667969], "perturbed_sampled_ll": -6.8082594871521, "perturbed_original_ll": -5.775016784667969, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "sampled": "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You also get these two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and used to escape shell name\n\nThe `ciphers' field has been eliminated", "perturbed_sampled": ["Use exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. Then we get these two other changes: The `csh' and <unk>sh' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand now return exit and status codes, instead of and errors (#11495) Now we use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command will continue to be handled and used to escape shell name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You also get two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) instead of errors (#11493) a path starting with '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and used to escape shell name\n\nThe `ciphers' command has been eliminated", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` are actually `site.Quitter` now. Also, you get these two other changes: The <unk>exec' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' <unk> ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' character is now handled and used to type the command name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() instead of exit() (#10414) The `exit` and <unk>quit<unk> commands are actually `site.Quitter` now. You also get these two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of exit and errors (#11495) uses '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and used to remove arguments that don't match the command name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() as a replacement for exit() (#10414) The all `quit` functions are actually `site.Quitter` now. You also get these two other things now: `csh' and `bash' commands now return exit and status codes, instead of and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and returned in escape shell name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You may want to also check out these two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status, instead of and errors (#11495) You can now use the <unk>file' command instead of `(file' .. ' (#11634)\n\ninstead of (#11634) The <unk>_ ' is now handled and used to escape shell name\n\nThe `ciphers' field is eliminated", "Use sys.exit() instead of exit() The `exit` and `quit` functions make me look like `site.Quitter` now. You also get these two other changes: The `csh' command now returns <unk>command<unk> instead of + and <unk>sh<unk> commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of <unk>file' The `commandname(name)' command is handled and used to escape shell name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You also get these two other changes: The `csh' and commands now return exit and status codes, instead of and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' function was now handled differently - now you use <unk>(name)' to escape shell name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You also get these two other changes: The exit and `bash' commands now return exit and status codes, instead of and error codes . The exit and <unk>sudo' commands now return exit and status codes, instead of and errors (#11495) You use <unk>file' instead of `(file' .. '/etc/passwd ...')' again (#11634) The `commandname(name)' command is now handled and used to get the command name\n\nThe `ciphers' field has been eliminated", "Use sys.exit() , instead of exit() (#10414) The `exit` and <unk>sh<unk> commands are actually `site.Quitter` now. You also get some other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' is now handled and used to escape shell name\n\nThe <unk>command name' operator has been eliminated"], "perturbed_original": ["Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and <unk>quit<unk> might not be present. It is recommended to use `sys.exit()` instead, as it is built into the interpreter and is guaranteed to be present. Previously, `exit()` is not guaranteed to be present and wouls fail if the interpreter is passed the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are created at start up, from `site.py`. However, if the interpreter is started with the \u00e2 flag, or a custom `site.py` is used, `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` only existed if the interpreter was passed a <unk>-S<unk> option, and wouls fail if it is passed the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` functions in <unk>sys<unk> and are loaded, at start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is created, `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is passed into the interpreter , so it is guaranteed to be present. Previously, `exit()` was used and would not always be available if the interpreter is passed the `-S` option.", "Use quitter instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, when the interpreter is started with the `-S` flag, or a custom `site.py` is loaded, the `exit` and <unk>quit<unk> objects may or may not be present. It is recommended the use of `sys.exit()` which is built into the interpreter and is not guaranteed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and <unk>quit<unk> functions are actually internal, and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and `quit` may not be present. It is better to use `sys.exit()` which is loaded as the interpreter starts and is guaranteed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is changed by the user or a custom path is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are defined, at each time the interpreter is started up, from `site.py`. If the interpreter is started with the `-S` flag, or else `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` was not guaranteed and wouls fail if the interpreter was started with the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually built into the interpreter and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the <unk>-S<unk> flag, or a custom script is used then `exit` and `quit` will not be used. It is recommended to use `sys.exit()` which is built into the interpreter and does not need to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "Use quit() instead of exit() (#10414) The `exit` and `quit` functions are defined in `site.Quitter` objects and are loaded at boot up, from `site.py`. However, if the interpreter is started with the `-S` option or a custom `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` was not guaranteed there to be present and exit() wouls fail if the interpreter is passed the `-S` option.", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions create `site.Quitter` objects and are loaded, at interpreter launch from <unk>site.txt<unk>. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and the only possibility to be present. This is the function that was used and wouls fail if the interpreter is passed the `-S` option."], "original_ll": -2.7601258754730225, "sampled_ll": -2.7118217945098877, "all_perturbed_sampled_ll": [-3.0782344341278076, -2.830577850341797, -3.1213037967681885, -2.9869003295898438, -3.043489694595337, -3.1207752227783203, -3.123126983642578, -3.098066568374634, -3.4380176067352295, -2.988779067993164], "all_perturbed_original_ll": [-2.880661725997925, -3.130438804626465, -2.961775541305542, -3.057934522628784, -3.06990647315979, -2.98622465133667, -2.741764783859253, -2.863945484161377, -2.933223009109497, -3.2085862159729004], "perturbed_sampled_ll": -3.0829271554946898, "perturbed_original_ll": -2.9834461212158203, "perturbed_sampled_ll_std": 0.1463496565766172, "perturbed_original_ll_std": 0.13098603890603483}, {"original": "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and ES, we discovered that airflow does now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "sampled": "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be returned to you if it is not found. All of this is available through the", "perturbed_sampled": ["[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running your virtual machine against an OSX host, this will auto-discover the local configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be returned to you if it is not found. Currently this is available through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover custom ES configuration. This does not work against OSX hosts running VirtualBox. A default configuration will be returned to you if it is not found. All of this can be done through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be returned to the ES user if it is not found. All of this is done through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs When running on an OSX host, this will auto-discover the configuration. This feature only works against OSX hosts running Linux. The default configuration will be returned to you if it is not found. All of this is available through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs : If you are on an OSX host, this will auto-discover the configuration you provided. This feature will work against OSX hosts running VMware as well. Your default configuration will be returned to you if it is not found. All of this is available through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against a Windows host while running VirtualBox. A default configuration will be returned to you if the configuration is not found. All of this is available through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES-configs. While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running this feature. The default configuration will be returned to you if it is not in use. An overview of this is available through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configurations. Once you start Java running on an OSX host, this will auto-discover the setup. This feature will work against OSX hosts running VirtualBox. A default configuration will be provided to you if it is not found. All of this is available through the", "[AIRFLOW-5139] Allow custom ES configs * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be returned to the host if it is not found. All information about the ES client is available through the", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be provided to you if an ES configuration is not found. All of this is available through the"], "perturbed_original": ["[AIRFLOW-5139] Allow custom ES configs (#5760) [AIRFLOW-5139] Allow custom ES configs While attempting to create a self-signed certificate between airflow and ES, we discovered that many users will now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a new connection between airflow and ES, we discovered that airflow does now allow users to modify or change the state of the elasticsearchtaskhandler. This commit allows users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and ES, we noticed that airflow does now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and ES, we discovered that ES does now allow users to modify the SSL state in the elasticsearchtaskhandler. This commit allows users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed certificate between airflow and ES, we discovered that airflow does now require the elasticsearch taskhandler to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in airflow.cfg", "configs * AIRFLOW-5142 Allow custom ES configs * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and ES, we discovered that airflow does now allow users to modify the SSL key of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed certificate to establish a direct connection between airflow and ES, we discovered that airflow doesn't allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and elasticsearch, we discovered that airflow does now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to connect to a self-signed TLS connection using airflow and ES, we discovered that we can now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS server with airflow and ES, we discovered that airflow does now allow users to modify the ES state of the elasticsearchtaskhandler. This commit will allow users to define ES state in the airflow.cfg"], "original_ll": -4.223630428314209, "sampled_ll": -3.3788585662841797, "all_perturbed_sampled_ll": [-3.47558856010437, -3.3275656700134277, -3.376246929168701, -3.385443687438965, -3.4059245586395264, -3.4567227363586426, -3.502944231033325, -3.6569948196411133, -3.2945268154144287, -3.396892547607422], "all_perturbed_original_ll": [-4.06661319732666, -4.275659561157227, -4.2056708335876465, -4.2720866203308105, -4.239114761352539, -4.030004978179932, -4.052467346191406, -4.1481804847717285, -4.112218379974365, -4.305514335632324], "perturbed_sampled_ll": -3.427885055541992, "perturbed_original_ll": -4.170753049850464, "perturbed_sampled_ll_std": 0.0974635132396926, "perturbed_original_ll_std": 0.09690692055346203}, {"original": "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "sampled": "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "perturbed_sampled": ["[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) (http://github.io) Add a new option for controlling whether the client device is always offline (#5847) Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback for connection's project id in the connection path * [AIRFLOW-5427] Add a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device connection is paired or offline * [AIRFLOW-5423] Fix bug to detect", "[AIRFLOW-5435] Add the ability to set the current connection's project id from an incoming message (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback handler for project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback for arbitrary event id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback for device id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add ability to detect", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for connection detection, a new setting that indicates that the client device is offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add connection option for controlling whether the client device is active or not (#4899) * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device is live or offline * [AIRFLOW-5430] Change GKEConnectItemPropertyCallback to detect"], "perturbed_original": ["[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * Fixup! fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id * fixup! * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator"], "original_ll": -2.738751173019409, "sampled_ll": -3.9431238174438477, "all_perturbed_sampled_ll": [-4.810816287994385, -3.7743496894836426, -4.015256404876709, -3.7580044269561768, -3.9035935401916504, -3.9111292362213135, -3.7028512954711914, -3.9840331077575684, -3.839148759841919, -3.9664363861083984], "all_perturbed_original_ll": [-3.1092159748077393, -3.0875284671783447, -2.5247607231140137, -2.738751173019409, -2.3310420513153076, -2.928424596786499, -3.0875284671783447, -3.0875284671783447, -2.738751173019409, -2.738751173019409], "perturbed_sampled_ll": -3.9665619134902954, "perturbed_original_ll": -2.837228226661682, "perturbed_sampled_ll_std": 0.2981105205318425, "perturbed_original_ll_std": 0.2555774480350125}, {"original": "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "sampled": "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "perturbed_sampled": ["[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The"], "perturbed_original": ["[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)"], "original_ll": -6.613442897796631, "sampled_ll": -6.897268772125244, "all_perturbed_sampled_ll": [-6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244, -6.897268772125244], "all_perturbed_original_ll": [-6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631, -6.613442897796631], "perturbed_sampled_ll": -6.897268772125244, "perturbed_original_ll": -6.613442897796631, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "sampled": "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "perturbed_sampled": ["Fix failing backport packages test (#13497) In #13473 - I updated backport packages but looks like it broke backport packages: ```. This worked with #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "Add backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it was only updating the deprecated packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like I need to manually backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. <unk>, #14054. utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14011. (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx . utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (10edfec097), #13497. d2gfx.c2gfx-dev.c2gfx.d2gfx .c2gfx, utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - Fixed the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #14325. dsim.c2gfx (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated git-files because it looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #13473. d2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #14314. b2gfx.c2gfx (3bb4efe0), #14322. d2gfx.c2gfx (1ba06afe), #14321. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. <unk>, #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx .aio. utils.aio.aio", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio -util, #13578. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx , #13568. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio"], "perturbed_original": ["Fix failing import test (#13497) In #13473 - I updated the deprecated packages but looks like it broke the test. ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages test (#13497) In #13473 we updated the deprecated env.txt file and it looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages . In #13473 - I updated the deprecated packages but looks like it broke backport . in File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages test (#13497) In #13473 - fixed the deprecated packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator : ImportError: cannot import name 'chain' ```", "broke backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File : 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 86 <module> : cannot import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like i didn't backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, line 24: import from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages test (#13497) - edit - I updated my system packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Fix failing backport packages : In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import _cache cannot import name 'chain' ```", "Fix failing backport packages test (#13497) In #13473 it updated the deprecated packages but looks like that broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```"], "original_ll": -3.6533801555633545, "sampled_ll": -2.6417195796966553, "all_perturbed_sampled_ll": [-2.990854263305664, -2.6771116256713867, -3.053605794906616, -2.952200174331665, -2.806920051574707, -2.819934844970703, -2.833981990814209, -2.6981823444366455, -2.984344005584717, -2.7539074420928955], "all_perturbed_original_ll": [-3.588738441467285, -3.4763295650482178, -3.6872520446777344, -3.7430520057678223, -5.08794641494751, -3.8075039386749268, -3.820265054702759, -3.712315559387207, -3.7816925048828125, -3.6139183044433594], "perturbed_sampled_ll": -2.857104253768921, "perturbed_original_ll": -3.8319013833999636, "perturbed_sampled_ll_std": 0.1242871146942374, "perturbed_original_ll_std": 0.4309402814383702}, {"original": "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs", "sampled": "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "perturbed_sampled": ["Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) by Andrew G. Do not store configuration of OpenVPN agent in", "Add BREEZE.rst file to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of fan speeds in", "Add Airflow 2.0.1 to ``breeze-complete`` and Airflow 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store the OpenVPN agent in", "README 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of agent in", "Add Airflow 2.0.1 to ``breeze-complete`` and use Airflow 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of airflow properties in", "Add Airflow 2.0.1 to ``breeze-complete`` . (#14876) 2.0.1 bugfix: Do not store configuration of OpenVPN agent in"], "perturbed_original": ["Add Airflow 2.0.1 to the breeze-complete list and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs", "added breeze-first (#14876) 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs", "released 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the list and the docs", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was already on the breeze-complete list and the docs", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was updated to add Airflow to the breeze-complete list and the docs", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-init and the docs", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze list and the docs", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst as this file was missing from the breeze-complete list and the docs", "Add Airflow 2.0.1 to BREEZE and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs"], "original_ll": -4.0744757652282715, "sampled_ll": -3.986768960952759, "all_perturbed_sampled_ll": [-4.646064758300781, -3.8139519691467285, -4.044761657714844, -3.7347402572631836, -4.018575668334961, -3.907142162322998, -4.074199199676514, -3.6603078842163086, -4.07163667678833, -3.9914357662200928], "all_perturbed_original_ll": [-3.9503605365753174, -4.024358749389648, -4.150747776031494, -4.127830505371094, -4.107438087463379, -3.8250391483306885, -4.426240921020508, -4.300504684448242, -4.283556938171387, -3.9830374717712402], "perturbed_sampled_ll": -3.996281599998474, "perturbed_original_ll": -4.1179114818573, "perturbed_sampled_ll_std": 0.2567704542245824, "perturbed_original_ll_std": 0.17260436682667574}, {"original": "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "sampled": "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "perturbed_sampled": ["[AIRFLOW-6014] - handle rs that are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a kubernetes registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry (#6023) * [AIRFLOW-6011] - Add kubernetes-ip-config", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6454) + [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) + [AIRFLOW-6012] - kubernetes-ip-config", "better handle pods which are created and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "[AIRFLOW-6014] - handle pods that are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6014] - kubernetes-ip-config", "[AIRFLOW-6014] - Support for a new pod registry system, which are preempted and deleted on every reboot<unk> (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "[AIRFLOW-6014] - Support for pods which are preempted and deleted later<unk> (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "[AIRFLOW-6014] - handle rs are preempted and deleted by kuber\u2026 (#6220) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "[AIRFLOW-6014] - handle pods being preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "[AIRFLOW-6014] - A more flexible syntax for pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * Fixed kubernetes-ip-config"], "perturbed_original": ["[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 netes [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by . * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * Update to help handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are created and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are created and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - List pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - List pods which are preempted and deleted by kubernetes but not restarted", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted"], "original_ll": -3.676008701324463, "sampled_ll": -3.5043020248413086, "all_perturbed_sampled_ll": [-3.540085554122925, -3.531456470489502, -3.509244680404663, -3.692314863204956, -3.537428855895996, -3.390383720397949, -3.6474807262420654, -3.7483365535736084, -3.5673210620880127, -4.0820231437683105], "all_perturbed_original_ll": [-3.676008701324463, -3.7901952266693115, -3.39660382270813, -3.1662373542785645, -4.443158149719238, -3.708024740219116, -3.676008701324463, -3.676008701324463, -3.6138997077941895, -3.676008701324463], "perturbed_sampled_ll": -3.624607563018799, "perturbed_original_ll": -3.68221538066864, "perturbed_sampled_ll_std": 0.1801437498330912, "perturbed_original_ll_std": 0.30724388949910275}, {"original": "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "sampled": "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the kernel takes when", "perturbed_sampled": ["sigkill_handles_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the kernel takes when", "Quarantine test_process_sigterm_works_with_retries and check that is the actual syscall that the kernel takes when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual action the kernel takes when", "Quarantine test_process_sigterm_works_with_retries and test_process_sigterm is the actual syscall that the kernel takes when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall /step that your kernel takes when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the kernel takes when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the shell uses when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual behavior the kernel takes when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that RPC takes when", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the kernel takes when"], "perturbed_original": ["com) and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "Quarantine d test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in the current test suite. These tests are flaky and fail sometimes", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries ; (#17441) These tests are flaky and fail sometimes", "test_signkill and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (at least because the tests are flaky and fail sometimes", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are being observed to fail sometimes", "Quarantine test_process_sigterm_works_with_retries Tests in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in the github repository. These tests are flaky and fail sometimes", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) because they are not scalable and thus are flaky and fail sometimes"], "original_ll": -4.105998516082764, "sampled_ll": -3.2886242866516113, "all_perturbed_sampled_ll": [-3.7414305210113525, -4.538538932800293, -3.3900680541992188, -3.6567177772521973, -3.683157444000244, -3.2886242866516113, -3.2684600353240967, -3.509418487548828, -3.493807554244995, -3.2886242866516113], "all_perturbed_original_ll": [-5.049521446228027, -5.371641159057617, -3.424201726913452, -3.7967987060546875, -4.72763729095459, -4.061824321746826, -4.111449718475342, -5.5244526863098145, -3.5141849517822266, -4.290579795837402], "perturbed_sampled_ll": -3.5858847379684446, "perturbed_original_ll": -4.387229180335998, "perturbed_sampled_ll_std": 0.3572645052180741, "perturbed_original_ll_std": 0.7103782764656783}, {"original": "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not strictly necessary because Recently GitHub recognized this as being a problem and introduced new rules for scheduled workflows. But for people who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public URL explaining it yet): > Scheduled workflows will be disabled by default in", "sampled": "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not showing up to all the active developers. Fixes #9077\n\nFixed issue with not accepting a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "perturbed_sampled": ["Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't be run in the new repo. Fixed problem with new features not showing up to all the active developers. Fixes #9077\n\nFixed issue with adding a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that would trace to unknown problem. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have tags enabled. Fixes #11276\n\nFixed issue with \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes it possible to keep log messages separated. Adds #11229\n\nImproved", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not being available to all the active developers. Fixes #9077\n\nFixed issue with not accepting a log file in the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" which caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer fails users that don't have tags specified. Adds #11276\n\nFixed issue with \"show-logs\" which would not trigger a log merge and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the tag list on the \"List Tags\" tab. Makes it easy to keep log messages separated. Adds #11229\n\nImproved", "Limits tasks to run only in Apache Airflow repo (#11264) It has been raised a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not showing up to the currently active feature. Adds #9077\n\nFixed issue with not accepting a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"load-log-memory\" method that caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly on \"List Tags\" tab. Makes sure to keep log messages on \"List Tags\" tab. Fixes #11229\n\nImproved", "Limits CodeQL workflow to run only with Apache Airflow server support. It has been raised quite a few times that workflow added in the last patches can't run code ql (which is not possible without Apache Airflow). Fixes:\n\nFixed problem with new features not showing up to all the active developers. Fixes #9077\n\nFixed issue with not using same log message as the latest command line . Fixes #11296\n\nFixed a regression in \"set all\" which caused stack trace to be incorrect. Adds #11321\n\nAdded logs using \"list-tags\". No longer prompts users to \"add tags\" when no files are matched and don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "Limits CodeQL workflow to run only in the Airflow repo (#11264) It has been raised quite a few times that code generated in the last patches can't be executed locally in airflow.\n\nBug Fixes:\n\nFixed problem with some active code not showing up to the active developers. Fixes #9077\n\nFixed issue with not accepting a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that caused stack trace to be overwritten. Adds #11321\n\nAdded option on \"set tags\" which no longer prompts users if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not a trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches would break existing code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not popping up to all the active developers. Fixes #9077\n\nFixed issue with not accepting a log message. Also updated to the latest command line documentation. Fixes #11296\n\nFixed a regression in \"set all\" method which caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts for tags if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all text. Adds #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log message visible. Adds #11229\n\nImproved", "to the workflow s only in the Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in the spec. Fixes #9098 Fixed problem with workflows not showing up to all the active developers. Fixes #9077\n\nFixed issue with not accepting a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have anything specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add meta data tags on a project from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "Limits CodeQL workflow to run only in the Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with some new features not showing up to all the active developers. Fixes #9077\n\nFixed issue where list-items was not accepting a log message as the latest command line arguments. Fixes #11296\n\nFixed a regression in list-tags method that caused stack trace to be incorrect. Adds #11275 Fixed an issue on \"list-tags\". No longer prompts users if they don't have tags . Fixes #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep tags separated. Adds #11229\n\nImproved", "Limits CodeQL workflow to run only in the Apache Airflow environment. It has been mentioned a few times that workflow added in its last patches can't run code in Airflow. Fixes #11426 Avoided problem with new features not showing for all the active developers. Fixes #9077\n\nFixed issue with not accepting a log message as the command line option. Fixes #11296\n\nFixed a regression in \"set -tag\" that caused stack trace to hang. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts for log messages if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not showing up to all the active developers. Fixed an issue with \"init-bfs\" not accepting a git repository-generated file as the latest command line option. Fixed a regression in \"set all\" that caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have tags specified. Fixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Adds #11234 Added ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved"], "perturbed_original": ["Limits scheduled workflows to run only on Apache Airflow (#11264) It has been raised quite a few times that workflow added in forked repos can be pretty invasive on forks - especially when it comes to scheduled workflows as they might eat quota or disable scheduled jobs for organisations/people who fork repositories. This is not strictly necessary because Recently GitHub recognized this as being a problem and introduced new rules for forked repositories. But for people who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Apache (no public URL explaining it yet): > Scheduled workflows will be disabled by default in", "Limits CodeQL workflow access only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least remove access for those organisations/people who add it to their repositories. This is not strictly forbidden. Recently GitHub recognized this as being a problem and introduced some limits for scheduled workflows. But for those organisations who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the changelog by Github (no public URL explaining it ). Scheduled workflows will be disabled by default in", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs of the organisations/people who fork the repository. Of course this is not strictly necessary because Recently Apache recognized this as being a problem and introduced new rules for scheduled workflows. But for people who are forking the repository it would be nice to not have all actions. It is enough that the CodeQL check is disabled if the PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public URL explaining it yet): > <unk> This check will be disabled by default in", "schedule any workflow to run only in the Apache /Airflow repository (#11264) It has been raised quite a few times that workflow added in forked repositories can be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not strictly necessary but GitHub recognized this is a problem and introduced a job for scheduled workflows. For people who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is run when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public report is out on it yet): > Scheduled workflows will be disabled by default in", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that code added in forked repositories might be very disruptive and potentially invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not strictly necessary because Recently GitHub recognized this as being a problem and introduced new rules for scheduled workflows. But for those who are already forked, it would be nice to limit those actions. IMO it is enough that the CodeQL check is done when PR is in the \"apache/airflow\" repository. Quote from the emails received by Github (no documentation explaining it yet): > Scheduled workflows will be disabled by default in", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) : It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks themselves, when it comes to scheduled workflows . These actions might eat quota or at least jobs for those organisations/people who fork repositories. This is strictly true, it is just a small issue ! Recently GitHub has pointed it out as being invasive and introduced new rules for scheduled workflows. But for people creating repos that are already forked, it would be nice to not run those actions. It is enough that the \"code-ql\" is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public URL explaining it yet): > Scheduled workflows will be disabled by default in", "Limits CodeQL workflow to run only in forked Apache Airflow repo (#11264) It has been suggested quite a few times that workflow added in forked repositories might be pretty invasive for the forks - Especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not strictly necessary because Recently GitHub recognized this as being a problem and introduced new rules for scheduled workflows. But for people who are already forked, it would be nice to not take some additional actions. If it are enough that the check is done , then it is possible to add this to the \"apache/airflow\" repository. Quote from the emails received from the forked repo (no public URL explaining it yet): > Scheduled workflows will be disabled by default in", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the users, especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who want to run this type functions. This is not strictly necessary because Recently GitHub recognized this as being a big problem and introduced quota policy for scheduled workflows. But for people who are really worried it would be better to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repo. Here is the official response from the emails received by Github (no link explaining it yet): > Scheduled workflows will be disabled by default in", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This solution might not be strictly necessary because Recently GitHub identified this as being a problem and fixed a few rules for scheduled workflows. But for people who are already forked, it is nice to not have to run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the received bug report (no public URL explaining it yet): > Scheduled workflows will be ignored by default in", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow s in forked repositories might be pretty invasive for those forks - especially when it comes to scheduled workflows as they might need logging, or at least jobs for those organisations/people who are forked. This might not be strictly necessary because Recently GitHub recognized this as being a problem and set some new rules for scheduled workflows. But for people who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no URL explaining it yet): > Scheduled workflows will be disabled by default in"], "original_ll": -4.258916854858398, "sampled_ll": -3.131984233856201, "all_perturbed_sampled_ll": [-3.251715898513794, -3.2018239498138428, -3.0497961044311523, -3.251537322998047, -3.172008752822876, -3.220668315887451, -3.298844575881958, -3.133476734161377, -3.246946334838867, -3.5039749145507812], "all_perturbed_original_ll": [-4.267278671264648, -4.168288230895996, -4.251739501953125, -4.206611633300781, -4.206758499145508, -4.233626842498779, -4.0762739181518555, -4.1480393409729, -4.211566925048828, -4.109262943267822], "perturbed_sampled_ll": -3.2330792903900147, "perturbed_original_ll": -4.187944650650024, "perturbed_sampled_ll_std": 0.1126781982643661, "perturbed_original_ll_std": 0.05860966956677849}, {"original": "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "sampled": "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "perturbed_sampled": ["Fix failing spelling check on command_debug() in this branch. For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request, but due to the change, we have to revert Master", "Fix ing a broken check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the need to revert that, I have to revert Master", "Fix failing spelling check on Master (#15998) for the reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "Fix failing merge test, on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "Fix failing spelling check on Master (#15998) For some reason , Master has not been green-lit for this Pull Request. Due to the change, we have to revert Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we 'll revert Master", "Fixed spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "Fix error after security check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master"], "perturbed_original": ["Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green , and checking master docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Master (#15998) For some reason, https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build build was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was skipped. Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which is fixed on Master", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Build docs. For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix failing spelling check on Master (#15998) For some reason the warning color was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Fix for an incomplete run completion check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master"], "original_ll": -4.303107738494873, "sampled_ll": -3.8130943775177, "all_perturbed_sampled_ll": [-3.7985517978668213, -3.8159377574920654, -3.907144784927368, -3.867905378341675, -4.023247241973877, -3.750969171524048, -4.363554954528809, -4.051498889923096, -3.747857093811035, -3.699479341506958], "all_perturbed_original_ll": [-4.174617290496826, -4.234528541564941, -4.201179027557373, -4.015984058380127, -4.166890621185303, -4.153842926025391, -4.303107738494873, -4.100584983825684, -5.326119899749756, -4.234435081481934], "perturbed_sampled_ll": -3.902614641189575, "perturbed_original_ll": -4.291129016876221, "perturbed_sampled_ll_std": 0.18893441346925227, "perturbed_original_ll_std": 0.35294869923307776}, {"original": "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "sampled": "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example", "perturbed_sampled": ["Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr If you want to use the MySQL drivers you need to install: $ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser, like this example", "Thanks to Michael (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install install mysql -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example", "Debian (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a Windows application. For example", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser using your favourite database clients. For example", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can switch to the MySQL database from a web browser. For example", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to : sudo install libmysqlclient-dev -y\n\nOr if you want to use the MySQL server you need to : sudo install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example", "Update install_mysql.sh (#12101) After Debian and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you will only use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL server from a web browser. For example", "Update install_mysql.sh (#12101) After that the following command should be executed and according to the version you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you only use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the new drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the server from a web browser. For example", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install: $ apt-get install libmysqlclient-dev -y\n\nOr if you want to use Debian MPG123 MySQL drivers you need to install:\n\n$ apt-get install libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example"], "perturbed_original": ["Update install_mysql.sh (#12101) After Debian 9 and according to the \"git keylist\" after Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the apt repository itself with a descriptive name and \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "Update install_mysql.sh for the new Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 installed using \"apt-key add\" a keyring should be placed directly in the terminal with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "Update install_mysql.sh (#12101) after Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of using \"apt-key add\" the script should be placed directly into /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, in Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the /etc/apt/key directory with a descriptive name and either \"gpg\" or \"frv\" as gpg file extension. Also added better redirection on the apt-key list command.", "Update install_mysql.sh (#12101) After a few updates and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \".gpg.key\" as file extension. That will ensure better redirection on the apt-key list command.", "Update install_mysql.sh (#12101) under Debian 9 and according to the notes after Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also there is no redirection on the apt-key list command.", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 and using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \".key\" or \"asc\" as file extension. Also added an esoteric field on the apt-key list command.", "Update install_mysql.sh (#12101) to reflect new requirements for Debian 9 and . According to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "Update install_mysql.sh (#12101) After Debian 12, according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of using \"apt-key add\" a keyring should be installed in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. This gives better redirection on the apt-key list command.", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 and using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file path. I updated the apt-key and added better redirection on the apt-key list command."], "original_ll": -3.173780918121338, "sampled_ll": -2.1254050731658936, "all_perturbed_sampled_ll": [-2.2867236137390137, -2.310694694519043, -2.193288803100586, -2.185487985610962, -2.1971426010131836, -2.5306992530822754, -2.1927876472473145, -2.288480758666992, -2.1807467937469482, -2.531611442565918], "all_perturbed_original_ll": [-3.8854329586029053, -3.542541742324829, -3.1584177017211914, -3.4861018657684326, -3.252779483795166, -3.2903881072998047, -3.381321668624878, -3.183134078979492, -3.160327196121216, -3.2279913425445557], "perturbed_sampled_ll": -2.2897663593292235, "perturbed_original_ll": -3.356843614578247, "perturbed_sampled_ll_std": 0.12924310613110754, "perturbed_original_ll_std": 0.21707429466462555}, {"original": "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "sampled": "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "perturbed_sampled": ["Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint"], "perturbed_original": ["Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)"], "original_ll": -4.547948837280273, "sampled_ll": -5.002066612243652, "all_perturbed_sampled_ll": [-5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652, -5.002066612243652], "all_perturbed_original_ll": [-4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273, -4.547948837280273], "perturbed_sampled_ll": -5.002066612243652, "perturbed_original_ll": -4.547948837280273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "sampled": "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "perturbed_sampled": ["[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)"], "perturbed_original": ["[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)"], "original_ll": -4.913494110107422, "sampled_ll": -4.913494110107422, "all_perturbed_sampled_ll": [-4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422], "all_perturbed_original_ll": [-4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422, -4.913494110107422], "perturbed_sampled_ll": -4.913494110107422, "perturbed_original_ll": -4.913494110107422, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "sampled": "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "perturbed_sampled": ["Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing"], "perturbed_original": ["Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)"], "original_ll": -4.973775863647461, "sampled_ll": -5.327398300170898, "all_perturbed_sampled_ll": [-5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898, -5.327398300170898], "all_perturbed_original_ll": [-4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461, -4.973775863647461], "perturbed_sampled_ll": -5.327398300170898, "perturbed_original_ll": -4.973775863647461, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of dependencies broke them: 1) SQS moto 2.2.6 broke SQS tests - the queue url in the 2.2.6+ version has to start with http:// or https:// 2) DataCatalog part of Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "sampled": "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of the database engines had changed the way the tests were created and they didn't have enough time to find the missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "perturbed_sampled": ["Fix providers in SQS and DB tests in main branch with eager upgrades (#18040) The SQS and DB tests were failing tests in main branch because some recent release of the database engines has changed the way the tests were created and they didn't have enough time to find the missing dependencies in each test suite, causing the tests to fail immediately upon attempting to run them. This should be fixed now.\n\nBug Fixes:\n\nFix some regression", "failing tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch with recent release plans. The database engines had changed the way the tests were created and did not have enough time to find the missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "Fix ing Sqs failing testing in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch with eager upgrades owing to some recent release of the database engines had changed the way new dependencies were created and they didn't have enough time to find the missing dependencies in the test suite, causing the tests to crash when attempting to write them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in this branch because some recent release of the engines had changed the way the tests were created and they didn't have enough time to find the missing dependencies in each test , causing the tests to crash when attempting to run the tests. The problem is fixed though there is still some regression", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were fixed in main branch because some recent release of the provider tests had changed the way the tests were created and they didn't have enough time to grab the missing dependencies in each test suite, causing the tests to crash when attempting to run. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "Fix providers tests in main branch with eager upgrades (#18040) - Provider tests for DataEdge and DataCatalog were failing in main branch because an early release of the database engines had changed the way the provider test suites were created and they didn't have enough time to find the missing providers for each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "Fix providers tests in main branch with eager upgrades . SQS and DataCatalog were failing tests in main branch because some recent release of the database engines had changed the way the tests were created and they didn't have enough time to find the missing dependencies after submitting the test suite, causing tests to crash when trying to run them. This is fixed . Fix some regression", "Fix providers tests in main branch with eager upgrades . Bug Fixes: The SQS and DataCatalog providers tests were removed in main branch because some recent release of the database engines had changed the way the tests were created and they didn't have enough time to update the missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch. This was because some recent release of the database engines had changed the way the tests were created and they didn't have the ability to find any dependencies in the test suite, causing the tests to fail when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "Fix providers tests in main branch with eager upgrades : SQS QL and SQLite DB were failing tests in some providers because some recent release of the database engines had changed the way the tests were created and they did not have enough time to add any missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression"], "perturbed_original": ["Fix providers tests in GitHub main branch with eager -fix The SQS and DataCatalog were failing tests in main branch because some configuration of dependencies broke them: 1) SQS moto 2.2.6 broke SQS tests - every url in the 2.2.6+ code had to start with http:// or https:// 2) DataCatalog part of Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "). Fixed erroneous tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some set of dependencies broke them: 1) SQS moto 2.2.6 broke SQS tests (because the queue url in the 2.2.6+ version has to start with http:// or the URL will not load correctly on startup) 2) DataCatalog with Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "Fix providers tests in case of SQS 2.0 with moto 2.2.6+ (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of Google Provider broke them: 1) SQS moto 2.2.6 broke tests - the queue url in the 2.2.6+ version has to start with http:// or https:// 2) DataCatalog part of Google Provider changes path types and broke tests (used beta instad of datacatalog path)", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and Google provider tests are failing tests in main branch because some recent release of dependencies broke them: 1) SQS moto 2.2.6 broke SQS tests - the queue url in the 2.2.6+ version has been broken (changing it to https:// instead of with http:// or https:// 2) 3.2 release of Google Provider incorrectly imported types and broke tests. (fix beta instad of datacatalog path)", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing in main branch because some recent release of ffSense affected them: 1) New 2.2.6 broke SQS tests - the queue url in the 2.2.6+ version has to start with http:// or https:// 2) DataCatalog tests have fail - Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were stopped in main branch because some recent release of dependencies broke tests in them: 1) SQS moto 2.2.6 broke SQS tests - the queue url in the 2.2.6+ version has to be http:// or https:// 2) DataCatalog part of 4.2.0 incorrectly imported types and broke tests (such as wrong instad of datacatalog path)", "Fix providers tests in main branch for eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because the release of dependencies broke them: 1) SQS provider broke SQS tests - the queue url in the 2.2.6+ version does not always start with http:// or https:// 2) DataCatalog part of Google Provider incorrectly imported types and path (used beta instad of datacatalog path)", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch . The most recent release of dependencies broke them: 1) SQS : Recent release of dependencies broke SQS tests as the queue url in the 2.2.6+ version has to start with http:// or https:// 2) DataCatalog part : Version Provider incorrectly imported types and attributes only (used beta instad of datacatalog path)", "Fix providers tests in main branch for new upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of our libraries broke them: 1) SQS moto 2.2.6 broke SQS tests - indexed url in previous version has to start with http:// or https:// 2) DataCatalog part of Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "Fix providers tests in main branch and after upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of dependencies broke them: 1) SQS moto _qt broke SQS tests - the queue url in the 2.2.6+ version has to start with http:// audit//?qt= 2) Release of v2 of Google datacatalog imported types and broke tests (used beta instad of datacatalog path)"], "original_ll": -4.631836891174316, "sampled_ll": -3.396033525466919, "all_perturbed_sampled_ll": [-3.25165057182312, -3.526172161102295, -3.707371711730957, -3.8313233852386475, -3.630169153213501, -3.486577272415161, -3.9225330352783203, -3.5185554027557373, -3.3086817264556885, -3.667290687561035], "all_perturbed_original_ll": [-4.775327682495117, -4.595754146575928, -4.141733646392822, -4.454361438751221, -4.876638412475586, -4.451718807220459, -4.691701412200928, -4.726941108703613, -4.75923490524292, -4.993081092834473], "perturbed_sampled_ll": -3.585032510757446, "perturbed_original_ll": -4.646649265289307, "perturbed_sampled_ll_std": 0.20085488715520822, "perturbed_original_ll_std": 0.23304993959030915}, {"original": "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "sampled": "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "perturbed_sampled": ["[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The"], "perturbed_original": ["[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)"], "original_ll": -6.411542892456055, "sampled_ll": -6.769205093383789, "all_perturbed_sampled_ll": [-6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789, -6.769205093383789], "all_perturbed_original_ll": [-6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055, -6.411542892456055], "perturbed_sampled_ll": -6.769205093383789, "perturbed_original_ll": -6.411542892456055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "sampled": "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "perturbed_sampled": ["Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The"], "perturbed_original": ["Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112"], "original_ll": -3.9904048442840576, "sampled_ll": -4.237772464752197, "all_perturbed_sampled_ll": [-4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197, -4.237772464752197], "all_perturbed_original_ll": [-3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576, -3.9904048442840576], "perturbed_sampled_ll": -4.237772464752197, "perturbed_original_ll": -3.9904048442840576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow ./run_tmux.sh script to run standalone (#13420)", "sampled": "Allow ./run_tmux.sh script to run standalone (#13420)I", "perturbed_sampled": ["Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I", "Allow ./run_tmux.sh script to run standalone (#13420)I"], "perturbed_original": ["Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)", "Allow ./run_tmux.sh script to run standalone (#13420)"], "original_ll": -4.795127868652344, "sampled_ll": -5.527097702026367, "all_perturbed_sampled_ll": [-5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367, -5.527097702026367], "all_perturbed_original_ll": [-4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344, -4.795127868652344], "perturbed_sampled_ll": -5.527097702026367, "perturbed_original_ll": -4.795127868652344, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "sampled": "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "perturbed_sampled": ["fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of"], "perturbed_original": ["fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id"], "original_ll": -3.8451452255249023, "sampled_ll": -3.963820457458496, "all_perturbed_sampled_ll": [-3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496, -3.963820457458496], "all_perturbed_original_ll": [-3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023, -3.8451452255249023], "perturbed_sampled_ll": -3.963820457458496, "perturbed_original_ll": -3.8451452255249023, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by `mypy --install-types` command. Part of #19891", "sampled": "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only warned about missing mypy types that it knew about.)", "perturbed_sampled": ["Adds missing mypy types (#20324) : This adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only complained about the missing mypy types that it knew about.)", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only warned about a few additional types that we did have but it did not complain about.)", "Adds missing mypy types . This PR adds a few missing type names that we have but so far MyPy did not complain about. (It only warned about missing mypy types that it knew about.)", "Adds missing mypy types (#20324) : This adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only complained about missing mypy types that it knew about.)", "Adds missing mypy packages to MyPy. This PR adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only warned about missing mypy types that it knew about.)", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but which MyPy did not complain about. (It only added missing mypy types that it knew about.)", "Adds missing mypy types (#20324) This PR adds a few missing mypy types to some stub packages that we have but so far MyPy did not complain about. (It only acted on the missing mypy types that it knew about.)", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far did not complain about. (It only warned about missing mypy types that a new PR complained about.)", "Adds missing mypy types (#20324) This PR adds a few missing type classes that we have but that MyPy did not complain about. (It only warned about missing mypy types that it knew about.)", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages. These are type stub packages that we have but so far MyPy did not complain about. (It only warned about mypy types that it knew about.)"], "perturbed_original": ["Adds missing mypy types (#20324) This PR adds some of the missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by `mypy -devel> 8 months ago. Part of #19891", "Adds missing package types. (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by spm command. Part of #19891", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far the dev is unable to or does not complain about in those. Added by `mypy --install-types` command. Part of #19891", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about lack of type packages installed by `mypy --install-types` command. Changed in #19891", "Adds several missing Python types (#20324) This PR adds a few missing type stub packages that we have but so far we can not complain about lack of those. Added by `mypy --install-types` command. Part of #19891", "Adds missing mypy types (#20324) This PR adds a few missing type stub s which we have but so far MyPy did not detect or noter on a lack of those. Added by `mypy --install-types` command. Part of #19891", "Adds missing mypy types (#20324) : This branch adds a few missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by `mypy _code> . Part of #19891", "Adds some missing types packages. This PR adds a few missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by `mypy --install-types` command. Part of #19891", "Adds missing mypy types (#20324) This PR adds a few missing mypy types for specific packages that we have but so far we can not complain about lack of those. Added by `mypy --install-types` command. Part of #19891", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that are not quite ready but so far mypy does not complain about lack of those. Added by `mypy --install-types` command. Part of #19891"], "original_ll": -4.964541435241699, "sampled_ll": -4.3235249519348145, "all_perturbed_sampled_ll": [-4.219069957733154, -4.169429779052734, -4.02807092666626, -4.264926433563232, -4.220299243927002, -4.33027982711792, -4.137348175048828, -4.531868934631348, -4.06689453125, -4.223500728607178], "all_perturbed_original_ll": [-5.18613338470459, -5.561209201812744, -4.966000556945801, -4.853643417358398, -4.809572219848633, -5.2081732749938965, -5.425858974456787, -4.666792392730713, -4.570403099060059, -4.596349239349365], "perturbed_sampled_ll": -4.219168853759766, "perturbed_original_ll": -4.984413576126099, "perturbed_sampled_ll_std": 0.13457907855305073, "perturbed_original_ll_std": 0.3300329849929016}, {"original": "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "sampled": "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "perturbed_sampled": ["[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From"], "perturbed_original": ["[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)"], "original_ll": -4.9959516525268555, "sampled_ll": -5.436398029327393, "all_perturbed_sampled_ll": [-5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393, -5.436398029327393], "all_perturbed_original_ll": [-4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555, -4.9959516525268555], "perturbed_sampled_ll": -5.436398029327393, "perturbed_original_ll": -4.9959516525268555, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25th", "sampled": "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "perturbed_sampled": ["Doc: Update Helm Chart 1.1.0 to a new Helm Chart 2.2.0. (#17244) We released it on 26th July 2021 instead of 25thThis", "Doc: Update s: Minecraft 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "Doc: Excel Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) to include it on 26th July 2021 instead of 25thThis", "Doc: Update Helm et Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "Doc: Update Helm a Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "Doc: Update Helm Chart 1.1.0 Release Date :. They released it on 26th July 2021 instead of 25thThis", "Doc: Update Helm Chart Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 and we announced on 25thThis", "Doc: Update Helm Chart 1.1.0 version (#17244) We released it on 26th July 2021 instead of 25thThis"], "perturbed_original": [". Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25th", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 , not 25th", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th November 2018 instead of 25th", "Doc: Update Helm Chart 1.1.0 : (#17244) We released it on 26th July 2021 instead of 25th", "Doc: Update Helm Chart 1.1.0 Release Date. We released it on 26th July 2021 instead of 25th", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) to release it on 26th July 2021 instead of 25th", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We need to release on 26th July 2021 instead of 25th", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25th", "Doc: Update Helm a Release Date (#17244) We released it on 26th July 2021 instead of 25th", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2017 instead of 25th"], "original_ll": -4.932730674743652, "sampled_ll": -5.283536434173584, "all_perturbed_sampled_ll": [-4.499969959259033, -5.0265703201293945, -5.155865669250488, -5.196848392486572, -5.986898899078369, -5.783012866973877, -5.199394702911377, -5.943305015563965, -5.302724838256836, -5.378647327423096], "all_perturbed_original_ll": [-5.322382926940918, -5.226871013641357, -4.712696552276611, -4.916090965270996, -4.600992202758789, -4.738874435424805, -4.808583736419678, -4.932730674743652, -5.369091987609863, -4.6003642082214355], "perturbed_sampled_ll": -5.347323799133301, "perturbed_original_ll": -4.92286787033081, "perturbed_sampled_ll_std": 0.4311749666475343, "perturbed_original_ll_std": 0.27385915057366184}, {"original": "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test with `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add new UI node files to rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "sampled": "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created (#14920) * make sure initial route is set to something we want before running the test suite (#15036)* Fixes a regression that was previously causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "perturbed_sampled": ["Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router : fix linting issues when router is set to default * make sure initial route is set to what we want before running the test suite (#15036)* Fixes a regression that was previously causing linting issues with the initial router * Please see the new test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views (#14927) * add initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created * make sure initial route is set to something we want before running the tests (#15036)* fix the regression that was causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views * Adds initial router, routes, and placeholder views * fix router tests from linting if the config file for router is created (#14920) * make sure initial route is set to something we want before running the test suite (#15036)* Fixes a regression that was previously causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues on test-suite. Test is created inside the router to make sure it is set to something we want before running the test . Fixes a regression that was causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "Initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created (#14920) * make sure initial route is set to something we want before running the test suite test-suite (#15036)* Fixes a configuration file that was previously causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views. * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created (#14920) * make sure initial route is set to something we want before running the test suite (#15036)* fix a regression that was previously introduced * fix issues with the test suite - see the test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests for linting issues when router s are set to default (#14920) * make sure initial route is set to the route you want before running the test suite (#15036)* Fixes a regression that was previously known to cause issues with the test suite. Please see the new test documentation for more information. * fix", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views (#14931) * fix router tests - fix linting if initial router is created (#14920) * make sure initial route is set to something we want before running the test suite (#15036)* Fixes a regression that is causing linting issues with the test suite. See the new test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router creation issue (#14928) * fix linting issues when router is created (#14920) * make sure initial route is set so that we never get any errors when running the test suite (#15036)* Fixes a regression that was previously causing linting issues with the test suite. See the new test suite test-suite for more information. * fix", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created (#14920) Make sure initial route is set to the settings you want before running the test suite * Fixes a security flaw that was previously causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix"], "perturbed_original": ["Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views (#14930) * Add initial router tests - fix linting with `skipLibCheck` - Add a warning on first run * Remove `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add UI node - fix rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Add initial router, routes, and placeholder views (#14927) * fix router tests - fix linting with loop * fix motion warning on test with `resolutions` in package.json * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add new UI node files to rebuild check * Update dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * new router tests - fix motion warning on test with `skipLibCheck` - fix motion warning on test with moveA* * update route definitions in `package.json` * remove resolutions in package.json * Upgrade to Chakra release * Use username instead of id in route * Add new UI node files to rebuild check * Add linting tests: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Add initial router, routes, and placeholder views * fix motion warning on router tests - fix motion warning on test with `skipLibCheck` - fix motion warning on test with `resolutions` * Add resolutions in package.json * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add new UI node files to rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test _view * remove resolutions in `package.json` * remove resolutions in package.json * Upgrade to latest build * Add ip instead of id in route * Add new tests * add files to rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) Add initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test with `resolutions` in `package.json` * Fix motion warning on test with resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add UI node files to controllers * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test with `resolutions` in `package.json` * fix resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add new config files * Preload check for additional linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with version * fix motion warning on test with `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of user in route * Use the UI node files in route check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix motion warning on test with `skipLibCheck` - fix motion warning on test with `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Use UI node files to rebuild check * Add linting by Brent Bovenzi <brent.bovenzi@gmail.com>", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test with `resolutions` in package.json - remove resolutions in package.json * Upgrade to latest Chakra release * Use type of id in routes tests * Add new UI node files to rebuild components * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>"], "original_ll": -3.9776809215545654, "sampled_ll": -3.11045503616333, "all_perturbed_sampled_ll": [-3.3242530822753906, -3.1930854320526123, -3.3563311100006104, -3.3196730613708496, -3.218979835510254, -3.42160964012146, -3.31988787651062, -3.0728366374969482, -3.008023977279663, -3.226790428161621], "all_perturbed_original_ll": [-3.7369701862335205, -3.8510427474975586, -4.171917915344238, -3.9030356407165527, -3.8776133060455322, -3.7551732063293457, -3.834803581237793, -3.9068572521209717, -3.9724276065826416, -4.014223575592041], "perturbed_sampled_ll": -3.246147108078003, "perturbed_original_ll": -3.9024065017700194, "perturbed_sampled_ll_std": 0.12269447918540145, "perturbed_original_ll_std": 0.1212157294309891}, {"original": "provide_session keep return type (#9787)", "sampled": "provide_session keep return type (#9787)What", "perturbed_sampled": ["provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What", "provide_session keep return type (#9787)What"], "perturbed_original": ["provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)", "provide_session keep return type (#9787)"], "original_ll": -6.97110652923584, "sampled_ll": -7.808758735656738, "all_perturbed_sampled_ll": [-7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738, -7.808758735656738], "all_perturbed_original_ll": [-6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584, -6.97110652923584], "perturbed_sampled_ll": -7.808758735656738, "perturbed_original_ll": -6.97110652923584, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows sidecars with webservers in them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress.", "sampled": "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "perturbed_sampled": ["Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug fix: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) add support to check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug : Print DNS service status code (#16558) Print DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, change to check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding the networkpolicy on the webserver and flower. Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() gcr, gcr8: check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code dnsmasq: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check system files when generating DNS", "Chart: refactor webserver to use flower networkpolicy (#16619) This adds support for overriding ports on the webserver using flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "for config ports on the webserver and flower networkpolicy (#16619) This adds support for config ports on the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding default configuration and the ability to bind a single proxy to the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy .. adds support for overriding ports on the webserver and flower networkpolicy .. Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports for webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve return value for querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS"], "perturbed_original": ["Chart: Support for overriding ports on the webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows the webservers in them to function when networkpolicy is off. I also renamed the existing parameter used to define `from` in the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy . Note: This adds support for overriding ports and aliases in webserver and flower networkpolicies. This allows sidecars with webservers in them to function when rewrite is not enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicies. This adds support for reworking the behavior on the webserver and flower networkpolicies. This allows sidecars with webservers in them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in gress, to <unk>in<unk> ingress.", "Support for overriding ports on the webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows projects with webservers in them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows sidecars with webservers in gresses and flower webservers to function when networkpolicy is enabled. This also renamed the existing server port and added support to address the webserver ports in the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows sidecars with webservers in them to function when port firewall is enabled. This also renamed the existing HTTP ingress ingress to HTTP egress in the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for ingress on the webserver and flower networkpolicies. This allows sidecars with webservers in them to be detected when the networkpolicy is enabled. This also renamed the existing parameter used to define the gateway that is needed for the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy support. This adds support for overriding ports on the webserver and flower networkpolicies. This allows sidecars with override ports specified on them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` for networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicy. This allows sidecars with webservers in them to function when they do not need the webserver enabled. This also fixes the existing parameter used to define `from` in the networkpolicies ingress.", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for <unk>from<unk> for <unk>to<unk> on the webserver and flower networkpolicies. This allows sidecars to use <unk>from<unk> in them for when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress."], "original_ll": -4.04581880569458, "sampled_ll": -2.488743305206299, "all_perturbed_sampled_ll": [-2.762394905090332, -3.178292751312256, -2.919929027557373, -2.68367862701416, -2.5404767990112305, -2.458815574645996, -2.515289783477783, -2.585078477859497, -3.0698795318603516, -2.9973013401031494], "all_perturbed_original_ll": [-3.7106211185455322, -4.301949501037598, -4.312000751495361, -3.7403368949890137, -3.9969277381896973, -3.991889238357544, -3.7280185222625732, -4.193545818328857, -3.97809100151062, -3.7854578495025635], "perturbed_sampled_ll": -2.7711136817932127, "perturbed_original_ll": -3.973883843421936, "perturbed_sampled_ll_std": 0.24225235624816333, "perturbed_original_ll_std": 0.2215581117289448}, {"original": "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "sampled": "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "perturbed_sampled": ["[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What"], "perturbed_original": ["[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)"], "original_ll": -5.757751941680908, "sampled_ll": -6.2818827629089355, "all_perturbed_sampled_ll": [-6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355, -6.2818827629089355], "all_perturbed_original_ll": [-5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908, -5.757751941680908], "perturbed_sampled_ll": -6.2818827629089355, "perturbed_original_ll": -5.757751941680908, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "sampled": "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "perturbed_sampled": ["Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb"], "perturbed_original": ["Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>"], "original_ll": -4.6524786949157715, "sampled_ll": -4.803845405578613, "all_perturbed_sampled_ll": [-4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613, -4.803845405578613], "all_perturbed_original_ll": [-4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715, -4.6524786949157715], "perturbed_sampled_ll": -4.803845405578613, "perturbed_original_ll": -4.6524786949157715, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "sampled": "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "perturbed_sampled": ["Add support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish immediately, instead of waiting for the pod to destroy it.", "Add on_kill support for pods (#10666) This PR ensures that when a user runs an operation on a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "Add on_kill support for the user when killing Kubernetes pods. This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "Add on_kill support to KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "Add on_kill support for Kubernetes pods. (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will stop right away instead of waiting for the pod to destroy it.", "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation should be executed right away instead of waiting for the system to destroy it.", "Add on_kill support for the kill event. This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to finish it.", "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the user to destroy it.", "Add on_kill support for the KubernetesPodOperator (#10666) This ensures that when a user or application creates a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it."], "perturbed_original": ["Add on_kill support for the KubernetesPodOperator (#10666) This makes it so that when the user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "Add on_kill support for the KubernetesPodOperator (#10666) This PR makes it so that when a pod is terminated via a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a pod running a specific service in the airflow UI, that the associated pod is also killed using the on_kill method.", "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a task in the airflow UI, that the associated pod operator is killed using the on_kill method.", "Added Kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a KubernetesPodOperator task from the airflow UI, that the associated pod is also killed using the on_kill method.", "Add on_kill support for the KubernetesPodOperator (#10666) This PR will ensure that when a user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "for killing tasks: On_kill method support for KubernetesPodOperator (#10666) This PR ensures that when a user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "Adds on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a KVM pod in the airflow UI, that the associated pod is also killed using the on_kill method.", "Add support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a task in the airflow UI, that the associated pod is also killed using the on_kill method.", "Add on_kill support for the KubernetesPodOperator (#10666) This ensures that when a user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method."], "original_ll": -3.674380302429199, "sampled_ll": -3.393378496170044, "all_perturbed_sampled_ll": [-3.250117301940918, -3.351548433303833, -3.4532053470611572, -2.943589925765991, -3.444817304611206, -3.0502164363861084, -3.4353549480438232, -3.3239448070526123, -3.394014358520508, -3.4129810333251953], "all_perturbed_original_ll": [-3.5331008434295654, -3.6294972896575928, -4.079392910003662, -4.139688968658447, -3.881985902786255, -3.6561105251312256, -4.083754062652588, -4.413144111633301, -4.16528844833374, -3.5986599922180176], "perturbed_sampled_ll": -3.3059789896011353, "perturbed_original_ll": -3.9180623054504395, "perturbed_sampled_ll_std": 0.1670777275310339, "perturbed_original_ll_std": 0.28499642735897396}, {"original": "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "sampled": "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "perturbed_sampled": ["Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This"], "perturbed_original": ["Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Rename second pylint pre-commit hook to distinguish it from first (#13303)"], "original_ll": -5.093808650970459, "sampled_ll": -5.512474536895752, "all_perturbed_sampled_ll": [-5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752, -5.512474536895752], "all_perturbed_original_ll": [-5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459, -5.093808650970459], "perturbed_sampled_ll": -5.512474536895752, "perturbed_original_ll": -5.093808650970459, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixing bug which restricted the visibility of ImportErrors (#17924)", "sampled": "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "perturbed_sampled": ["Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Fixing bug which restricted the visibility of ImportErrors (#17924)The"], "perturbed_original": ["Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Fixing bug which restricted the visibility of ImportErrors (#17924)"], "original_ll": -5.1197614669799805, "sampled_ll": -5.636753559112549, "all_perturbed_sampled_ll": [-5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549, -5.636753559112549], "all_perturbed_original_ll": [-5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805, -5.1197614669799805], "perturbed_sampled_ll": -5.636753559112549, "perturbed_original_ll": -5.1197614669799805, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add missing values entries to Parameters in chart/README.md (#11477)", "sampled": "Add missing values entries to Parameters in chart/README.md (#11477)The", "perturbed_sampled": ["Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "Add missing values entries to Parameters in chart/README.md (#11477)The"], "perturbed_original": ["Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)", "Add missing values entries to Parameters in chart/README.md (#11477)"], "original_ll": -5.473202705383301, "sampled_ll": -5.898624897003174, "all_perturbed_sampled_ll": [-5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174, -5.898624897003174], "all_perturbed_original_ll": [-5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301, -5.473202705383301], "perturbed_sampled_ll": -5.898624897003174, "perturbed_original_ll": -5.473202705383301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different than in tests and makes it difficult to run tests outside of the main directory.", "sampled": "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode", "perturbed_sampled": ["[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze in an environment that Breeze did not have . Set your shell options up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options like: environment settings for shells - did not work properly (#6915) Text mode", "[AIRFLOW-6634] missing user environment options in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly in Shell mode", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) The user does not have PYTHONPATH set up for interactive Breeze (#7584) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When the user didn't have a shell, Emacs used global options which didn't work properly (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user profile inside a shell, Emacs used global options which did not work properly (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH in Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't allow (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH for interactive Breeze (#7254) Breeze did not detect directory, set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have a PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which did not display properly (#6915) Text mode", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze 2.7.0 doesn't have PYTHONPATH set for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode"], "perturbed_original": ["[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different than in a normal entry. This makes it difficult to run tests outside of the testing directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive directory. Only in executable directory they had it set - which is different than in tests and makes it difficult to run tests outside of the test directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not support PythonPATH set in the interactive Breeze entry - which is required in tests and makes it difficult to run tests outside of the main directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different than in tests and makes it difficult to run tests outside of a Breeze main directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different to the standard tests and makes it difficult to find other libraries in Breeze outside of the main directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze entry. The test did not have PYTHONPATH set in the interactive Breeze entry - which is different than the Breeze entry and makes it difficult to run tests outside of the main directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive entries? Breeze did not have PYTHONPATH set in the interactive entry - which is different than in tests and makes it difficult to run tests outside of the main directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have any PythonPATH in the interactive Breeze entry - which is different than in tests and makes it difficult to run tests outside of the main directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH for tests. Note the interactive Breeze entry - which is different than in tests and makes it difficult to run tests outside of Breeze outside of test directory.", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different than in the main Breeze entry. This makes it impossible to run tests outside of the main directory."], "original_ll": -3.9104323387145996, "sampled_ll": -3.671786069869995, "all_perturbed_sampled_ll": [-4.37537145614624, -4.001459121704102, -4.075170040130615, -3.7151525020599365, -3.668470621109009, -3.7653512954711914, -4.023387432098389, -4.111613750457764, -3.7010040283203125, -3.5344467163085938], "all_perturbed_original_ll": [-3.7061750888824463, -3.9449806213378906, -4.435066223144531, -3.844172477722168, -3.932725191116333, -3.6334755420684814, -3.971465826034546, -4.405325889587402, -4.0864481925964355, -3.545292377471924], "perturbed_sampled_ll": -3.897142696380615, "perturbed_original_ll": -3.950512742996216, "perturbed_sampled_ll_std": 0.24617332064538933, "perturbed_original_ll_std": 0.2822475554542281}, {"original": "Fixed failed pylint in master (#12938)", "sampled": "Fixed failed pylint in master (#12938)A", "perturbed_sampled": ["Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A", "Fixed failed pylint in master (#12938)A"], "perturbed_original": ["Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)", "Fixed failed pylint in master (#12938)"], "original_ll": -5.886841773986816, "sampled_ll": -6.618101596832275, "all_perturbed_sampled_ll": [-6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275, -6.618101596832275], "all_perturbed_original_ll": [-5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816, -5.886841773986816], "perturbed_sampled_ll": -6.618101596832275, "perturbed_original_ll": -5.886841773986816, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "sampled": "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "perturbed_sampled": ["Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook"], "perturbed_original": ["Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)"], "original_ll": -5.150719165802002, "sampled_ll": -5.6031389236450195, "all_perturbed_sampled_ll": [-5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195, -5.6031389236450195], "all_perturbed_original_ll": [-5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002, -5.150719165802002], "perturbed_sampled_ll": -5.6031389236450195, "perturbed_original_ll": -5.150719165802002, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "sampled": "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "perturbed_sampled": ["[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd"], "perturbed_original": ["[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)"], "original_ll": -4.416003704071045, "sampled_ll": -3.90499210357666, "all_perturbed_sampled_ll": [-3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666, -3.90499210357666], "all_perturbed_original_ll": [-4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045, -4.416003704071045], "perturbed_sampled_ll": -3.90499210357666, "perturbed_original_ll": -4.416003704071045, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "sampled": "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "perturbed_sampled": ["[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In"], "perturbed_original": ["[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)"], "original_ll": -6.6061553955078125, "sampled_ll": -6.9774041175842285, "all_perturbed_sampled_ll": [-6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285, -6.9774041175842285], "all_perturbed_original_ll": [-6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125, -6.6061553955078125], "perturbed_sampled_ll": -6.9774041175842285, "perturbed_original_ll": -6.6061553955078125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) its task_1. if task_2 depends on task_1 this means task_2 is set downstream of tastk_1 - wondering if I am missing something here - or misreading it", "sampled": "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "perturbed_sampled": ["[AIRFLOW-XXXX] Fix typo from upstream to downstream: This is very important. This is not as it used to be: Each DAG Run will consume a separate channel. That is, one DAG will read inputs and outputs and send output.\n\nOne DAG will read inputs and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will also consume parameters and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] DAGs switching channels from upstream to downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will also consume inputs and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel. One DAG will also consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] Fix typo from upstream to downstream ! This is how it used to be: Each DAG will consume a separate channel. That means: One channel will read inputs and outputs and send output.\n\nOne channel will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] Fix typo in line 9 when changing channels to downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and send input and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This will lead to: *", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it's going to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send them to the upstream channel. One DAG will read input and outputs and send them to the downstream channel. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the downstream channel. This allows you to: *", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will also read input s and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] Fix typo from upstream + downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will also consume inputs and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will read inputs and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] Fix typo from upstream to downstream interface. Now this is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read input and outputs and send output. One DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] send output from an upstream channel through to a downstream (#7595) This is different than it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will consume input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read input and outputs and send output. One DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will also consume inputs and outputs and receive output from the upstream channel. This allows you to: *"], "perturbed_original": ["[AIRFLOW-XXXX] Fix task_2 flow from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances ' ``execution_date`` equal s the DAG Run's ``execution_date``, and each Task Instance will be *upstream* from (depending on) its task_1. if task_2 depends on task_1 this means task_2 is set downstream of tastk_1 - wondering if I am missing something here - or misreading it", "[AIRFLOW-XXXX] Fix Task dependency upstream to downstream (#7595) ? Here\u2019s how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) its task_1. if task_2 depends on task_1 this means task_2 is *downstream* of tastk_1 - wondering if I am understand here - or misreading it", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task Instance. In this case, the Task_1 Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and task_2 will be *upstream* of (depends on) task_1, if task_2 depends on task_1. So, this means task_2 is set downstream of tastk_1 - wondering if I am correct here - or misreading it", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be. Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) its task_1. if task_2 depends on task_1 this task_2 is set downstream of tastk_1 <unk>execution_date<unk> - feel free to comment if I am missing something here - or remove it", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how the upstream/downstream relationships are supposed to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task Instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) task_1. if task_2 depends on task_1 this means task_2 is downstream of tastk_1 - wondering if I am missing something here - or misreading it", "- typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the Run's ``execution_date``, and each task_2 task instance is set *upstream* of (depends on) its task_1. if task_2 depends on task_1 this means task_2 is set downstream of tastk_1 - wondering if I am doing something wrong here - I don't want to edit it", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be - Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to that DAG Run's ``execution_date``, and task_2 will be *upstream* of (depends on) its task_1. if no DAG Run has a task_2 task instance on task_1 this means task_2 is set downstream of tastk_1 - wondering if I missed something here - or misreading it", "[AIRFLOW-XXXX] Fix typo from upstream docs. (#7595) This is how it used to be: Each DAG Run will have a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, so task_2 will be *upstream* of (depends on) its task_1. if task_2 depends on task_1 this means task_2 must be downstream of tastk_1 - wondering if I am missing something here - or misreading it", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: A DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) its task_1. Since task_2 depends on task_1 this means task_2 is set up downstream of tastk_1 - wondering if I am missing something here - or misreading it", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to DAG Run's ``execution_date``, and each task_2 will be bound to (depends on) its task_1. The task_2 depends on task_1 this means task_2 is set downstream of tastk_1 - wondering if I am missing something here or misreading it"], "original_ll": -3.6332876682281494, "sampled_ll": -2.750288486480713, "all_perturbed_sampled_ll": [-2.854187488555908, -2.7430074214935303, -2.7335364818573, -2.87712025642395, -2.7017014026641846, -2.8373537063598633, -2.9217844009399414, -2.898341178894043, -2.676238775253296, -2.922551393508911], "all_perturbed_original_ll": [-3.8550872802734375, -3.8315491676330566, -3.577155828475952, -3.702669858932495, -3.4226760864257812, -3.5762617588043213, -3.702854633331299, -3.55959415435791, -3.6345436573028564, -3.7197446823120117], "perturbed_sampled_ll": -2.8165822505950926, "perturbed_original_ll": -3.658213710784912, "perturbed_sampled_ll_std": 0.08927244085662021, "perturbed_original_ll_std": 0.1247438046730885}, {"original": "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "sampled": "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "perturbed_sampled": ["Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer"], "perturbed_original": ["Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer."], "original_ll": -4.25317907333374, "sampled_ll": -4.343189239501953, "all_perturbed_sampled_ll": [-4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953, -4.343189239501953], "all_perturbed_original_ll": [-4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374, -4.25317907333374], "perturbed_sampled_ll": -4.343189239501953, "perturbed_original_ll": -4.25317907333374, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "sampled": "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "perturbed_sampled": ["[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks"], "perturbed_original": ["[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "[AIRFLOW-5406] allow spark without kubernetes (#6921)"], "original_ll": -6.072336196899414, "sampled_ll": -6.523364067077637, "all_perturbed_sampled_ll": [-6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637, -6.523364067077637], "all_perturbed_original_ll": [-6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414, -6.072336196899414], "perturbed_sampled_ll": -6.523364067077637, "perturbed_original_ll": -6.072336196899414, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "sampled": "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we may not always get these errors anymore. The latest commit gives us a workaround", "perturbed_sampled": ["Remove workaround for 254 exit code. Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we may not always get that anymore. The latest commit gives us a workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures , where the script simply got the 254 exit code without a traceback. However now we may not always get such errors anymore. The latest commit gives us a workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we don't always get these errors anymore. The latest commit gives a convenient workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we may not get these errors anymore. The issue gives us a workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned raw build code without a traceback. However now you do not always get these errors anymore. The latest commit gives us a workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown doesker-compose failures that returned 254 exit code without a traceback. However now we may not always get these errors anymore. The latest release provides us a workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit codes in a traceback. However now we may not always get these errors. The latest commit gives us a workaround", "Remove workaround for docker-compose-failures (#18539) Long time ago there were some unknown docker-compose failures that returned 254 exit code without a console error. Maybe now we may not always get these errors anymore. The latest commit gives us a workaround", "Remove workaround for docker-compose-failures (#18539) \" A while ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we may not always get these errors anymore. This commit gives us a workaround", "Remove workaround code with this commit. Fix (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. Fortunately we may not always get these errors anymore. The latest commit gives us a workaround"], "perturbed_original": ["Remove d unknown docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has long been removed with decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long ago we had unknown docker-compose failures that returned 254 exit code. This has long been improved by decreasing memory pressure for CI dockers and Docker VMs and is no longer needed.", "Remove workaround . (#18539) Long time ago we had unknown docker-compose failures that returned 254 errors. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has been improved by decreasing memory pressure for CI dockers and healthiness checks are no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has long been improved by additional pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long before CI, we had docker-compose failures that returned 254 exit code. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long time ago we added workarounds for docker-compose failures thrown by 254 exit code. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures , and we still get the 254 exit code. This has been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that would exit with the 254 exit code. This has long been moved to decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose -errors that returned no code. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed."], "original_ll": -4.3843584060668945, "sampled_ll": -3.666752815246582, "all_perturbed_sampled_ll": [-4.027044773101807, -3.812676429748535, -3.713390588760376, -3.758809804916382, -3.847659111022949, -4.196563243865967, -3.6248700618743896, -3.817202568054199, -3.7569210529327393, -4.422401428222656], "all_perturbed_original_ll": [-4.534414291381836, -4.203558921813965, -5.07449197769165, -4.362279891967773, -4.378780841827393, -4.247272491455078, -4.143468379974365, -4.415253162384033, -4.224412441253662, -4.488102436065674], "perturbed_sampled_ll": -3.89775390625, "perturbed_original_ll": -4.407203483581543, "perturbed_sampled_ll_std": 0.23347144057411925, "perturbed_original_ll_std": 0.25316195158289934}, {"original": "Strict type check for google ads and cloud hooks (#11390)", "sampled": "Strict type check for google ads and cloud hooks (#11390)A", "perturbed_sampled": ["Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A", "Strict type check for google ads and cloud hooks (#11390)A"], "perturbed_original": ["Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)", "Strict type check for google ads and cloud hooks (#11390)"], "original_ll": -6.086731433868408, "sampled_ll": -6.577614784240723, "all_perturbed_sampled_ll": [-6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723, -6.577614784240723], "all_perturbed_original_ll": [-6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408, -6.086731433868408], "perturbed_sampled_ll": -6.577614784240723, "perturbed_original_ll": -6.086731433868408, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Dev: Clarify file naming in release verification doc (#19233)", "sampled": "Dev: Clarify file naming in release verification doc (#19233)If", "perturbed_sampled": ["Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If", "Dev: Clarify file naming in release verification doc (#19233)If"], "perturbed_original": ["Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)", "Dev: Clarify file naming in release verification doc (#19233)"], "original_ll": -6.128222465515137, "sampled_ll": -6.858707904815674, "all_perturbed_sampled_ll": [-6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674, -6.858707904815674], "all_perturbed_original_ll": [-6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137, -6.128222465515137], "perturbed_sampled_ll": -6.858707904815674, "perturbed_original_ll": -6.128222465515137, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve KubernetesPodOperator guide (#9079)", "sampled": "Improve KubernetesPodOperator guide (#9079)A", "perturbed_sampled": ["Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A", "Improve KubernetesPodOperator guide (#9079)A"], "perturbed_original": ["Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)", "Improve KubernetesPodOperator guide (#9079)"], "original_ll": -5.9975409507751465, "sampled_ll": -6.366732120513916, "all_perturbed_sampled_ll": [-6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916, -6.366732120513916], "all_perturbed_original_ll": [-5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465, -5.9975409507751465], "perturbed_sampled_ll": -6.366732120513916, "perturbed_original_ll": -5.9975409507751465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Small typo in JdbcOperator (#18593)", "sampled": "Small typo in JdbcOperator (#18593)With", "perturbed_sampled": ["Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With", "Small typo in JdbcOperator (#18593)With"], "perturbed_original": ["Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)", "Small typo in JdbcOperator (#18593)"], "original_ll": -5.63859224319458, "sampled_ll": -6.444066524505615, "all_perturbed_sampled_ll": [-6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615, -6.444066524505615], "all_perturbed_original_ll": [-5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458, -5.63859224319458], "perturbed_sampled_ll": -6.444066524505615, "perturbed_original_ll": -5.63859224319458, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "sampled": "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all", "perturbed_sampled": ["Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all fields to follow 'month' schedule * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days in the 'month' schedule * Update all", "Refactor days (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor DatetimeBreak (#18438) * Update all days to follow 'month' schedule * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow branch day * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor all (#18438) * Update all days to follow 'month' schedule * Update all", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all"], "perturbed_original": ["Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Move this code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, * Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * and DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor . 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor . Refactor / Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor. * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor . 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) : Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow to use variable branchday as week_day."], "original_ll": -4.110437870025635, "sampled_ll": -3.4426755905151367, "all_perturbed_sampled_ll": [-3.468153238296509, -3.4177849292755127, -4.353374004364014, -4.716466903686523, -3.443413496017456, -3.613128662109375, -3.420576333999634, -4.130972385406494, -4.6572370529174805, -3.4426755905151367], "all_perturbed_original_ll": [-3.942452907562256, -4.736943244934082, -4.944310188293457, -4.134191513061523, -5.294034004211426, -4.077206611633301, -3.83538556098938, -5.294034004211426, -4.037899017333984, -4.011277675628662], "perturbed_sampled_ll": -3.8663782596588137, "perturbed_original_ll": -4.430773472785949, "perturbed_sampled_ll_std": 0.5134588827477832, "perturbed_original_ll_std": 0.5462726941224465}, {"original": "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "sampled": "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "perturbed_sampled": ["Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski"], "perturbed_original": ["Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>"], "original_ll": -3.785753011703491, "sampled_ll": -4.0247392654418945, "all_perturbed_sampled_ll": [-4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945, -4.0247392654418945], "all_perturbed_original_ll": [-3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491, -3.785753011703491], "perturbed_sampled_ll": -4.0247392654418945, "perturbed_original_ll": -3.785753011703491, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "sampled": "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "perturbed_sampled": ["[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse"], "perturbed_original": ["[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)"], "original_ll": -6.541927814483643, "sampled_ll": -6.67662239074707, "all_perturbed_sampled_ll": [-6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707, -6.67662239074707], "all_perturbed_original_ll": [-6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643, -6.541927814483643], "perturbed_sampled_ll": -6.67662239074707, "perturbed_original_ll": -6.541927814483643, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "sampled": "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "perturbed_sampled": ["Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Made models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible (#8068) * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible (#8068) * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible (#8068) * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible (#7974) * Fixed for isortable/pump_tables.py * Fixed for", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible (#8006) * Fixed for isortable/pump_tables.py * Fixed for"], "perturbed_original": ["Make isort.msg compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for 2.6 by matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort . Matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Add isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort _resizing <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible (#8073) Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for the above issues * matsubara <matsubara@matsubaranoMacBook-Pro.local>", "Make models/pool.py pylint compatible (#8068) Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>"], "original_ll": -4.075239658355713, "sampled_ll": -3.68587064743042, "all_perturbed_sampled_ll": [-3.68587064743042, -3.730529308319092, -3.465177059173584, -3.33197021484375, -2.5940120220184326, -3.68587064743042, -3.33197021484375, -3.33197021484375, -3.4769985675811768, -3.446887493133545], "all_perturbed_original_ll": [-4.86492395401001, -4.075239658355713, -3.8180577754974365, -3.910895347595215, -3.9933738708496094, -4.461065769195557, -4.075239658355713, -3.908266067504883, -3.9656553268432617, -4.114811420440674], "perturbed_sampled_ll": -3.408125638961792, "perturbed_original_ll": -4.118752884864807, "perturbed_sampled_ll_std": 0.3078813828112295, "perturbed_original_ll_std": 0.29913834197901323}, {"original": "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "sampled": "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "perturbed_sampled": ["[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The"], "perturbed_original": ["[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)"], "original_ll": -6.36091947555542, "sampled_ll": -6.634527206420898, "all_perturbed_sampled_ll": [-6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898, -6.634527206420898], "all_perturbed_original_ll": [-6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542, -6.36091947555542], "perturbed_sampled_ll": -6.634527206420898, "perturbed_original_ll": -6.36091947555542, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "sampled": "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "perturbed_sampled": ["Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For"], "perturbed_original": ["Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)"], "original_ll": -6.718223571777344, "sampled_ll": -7.345280170440674, "all_perturbed_sampled_ll": [-7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674, -7.345280170440674], "all_perturbed_original_ll": [-6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344, -6.718223571777344], "perturbed_sampled_ll": -7.345280170440674, "perturbed_original_ll": -6.718223571777344, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "sampled": "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "perturbed_sampled": ["Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A"], "perturbed_original": ["Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)"], "original_ll": -6.434218406677246, "sampled_ll": -6.828874588012695, "all_perturbed_sampled_ll": [-6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695, -6.828874588012695], "all_perturbed_original_ll": [-6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246, -6.434218406677246], "perturbed_sampled_ll": -6.828874588012695, "perturbed_original_ll": -6.434218406677246, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod 3. reconcile with any of the", "sampled": "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "perturbed_sampled": ["Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs Fixes #113432 * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod for ipa_parse and ipa_parse-parse (#11182) * Fix full_parsepod for full_pod, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_pod on platforms requiring all of the required APIs (#12475) * Fix the API failures for ipa_parse_spokes (#10458) * Fix API failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Fix full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "e for k8spodoperator (#12354) * Fix full_pod_spec for full_pod Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12456) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and full_pod (#12481) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on supported platforms requiring all of the required APIs (#12475) * Support full_pod on supported platforms requiring all of the required APIs (#12475) * Fix full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#10458) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for fullpod (#12976) * Fix full_parsepod for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on platform ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Fix test failures on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for K8spodoperator Features (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_pip_skip_spokes, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse _spokes (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_pod on ipa_parse_spokes (#10458) * Fix test and full_pod for ipa_parse_spokes, ipa_skip_spokes, full_parse, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Fix full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, and ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, and full_parse_spokes (#11925) * Fix", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12358) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures in ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and full_pod (#11913) * Fix full_parsepod for ipa_skip_spokes, full_pod, ipa_jump_spokes (#11925) * Fix"], "perturbed_original": ["Fix full_pod_spec for k8spodoperator ? Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The new order for processing pods is as follows: 1. Check to see if there is pod_template_file and if so start with empty pod and then return the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile with the empty pod and the full_pod_spec pod 3. reconcile with any of the", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes the issue previously reported where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else create the empty pod 2. if there is a fill_pod_spec pod, create the original first and reconcile the pod_template_file with the full_pod_spec pod 3. reconcile with any of the", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator in the Pod operator. Fixed the bug where the `full_pod_spec` was never factored into the initial Pod operation. The new order of operations in as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec 3. check and reconcile with any of the", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument wouldn<unk>t be factored into the initial pod. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , read the pod_template_file and create the full_pod_spec pod 3. reconcile with any of the", "Fix a bug with the k8spodoperator (#12354) * Fix full_pod_spec bug Fix a bug. Fixes a bug where the `full_pod_spec` argument is never found in the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the template pod else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod 3. reconcile the pod of the", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator * Fix a bug where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, then reconcile with empty pod 2. If there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod 3. reconcile with any of the", "Fix full_pod_spec for k8spodoperator (#12354) : New order now for full_pod_spec operation - Fixes a bug where the `full_pod_spec` argument is never passed to the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod to start with empty pod 2. if there is a full_pod_spec , reconcile between the empty pod and the full_pod_spec pod 3. reconcile with any of the", "Fix full_pod_spec for k8spodoperator | Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile this pod with the full_pod_spec pod 3. reconcile with any of the", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes a situation where the `full_pod_spec` was never factored into the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a full_pod_spec and if so , compare with initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the standard pod 3. reconcile with any of the", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored into the operation The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the pod template_file pod, else start with empty pod 2. if there is a full_pod_spec go to the pod_template_file pod and then do a full_pod_spec pod 3. reconcile with any of the"], "original_ll": -3.1907310485839844, "sampled_ll": -1.9140031337738037, "all_perturbed_sampled_ll": [-2.192979097366333, -2.0788161754608154, -2.129134178161621, -1.983871340751648, -2.085035562515259, -1.9320605993270874, -2.113860845565796, -2.1001670360565186, -2.0098233222961426, -2.187791347503662], "all_perturbed_original_ll": [-3.2789461612701416, -3.26326060295105, -3.174701690673828, -3.094315528869629, -3.427262544631958, -3.1992502212524414, -3.461405038833618, -3.2802841663360596, -3.3500044345855713, -3.0560379028320312], "perturbed_sampled_ll": -2.081353950500488, "perturbed_original_ll": -3.258546829223633, "perturbed_sampled_ll_std": 0.08025625378275794, "perturbed_original_ll_std": 0.1254670012016942}, {"original": "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "sampled": "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "perturbed_sampled": ["[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon"], "perturbed_original": ["[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-6929] Add OpenAPI spec (#7549)"], "original_ll": -6.245003700256348, "sampled_ll": -6.41998291015625, "all_perturbed_sampled_ll": [-6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625, -6.41998291015625], "all_perturbed_original_ll": [-6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348, -6.245003700256348], "perturbed_sampled_ll": -6.41998291015625, "perturbed_original_ll": -6.245003700256348, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "sampled": "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "perturbed_sampled": ["[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A"], "perturbed_original": ["[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)"], "original_ll": -6.1467180252075195, "sampled_ll": -6.492127418518066, "all_perturbed_sampled_ll": [-6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066, -6.492127418518066], "all_perturbed_original_ll": [-6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195, -6.1467180252075195], "perturbed_sampled_ll": -6.492127418518066, "perturbed_original_ll": -6.1467180252075195, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "sampled": "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "perturbed_sampled": ["Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The"], "perturbed_original": ["Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`"], "original_ll": -5.098919868469238, "sampled_ll": -5.450592994689941, "all_perturbed_sampled_ll": [-5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941, -5.450592994689941], "all_perturbed_original_ll": [-5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238, -5.098919868469238], "perturbed_sampled_ll": -5.450592994689941, "perturbed_original_ll": -5.098919868469238, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "sampled": "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "perturbed_sampled": ["Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99"], "perturbed_original": ["Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8."], "original_ll": -4.402935028076172, "sampled_ll": -4.140133857727051, "all_perturbed_sampled_ll": [-4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051, -4.140133857727051], "all_perturbed_original_ll": [-4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172, -4.402935028076172], "perturbed_sampled_ll": -4.140133857727051, "perturbed_original_ll": -4.402935028076172, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "sampled": "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "perturbed_sampled": ["[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The"], "perturbed_original": ["[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)"], "original_ll": -6.262882232666016, "sampled_ll": -6.579794406890869, "all_perturbed_sampled_ll": [-6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869, -6.579794406890869], "all_perturbed_original_ll": [-6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016, -6.262882232666016], "perturbed_sampled_ll": -6.579794406890869, "perturbed_original_ll": -6.262882232666016, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape problem areas, this commit just fixes the last instances so that we can assert that `|safe` is never used.", "sampled": "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "perturbed_sampled": ["Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the <unk>|safe<unk> filter in code, it's risky (#9180) Most", "Don't use the <unk>|safe<unk> filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode information so you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in code, it's risky (#9180) Most things I create use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use that class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, though. Some people use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the safe filter to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the <unk>|safe<unk> filter in code, it's risky (#9180) Most code files already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that . Don't use the <unk>|safe<unk> filter in code, it's risky (#9180) Most", "Don't use the `|safe` filter in your code because it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in your code because it's risky (#9180) Most"], "perturbed_original": ["Don't use the <unk>|safe<unk> in code, it's risky (#9180) Most things already use the safe<unk> structure to correctly escape problem areas, this commit just fixes the last instances so that we can assert that `|safe` is never used.", "Don't use the `|safe` filter in code, it's risky (#9180) Most of the previous commit to this commit was to use the `Markup` class to correctly escape problem areas, this commit just fixes the last instances so that they automatically assert that `|safe` is never used.", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the filter to correctly escape problem areas, this commit just fixes the last instances so that we ensure that `|safe` is never used.", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the filter to correctly escape problem areas, this commit just removes all last instances so that we can assert that `|safe` is never used.", "Don't ever use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape problem areas, this commit just fixes the last instances so you can assert that `|safe` is never used.", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` filter for their code to correctly escape problem areas, this commit just fixes the last instances so that we can assert that `|safe` is never used.", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape problem areas, this commit just fixes the last instances where we can assert that `|safe` should not be used.", "(#9180) Never use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape problem areas, this class fixes the last instances so that we can assert that `|safe` is never used.", "Don't use this filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape this filter, this commit just fixes the last instances so that we can assert that `|safe` is never used.", "Don't use the `|safe` filter in code, never. (#9180) Most things already use the <unk>|safe<unk> filter to correctly escape problem areas, this commit just fixes the last instances so that we can assert that `|safe` is never used."], "original_ll": -4.155300140380859, "sampled_ll": -2.714235305786133, "all_perturbed_sampled_ll": [-3.2242703437805176, -3.1199557781219482, -2.820005178451538, -2.798673391342163, -2.7925753593444824, -2.8620758056640625, -2.6524481773376465, -3.126919746398926, -3.4727718830108643, -2.6181695461273193], "all_perturbed_original_ll": [-4.362241268157959, -4.091309547424316, -4.233344554901123, -4.201075553894043, -4.2107086181640625, -4.061892509460449, -4.171142101287842, -4.187333106994629, -4.339685440063477, -4.280996322631836], "perturbed_sampled_ll": -2.9487865209579467, "perturbed_original_ll": -4.213972902297973, "perturbed_sampled_ll_std": 0.26070013729431163, "perturbed_original_ll_std": 0.09140593423529435}, {"original": "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in processing environment variable file which prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "sampled": "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "perturbed_sampled": ["Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be fixed by #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) #16100 docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of #17115 - docker-compose workaround buggy docker-compose v2 and v2 documentation has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1595) using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode \"docker\" had no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of trying to workaround buggy docker-compose v2 Beta. #17074 Docker Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in SELinux. (#1599)\n\nargument has no effect when run in SELinux. (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of trying to run docker-compose v2 (#16989) Docker-Compose now has an error in the documentation that is considered to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to do this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out of date documentation while trying to workaround buggy docker-compose v2 (#16989) Docker-Compose V2 has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of trying to workaround buggy docker-compose v2 Beta. docker-compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker node, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of \"out\". workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out instead of out (#16989) workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error about removing documentation that is expected to be present. #17078\n\ndocker's --continue arguments have no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "Errors out , trying to workaround in v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode(#16562 docker's --continue argument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is needed in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is"], "perturbed_original": ["Errors out instead . Try to workaround buggy docker-compose v2 (#16989) . Beta has an error in processing environment variable file which prevents Breeze from running. Until it is fixed we are going to post an error, explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in processing environment variable file name that prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to disable it and close because the workaround may cause more problems than it solves (passing environment variable to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in processing environment variable file which prevents executable instances from running. Until it is fixed, we are going to print an error, explain how to disable v2 and exit - because it introduces more problems than it solves . (Although, attaching environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy docker-compose (and rewriting code to fix bug fixes too). Docker-Compose v2 Beta has an error in processing environment variable file which prevents Breeze from running. Until it is fixed, we are going to show an error, explain how to disable it and exit - because the fix causes more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy behavior. (#16989) Docker-Compose v2 Beta has an error in processing environment variable file which prevents Breeze from binding. After it is fixed, I am going to print it and explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in processing environment variable file which prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to run it and exit . Actually implementing the workaround seems to cause more problems than it solves (as env variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out of the box - trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta 2 contains an error in shell_environment variable file which prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variables to container works partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to fix? docker-compose v2 Beta docker-compose v2 Beta has an error in processing environment variable file which prevents Breeze from starting. Even if it is fixed, we are going to print an error, explain how to fix it and exit - because the workaround introduces more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an issue in processing environment variable file which prevents Breeze from running. Until this is fixed, we are going to print an error, explain how to work around that and exit - because the workaround introduces more problems than it fixes (i.e. passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in its environment variable check that prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variable to running container is broken ). For more information, please see https://github.com/docker/compose-cli/issues/1917"], "original_ll": -3.6711905002593994, "sampled_ll": -2.811732292175293, "all_perturbed_sampled_ll": [-2.9164698123931885, -3.0390310287475586, -3.198737144470215, -2.6622419357299805, -2.6804420948028564, -2.7285244464874268, -2.8639283180236816, -2.883540391921997, -2.923895835876465, -3.196626901626587], "all_perturbed_original_ll": [-3.9844775199890137, -3.670671224594116, -3.8588171005249023, -3.6546175479888916, -3.8727409839630127, -3.8911426067352295, -3.5656137466430664, -3.374861001968384, -3.6469480991363525, -3.436321496963501], "perturbed_sampled_ll": -2.909343791007996, "perturbed_original_ll": -3.695621132850647, "perturbed_sampled_ll_std": 0.18227472393313676, "perturbed_original_ll_std": 0.1927094092076591}, {"original": "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "sampled": "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "perturbed_sampled": ["AwsBaseHook () in & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( socket_socket & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type & resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make <unk>client_type<unk> & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet resource_type = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = socket_socket( request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & <unk>get_resource_type<unk>; let client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type & resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet"], "perturbed_original": ["* AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for <unk>get_client_type<unk> & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make client_type & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & get_resource_type<unk>. (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & resource_type optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & get_resource_type. * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) and make client_type & resource_type optional params for get_client_type & get_resource_type", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type"], "original_ll": -2.6223320960998535, "sampled_ll": -2.1332950592041016, "all_perturbed_sampled_ll": [-2.3612735271453857, -2.1332950592041016, -2.1465728282928467, -2.601722240447998, -2.4632153511047363, -2.1332950592041016, -2.0879313945770264, -2.1992690563201904, -2.5509374141693115, -2.601722240447998], "all_perturbed_original_ll": [-2.490171432495117, -3.0459539890289307, -2.6223320960998535, -2.811896562576294, -3.0938589572906494, -2.7801575660705566, -2.6223320960998535, -2.708345890045166, -2.7139997482299805, -2.6223320960998535], "perturbed_sampled_ll": -2.3279234170913696, "perturbed_original_ll": -2.7511380434036257, "perturbed_sampled_ll_std": 0.20046932891259114, "perturbed_original_ll_std": 0.18165756425411234}, {"original": "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "sampled": "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG entities (#17666)", "perturbed_sampled": ["Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property to DAG entities (#17666)", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Added property on DAG entities (#17666)", "* ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG entities (#17666)", "Docs: Make ``DAG.is_active`` read-only in DAG properties. Add readOnly=True property on DAG entities (#17666)", "Docs: Make ``DAG.is_active`` read-only in API -based readOnly=True property on DAG entities (#17666)", "Docs: Make ``DAG.is_active`` value on DAGs public API (#17667) Add readOnly=True property on DAG entities (#17666)", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property to all DAG entities (#17666)", "Docs: Set MSG to read-only in API (#17667) Add readOnly=True property on DAG entities (#17666)", "Docs: Make ``DAG.is_active`` read-only in DAGs (#17774) Add readOnly=True property on DAG entities (#17666)", "Docs: Make ``DAG.is_active`` read-only property on Active DAG entities (#17667) Add readOnly=True property on DAG entities (#17666)"], "perturbed_original": ["<unk>DAG.is_active<unk>: ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "Docs: Make ``DAG.is_active`` a new API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add error on DAG.is_active closes: #17639", "Docs: Make it easy to insert in API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property in closes: #17639", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add a hint on DAG.is_active closes: #17639", "Docs: Make ``DAG.is_active`` a C# API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property to API 'opening' closes: #17639", "Docs: Make ``DAG.is_active`` read-only property (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "Docs: Make ``DAG.is_active`` read-only in OpenSocket: Add readOnly=True property on DAG.is_active closes: #17639"], "original_ll": -4.130771636962891, "sampled_ll": -3.9657154083251953, "all_perturbed_sampled_ll": [-3.8927886486053467, -4.325709819793701, -4.203670024871826, -4.077256202697754, -4.748590469360352, -4.000730037689209, -3.8884925842285156, -4.377230644226074, -3.7940549850463867, -3.8936100006103516], "all_perturbed_original_ll": [-4.090927600860596, -4.2946929931640625, -4.175492286682129, -4.975643634796143, -4.813410758972168, -4.130102157592773, -4.318416118621826, -4.758497714996338, -4.108255863189697, -4.341558456420898], "perturbed_sampled_ll": -4.120213341712952, "perturbed_original_ll": -4.400699758529663, "perturbed_sampled_ll_std": 0.2809583515128776, "perturbed_original_ll_std": 0.3091665606605131}, {"original": "Move the contribution workflow to the beginning of the file (#10092)", "sampled": "Move the contribution workflow to the beginning of the file (#10092)A", "perturbed_sampled": ["Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A", "Move the contribution workflow to the beginning of the file (#10092)A"], "perturbed_original": ["Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)", "Move the contribution workflow to the beginning of the file (#10092)"], "original_ll": -4.822419166564941, "sampled_ll": -5.323852062225342, "all_perturbed_sampled_ll": [-5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342, -5.323852062225342], "all_perturbed_original_ll": [-4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941, -4.822419166564941], "perturbed_sampled_ll": -5.323852062225342, "perturbed_original_ll": -4.822419166564941, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "shorten name of hook re imports of provide_session and create_session (#12936)", "sampled": "shorten name of hook re imports of provide_session and create_session (#12936)If", "perturbed_sampled": ["shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If", "shorten name of hook re imports of provide_session and create_session (#12936)If"], "perturbed_original": ["shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)", "shorten name of hook re imports of provide_session and create_session (#12936)"], "original_ll": -5.920376777648926, "sampled_ll": -6.436463832855225, "all_perturbed_sampled_ll": [-6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225, -6.436463832855225], "all_perturbed_original_ll": [-5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926, -5.920376777648926], "perturbed_sampled_ll": -6.436463832855225, "perturbed_original_ll": -5.920376777648926, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update max_tis_per_query to better render on the webpage (#17971)", "sampled": "Update max_tis_per_query to better render on the webpage (#17971)A", "perturbed_sampled": ["Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A", "Update max_tis_per_query to better render on the webpage (#17971)A"], "perturbed_original": ["Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)", "Update max_tis_per_query to better render on the webpage (#17971)"], "original_ll": -5.869471549987793, "sampled_ll": -6.299273490905762, "all_perturbed_sampled_ll": [-6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762, -6.299273490905762], "all_perturbed_original_ll": [-5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793, -5.869471549987793], "perturbed_sampled_ll": -6.299273490905762, "perturbed_original_ll": -5.869471549987793, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the main shell is zsh on MacOS (which is the default) this PR changes it so that the output of `command -v bash` is used instead. Fixes #14754", "sampled": "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "perturbed_sampled": ["Fixes recent scripting breeze fix ; also with zsh (#14787) The BASH variable introduced in #14579 is not set when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed back to the script are valid or not (and thus corrupt). This", "Fixes recent scripting breeze fix to work against zsh (#14787) The BASH variable introduced in the zsh patch was not set when the batch file was being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fixes recent problems in #14781 This fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not pointing where the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fixes recent #16365 fix to work also with zsh (#14787) The BASH variable introduced in #14579 is n't being pushed when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fixes recent scripting breeze fix to work with BASH. The BASH variable introduced in #14579 is not set when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the shell are still valid. This", "Fixes recent scripting breeze fix to work with zsh (#14787) The BASH variable introduced in #14579 is not set when scripts are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are set properly or not. Comment: This", "Fixes recent scripting breeze fix to work also with shellsignatures. The BASH variable introduced in #14579 is not set when scriptsignatures are being checked again , which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fixes recent scripting breeze fix to work at boot time. zsh (#14787) The BASH variable introduced in the scripting breeze install is not called when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fixes recent scripting breeze bugs That seem to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when shell scriptsignatures are being checked again or updated, but the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fixes recent scripting breeze fix to work also with scripting breeze. Description: The BASH variable introduced in #14579 is not set when scripts are being checked again or previously used in an older script. This means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This"], "perturbed_original": ["Fixes recent scripting breeze fix to work also on MacOS (#14787) The BASH variable introduced in #14579 is not set when the main shell is zsh on MacOS (which is the default) this PR changes it so the output of `command > is used instead. Fixes #14754", "Fixes recent bug fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the main shell is started on MacOS (which is the default) this PR changes it so that the output of `command -v -r <unk> is used instead. Fixes #14754", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is used by scripting breeze when the main shell is zsh /sh (which is the default) this PR changes it so that the output of `command line/shell/<unk>command is used instead. Fixes #14754", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced last night is not set when the main shell is zsh on MacOS (which is the default) this PR changes it so that the output of `command -v >> is used instead. Fixes #14754", "Fix a scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the main shell is zsh on MacOS (which is the default) this PR changes it so that the output of <unk>execute bash` is used instead. Fixes #14754", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the shell is zsh on MacOS (which is the default) and the bugfix introduced by this bug changes it so that the output of the command bash` is used instead. Fixes #14754", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable for #14579 is not set when the running program is zsh on MacOS (which is the default) this PR changes that, so that the output of `command -v bash` is used instead. Fixes #14754", "Fixes #37444 and breeze fix to work also with zsh . Fixes #39441. Since the BASH variable introduced in #14579 is not set when the environment is zsh on MacOS (which is the default) this PR changes it so that the output of `command -v bash` is used instead. Fixes #14754", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in an earlier release is not set when the main shell is zsh on MacOS (which is the default) this PR changes this so that the output of <unk> bash` is used instead. Fixes #14754", "Fixes recent scripting issue caused by rcf to work also . <unk> (#14787) The BASH variable introduced in #14579 is not set when the main process is zsh on MacOS (which is the default) this PR changes it so that the output of `command -v bash` is used instead. Fixes #14754"], "original_ll": -3.9511241912841797, "sampled_ll": -3.859037399291992, "all_perturbed_sampled_ll": [-3.9078919887542725, -4.031243324279785, -3.8191845417022705, -3.9758193492889404, -3.6566274166107178, -4.0847697257995605, -3.9931929111480713, -3.920417547225952, -4.097939968109131, -4.02955436706543], "all_perturbed_original_ll": [-4.22324800491333, -3.8651559352874756, -4.525362491607666, -4.348172664642334, -4.3633503913879395, -3.9806807041168213, -3.962613105773926, -3.884594202041626, -4.361938953399658, -4.162141799926758], "perturbed_sampled_ll": -3.951664113998413, "perturbed_original_ll": -4.167725825309754, "perturbed_sampled_ll_std": 0.12700843466869774, "perturbed_original_ll_std": 0.22098321433288923}, {"original": "Fix capitalisation of boolean in config (#13569)", "sampled": "Fix capitalisation of boolean in config (#13569)The", "perturbed_sampled": ["Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The", "Fix capitalisation of boolean in config (#13569)The"], "perturbed_original": ["Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)", "Fix capitalisation of boolean in config (#13569)"], "original_ll": -6.264561653137207, "sampled_ll": -7.038703918457031, "all_perturbed_sampled_ll": [-7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031, -7.038703918457031], "all_perturbed_original_ll": [-6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207, -6.264561653137207], "perturbed_sampled_ll": -7.038703918457031, "perturbed_original_ll": -6.264561653137207, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add missing type of tests to breeze. (#18504)", "sampled": "Add missing type of tests to breeze. (#18504)From", "perturbed_sampled": ["Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From", "Add missing type of tests to breeze. (#18504)From"], "perturbed_original": ["Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)", "Add missing type of tests to breeze. (#18504)"], "original_ll": -6.598736763000488, "sampled_ll": -7.274570941925049, "all_perturbed_sampled_ll": [-7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049, -7.274570941925049], "all_perturbed_original_ll": [-6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488, -6.598736763000488], "perturbed_sampled_ll": -7.274570941925049, "perturbed_original_ll": -6.598736763000488, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move backport packages to GA (#8391)", "sampled": "Move backport packages to GA (#8391)The", "perturbed_sampled": ["Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The", "Move backport packages to GA (#8391)The"], "perturbed_original": ["Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)", "Move backport packages to GA (#8391)"], "original_ll": -6.16018009185791, "sampled_ll": -6.8451128005981445, "all_perturbed_sampled_ll": [-6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445, -6.8451128005981445], "all_perturbed_original_ll": [-6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791, -6.16018009185791], "perturbed_sampled_ll": -6.8451128005981445, "perturbed_original_ll": -6.16018009185791, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "sampled": "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "perturbed_sampled": ["[AIRFLOW-6118] [AIP-21] New operators and hook (#7046) PR contains changes regarding AIP-21 and implementation is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related Hooks are now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to PubSub API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators to the correct URI. (#7046) PR contains changes regarding AIP-21 and PUBSCUB parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators . FIXED: (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook into the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook s. PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR fixes major bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in branch, and fixed a misunderstanding. PR contains many bugs, notably missing /r/myapp.js as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes to this PR and related parts.\n\nPR is now in 0.0.6 release and contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "FIXED: FIXED: Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub hook to our API hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is currently in testing place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably with the AIP-21 module as a trigger to trigger the API.\n\nPR contains"], "perturbed_original": ["[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation in the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * added new warnings to the contrib pings * fixed tests * updated UPDATING.md", "#7046 Rename Pubsub operators and hook (#7046) ? ? Summary of changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * added deprecation warnings to GCP modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP operators * fixed <unk>* dev diff * adde deprecation warnings * removed obsolete contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation message to the .ps1 list * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR INT: Update regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR INT: Update regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings * removed contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains the following changes to AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib s * fixed tests * updated UPDATING.md", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks) * renamed GCP modules * added deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md"], "original_ll": -4.617997169494629, "sampled_ll": -4.130293369293213, "all_perturbed_sampled_ll": [-4.234513282775879, -4.325521469116211, -4.253779888153076, -4.0838823318481445, -4.329519271850586, -4.2806243896484375, -4.2508392333984375, -3.865900754928589, -4.060463905334473, -4.018863677978516], "all_perturbed_original_ll": [-4.728070259094238, -4.716598033905029, -4.575654029846191, -4.308494567871094, -4.742502689361572, -4.700189113616943, -4.835700511932373, -4.738748550415039, -4.569395065307617, -4.4122161865234375], "perturbed_sampled_ll": -4.1703908205032345, "perturbed_original_ll": -4.632756900787354, "perturbed_sampled_ll_std": 0.14639621446124196, "perturbed_original_ll_std": 0.15690292529709615}, {"original": "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "sampled": "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "perturbed_sampled": ["[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - add tests - add \"get_autocommit\" docs -", "[AIRFLOW-5582] Add new JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] : update to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add get_autocommit API - update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update the \"get_autocommit\" docs -", "- get_autocommit to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] : Changes made to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add docs, update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] Add get_autocommit to airflight cmake - add tests - update README, add \"get_autocommit\" docs -", "[AIRFLOW-5582] : Update to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -"], "perturbed_original": ["[AIRFLOW-5582] Add get_autocommit to airflow.h - add tests - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook - add tests - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs by Florian Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update tests - test case by Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - - Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs Co-Authored-By: Flores, <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - docs - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs Co-Authored-By: Felice Legero <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs - commit - commit. Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests and docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>"], "original_ll": -4.231499671936035, "sampled_ll": -4.002114772796631, "all_perturbed_sampled_ll": [-4.024662017822266, -4.499896049499512, -4.569478988647461, -3.519894599914551, -4.136611461639404, -3.903857469558716, -4.627269744873047, -3.9695353507995605, -4.369107723236084, -4.532819747924805], "all_perturbed_original_ll": [-4.280811309814453, -4.275691986083984, -4.3666486740112305, -4.388588905334473, -4.323805809020996, -4.32973051071167, -4.316618919372559, -4.045261859893799, -4.289525032043457, -4.203367233276367], "perturbed_sampled_ll": -4.2153133153915405, "perturbed_original_ll": -4.282005023956299, "perturbed_sampled_ll_std": 0.3441151626574376, "perturbed_original_ll_std": 0.09277762581237645}, {"original": "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "sampled": "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation interface. These have some new and interesting commands. The new commands are:", "perturbed_sampled": ["[AIRFLOW-XXXX] Add Gojek as an Automation Program (#8070) Gojek uses Airflow as its automation interface. These have some new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation tool We have some new and improved and support Gojek. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek was added as a user into the airflow interface. These have some new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as Airflow user (#8070) Gojek uses Airflow as a task automation interface. These have several new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task manager for his apps. These have some new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] ] New. as an Airflow user (#8070) Gojek uses Airflow as a command interface. These have some new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) and use Airflow as a task automation interface. These have some new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation framework. Gojek seems to have some new and interesting commands and some of these new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) and include Airflow as a task automation interface. These have added new and interesting commands. The new commands are:", "[AIRFLOW-XXXX] Add Gojek as an Airflow user who uses Airflow as a task automation interface. These have some new and improved commands. The new commands are:"], "perturbed_original": ["[AIRFLOW-XXXX] Add Airflow to Airflow. It's an API! (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and tool for managing data warehouses and machine learning pipelines.", "[AIRFLOW-XXXX] Add Gojek as a user ! Our team uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and automation transformation tool on our data warehouses and business pipelines.", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) : We use Airflow as a task automation scheduler and ETL tool on our data warehouses . We use Airflow to create deep learning pipelines.", "[AIRFLOW-XXXX] Add Gojek as an Airflow developer. Gojek uses Airflow as a task automation scheduler and ETL tool on data warehouses and machine learning pipelines.", "[AIRFLOW-XXXX] Add Gojek as an Airflow user. (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "[AIRFLOW-XXXX] \u00bb Gojek Joins Airflow as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses for machine learning pipelines.", "[AIRFLOW-XXXX] Gojek: as a Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and is a data scientist on our web data and machine learning pipelines."], "original_ll": -4.719353199005127, "sampled_ll": -4.406858444213867, "all_perturbed_sampled_ll": [-4.403887748718262, -4.499090194702148, -4.21372127532959, -4.528289318084717, -4.217710971832275, -4.843743324279785, -4.649210453033447, -4.170361518859863, -4.834223747253418, -4.5846171379089355], "all_perturbed_original_ll": [-4.819858074188232, -4.67249059677124, -5.197223663330078, -5.030250072479248, -4.8330397605896, -4.577852249145508, -4.627901554107666, -4.821527004241943, -4.86444091796875, -4.803953170776367], "perturbed_sampled_ll": -4.494485569000244, "perturbed_original_ll": -4.824853706359863, "perturbed_sampled_ll_std": 0.23223426233565608, "perturbed_original_ll_std": 0.17491953974831462}, {"original": "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "sampled": "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "perturbed_sampled": ["Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It"], "perturbed_original": ["Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)"], "original_ll": -6.1924638748168945, "sampled_ll": -6.631679534912109, "all_perturbed_sampled_ll": [-6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109, -6.631679534912109], "all_perturbed_original_ll": [-6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945, -6.1924638748168945], "perturbed_sampled_ll": -6.631679534912109, "perturbed_original_ll": -6.1924638748168945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "sampled": "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "perturbed_sampled": ["Restore base lineage backend (#14146) This restores the base lineage backend which can be used by default to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send back a lineage from a TLS instance to any custom backend. closes #14141\n\ntls2tls: fix security for TLS certificate (#11387) This", "Restore base lineage backend (#14146) This adds a base lineage backend which can be extended by importing lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for certificate (#11387) This", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended with a function to add lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for new users (#11387) This", "Restore lineage backend (#14146) This adds back the base lineage backend that could be extended to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "Restore d lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be used to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check on certificate (#11387) This", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to allow adding lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for tls (#11387) This", "Restore base lineage backend (#14146) This patch restores back the base lineage backend which can be extended to export lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This"], "perturbed_original": ["Restore base lineage backend (#14146) This patch restore the base lineage backend which can be used to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage functionality which can be extended to send lineage metadata to any backend closes: #14106 Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend that can be extended to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte s, Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send messages to any custom backend. closes: #14106 Co-authored-by: Josh Pepe <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to add new tags or metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte s <unk>joaopontres@gmail.com>, Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend , and can be extended to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any lineage that exists of the lineage. closes: #14106 Co-authored-by: Joao Ponte des; Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. Init Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend; it can be extended to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte s (asli) and Tomek Urbaszek <turbaszek@gmail.com>", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom user interface. #14106 Co-authored-by: Jordan Petersen <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>"], "original_ll": -3.789785385131836, "sampled_ll": -3.7949421405792236, "all_perturbed_sampled_ll": [-3.728912353515625, -3.986743211746216, -3.9450976848602295, -3.9181292057037354, -3.7771475315093994, -3.8871757984161377, -3.976740837097168, -3.813739538192749, -3.8547005653381348, -3.7827882766723633], "all_perturbed_original_ll": [-3.731597900390625, -3.901449680328369, -4.324437141418457, -3.8308846950531006, -4.326287746429443, -3.8453307151794434, -4.507019519805908, -3.8798232078552246, -4.403572082519531, -3.6939728260040283], "perturbed_sampled_ll": -3.867117500305176, "perturbed_original_ll": -4.044437551498413, "perturbed_sampled_ll_std": 0.08527783320253979, "perturbed_original_ll_std": 0.2922815305204666}, {"original": "Remove redundant code from breeze initialization (#9375)", "sampled": "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "perturbed_sampled": ["Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure"], "perturbed_original": ["Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)", "Remove redundant code from breeze initialization (#9375)"], "original_ll": -6.790793418884277, "sampled_ll": -4.725438594818115, "all_perturbed_sampled_ll": [-4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115, -4.725438594818115], "all_perturbed_original_ll": [-6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277, -6.790793418884277], "perturbed_sampled_ll": -4.725438594818115, "perturbed_original_ll": -6.790793418884277, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the UI and are helpful for observability of Ariflow", "sampled": "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "perturbed_sampled": ["[AIRFLOW-4419] Restore used_slots and queued_slots from Airflow (#5210) These are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These methods are used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods that are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still needed by the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) which are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in Pool methods but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the API , but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are useful methods in the API but are not part of the code.\n\n[AIRFLOW-4421]", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) and methods (#5229) are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]"], "perturbed_original": ["[AIRFLOW-4419] Restore used_slots and Pool methods (#5210) These are still used in the UI and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the API, which are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the Ariflow pool classes and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool s. These are still used in the UI and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods . These pool methods are still used in the UI and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in Ariflow and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the current release and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods and queued pools are still used in the UI and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods . These are still used in the UI and are helpful for observability of Ariflow", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the UI , but not directly and are not helpful for observability of Ariflow"], "original_ll": -5.324529647827148, "sampled_ll": -3.6911561489105225, "all_perturbed_sampled_ll": [-3.44293212890625, -3.709038257598877, -3.6911561489105225, -3.6032321453094482, -3.778742551803589, -3.7362687587738037, -3.813530445098877, -3.783700942993164, -3.867666721343994, -3.5751001834869385], "all_perturbed_original_ll": [-5.879180908203125, -5.297651290893555, -4.979381084442139, -5.32710075378418, -5.122859001159668, -5.178517818450928, -5.252604007720947, -5.3033223152160645, -5.302242755889893, -5.229295253753662], "perturbed_sampled_ll": -3.700136828422546, "perturbed_original_ll": -5.287215518951416, "perturbed_sampled_ll_std": 0.12113238229040871, "perturbed_original_ll_std": 0.22152900804096537}, {"original": "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "sampled": "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "perturbed_sampled": ["Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In"], "perturbed_original": ["Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Allow hvac pakage installation using 'hashicorp' extra (#7915)"], "original_ll": -5.687289237976074, "sampled_ll": -6.11829137802124, "all_perturbed_sampled_ll": [-6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124, -6.11829137802124], "all_perturbed_original_ll": [-5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074, -5.687289237976074], "perturbed_sampled_ll": -6.11829137802124, "perturbed_original_ll": -5.687289237976074, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic.", "sampled": "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be better at", "perturbed_sampled": ["Need more information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask component is quite limited in connection, pools, variables and dag. So the docs should be better at", "Use generic information in UpdateMask component (#13146) The generic information is used in connection, pools, variables and dag. So the docs should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and object management, etc So the docs should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, tbl and dag. So the docs should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask is the component providing generic hints for connection, pools, variables and dag. So the docs should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be at", "Use generic information in UpdateMask component (#13146) The UpdateMask is used by the component to set pools, variables and dag. So the docs should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the function should be better at", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables . So the docs should be better at"], "perturbed_original": ["Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be generic.", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the UpdateMask could use to be more generic.", "Use generic information in UpdateMask documentation. The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic.", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and so on. So the docs should be more generic.", "Use generic information in UpdateMask component (#13146) The UpdateMask component uses generic information in connection, pools, variables and dag. So the docs should be more generic.", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic.", "Use generic information in UpdateMask component (#13146) ? This specific tool is used in connection, pools, variables and dag. So the docs should be more generic.", "Use all docs in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic.", "Use generic information in UpdateMask component (#13146) The UpdateMask is used for update multiple pools, variables and dag. So the docs should be more generic.", "Use generic information on the UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic."], "original_ll": -5.623473167419434, "sampled_ll": -5.845587730407715, "all_perturbed_sampled_ll": [-5.9149065017700195, -5.5909647941589355, -5.799673557281494, -5.612070560455322, -5.810755252838135, -5.732430934906006, -5.9819111824035645, -5.35470724105835, -5.8009443283081055, -5.901558876037598], "all_perturbed_original_ll": [-5.79907751083374, -5.297911167144775, -5.389787197113037, -4.992507457733154, -5.272276878356934, -5.623473167419434, -6.11051607131958, -5.515261173248291, -5.566336631774902, -5.441603183746338], "perturbed_sampled_ll": -5.749992322921753, "perturbed_original_ll": -5.500875043869018, "perturbed_sampled_ll_std": 0.17708269116529554, "perturbed_original_ll_std": 0.2911789992471804}, {"original": "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "sampled": "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "perturbed_sampled": ["Add Migration guide from the experimental API to the official API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added some more RPC", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Saha Added a new RPC", "Add Migration guide from the RESTful API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the experimental documentation of the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the JSON API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the Web API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the web API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaitlin Atticak <kaxil.natik@gmail.com> Added a new RPC", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> . Activate a new RPC"], "perturbed_original": ["Add Migration guide from the experimental API to the new REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> and Michel Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the REST API (#9771) by: Kapil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API into the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the stable API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental installation-guides (#9759) Enhance migrations by using Migration guides and enabling the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the REST API Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> , Miguel Laj <unk>mik-laj@gmail.com>, Jorge Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the standard API. (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add Migration guide from the experimental API to the Public API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>"], "original_ll": -3.3390846252441406, "sampled_ll": -3.9371025562286377, "all_perturbed_sampled_ll": [-3.833740711212158, -4.0212578773498535, -5.314290523529053, -3.646066904067993, -4.091416358947754, -3.734205722808838, -3.7115108966827393, -3.764885902404785, -4.512322902679443, -4.280567169189453], "all_perturbed_original_ll": [-3.3426640033721924, -3.779693603515625, -3.7028684616088867, -3.390730381011963, -3.257957696914673, -3.563704490661621, -3.283097743988037, -3.764064073562622, -3.2898058891296387, -3.342747211456299], "perturbed_sampled_ll": -4.091026496887207, "perturbed_original_ll": -3.4717333555221557, "perturbed_sampled_ll_std": 0.4854446288573909, "perturbed_original_ll_std": 0.1991024168872951}, {"original": "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "sampled": "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "perturbed_sampled": ["Change KPO node_selectors warning to show when node_selectors are removed. (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been made obsolete, see #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been declared as deprecated. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has deprecated. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change s warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate d KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPOe raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed from use. Fixed in #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` is removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove independencies from KPO_PREFIX_NAME Changes in dependencies from KPOE_PREFIX_NAME"], "perturbed_original": ["Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used, turns it into a `DeprecationWarning` and also simplifies that code path.", "Change KPO node_selectors warning to proper deprecationwarning . This turns the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Change KPO's default warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning displayed when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a proper deprecationwarning and also simplifies that code path.", "Change KPO node_selectors warning to proper deprecationwarning (#15507) - this update turns the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Change KPO node_selectors warning to deletion warning. (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Change s KPO warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", ") Switch node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises whenever this code is used into a `DeprecationWarning` and also simplifies that code path."], "original_ll": -4.508952617645264, "sampled_ll": -4.120049953460693, "all_perturbed_sampled_ll": [-3.7341790199279785, -4.268105983734131, -4.104438781738281, -4.277055740356445, -4.33231258392334, -4.310650825500488, -4.163327693939209, -4.2803568840026855, -4.163341999053955, -3.910743236541748], "all_perturbed_original_ll": [-4.273414611816406, -4.22228479385376, -4.513052940368652, -4.358710765838623, -4.652997016906738, -4.4706130027771, -4.5639872550964355, -4.759216785430908, -4.868305206298828, -5.024452209472656], "perturbed_sampled_ll": -4.1544512748718265, "perturbed_original_ll": -4.570703458786011, "perturbed_sampled_ll_std": 0.18398800954728758, "perturbed_original_ll_std": 0.24594067341981193}, {"original": "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.", "sampled": "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.\n\nAdded", "perturbed_sampled": ["Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have the wrong usage of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we might need to provide examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example . Added a couple places where we have incorrect examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where I think we have incorrect examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect ly created `extraSecrets`.\n\nAdded", "Chart docs: Fixed <unk>extraSecrets<unk> example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where there were incorrect examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places in chart docs that have incorrect examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` syntax in Chart docs. Found a couple places where we have incorrect examples of `extraSecrets`.\n\nAdded", "Chart docs: Fix ``extrasecrets`` example (#16305) A couple places where we have incorrect examples of `extraSecrets`.\n\nAdded"], "perturbed_original": ["Chart docs: Fix ``extrasecrets`` correctly! Found a couple places where we have incorrect examples of `extraSecrets`.", "Chart docs: Fix more errors with examples, examples. (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.", "Chart of the ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have n't used the right version of `extraSecrets`.", "Chart : ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect ly use `extraSecrets`.", "Chart docs: Fix ``extrasecrets`` example at a couple places where we have incorrect examples of `extraSecrets`.", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places that I wanted to correct. Chart docs have incorrect examples of `extraSecrets`.", "Chart docs: Fix ``extrasecrets`` example (#16305) : There are couple places where we have incorrect examples of `extraSecrets`.", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places that have incorrect examples of `extraSecrets`."], "original_ll": -5.29566764831543, "sampled_ll": -5.006393909454346, "all_perturbed_sampled_ll": [-4.806057453155518, -4.811290740966797, -5.189159870147705, -5.030969142913818, -5.538338661193848, -4.984248638153076, -4.893354892730713, -5.02968168258667, -5.053714752197266, -5.067794322967529], "all_perturbed_original_ll": [-5.573664665222168, -5.215927600860596, -5.243435859680176, -5.378197193145752, -5.253818988800049, -5.731429576873779, -5.5674824714660645, -5.452547073364258, -5.349108695983887, -5.435043811798096], "perturbed_sampled_ll": -5.040461015701294, "perturbed_original_ll": -5.420065593719483, "perturbed_sampled_ll_std": 0.20084487741652868, "perturbed_original_ll_std": 0.1583816577697124}, {"original": "Allow passing backend_kwargs to AWS SSM client (#8802)", "sampled": "Allow passing backend_kwargs to AWS SSM client (#8802)With", "perturbed_sampled": ["Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Allow passing backend_kwargs to AWS SSM client (#8802)With"], "perturbed_original": ["Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Allow passing backend_kwargs to AWS SSM client (#8802)"], "original_ll": -6.160957336425781, "sampled_ll": -6.720598220825195, "all_perturbed_sampled_ll": [-6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195, -6.720598220825195], "all_perturbed_original_ll": [-6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781, -6.160957336425781], "perturbed_sampled_ll": -6.720598220825195, "perturbed_original_ll": -6.160957336425781, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix kinesis test (#18337)", "sampled": "Fix kinesis test (#18337)On", "perturbed_sampled": ["Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On", "Fix kinesis test (#18337)On"], "perturbed_original": ["Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)", "Fix kinesis test (#18337)"], "original_ll": -6.094775199890137, "sampled_ll": -7.206113338470459, "all_perturbed_sampled_ll": [-7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459, -7.206113338470459], "all_perturbed_original_ll": [-6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137, -6.094775199890137], "perturbed_sampled_ll": -7.206113338470459, "perturbed_original_ll": -6.094775199890137, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clean up incorrect class names of Google system tests (#19956)", "sampled": "Clean up incorrect class names of Google system tests (#19956)A", "perturbed_sampled": ["Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A", "Clean up incorrect class names of Google system tests (#19956)A"], "perturbed_original": ["Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)", "Clean up incorrect class names of Google system tests (#19956)"], "original_ll": -6.384636402130127, "sampled_ll": -6.8947014808654785, "all_perturbed_sampled_ll": [-6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785, -6.8947014808654785], "all_perturbed_original_ll": [-6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127, -6.384636402130127], "perturbed_sampled_ll": -6.8947014808654785, "perturbed_original_ll": -6.384636402130127, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and above) to see all of the links.", "sampled": "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage.", "perturbed_sampled": ["Allow viewers to see all docs links (#14197) Currently, viewers can only see the previous link in the docs menu. This means that the users can't skip the docs of a webpage.", "Allow viewers to skip over docs . Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage.", "Allow viewers to see all page documentation (#14197) Currently, Viewers can only click on the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the viewer can skip the docs menu on a webpage.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the \"Viewer Info page\". This means that the users can't see all docs of a webpage.", "Allow viewers to see docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the text to see the description of a webpage.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only skip to the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage.", "Allow viewers to skip/displayed docs links (#14197) Currently, Viewers can only see the \"Documentation\" menu which is the docs menu. This means that the users can't skip the docs of a webpage.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the \"Documentations\" page of a webpage.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the webpage. This means that the users can only see the docs of a webpage."], "perturbed_original": ["Allow viewers to see all docs links (#14197) Currently, viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and above) to see all of the links.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This change allows Viewers (and above) to see all of the links.", "Allow viewers to see all of the links. (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers of all views to see all of the links.", "Allow Viewer to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR will enable Viewers on 0.9 (and above) to see all of the links.", "Viewers to see all docs menu links. Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and above) to see all of the links.", "Allow viewers to all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and other users) to see all of the links.", "Allow viewers to see all docs links . Currently Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and not administrators) to see all of the links.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the navigation menu. This PR allows Viewers (and also Developers) to see all of the links.", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the documents that are listed in the URL field. This PR allows Viewers (and above) to see all of the links.", "Allow viewers to see all links. (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This feature allows Viewers (and above) to see all of the links."], "original_ll": -3.4734418392181396, "sampled_ll": -3.4790704250335693, "all_perturbed_sampled_ll": [-3.7482845783233643, -3.357800006866455, -3.6292359828948975, -3.4837820529937744, -3.383715867996216, -3.4780144691467285, -3.630079746246338, -3.8804233074188232, -3.3745412826538086, -3.4551074504852295], "all_perturbed_original_ll": [-3.5565309524536133, -3.281107187271118, -3.2528598308563232, -3.3289906978607178, -3.332270622253418, -3.431875705718994, -3.457552194595337, -3.505063533782959, -3.682074546813965, -3.2943620681762695], "perturbed_sampled_ll": -3.5420984745025637, "perturbed_original_ll": -3.4122687339782716, "perturbed_sampled_ll_std": 0.1656838683456291, "perturbed_original_ll_std": 0.13210000949480905}, {"original": "Improve Google PubSub hook publish method (#7831)", "sampled": "Improve Google PubSub hook publish method (#7831)By", "perturbed_sampled": ["Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By", "Improve Google PubSub hook publish method (#7831)By"], "perturbed_original": ["Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)", "Improve Google PubSub hook publish method (#7831)"], "original_ll": -7.006582736968994, "sampled_ll": -7.602819919586182, "all_perturbed_sampled_ll": [-7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182, -7.602819919586182], "all_perturbed_original_ll": [-7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994, -7.006582736968994], "perturbed_sampled_ll": -7.602819919586182, "perturbed_original_ll": -7.006582736968994, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "sampled": "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "perturbed_sampled": ["Updates the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 - Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Updated the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Updates the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 Fixes\n\nFixed an issue where", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where"], "perturbed_original": ["Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update the default Airflow version ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to version 2.1.2 as it has been released.", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as version 2.1.2 has been released."], "original_ll": -3.213484764099121, "sampled_ll": -2.743013381958008, "all_perturbed_sampled_ll": [-2.7218096256256104, -2.743013381958008, -2.743013381958008, -2.743013381958008, -3.1926660537719727, -2.743013381958008, -2.717496156692505, -2.7218096256256104, -3.1830461025238037, -2.743013381958008], "all_perturbed_original_ll": [-3.213484764099121, -3.271479845046997, -3.213484764099121, -3.213484764099121, -3.3175137042999268, -3.271479845046997, -3.2367537021636963, -3.213484764099121, -3.213484764099121, -2.895230531692505], "perturbed_sampled_ll": -2.825189447402954, "perturbed_original_ll": -3.205988144874573, "perturbed_sampled_ll_std": 0.1816143139921272, "perturbed_original_ll_std": 0.10900187436599566}, {"original": "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "sampled": "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - do not use", "perturbed_sampled": ["Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of <unk> in the first line (#13484) - do not use", "Replace deprecated module and operator in example_tasks.py with new: `from airflow.operators.bash_operator import BashOperators` (#13476) - Fixed use of `.shcmd()` in some commands (#13484) - do not use", "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - Remove some restrictions on the use of `.shcmd()` in some commands (#13484) - Do not use", "Replace deprecated module and operator names (#13473) - `from airflow.operators.bash_operator import BashOperators` ; also fix the use of `.shcmd()` in some commands (#13484) - do not use", "Replace deprecated module by newer one in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - do not use", "Replace d by bash_operator and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - do not use", "Replace deprecated module and operator : (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - fix the use", "Replace deprecated module and operator in example_tasks.py (#13473) - do not import BashOperators` (#13476) - fix the use of a alias in some commands (#13484) - do not use", "Replace deprecated module and file example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - do not use", "Replace deprecated module <unk><unk>.hc in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the usage of `.shcmd()` in some commands (#13484) - do not use"], "perturbed_original": ["Replace deprecated module and operator in example_tasks.py (#13473) - `from user import BashOperator` to <unk>from user import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "the first module and correspondingly a new module was installed. example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "Replace deprecated module names in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - and (#) <unk>from airflow import chain` to `from airflow.models.baseoperator import chain`", "Replace deprecated module and operator import chains (#13473) - `from airflow.operators.bash_operator import BashOperator` to <unk>from airflow.operators.bash_operator import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import chain<unk> to `from airflow.operators.bash import chain<unk>; `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "Replace deprecated module and operator in build. - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.operators.bash import chain` to `from airflow.models.baseoperator import chain`", "Replace deprecated module and operator in example_tasks.py (#13473) - <unk>from airflow.utils import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import chain<unk> to `from airflow.models.baseoperator import chain`", "for each module and operator in example_tasks.py (#13473) - <unk>from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator <unk> to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import helpers<unk> to `from airflow.models.baseoperator import chain`", "Replace deprecated module and operator in package - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import BashOperator<unk> to `from airflow.models.baseoperator import chain`"], "original_ll": -3.35652494430542, "sampled_ll": -3.733330488204956, "all_perturbed_sampled_ll": [-3.950653553009033, -3.9041755199432373, -3.743849992752075, -4.410675525665283, -3.7523200511932373, -3.7551169395446777, -3.9777090549468994, -3.9994633197784424, -3.80745005607605, -3.970909595489502], "all_perturbed_original_ll": [-4.085870742797852, -3.489675760269165, -4.08350944519043, -3.7001864910125732, -3.7806339263916016, -3.1694912910461426, -4.188506126403809, -3.8914554119110107, -3.9239559173583984, -3.5892410278320312], "perturbed_sampled_ll": -3.927232360839844, "perturbed_original_ll": -3.790252614021301, "perturbed_sampled_ll_std": 0.18779384985896896, "perturbed_original_ll_std": 0.2973911961371652}, {"original": "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installation for quite some time at least verbally but also in the Helm Chart documentation. However we missed such recommendation in the general Postgres area of 'Setting Up the database` doc. This PR adds a note that we can refer to when explaining problems with connections and stability to the users who use Postgres without PGBouncer proxy (which is known to help in such cases)", "sampled": "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installations, for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in OpenWrt-1.9.0 (#18465) When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not", "perturbed_sampled": ["Add PGBouncer recommendation in \"setup-database' . We were recommending using PGBouncer for all Postgres installations, for clarity. We also simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in OpenWrt-1.9.0 (#18465) When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured by a user. (#18485) The \"--force pkg:openSSH-client\" has been disabled in Pkgfile in PGCouncer after this is not", "to use in our Postgres recommendation and php doc. (#18399) We were recommending using PGBouncer for all Postgres installations, for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used . (#18465) We are now limited to OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The directive \"use pkg:openSSH-client\" has been removed from the Pkgfile in v2.19. If this is not", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installations, with the caveat that they need to use the database engine. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Improve PGCouncer and PGBouncer recommendation . This has been used in OpenWrt-1.9.0 . After specifying the following configuration parameters in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile . If this is not", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installations for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer recommendation now uses a PGBouncer database engine. OpenSSH keys are still used in OpenWrt-1.9.0 (#18465) We will use OpenSSH keys in PGBouncer because we are still limited by what is configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres workloads for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and SQLServer recommendation are still used in OpenWrt-1.9.0 (#18465) If you enable OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are limited by OpenSSH keys configured in /etc/pki/. (#18485) \"pkg_openssh -W pkg:openSSH-client\" has been disabled for PHP in v2.19. If this is not", "Add link in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres for clarity. Now we simply recommend PGBouncer. (#18399) Our PGCouncer recommendation is still used with PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in PGCouncer's database engine. When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still using the OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not", "Add PGBouncer recommendation in \"setup-database' . (#18397) We were recommending using PGBouncer for all Postgres installations, including Postgres 5. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation and uses PGCouncer's database engine. (#18402) Our PGCouncer installation recommendation is used in OpenWrt-1.9.0 . (#18415) While specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not", "Add PGBouncer recommendation to doc. (#18399) We were recommending using PGBouncer for all Postgres installations, for clarity. Now we simply recommend PostgreSQL. (#18399) Our PGCouncer recommendation now reflects the PGCouncer database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in OpenWrt-1.9.0 (#18465) When specifying the PKI in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for PHP in v2.19. If this is not", "Add PGBouncer recommendation to doc. (#18399) We were recommending using PGBouncer for all Postgres installations, for clarity. Now we recommend using PGBouncer. Our PGBouncer and PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in OpenWrt-1.9.0 (#18465) When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by the key configured in systemd. The \"systemd -W pkg:openSSH-client\" has been disabled for v2.19. Not enabled in v2.19. If this is not", "and new recommendation in \"setup-database' doc. (#18399) We were using PGBouncer for all Postgres installations, before this. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's recommendation. (#18402) Our PGCouncer and PGBouncer recommendation are still used in different places, for different installations. When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -trunk\" argument has been disabled for the Pkgfile in v2.19. If this is not"], "perturbed_original": ["Add recommendation in \"setup-database' doc. (#18399) We were recommending using Helm Chart for all Postgres installation for quite some time at least verbally but also in the Helm Chart documentation. However we missed such recommendation in the general Postgres area of 'Setting Up the database` in the Helm Chart documentation. Thus this PR adds a note that we can refer to when addressing problems with connections and stability to the users who use Postgres without Helm Chart (which is known to help in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for Postgres installation for quite some time at least verbally but also in the Helm ut Repository. However we missed such recommendation in the general Postgres documentation, especially in 'Setting Up Postgres Database' doc. This PR adds a note that we can use when explaining problems with connections and stability to the users who use Postgres without PGBouncer proxy (which is known to help in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. We were recommending using PGBouncer for all Postgres installation for some time at least verbally but also in the Helm Chart s, but we missed such recommendation in the general Postgres area of 'Setting Up the database` doc. This PR is simply to add a note that we can refer to when faced with connections and stability issues from users who use Postgres without PGBouncer proxy (which is known to help in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installation for quite some time at least verbally but also in the Helm Chart documentation. However we never actually included that recommendation in the docs in the Postgres area of 'Setting Up Database\u2019 doc. This PR adds a note that we can refer to when explaining how this will improve connections and stability to the users who use Postgres without PGBouncer proxy (which is supposed to help in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending PGBouncer for all Postgres installation for quite some time at least verbally but also in RFCs and in Postgres Chart documentation. However we missed such recommendation in the general Postgres area in the <unk>Setting Up the database` doc. This PR adds a note that we can refer to provide a note about problems with connections and stability to the users who use Postgres without PGBouncer installed (it is known to fail in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending PGBouncer for all Postgres for quite some time, at least verbally but also in the Helm Chart documentation. However we missed such recommendation in the general Postgres area of 'Setting up database` doc. This PR adds a note that we can refer to when explaining problems with connections and stability issues to users who use Postgres without PGBouncer proxy (eg; PGBouncer is already known to help in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer proxy on all Postgres installation for quite some time at least verbally but also in the Helm Chart . After that recommendation we added recommendation in the general Postgres area of 'Setting Up the database` doc. This PR adds a note that we can refer to when we have issues with connections and stability to the database and where we can use the PGBouncer proxy (which is known to help in such cases)", "Add PGBouncer recommendation in Postgres/Helm Chart installation doc. (#18399) We were recommending using PGBouncer for all Postgres installation for quite some time now, at least verbally but also in the Helm Chart documentation. However we missed such recommendation in the same area of 'Setting Up the database` doc. This PR adds a note that we can give help and suggestions when faced with connections and stability to the users who use the PGBouncer proxy (which is known to help in such cases)", "Add PGBouncer in Postgres \"setup-database' doc. (#18399) We were recommending using PGBouncer in all Postgres installation for quite some time at least verbally but also in PPG Chart documentation. But we missed adding a mention to this in the general Postgres area of 'Setting Up the database` doc. This PR adds a note that we can refer to when explaining problems related to database performance and stability to the users who use Postgres without PGBouncer proxy (which is known to help in such cases)", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installation for quite some time at least verbally but not in print or in the Helm Chart documentation. I missed such recommendation in the general Postgres area of Helm Chart and therefore the Postgres docs. This PR adds a note that we can refer to when explaining problems with connections and updates for the users who use Postgres to connect through the HTTP proxy (which is known to help in such cases)"], "original_ll": -4.387980937957764, "sampled_ll": -2.8348398208618164, "all_perturbed_sampled_ll": [-3.148005485534668, -2.8607828617095947, -3.041200876235962, -2.9475934505462646, -2.962918519973755, -2.87800931930542, -3.0209741592407227, -3.0397274494171143, -2.957069158554077, -3.1135413646698], "all_perturbed_original_ll": [-4.376868724822998, -4.249365329742432, -4.38512659072876, -4.4377312660217285, -4.2923150062561035, -4.2147440910339355, -4.298574924468994, -4.285378932952881, -4.212268352508545, -4.397598743438721], "perturbed_sampled_ll": -2.9969822645187376, "perturbed_original_ll": -4.31499719619751, "perturbed_sampled_ll_std": 0.08868459464003633, "perturbed_original_ll_std": 0.07563447766456219}, {"original": "Add template fields to neo4j operator (#20043)", "sampled": "Add template fields to neo4j operator (#20043)In", "perturbed_sampled": ["Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In", "Add template fields to neo4j operator (#20043)In"], "perturbed_original": ["Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)", "Add template fields to neo4j operator (#20043)"], "original_ll": -6.545008659362793, "sampled_ll": -7.165198802947998, "all_perturbed_sampled_ll": [-7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998, -7.165198802947998], "all_perturbed_original_ll": [-6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793, -6.545008659362793], "perturbed_sampled_ll": -7.165198802947998, "perturbed_original_ll": -6.545008659362793, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in example (#13321) False should not be passed as a string", "sampled": "Fix typo in example (#13321) False should not be passed as a stringDescription", "perturbed_sampled": ["Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Fix typo in example (#13321) False should not be passed as a stringDescription"], "perturbed_original": ["Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string", "Fix typo in example (#13321) False should not be passed as a string"], "original_ll": -5.053643226623535, "sampled_ll": -5.650160312652588, "all_perturbed_sampled_ll": [-5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588, -5.650160312652588], "all_perturbed_original_ll": [-5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535, -5.053643226623535], "perturbed_sampled_ll": -5.650160312652588, "perturbed_original_ll": -5.053643226623535, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "sampled": "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "perturbed_sampled": ["Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded"], "perturbed_original": ["Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E"], "original_ll": -4.646146774291992, "sampled_ll": -4.444993019104004, "all_perturbed_sampled_ll": [-4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004, -4.444993019104004], "all_perturbed_original_ll": [-4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992, -4.646146774291992], "perturbed_sampled_ll": -4.444993019104004, "perturbed_original_ll": -4.646146774291992, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "sampled": "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "perturbed_sampled": ["[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The"], "perturbed_original": ["[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)"], "original_ll": -6.095162391662598, "sampled_ll": -6.4268951416015625, "all_perturbed_sampled_ll": [-6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625, -6.4268951416015625], "all_perturbed_original_ll": [-6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598, -6.095162391662598], "perturbed_sampled_ll": -6.4268951416015625, "perturbed_original_ll": -6.095162391662598, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard names for new users endpoints. * Document user access. * Remove unused tests. * Make roles tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "sampled": "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "perturbed_sampled": ["Standardize default custom view test. (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add custom views tests. * fix missing clear permission. * remove obsolete custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "* Ensure fab perms (#14946) * Add back wards compatible tests. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom view test. -------------- * add a custom view test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize default fab tests. * Add back changes. * Add custom view classes tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * remove obsolete test. * remove obsolete test.", "Standardize default fab perms (#14946) which enables rolling back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize default fab perms (#14946) * Add back changes. Add custom view class tests. * Cover missing clear permissions. Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * fix missing clear permission. * fix missing clear permission. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize default fab perms (#14946) - Roll back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize default fab perms (#14946) * Add custom view classes tests. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize default fab perms (#14946) * Roll back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- Add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "Standardize view perms (#14946) * Fix changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * fix missing clear permission. * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test."], "perturbed_original": ["Standardize default fab . * Add back changes. * Add permission class tests. * Cover missing clear permission. * Remove some of mappings. * Add the rest of the mappings. * Fix permission validation. * Fix permission names. * Use standard names for new users endpoints. * Document user access. * Remove permission tests. * Make roles tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "Standardize default fab perms (#14946) * Add back changes. * Add custom perms. * Add new tests. * Test clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use the same for new users endpoints. * Document user access. * Remove unused tests. * Make tests pass by cleaning test roles from test_views.py. * Remove old tests. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "Standardize default fab perms (#14946) * Make the needed changes. * Add custom view class tests. * Cover missing clear permission. * Add the rest of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard names for new users endpoints. * Document user endpoints. * Remove unused tests. * Make roles tests pass by cleaning test _views.py and testing test_views.py. * Remove existing permission names. * Update role tests. * Reorder permissions. * Update roles tests. * Remove old permissions. * Remove db merge.", "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Add a clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Replace with correct names for new users endpoints. * Document user access. * Remove unused views. * Make roles tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Update role tests. * Update role tests. * Remove RESOURCE_ROLE_MODEL_VIEW. * Test merge.", "Standardize default fab perms (#14946) * Add back up role to index. * Add custom view class . * Cover missing clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix user definitions. * Fix permission names. * Use standard names for new users endpoints. * Document user access. * Remove unused roles. * Make roles more useful by cleaning test roles from test_views.py. * Remove old permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing features. * Add the rest of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard ize new users endpoints. * Document user access. * Remove unused tests. * Make roles tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Add web role tests. * Clean permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "Standardize default fab perms (#14946) * Add back changes. * Add some class tests. * Cover missing clear permission. * Add some new mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Add standard names for new users endpoints. * Document user access. * Remove unused tests. * Make roles tests pass . test roles from test_views.py. * Remove old permission names. * Update role schema. * Reorder permissions. * Remove old permissions. Remove db merge.", "Standardize default fab perms (#14946) * Add back changes. * Add custom view class . * Cover missing clear permission. * Add some of the mappings. * Add the rest of the mappings. * Add explicit permission names. * Fix permission names. * Use the standard names . * Remove unnecessary users endpoints. * Document user access. * Remove unused user roles. * Make roles tests pass by cleaning up some data from test_views.py. * Remove old permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "Standardize default fab perms (#14946) * Add back end classes tests. * Add custom view class tests. * Add missing clear permission. * Add some of the mappings. * Add the new permission mappings in. * Add some of the mappings. * Fix permission names. * Fix permission names. * Use standard names for new users . * Fixed permission names. * Document user access. * Remove unused tests. * Make roles tests pass by cleaning test from test_views.py. * Update permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "Standardize default fab perms (#14946) * Add back changes. * Add view class tests. * Cover missing clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard names for new users endpoints. * Use standard user access. * Remove role tests. * Make roles tests pass by cleaning roles up. * Remove permission tests from test_views.py. * Clear old permission names. * Update role tests. * Remove authorization tests. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge."], "original_ll": -3.7460806369781494, "sampled_ll": -2.2837164402008057, "all_perturbed_sampled_ll": [-2.0136799812316895, -2.43048357963562, -2.1016294956207275, -2.1654720306396484, -2.3395285606384277, -2.1893529891967773, -2.1996750831604004, -2.0770153999328613, -2.212888479232788, -1.9084402322769165], "all_perturbed_original_ll": [-3.823167324066162, -3.468261241912842, -3.459228515625, -3.6110079288482666, -3.898554801940918, -3.7883851528167725, -3.8952784538269043, -3.7693591117858887, -3.5068063735961914, -3.6166908740997314], "perturbed_sampled_ll": -2.163816583156586, "perturbed_original_ll": -3.6836739778518677, "perturbed_sampled_ll_std": 0.14335078537628818, "perturbed_original_ll_std": 0.16326133694537623}, {"original": "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged", "sampled": "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging", "perturbed_sampled": ["[AIRFLOW-5444] Fix action_logging so that request.form for POST is available, not request.values. (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is clear. (#6055) Log request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is reported only. Log request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged along with request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for a form is logged (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is included. (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values if user has multiple requests (#6054 Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values if user fills forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged but it will ignore request.values if user has multiple forms. (#6057) Fix action_logging", "[AIRFLOW-5444] Ensure actions log correctly, so that request.form for POST is logged (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging"], "perturbed_original": ["[AIRFLOW-5444] Fix action_logging on action.page to make sure request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged", "[AIRFLOW-5444] Fix so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged", "[AIRFLOW-5444] Log request.forms so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is properly logged. [AIRFLOW-5449] Log request.values so both GET and POST are properly logged", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged correctly. [AIRFLOW-5448] Fix request.values so both GET and POST are properly logged", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values so both GET and POST requests from NS functions are properly logged", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values so FOLLOW and POST are properly logged", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) and so both GET and POST are properly logged", "[AIRFLOW-5444] Fix action_logging so all request_values and request_actions for POST is logged (#6064) Log request.values so both GET and POST are properly logged"], "original_ll": -5.176823616027832, "sampled_ll": -4.477470874786377, "all_perturbed_sampled_ll": [-4.263115406036377, -4.360409259796143, -4.837255001068115, -4.415210723876953, -4.330671787261963, -4.381533145904541, -5.04046106338501, -4.737823963165283, -4.554481506347656, -4.931825637817383], "all_perturbed_original_ll": [-5.198517322540283, -5.2547926902771, -5.021309852600098, -4.053481101989746, -3.9959166049957275, -5.33109188079834, -5.176823616027832, -5.317041873931885, -5.078836917877197, -4.937072277069092], "perturbed_sampled_ll": -4.585278749465942, "perturbed_original_ll": -4.93648841381073, "perturbed_sampled_ll_std": 0.2654452571140942, "perturbed_original_ll_std": 0.47118000353068357}], "metrics": {"roc_auc": 0.6023540000000001, "fpr": [0.0, 0.0, 0.0, 0.002, 0.002, 0.01, 0.01, 0.012, 0.012, 0.014, 0.014, 0.016, 0.016, 0.018, 0.018, 0.02, 0.02, 0.022, 0.022, 0.024, 0.024, 0.03, 0.03, 0.032, 0.032, 0.034, 0.034, 0.036, 0.036, 0.038, 0.038, 0.042, 0.042, 0.044, 0.044, 0.046, 0.046, 0.048, 0.048, 0.05, 0.05, 0.056, 0.056, 0.058, 0.058, 0.064, 0.064, 0.066, 0.066, 0.068, 0.068, 0.07, 0.07, 0.078, 0.078, 0.08, 0.08, 0.084, 0.084, 0.086, 0.086, 0.088, 0.088, 0.092, 0.092, 0.094, 0.094, 0.096, 0.096, 0.098, 0.098, 0.104, 0.104, 0.108, 0.108, 0.11, 0.11, 0.112, 0.112, 0.116, 0.116, 0.118, 0.118, 0.122, 0.122, 0.132, 0.132, 0.136, 0.136, 0.138, 0.138, 0.14, 0.14, 0.15, 0.15, 0.162, 0.162, 0.178, 0.178, 0.18, 0.18, 0.182, 0.182, 0.184, 0.184, 0.188, 0.188, 0.19, 0.19, 0.192, 0.192, 0.194, 0.194, 0.196, 0.196, 0.204, 0.204, 0.21, 0.21, 0.216, 0.216, 0.22, 0.22, 0.224, 0.224, 0.226, 0.226, 0.232, 0.232, 0.236, 0.236, 0.248, 0.248, 0.256, 0.256, 0.262, 0.262, 0.268, 0.268, 0.27, 0.828, 0.828, 0.832, 0.832, 0.846, 0.846, 0.852, 0.852, 0.854, 0.854, 0.866, 0.866, 0.87, 0.87, 0.876, 0.876, 0.878, 0.878, 0.882, 0.882, 0.884, 0.884, 0.892, 0.892, 0.912, 0.912, 0.914, 0.914, 0.916, 0.916, 0.926, 0.926, 0.942, 0.942, 0.948, 0.948, 0.958, 0.958, 0.96, 0.96, 0.962, 0.962, 0.97, 0.97, 0.98, 0.98, 0.982, 0.982, 0.984, 0.984, 0.99, 0.99, 1.0], "tpr": [0.0, 0.002, 0.016, 0.016, 0.024, 0.024, 0.044, 0.044, 0.06, 0.06, 0.08, 0.08, 0.082, 0.082, 0.108, 0.108, 0.116, 0.116, 0.134, 0.134, 0.144, 0.144, 0.15, 0.15, 0.152, 0.152, 0.156, 0.156, 0.164, 0.164, 0.17, 0.17, 0.18, 0.18, 0.186, 0.186, 0.188, 0.188, 0.19, 0.19, 0.192, 0.192, 0.194, 0.194, 0.206, 0.206, 0.208, 0.208, 0.21, 0.21, 0.212, 0.212, 0.216, 0.216, 0.22, 0.22, 0.224, 0.224, 0.228, 0.228, 0.23, 0.23, 0.232, 0.232, 0.24, 0.24, 0.244, 0.244, 0.248, 0.248, 0.254, 0.254, 0.256, 0.256, 0.26, 0.26, 0.262, 0.262, 0.264, 0.264, 0.276, 0.276, 0.284, 0.284, 0.286, 0.286, 0.294, 0.294, 0.298, 0.298, 0.302, 0.302, 0.304, 0.304, 0.31, 0.31, 0.314, 0.314, 0.316, 0.316, 0.318, 0.318, 0.324, 0.324, 0.326, 0.326, 0.328, 0.328, 0.33, 0.33, 0.334, 0.334, 0.338, 0.338, 0.34, 0.34, 0.342, 0.342, 0.344, 0.344, 0.346, 0.346, 0.348, 0.348, 0.35, 0.35, 0.352, 0.352, 0.36, 0.36, 0.362, 0.362, 0.368, 0.368, 0.37, 0.37, 0.372, 0.372, 0.374, 0.374, 0.932, 0.934, 0.934, 0.94, 0.94, 0.942, 0.942, 0.944, 0.944, 0.946, 0.946, 0.948, 0.948, 0.95, 0.95, 0.952, 0.952, 0.954, 0.954, 0.956, 0.956, 0.96, 0.96, 0.962, 0.962, 0.964, 0.964, 0.966, 0.966, 0.972, 0.972, 0.974, 0.974, 0.976, 0.976, 0.978, 0.978, 0.98, 0.98, 0.982, 0.982, 0.984, 0.984, 0.986, 0.986, 0.988, 0.988, 0.992, 0.992, 0.998, 0.998, 1.0, 1.0]}, "pr_metrics": {"pr_auc": 0.6306692433496267, "precision": [0.5, 0.5005005005005005, 0.501002004008016, 0.5015045135406219, 0.5020080321285141, 0.5025125628140703, 0.5020120724346077, 0.5025176233635448, 0.5030241935483871, 0.5035317860746721, 0.503030303030303, 0.5025278058645096, 0.5020242914979757, 0.502532928064843, 0.5020283975659229, 0.5015228426395939, 0.5020325203252033, 0.5015259409969481, 0.5020366598778004, 0.5025484199796126, 0.503061224489796, 0.5035750766087844, 0.5040899795501023, 0.503582395087001, 0.5040983606557377, 0.5046153846153846, 0.5051334702258727, 0.5056526207605344, 0.5051440329218106, 0.505664263645726, 0.5051546391752577, 0.5056759545923633, 0.5051652892561983, 0.5056876938986556, 0.5062111801242236, 0.5067357512953368, 0.5072614107883817, 0.5077881619937694, 0.5072765072765073, 0.5078043704474505, 0.5083333333333333, 0.5088633993743483, 0.5083507306889353, 0.5088819226750261, 0.5094142259414226, 0.5099476439790576, 0.510482180293501, 0.5110178384050367, 0.5115546218487395, 0.5120925341745531, 0.5126315789473684, 0.512118018967334, 0.5126582278481012, 0.5131995776135164, 0.5137420718816068, 0.5142857142857142, 0.5148305084745762, 0.5143160127253447, 0.5138004246284501, 0.5132837407013815, 0.5138297872340426, 0.5133120340788072, 0.5138592750533049, 0.5133404482390609, 0.5138888888888888, 0.5144385026737968, 0.5149892933618844, 0.5155412647374062, 0.5160944206008584, 0.5166487647690655, 0.5172043010752688, 0.5177610333692142, 0.5183189655172413, 0.5188781014023732, 0.5183585313174947, 0.518918918918919, 0.5194805194805194, 0.5200433369447454, 0.5206073752711496, 0.5200868621064061, 0.5195652173913043, 0.5201305767138193, 0.5196078431372549, 0.520174482006543, 0.5207423580786026, 0.5202185792349727, 0.5207877461706784, 0.52026286966046, 0.5208333333333334, 0.5214050493962679, 0.521978021978022, 0.5214521452145214, 0.5220264317180616, 0.5226019845644984, 0.522075055187638, 0.5226519337016574, 0.5232300884955752, 0.5238095238095238, 0.524390243902439, 0.5249722530521642, 0.5255555555555556, 0.525027808676307, 0.5256124721603563, 0.5250836120401338, 0.5256696428571429, 0.5262569832402234, 0.5268456375838926, 0.5263157894736842, 0.5269058295964125, 0.5274971941638609, 0.5280898876404494, 0.5286839145106862, 0.5292792792792793, 0.5298759864712514, 0.5304740406320542, 0.5299435028248588, 0.5294117647058824, 0.5288788221970555, 0.5294784580498866, 0.5300794551645857, 0.5295454545454545, 0.5807453416149069, 0.5825545171339563, 0.58125, 0.5830721003134797, 0.5849056603773585, 0.5867507886435331, 0.5854430379746836, 0.5873015873015873, 0.589171974522293, 0.5910543130990416, 0.5897435897435898, 0.5916398713826366, 0.5935483870967742, 0.5954692556634305, 0.5974025974025974, 0.5960912052117264, 0.5947712418300654, 0.5934426229508196, 0.5953947368421053, 0.5973597359735974, 0.5993377483443708, 0.6013289036544851, 0.6033333333333334, 0.6053511705685619, 0.6040268456375839, 0.6060606060606061, 0.6081081081081081, 0.6067796610169491, 0.6054421768707483, 0.6040955631399317, 0.6027397260273972, 0.6048109965635738, 0.6068965517241379, 0.6089965397923875, 0.6076388888888888, 0.6097560975609756, 0.6083916083916084, 0.6105263157894737, 0.6126760563380281, 0.6113074204946997, 0.6134751773049646, 0.6156583629893239, 0.6142857142857143, 0.6164874551971327, 0.6187050359712231, 0.6209386281588448, 0.6195652173913043, 0.6218181818181818, 0.6240875912408759, 0.6263736263736264, 0.625, 0.6273062730627307, 0.6296296296296297, 0.6319702602230484, 0.6343283582089553, 0.6329588014981273, 0.6353383458646616, 0.6339622641509434, 0.6325757575757576, 0.6349809885931559, 0.6335877862595419, 0.632183908045977, 0.6346153846153846, 0.6332046332046332, 0.6356589147286822, 0.6342412451361867, 0.63671875, 0.6392156862745098, 0.6377952755905512, 0.6403162055335968, 0.6388888888888888, 0.6374501992031872, 0.636, 0.6385542168674698, 0.6370967741935484, 0.6396761133603239, 0.6382113821138211, 0.6408163265306123, 0.6434426229508197, 0.6460905349794238, 0.6487603305785123, 0.6514522821576764, 0.6541666666666667, 0.6569037656903766, 0.6596638655462185, 0.6582278481012658, 0.6567796610169492, 0.6595744680851063, 0.6623931623931624, 0.6652360515021459, 0.6681034482758621, 0.670995670995671, 0.6739130434782609, 0.6724890829694323, 0.6710526315789473, 0.6696035242290749, 0.672566371681416, 0.6755555555555556, 0.6785714285714286, 0.6816143497757847, 0.6846846846846847, 0.6832579185520362, 0.6863636363636364, 0.684931506849315, 0.6834862385321101, 0.6866359447004609, 0.6851851851851852, 0.6837209302325581, 0.6869158878504673, 0.6901408450704225, 0.6886792452830188, 0.6872037914691943, 0.6857142857142857, 0.6842105263157895, 0.6875, 0.6908212560386473, 0.6941747572815534, 0.697560975609756, 0.7009803921568627, 0.6995073891625616, 0.7029702970297029, 0.7064676616915423, 0.705, 0.7035175879396985, 0.702020202020202, 0.700507614213198, 0.7040816326530612, 0.7025641025641025, 0.7010309278350515, 0.6994818652849741, 0.6979166666666666, 0.6963350785340314, 0.6947368421052632, 0.6984126984126984, 0.7021276595744681, 0.7005347593582888, 0.7043010752688172, 0.7027027027027027, 0.7065217391304348, 0.7049180327868853, 0.7032967032967034, 0.7071823204419889, 0.7111111111111111, 0.7094972067039106, 0.7134831460674157, 0.7175141242937854, 0.7215909090909091, 0.72, 0.7183908045977011, 0.7167630057803468, 0.7209302325581395, 0.7192982456140351, 0.7176470588235294, 0.7218934911242604, 0.7202380952380952, 0.718562874251497, 0.7228915662650602, 0.7212121212121212, 0.7195121951219512, 0.7177914110429447, 0.7160493827160493, 0.7204968944099379, 0.725, 0.7232704402515723, 0.7278481012658228, 0.7261146496815286, 0.7307692307692307, 0.7290322580645161, 0.7272727272727273, 0.7320261437908496, 0.7368421052631579, 0.7350993377483444, 0.7333333333333333, 0.738255033557047, 0.7364864864864865, 0.7346938775510204, 0.7397260273972602, 0.7448275862068966, 0.75, 0.7552447552447552, 0.7535211267605634, 0.75177304964539, 0.7571428571428571, 0.7553956834532374, 0.7608695652173914, 0.7591240875912408, 0.7647058823529411, 0.762962962962963, 0.7686567164179104, 0.7744360902255639, 0.7803030303030303, 0.7786259541984732, 0.7769230769230769, 0.7751937984496124, 0.7734375, 0.7716535433070866, 0.7698412698412699, 0.776, 0.7741935483870968, 0.7804878048780488, 0.7868852459016393, 0.7933884297520661, 0.7916666666666666, 0.7983193277310925, 0.7966101694915254, 0.8034188034188035, 0.8017241379310345, 0.808695652173913, 0.8070175438596491, 0.8053097345132744, 0.8035714285714286, 0.8108108108108109, 0.8090909090909091, 0.8073394495412844, 0.8055555555555556, 0.8037383177570093, 0.8018867924528302, 0.8095238095238095, 0.8173076923076923, 0.8155339805825242, 0.8137254901960784, 0.8118811881188119, 0.82, 0.8181818181818182, 0.8163265306122449, 0.8144329896907216, 0.8125, 0.8210526315789474, 0.8191489361702128, 0.8172043010752689, 0.8260869565217391, 0.8241758241758241, 0.8333333333333334, 0.8314606741573034, 0.8295454545454546, 0.8275862068965517, 0.8372093023255814, 0.8470588235294118, 0.8571428571428571, 0.8554216867469879, 0.8536585365853658, 0.8518518518518519, 0.85, 0.8481012658227848, 0.8589743589743589, 0.8571428571428571, 0.8552631578947368, 0.8533333333333334, 0.8513513513513513, 0.8493150684931506, 0.8472222222222222, 0.8450704225352113, 0.8428571428571429, 0.8405797101449275, 0.8529411764705882, 0.8507462686567164, 0.8484848484848485, 0.8461538461538461, 0.84375, 0.8571428571428571, 0.8548387096774194, 0.8524590163934426, 0.85, 0.847457627118644, 0.8448275862068966, 0.8421052631578947, 0.8392857142857143, 0.8363636363636363, 0.8333333333333334, 0.8301886792452831, 0.8269230769230769, 0.8235294117647058, 0.82, 0.8367346938775511, 0.8333333333333334, 0.851063829787234, 0.8478260869565217, 0.8444444444444444, 0.8409090909090909, 0.8372093023255814, 0.8333333333333334, 0.8292682926829268, 0.825, 0.8205128205128205, 0.8157894736842105, 0.8108108108108109, 0.8333333333333334, 0.8285714285714286, 0.8235294117647058, 0.8181818181818182, 0.8125, 0.8064516129032258, 0.8, 0.7931034482758621, 0.7857142857142857, 0.8148148148148148, 0.8076923076923077, 0.8, 0.7916666666666666, 0.782608695652174, 0.7727272727272727, 0.7619047619047619, 0.75, 0.7368421052631579, 0.7222222222222222, 0.7058823529411765, 0.75, 0.8, 0.8571428571428571, 0.9230769230769231, 0.9166666666666666, 0.9090909090909091, 0.9, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "recall": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998, 0.998, 0.998, 0.998, 0.996, 0.994, 0.992, 0.992, 0.99, 0.988, 0.988, 0.986, 0.986, 0.986, 0.986, 0.986, 0.986, 0.984, 0.984, 0.984, 0.984, 0.984, 0.982, 0.982, 0.98, 0.98, 0.978, 0.978, 0.978, 0.978, 0.978, 0.978, 0.976, 0.976, 0.976, 0.976, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.97, 0.968, 0.966, 0.966, 0.964, 0.964, 0.962, 0.962, 0.962, 0.962, 0.962, 0.962, 0.962, 0.962, 0.962, 0.962, 0.962, 0.96, 0.96, 0.96, 0.96, 0.96, 0.958, 0.956, 0.956, 0.954, 0.954, 0.954, 0.952, 0.952, 0.95, 0.95, 0.95, 0.95, 0.948, 0.948, 0.948, 0.946, 0.946, 0.946, 0.946, 0.946, 0.946, 0.946, 0.944, 0.944, 0.942, 0.942, 0.942, 0.942, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.938, 0.936, 0.934, 0.934, 0.934, 0.932, 0.374, 0.374, 0.372, 0.372, 0.372, 0.372, 0.37, 0.37, 0.37, 0.37, 0.368, 0.368, 0.368, 0.368, 0.368, 0.366, 0.364, 0.362, 0.362, 0.362, 0.362, 0.362, 0.362, 0.362, 0.36, 0.36, 0.36, 0.358, 0.356, 0.354, 0.352, 0.352, 0.352, 0.352, 0.35, 0.35, 0.348, 0.348, 0.348, 0.346, 0.346, 0.346, 0.344, 0.344, 0.344, 0.344, 0.342, 0.342, 0.342, 0.342, 0.34, 0.34, 0.34, 0.34, 0.34, 0.338, 0.338, 0.336, 0.334, 0.334, 0.332, 0.33, 0.33, 0.328, 0.328, 0.326, 0.326, 0.326, 0.324, 0.324, 0.322, 0.32, 0.318, 0.318, 0.316, 0.316, 0.314, 0.314, 0.314, 0.314, 0.314, 0.314, 0.314, 0.314, 0.314, 0.312, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.308, 0.306, 0.304, 0.304, 0.304, 0.304, 0.304, 0.304, 0.302, 0.302, 0.3, 0.298, 0.298, 0.296, 0.294, 0.294, 0.294, 0.292, 0.29, 0.288, 0.286, 0.286, 0.286, 0.286, 0.286, 0.286, 0.284, 0.284, 0.284, 0.282, 0.28, 0.278, 0.276, 0.276, 0.274, 0.272, 0.27, 0.268, 0.266, 0.264, 0.264, 0.264, 0.262, 0.262, 0.26, 0.26, 0.258, 0.256, 0.256, 0.256, 0.254, 0.254, 0.254, 0.254, 0.252, 0.25, 0.248, 0.248, 0.246, 0.244, 0.244, 0.242, 0.24, 0.24, 0.238, 0.236, 0.234, 0.232, 0.232, 0.232, 0.23, 0.23, 0.228, 0.228, 0.226, 0.224, 0.224, 0.224, 0.222, 0.22, 0.22, 0.218, 0.216, 0.216, 0.216, 0.216, 0.216, 0.214, 0.212, 0.212, 0.21, 0.21, 0.208, 0.208, 0.206, 0.206, 0.206, 0.206, 0.204, 0.202, 0.2, 0.198, 0.196, 0.194, 0.194, 0.192, 0.192, 0.192, 0.192, 0.19, 0.19, 0.188, 0.188, 0.186, 0.186, 0.184, 0.182, 0.18, 0.18, 0.178, 0.176, 0.174, 0.172, 0.17, 0.17, 0.17, 0.168, 0.166, 0.164, 0.164, 0.162, 0.16, 0.158, 0.156, 0.156, 0.154, 0.152, 0.152, 0.15, 0.15, 0.148, 0.146, 0.144, 0.144, 0.144, 0.144, 0.142, 0.14, 0.138, 0.136, 0.134, 0.134, 0.132, 0.13, 0.128, 0.126, 0.124, 0.122, 0.12, 0.118, 0.116, 0.116, 0.114, 0.112, 0.11, 0.108, 0.108, 0.106, 0.104, 0.102, 0.1, 0.098, 0.096, 0.094, 0.092, 0.09, 0.088, 0.086, 0.084, 0.082, 0.082, 0.08, 0.08, 0.078, 0.076, 0.074, 0.072, 0.07, 0.068, 0.066, 0.064, 0.062, 0.06, 0.06, 0.058, 0.056, 0.054, 0.052, 0.05, 0.048, 0.046, 0.044, 0.044, 0.042, 0.04, 0.038, 0.036, 0.034, 0.032, 0.03, 0.028, 0.026, 0.024, 0.024, 0.024, 0.024, 0.024, 0.022, 0.02, 0.018, 0.016, 0.016, 0.014, 0.012, 0.01, 0.008, 0.006, 0.004, 0.002, 0.0]}, "loss": 0.3693307566503733}