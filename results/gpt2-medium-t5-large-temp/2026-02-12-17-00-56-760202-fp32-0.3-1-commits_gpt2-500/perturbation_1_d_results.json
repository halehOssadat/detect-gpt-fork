{"name": "perturbation_1_d", "predictions": {"real": [0.0, 0.0, 0.06696534156799316, 0.0, 0.0, 0.0, 0.0, 0.18781661987304688, 0.0893259048461914, 0.15532636642456055, 0.0, -0.16495656967163086, -0.01942586898803711, 0.0, 0.0, -0.2636563777923584, 0.20129966735839844, 0.25632715225219727, 0.0, -0.016270160675048828, 0.04167819023132324, 0.0, 0.0, 0.0, 0.0, 0.7037434577941895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.23468875885009766, 0.031789541244506836, 0.0, -0.0767970085144043, 0.0, 0.0, 0.062168121337890625, 0.0, -0.18546462059020996, 0.0, 0.0, -0.1731858253479004, -0.012240886688232422, 0.0, -0.08060431480407715, 0.0, 0.0, 0.23759841918945312, 0.0, 0.12587356567382812, 0.0, 0.11078977584838867, 0.34349966049194336, -0.44948601722717285, 0.1995067596435547, 0.0, 0.0, 0.0, 0.3820030689239502, 0.0, 0.0, 0.03928041458129883, 0.0, 0.017692089080810547, -0.3324427604675293, -0.4842510223388672, 0.0, 0.0, 0.29498839378356934, 0.0, 0.4524064064025879, 0.0, 0.0, 0.0, 0.0, 0.2757077217102051, 0.0, 0.0, -0.23490643501281738, 0.0, 0.0, 0.0, 0.0, 0.0003528594970703125, -0.16904592514038086, 0.0, -0.01777505874633789, -0.02506542205810547, 0.0, 0.13060593605041504, 0.0, 0.0, 0.09502792358398438, 0.0, -0.13235712051391602, 0.0, 0.0, -0.24664711952209473, 0.0, 0.0, 0.0, 0.0, 0.03723335266113281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014448165893554688, 0.0, 0.0, 0.04016685485839844, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2073221206665039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008540630340576172, 0.0, 0.0, 0.0, 0.09652924537658691, 0.0, 0.12314176559448242, 0.0, 0.0, 0.0, 0.0, 0.0, -0.35062742233276367, 0.30022478103637695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25704503059387207, 0.0, 0.0, 0.07243537902832031, 0.0, 0.0, 0.7794589996337891, 0.017214298248291016, -0.14430761337280273, 0.2861354351043701, 0.23244524002075195, 0.2729380130767822, 0.3317294120788574, 0.004525661468505859, 0.0, 0.02391672134399414, 0.025043964385986328, -0.2914466857910156, 0.0, 0.030869483947753906, -0.6247682571411133, 0.0, 0.0, 0.010682106018066406, -0.04330849647521973, 0.25005149841308594, -0.06303596496582031, 0.0, 0.24137520790100098, 0.12054824829101562, -0.24955344200134277, 0.0, -0.3049349784851074, 0.0, 0.13606953620910645, 0.0, 0.5625438690185547, 0.1279311180114746, 0.0, 0.0, 0.74420166015625, 0.3117082118988037, -0.3882021903991699, 0.0, 0.0, 0.38929319381713867, 0.14765024185180664, 0.0, 0.0, 0.0, -0.12033939361572266, 0.0, 0.0, -0.19805288314819336, 0.0, 0.0, 0.0, -0.06619715690612793, 0.0, 0.0, 0.0, -0.16062712669372559, 0.08589529991149902, -0.03378558158874512, 0.0, 0.0, 0.0, 0.0, 0.0, -0.012150287628173828, 0.0, 0.0, 0.0, 0.0, 0.03266716003417969, -0.05090188980102539, 0.0, 0.18645405769348145, 0.0, 0.09745550155639648, 0.0, 0.007353067398071289, 0.0, 0.0, 0.0, -0.07156896591186523, 0.0316009521484375, 0.1381063461303711, -0.06564474105834961, -0.12578225135803223, 0.0, 0.0, 0.0, 0.05038332939147949, -0.36830878257751465, 0.3546640872955322, 0.3938894271850586, 0.0, -0.04131889343261719, -0.10572981834411621, -0.1987168788909912, -0.4996204376220703, 0.0, 0.0, 0.45918941497802734, 0.0, 0.0, 0.045247554779052734, 0.07686138153076172, -0.18056344985961914, 0.0, -0.03562664985656738, 0.0, 0.0, 0.24967002868652344, 0.0, 0.34487128257751465, 0.29096317291259766, 0.0, 0.013285636901855469, 0.09031486511230469, 0.0, 0.0, 0.0, 0.0, -0.0027265548706054688, -0.2493293285369873, 0.4075775146484375, 0.0, 0.0761098861694336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10236454010009766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3667724132537842, 0.23239898681640625, 0.0, 0.0, 0.00719451904296875, 0.0, -0.12262225151062012, 0.04567861557006836, 0.19977807998657227, 0.0, -0.17156314849853516, 0.021146774291992188, -0.25358104705810547, 0.38007068634033203, 0.0, 0.0, 0.15808749198913574, 0.0, -0.04539966583251953, -0.17607736587524414, -0.004672050476074219, 0.0, 0.2989826202392578, -0.05512714385986328, 0.057062625885009766, 0.0, 0.0, 0.03682518005371094, 0.0, -0.03187060356140137, 0.32753872871398926, 0.0, -0.30043578147888184, 0.0, 0.010917186737060547, 0.0, 0.0, 0.0, 0.40711116790771484, -0.21932458877563477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03867912292480469, 0.0, 0.0, 0.21926498413085938, 0.0, 0.0, 0.08992648124694824, 0.0, 0.34479427337646484, 0.0, 0.0, 0.21724653244018555, -0.023964881896972656, 0.0, 0.0, 0.0, 0.0, 0.13547086715698242, 0.0, 0.0, 0.0, 0.0, 0.06990981101989746, -0.031203269958496094, 0.18118619918823242, 0.0, 0.0, 0.0, -0.21747446060180664, -0.04305267333984375, -0.20526838302612305, 0.0, 0.0, -0.005828142166137695, 0.0, 0.0, 0.0, 0.18057584762573242, 0.0, 0.0, 0.0, 0.0, 0.012410879135131836, 0.0013856887817382812, 0.0, -0.05683088302612305, 0.6427738666534424, 0.1717362403869629, 0.0, -0.04067516326904297, -0.41815662384033203, 0.0, 0.06602859497070312, 0.0, 0.0, -0.0755612850189209, 0.0, 0.0, 0.0, 0.0, -0.3843097686767578, 0.0, -0.33384013175964355, 0.0, -0.08359575271606445, 0.0, 0.0, 0.0, 0.45892953872680664, -0.10437679290771484, 0.0, 0.0, 1.7907941341400146, -0.17266297340393066, 0.0, 0.026964664459228516, 0.06134176254272461, 0.12395334243774414, 0.2861506938934326, 0.0, 0.0, 0.0, 0.4913363456726074, 0.0, 0.0, 0.0, 0.0, -0.23987913131713867, 0.0, -0.03451967239379883, -0.0511322021484375, 0.0, 0.7909603118896484, 0.0, 0.0, 0.18413352966308594, 0.0, 0.0, 0.0, -0.0468447208404541, 0.0, 0.0, 0.0, 0.0, 0.17012524604797363, 0.0, 0.0, -0.255124568939209, 0.0, 0.0, 0.0, 0.0, 0.006942272186279297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.052561283111572266, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1771101951599121, -0.16569185256958008, 0.18956446647644043, 0.31082677841186523, 0.0, 0.0, 0.0, 0.191650390625, 0.0, 0.0, 0.0, 0.15886545181274414, 0.07281875610351562, 0.05361127853393555, 0.0, 0.38251352310180664, 0.0, -0.03647947311401367, 0.0, -0.29033994674682617, -0.1890428066253662, 0.09385490417480469, 0.26122522354125977, 0.0, 0.0, 0.0, 0.3645312786102295, 0.0, 0.0819089412689209, 0.9279847145080566, -0.41005778312683105, 0.0, 0.0, 0.0, 0.0, -0.4336214065551758, -0.3846454620361328], "samples": [0.0, 0.0, 0.03000044822692871, 0.0, 0.0, 0.0, 0.0, 0.004948616027832031, -0.022532939910888672, -0.4305438995361328, 0.0, 0.00885462760925293, 0.16327571868896484, 0.0, 0.0, 0.17785882949829102, 0.5247094631195068, 0.19246864318847656, 0.0, 0.003990888595581055, -0.12929511070251465, 0.0, 0.0, 0.0, 0.0, 0.13859272003173828, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.22255992889404297, 0.250913143157959, 0.0, 0.181992769241333, 0.0, -0.08017539978027344, 0.2122642993927002, 0.0, 0.5046212673187256, 0.0, 0.0, -0.06058549880981445, 0.06353878974914551, 0.0, 0.20610451698303223, 0.0, 0.0, -0.019022464752197266, 0.0, 0.0, 0.0, 0.025282859802246094, -0.13806629180908203, 0.011421680450439453, 0.11886096000671387, 0.0, 0.0, 0.0, 0.04888486862182617, 0.0, 0.0, -0.3403940200805664, 0.0, 0.2826138734817505, 0.21497249603271484, -0.054590702056884766, 0.0, 0.0, 0.08647918701171875, 0.0, -0.03955888748168945, 0.0, 0.0, 0.0, 0.0, -0.16904664039611816, 0.0, 0.0, 0.1982865333557129, 0.0, 0.0, 0.0, 0.0, -0.05665755271911621, 0.3845329284667969, 0.0, -0.2642524242401123, 0.25017738342285156, 0.0, -0.12553858757019043, 0.0, 0.0, -0.05222606658935547, 0.0, -0.41919946670532227, 0.0, 0.0, -0.033963680267333984, 0.0, 0.0, 0.0, 0.0, 0.0045948028564453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4780914783477783, 0.0, 0.0, -0.681422233581543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04830026626586914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06316900253295898, 0.0, 0.0, 0.0, 0.25324273109436035, 0.0, 0.010206937789916992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16417407989501953, 0.09828829765319824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0011391639709472656, 0.0, 0.0, -0.0992279052734375, 0.0, 0.0, 0.2728455066680908, 0.1868581771850586, -0.08515429496765137, 0.14098739624023438, -0.008820056915283203, 0.33861589431762695, -0.03310441970825195, 0.10169267654418945, 0.0, 0.21141505241394043, 0.23090648651123047, -0.12280797958374023, 0.0, 0.3591649532318115, -0.47921276092529297, 0.0, 0.0, 0.13266468048095703, 0.30956006050109863, 0.298799991607666, 0.05705404281616211, 0.0, 0.0385744571685791, 0.13760852813720703, 0.5005826950073242, 0.0, -0.1698017120361328, 0.0, 0.09146523475646973, 0.0, 0.07401573657989502, 0.17647075653076172, 0.0, 0.0, 0.8532466888427734, 0.08865714073181152, -0.5325174331665039, 0.0, 0.0, 1.0729789733886719, -0.009418010711669922, 0.0, 0.0, 0.0, 0.1236720085144043, 0.0, 0.0, 0.06210756301879883, 0.0, 0.0, 0.0, 0.4098830223083496, 0.0, 0.0, 0.0, -0.06218385696411133, 0.04545426368713379, -0.011081933975219727, 0.0, 0.0, 0.0, 0.0, 0.0, -0.432431697845459, 0.0, 0.0, 0.0, 0.0, 0.017208099365234375, -0.17425298690795898, 0.0, 0.0, 0.0, 0.3207693099975586, 0.0, 0.5196154117584229, 0.0, 0.0, 0.0, 0.2689814567565918, 0.3686521053314209, 0.42875170707702637, 0.4551961421966553, 0.17299985885620117, 0.0, 0.0, 0.0, 0.08781886100769043, -0.5357513427734375, 0.236098051071167, -0.15266990661621094, 0.0, 0.015014171600341797, -0.055582523345947266, -0.14743030071258545, 0.0076694488525390625, 0.0, 0.0, 0.005317211151123047, 0.0, 0.0, 0.20693111419677734, 1.050811529159546, -0.16378068923950195, 0.0, 0.32466816902160645, 0.0, 0.0, -0.10305380821228027, 0.0, 0.07144451141357422, 0.22599148750305176, 0.0, 0.05271601676940918, -0.09909200668334961, 0.0, 0.0, 0.0, 0.0, 0.031977176666259766, 0.07912707328796387, 0.18132448196411133, 0.0, 0.050925254821777344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.47326111793518066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05730104446411133, 0.22584104537963867, 0.0, 0.0, -0.10756039619445801, 0.0, 0.21542906761169434, 0.09120535850524902, 0.34822678565979004, 0.0, 0.04535198211669922, -0.04754281044006348, 0.34371519088745117, 0.0, 0.0, 0.0, 0.09579300880432129, 0.0, -0.16019105911254883, 0.00809478759765625, 0.5605928897857666, 0.0, -0.005751609802246094, -0.11344575881958008, -0.12171363830566406, 0.0, 0.0, -0.13064146041870117, 0.0, 0.0, 0.14806056022644043, 0.0, 0.0, 0.0, 0.007040977478027344, 0.0, 0.0, 0.0, 0.14513397216796875, 0.20544636249542236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218958854675293, 0.0, 0.0, 0.023090362548828125, 0.0, 0.0, 0.10479545593261719, 0.0, 0.08865475654602051, 0.0, 0.0, 0.011201620101928711, 0.6056931018829346, 0.0, 0.0, 0.0, 0.0, 0.14735960960388184, 0.0, 0.0, 0.08288908004760742, 0.0, 0.03005695343017578, 0.11382031440734863, 0.1476597785949707, 0.0, 0.0, 0.0, -0.04350423812866211, 0.0, -0.08270978927612305, 0.0, 0.0, 0.0707249641418457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16079187393188477, -0.07109689712524414, 0.0, 0.0899806022644043, 0.5432205200195312, 0.28789639472961426, 0.0, 0.473569393157959, 0.045350074768066406, 0.0, 0.16185927391052246, 0.0, 0.0, 0.2724573612213135, 0.0, 0.0, 0.0, 0.0, 0.5108819007873535, 0.0, 0.047222137451171875, 0.0, -0.1994922161102295, 0.0, 0.0, 0.0, 0.7888011932373047, 0.06927967071533203, 0.5944294929504395, 0.0, 0.08760857582092285, 0.5431993007659912, 0.035535573959350586, 0.025461912155151367, -0.12164425849914551, -0.03795790672302246, 0.3539440631866455, 0.0, 0.0, 0.0, 0.08617806434631348, 0.0, 0.0, 0.0, 0.0, -0.06862735748291016, 0.0, 0.9549603462219238, 0.09477400779724121, 0.0, 0.2350025177001953, 0.0, 0.0, 0.11700582504272461, 0.0, 0.0, 0.0, -0.11547613143920898, 0.0, 0.0, 0.0, 0.0, -0.03919672966003418, 0.0, 0.0, 0.2768442630767822, 0.0, 0.0, 0.0, 0.0, 0.6737842559814453, 0.0, 0.0, -0.22069358825683594, 0.0, 0.0, 0.0, 0.2648148536682129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027431249618530273, -0.11067318916320801, 0.33267641067504883, 0.40481090545654297, 0.0, 0.0, 0.0, 0.0816335678100586, 0.0, 0.0, 0.0, -0.22910237312316895, 0.6200695037841797, -0.012415885925292969, 0.0, 0.09471845626831055, 0.0, 0.14252519607543945, 0.0, -0.3525733947753906, 0.251143217086792, 0.21194934844970703, 0.5405726432800293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.025489330291748047, 0.36908864974975586, 0.0, 0.0, 0.0, 0.0, -0.05183982849121094, 0.6642460823059082]}, "info": {"pct_words_masked": 0.3, "span_length": 2, "n_perturbations": 1, "n_samples": 500}, "raw_results": [{"original": "Correct command for starting Celery Flower (#9483)", "sampled": "Correct command for starting Celery Flower (#9483)This", "perturbed_sampled": ["Correct command for starting Celery Flower (#9483)This"], "perturbed_original": ["Correct command for starting Celery Flower (#9483)"], "original_ll": -6.312844753265381, "sampled_ll": -6.824895858764648, "all_perturbed_sampled_ll": [-6.824895858764648], "all_perturbed_original_ll": [-6.312844753265381], "perturbed_sampled_ll": -6.824895858764648, "perturbed_original_ll": -6.312844753265381, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixing mypy issues inside tests model (#20026)", "sampled": "Fixing mypy issues inside tests model (#20026)Coding", "perturbed_sampled": ["Fixing mypy issues inside tests model (#20026)Coding"], "perturbed_original": ["Fixing mypy issues inside tests model (#20026)"], "original_ll": -6.958688259124756, "sampled_ll": -7.136562824249268, "all_perturbed_sampled_ll": [-7.136562824249268], "all_perturbed_original_ll": [-6.958688259124756], "perturbed_sampled_ll": -7.136562824249268, "perturbed_original_ll": -6.958688259124756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "sampled": "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "perturbed_sampled": ["Make the podoperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Kubernetes 25 and that this is a separate PR, so earlier versions of Kubernetes will not be affected"], "perturbed_original": ["Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the effort of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to be upgraded using previous versions and the same APIs. * clear custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10"], "original_ll": -3.1624696254730225, "sampled_ll": -3.173940658569336, "all_perturbed_sampled_ll": [-3.2039411067962646], "all_perturbed_original_ll": [-3.2294349670410156], "perturbed_sampled_ll": -3.2039411067962646, "perturbed_original_ll": -3.2294349670410156, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "sampled": "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "perturbed_sampled": ["[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One"], "perturbed_original": ["[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)"], "original_ll": -5.5672454833984375, "sampled_ll": -5.9538984298706055, "all_perturbed_sampled_ll": [-5.9538984298706055], "all_perturbed_original_ll": [-5.5672454833984375], "perturbed_sampled_ll": -5.9538984298706055, "perturbed_original_ll": -5.5672454833984375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix spelling (#11404)", "sampled": "Fix spelling (#11404)This", "perturbed_sampled": ["Fix spelling (#11404)This"], "perturbed_original": ["Fix spelling (#11404)"], "original_ll": -6.915605068206787, "sampled_ll": -8.098139762878418, "all_perturbed_sampled_ll": [-8.098139762878418], "all_perturbed_original_ll": [-6.915605068206787], "perturbed_sampled_ll": -8.098139762878418, "perturbed_original_ll": -6.915605068206787, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "sampled": "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "perturbed_sampled": ["[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash"], "perturbed_original": ["[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)"], "original_ll": -6.001043796539307, "sampled_ll": -6.292699813842773, "all_perturbed_sampled_ll": [-6.292699813842773], "all_perturbed_original_ll": [-6.001043796539307], "perturbed_sampled_ll": -6.292699813842773, "perturbed_original_ll": -6.001043796539307, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5614] Enable Fernet by default (#6282)", "sampled": "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "perturbed_sampled": ["[AIRFLOW-5614] Enable Fernet by default (#6282)One"], "perturbed_original": ["[AIRFLOW-5614] Enable Fernet by default (#6282)"], "original_ll": -5.7969746589660645, "sampled_ll": -6.356044769287109, "all_perturbed_sampled_ll": [-6.356044769287109], "all_perturbed_original_ll": [-5.7969746589660645], "perturbed_sampled_ll": -6.356044769287109, "perturbed_original_ll": -5.7969746589660645, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was rised during git add.", "sampled": "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "perturbed_sampled": ["Make Cloud Build er setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash"], "perturbed_original": ["Make Cloud Build system tests setup runnable (#10692) This change seems to be associated with open(quickstart.sh): Permission denied that was rised during git add."], "original_ll": -5.5679779052734375, "sampled_ll": -4.530129432678223, "all_perturbed_sampled_ll": [-4.535078048706055], "all_perturbed_original_ll": [-5.755794525146484], "perturbed_sampled_ll": -4.535078048706055, "perturbed_original_ll": -5.755794525146484, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it for now. This is captured in #18777", "sampled": "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining more than one adult animal at a time", "perturbed_sampled": ["Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining more than one animal at a time"], "perturbed_original": ["Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it for now. Quarantining fixes captured in #18777"], "original_ll": -4.738646507263184, "sampled_ll": -4.5175275802612305, "all_perturbed_sampled_ll": [-4.494994640350342], "all_perturbed_original_ll": [-4.827972412109375], "perturbed_sampled_ll": -4.494994640350342, "perturbed_original_ll": -4.827972412109375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company", "sampled": "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen", "perturbed_sampled": ["Update Thumbtack points to: You are not listed in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen"], "perturbed_original": ["The company moved the points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company"], "original_ll": -5.169980049133301, "sampled_ll": -5.450675010681152, "all_perturbed_sampled_ll": [-5.0201311111450195], "all_perturbed_original_ll": [-5.325306415557861], "perturbed_sampled_ll": -5.0201311111450195, "perturbed_original_ll": -5.325306415557861, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Mark trigger-controller-dag test as xfail (#8015)", "sampled": "Mark trigger-controller-dag test as xfail (#8015)The", "perturbed_sampled": ["Mark trigger-controller-dag test as xfail (#8015)The"], "perturbed_original": ["Mark trigger-controller-dag test as xfail (#8015)"], "original_ll": -7.303321838378906, "sampled_ll": -7.655359745025635, "all_perturbed_sampled_ll": [-7.655359745025635], "all_perturbed_original_ll": [-7.303321838378906], "perturbed_sampled_ll": -7.655359745025635, "perturbed_original_ll": -7.303321838378906, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "sampled": "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "perturbed_sampled": ["[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on some changes to the way we run Kubernetes, we decided to try to figure out how to handle some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for awhile. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\""], "perturbed_original": ["[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of Breeze parameters - a check of the Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)"], "original_ll": -4.156164169311523, "sampled_ll": -2.9110028743743896, "all_perturbed_sampled_ll": [-2.9198575019836426], "all_perturbed_original_ll": [-3.9912075996398926], "perturbed_sampled_ll": -2.9198575019836426, "perturbed_original_ll": -3.9912075996398926, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only used for development, not production.", "sampled": "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You can create subplanets when adding a sub-column to existing charts or sub-colors in", "perturbed_sampled": ["Chart: Update postgres subchart to 10.5.3 <unk> We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these two tricks to plot your sub-data: Note You can create subplanets when adding a new plot to any existing charts or sub-colors in"], "perturbed_original": ["Chart: Update postgres subchart to latest version. We were on 6.3.12 and the current latest version is 10.5.3. We have the necessary postgres support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only be used in development, not production."], "original_ll": -3.8848581314086914, "sampled_ll": -3.266268491744995, "all_perturbed_sampled_ll": [-3.42954421043396], "all_perturbed_original_ll": [-3.8654322624206543], "perturbed_sampled_ll": -3.42954421043396, "perturbed_original_ll": -3.8654322624206543, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Enable Black on Connexion API folders (#10545)", "sampled": "Enable Black on Connexion API folders (#10545)The", "perturbed_sampled": ["Enable Black on Connexion API folders (#10545)The"], "perturbed_original": ["Enable Black on Connexion API folders (#10545)"], "original_ll": -6.662216663360596, "sampled_ll": -7.110589981079102, "all_perturbed_sampled_ll": [-7.110589981079102], "all_perturbed_original_ll": [-6.662216663360596], "perturbed_sampled_ll": -7.110589981079102, "perturbed_original_ll": -6.662216663360596, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix documentation for provider's release (#14654)", "sampled": "Fix documentation for provider's release (#14654)We", "perturbed_sampled": ["Fix documentation for provider's release (#14654)We"], "perturbed_original": ["Fix documentation for provider's release (#14654)"], "original_ll": -6.447770595550537, "sampled_ll": -7.337686061859131, "all_perturbed_sampled_ll": [-7.337686061859131], "all_perturbed_original_ll": [-6.447770595550537], "perturbed_sampled_ll": -7.337686061859131, "perturbed_original_ll": -6.447770595550537, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the third party action.", "sampled": "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodule update commands to update only those we need... which doesn't require extra dependencies at all. This requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "perturbed_sampled": ["Run \"third party\" github actions from submodules instead (#13514) Rather than having to maintain all the repos we can instead use git submodule update commands to change those we need... which doesn't require extra dependencies at all. This requires only a couple of code changes (a single dependency) required to be passed down to each new repo, as opposed to manually re-compiling changes we made before installing to the repo, so can be implemented more quickly in many cases. (#13515) Add"], "perturbed_original": ["Run \"third party\" github actions code instead - rather than having to modify the repos we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the repo to use the third party action."], "original_ll": -3.680201292037964, "sampled_ll": -3.4457101821899414, "all_perturbed_sampled_ll": [-3.6235690116882324], "all_perturbed_original_ll": [-3.4165449142456055], "perturbed_sampled_ll": -3.6235690116882324, "perturbed_original_ll": -3.4165449142456055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can take forever where it needs to be compiled. So for first-time users this can be a turn off. If pandas is already installed this will work fine, but if not users have an option to run `pip install apache-airflow[pandas]` closes #12500", "sampled": "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "perturbed_sampled": ["Make `pandas` an optional core dependency (#17575) We only use <unk>pandas<unk> in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17526) is needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards exceptions if"], "perturbed_original": ["pandas is an optional dependency. (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, and even though `pandas` now supports many pre-compiled packages it still can 't use that where it needs to be compiled. So for first-time users this can be a turn off. If pandas is already installed this will work fine, but if not users have an option to do <unk>ps.deb install apache-airflow[pandas]` closes #12500"], "original_ll": -3.469585657119751, "sampled_ll": -2.036691427230835, "all_perturbed_sampled_ll": [-2.561400890350342], "all_perturbed_original_ll": [-3.6708853244781494], "perturbed_sampled_ll": -2.561400890350342, "perturbed_original_ll": -3.6708853244781494, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * add test to all executors * fix test", "sampled": "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *", "perturbed_sampled": ["[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove missing import * fix missing plugin *"], "perturbed_original": ["[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * export test to all executors * fix test"], "original_ll": -4.977297782897949, "sampled_ll": -4.835750579833984, "all_perturbed_sampled_ll": [-5.028219223022461], "all_perturbed_original_ll": [-5.2336249351501465], "perturbed_sampled_ll": -5.028219223022461, "perturbed_original_ll": -5.2336249351501465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "sampled": "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "perturbed_sampled": ["Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As"], "perturbed_original": ["Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)"], "original_ll": -6.876770496368408, "sampled_ll": -7.274985313415527, "all_perturbed_sampled_ll": [-7.274985313415527], "all_perturbed_original_ll": [-6.876770496368408], "perturbed_sampled_ll": -7.274985313415527, "perturbed_original_ll": -6.876770496368408, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result on custom XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` as it handled this directly.", "sampled": "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as while iterating over a property list or a list of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "perturbed_sampled": ["Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method extensively (such as while iterating through a property list or a list of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: Added support in the code for retrieving"], "perturbed_original": ["Make `XCom.get_one` return full, not abbreviated values (#18274) If we used this class method directly (such as in a custom operator ), the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result . This isn't currently the case for legacy XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` which handled this directly."], "original_ll": -4.204904556274414, "sampled_ll": -3.348747968673706, "all_perturbed_sampled_ll": [-3.352738857269287], "all_perturbed_original_ll": [-4.188634395599365], "perturbed_sampled_ll": -3.352738857269287, "perturbed_original_ll": -4.188634395599365, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1", "sampled": "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "perturbed_sampled": ["Upgrade FAB to 3.1.1 (#11884) We can now use the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types"], "perturbed_original": ["Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) that is merged and release in FAB 3.1.1"], "original_ll": -3.8137173652648926, "sampled_ll": -3.6194043159484863, "all_perturbed_sampled_ll": [-3.4901092052459717], "all_perturbed_original_ll": [-3.855395555496216], "perturbed_sampled_ll": -3.4901092052459717, "perturbed_original_ll": -3.855395555496216, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "sampled": "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "perturbed_sampled": ["[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For"], "perturbed_original": ["[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)"], "original_ll": -5.319941997528076, "sampled_ll": -5.743818759918213, "all_perturbed_sampled_ll": [-5.743818759918213], "all_perturbed_original_ll": [-5.319941997528076], "perturbed_sampled_ll": -5.743818759918213, "perturbed_original_ll": -5.319941997528076, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-7041] make bowler dependency local (#7691)", "sampled": "[AIRFLOW-7041] make bowler dependency local (#7691)How", "perturbed_sampled": ["[AIRFLOW-7041] make bowler dependency local (#7691)How"], "perturbed_original": ["[AIRFLOW-7041] make bowler dependency local (#7691)"], "original_ll": -6.978909492492676, "sampled_ll": -7.5300517082214355, "all_perturbed_sampled_ll": [-7.5300517082214355], "all_perturbed_original_ll": [-6.978909492492676], "perturbed_sampled_ll": -7.5300517082214355, "perturbed_original_ll": -6.978909492492676, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix Python Docstring parameters (#12513)", "sampled": "Fix Python Docstring parameters (#12513)The", "perturbed_sampled": ["Fix Python Docstring parameters (#12513)The"], "perturbed_original": ["Fix Python Docstring parameters (#12513)"], "original_ll": -6.606939315795898, "sampled_ll": -7.411397457122803, "all_perturbed_sampled_ll": [-7.411397457122803], "all_perturbed_original_ll": [-6.606939315795898], "perturbed_sampled_ll": -7.411397457122803, "perturbed_original_ll": -6.606939315795898, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "sampled": "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "perturbed_sampled": ["Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The"], "perturbed_original": ["Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)"], "original_ll": -6.3373517990112305, "sampled_ll": -6.62135648727417, "all_perturbed_sampled_ll": [-6.62135648727417], "all_perturbed_original_ll": [-6.3373517990112305], "perturbed_sampled_ll": -6.62135648727417, "perturbed_original_ll": -6.3373517990112305, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not start due to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "sampled": "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "perturbed_sampled": ["[AIRFLOW-6624] Create command with pidfile checking (#7245) * [AIRFLOW-6624] Create command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may"], "perturbed_original": ["[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * Improve webserver command with pidfile checking * Fixup! For install webserver as daemon it can happend that it will not start due to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! Improve webserver command with pidfile checking"], "original_ll": -3.0690603256225586, "sampled_ll": -2.5040814876556396, "all_perturbed_sampled_ll": [-2.642674207687378], "all_perturbed_original_ll": [-3.772803783416748], "perturbed_sampled_ll": -2.642674207687378, "perturbed_original_ll": -3.772803783416748, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "UX Enhancement: Add button to clear search query from DAG search (#11583)", "sampled": "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "perturbed_sampled": ["UX Enhancement: Add button to clear search query from DAG search (#11583)I"], "perturbed_original": ["UX Enhancement: Add button to clear search query from DAG search (#11583)"], "original_ll": -5.474490642547607, "sampled_ll": -6.062567710876465, "all_perturbed_sampled_ll": [-6.062567710876465], "all_perturbed_original_ll": [-5.474490642547607], "perturbed_sampled_ll": -6.062567710876465, "perturbed_original_ll": -5.474490642547607, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "sampled": "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "perturbed_sampled": ["[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get"], "perturbed_original": ["[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)"], "original_ll": -5.80667781829834, "sampled_ll": -6.214869022369385, "all_perturbed_sampled_ll": [-6.214869022369385], "all_perturbed_original_ll": [-5.80667781829834], "perturbed_sampled_ll": -6.214869022369385, "perturbed_original_ll": -5.80667781829834, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "sampled": "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "perturbed_sampled": ["[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint"], "perturbed_original": ["[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)"], "original_ll": -5.340404510498047, "sampled_ll": -5.649989128112793, "all_perturbed_sampled_ll": [-5.649989128112793], "all_perturbed_original_ll": [-5.340404510498047], "perturbed_sampled_ll": -5.649989128112793, "perturbed_original_ll": -5.340404510498047, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6635] Speed up static checks (#7256)", "sampled": "[AIRFLOW-6635] Speed up static checks (#7256)By", "perturbed_sampled": ["[AIRFLOW-6635] Speed up static checks (#7256)By"], "perturbed_original": ["[AIRFLOW-6635] Speed up static checks (#7256)"], "original_ll": -5.906435489654541, "sampled_ll": -6.376589775085449, "all_perturbed_sampled_ll": [-6.376589775085449], "all_perturbed_original_ll": [-5.906435489654541], "perturbed_sampled_ll": -6.376589775085449, "perturbed_original_ll": -5.906435489654541, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "sampled": "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "perturbed_sampled": ["[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The"], "perturbed_original": ["[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)"], "original_ll": -5.909849166870117, "sampled_ll": -6.192701816558838, "all_perturbed_sampled_ll": [-6.192701816558838], "all_perturbed_original_ll": [-5.909849166870117], "perturbed_sampled_ll": -6.192701816558838, "perturbed_original_ll": -5.909849166870117, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "sampled": "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "perturbed_sampled": ["[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The"], "perturbed_original": ["[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)"], "original_ll": -5.948135852813721, "sampled_ll": -6.146110534667969, "all_perturbed_sampled_ll": [-6.146110534667969], "all_perturbed_original_ll": [-5.948135852813721], "perturbed_sampled_ll": -6.146110534667969, "perturbed_original_ll": -5.948135852813721, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move out sendgrid emailer from airflow.contrib (#9355)", "sampled": "Move out sendgrid emailer from airflow.contrib (#9355)So", "perturbed_sampled": ["Move out sendgrid emailer from airflow.contrib (#9355)So"], "perturbed_original": ["Move out sendgrid emailer from airflow.contrib (#9355)"], "original_ll": -7.032699108123779, "sampled_ll": -7.654195308685303, "all_perturbed_sampled_ll": [-7.654195308685303], "all_perturbed_original_ll": [-7.032699108123779], "perturbed_sampled_ll": -7.654195308685303, "perturbed_original_ll": -7.032699108123779, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "sampled": "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "perturbed_sampled": ["Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314"], "perturbed_original": ["Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704"], "original_ll": -4.304908275604248, "sampled_ll": -3.866276741027832, "all_perturbed_sampled_ll": [-3.866276741027832], "all_perturbed_original_ll": [-4.304908275604248], "perturbed_sampled_ll": -3.866276741027832, "perturbed_original_ll": -4.304908275604248, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Pin pandas-gbq to <0.15.0 (#15114)", "sampled": "Pin pandas-gbq to <0.15.0 (#15114)Description:", "perturbed_sampled": ["Pin pandas-gbq to <0.15.0 (#15114)Description:"], "perturbed_original": ["Pin pandas-gbq to <0.15.0 (#15114)"], "original_ll": -5.486528396606445, "sampled_ll": -5.660397052764893, "all_perturbed_sampled_ll": [-5.660397052764893], "all_perturbed_original_ll": [-5.486528396606445], "perturbed_sampled_ll": -5.660397052764893, "perturbed_original_ll": -5.486528396606445, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "sampled": "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "perturbed_sampled": ["[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence for Lucas-C by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is likely to be introduced sometime in Firefox 50 or 51.\n\nSee also the"], "perturbed_original": ["[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek has been accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to using it."], "original_ll": -4.546263217926025, "sampled_ll": -4.400304794311523, "all_perturbed_sampled_ll": [-4.1777448654174805], "all_perturbed_original_ll": [-4.311574459075928], "perturbed_sampled_ll": -4.1777448654174805, "perturbed_original_ll": -4.311574459075928, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "sampled": "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their PgBouncer instance. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "perturbed_sampled": ["Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a given user, they had to specify an existing secret for their PgBouncer configuration. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class now shows unformatted and (un)formatted HTML-formatted-HTML attribute on"], "perturbed_original": ["Chart: Allow setting an out of band <unk>secret<unk> for PgBouncer users Previously, if a user wanted to supply the user name and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` line of the secret. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly."], "original_ll": -3.481090545654297, "sampled_ll": -2.8832123279571533, "all_perturbed_sampled_ll": [-3.1341254711151123], "all_perturbed_original_ll": [-3.5128800868988037], "perturbed_sampled_ll": -3.1341254711151123, "perturbed_original_ll": -3.5128800868988037, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Added Viscovery to the list of companies using Apache Airflow (#18683)", "sampled": "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "perturbed_sampled": ["Added Viscovery to the list of companies using Apache Airflow (#18683)The"], "perturbed_original": ["Added Viscovery to the list of companies using Apache Airflow (#18683)"], "original_ll": -5.624001979827881, "sampled_ll": -6.033973693847656, "all_perturbed_sampled_ll": [-6.033973693847656], "all_perturbed_original_ll": [-5.624001979827881], "perturbed_sampled_ll": -6.033973693847656, "perturbed_original_ll": -5.624001979827881, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: They both operate on system time, and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Also the `utcnow()` style I have replaced will be much", "sampled": "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format", "perturbed_sampled": ["that type, time.time() or timezone.utcnow() for other types (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix which made non-time object types default to \"timezone\" The use of `format` was introduced to improve the parsing of Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the format type of output arguments for Time output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format"], "perturbed_original": ["Don't use time.time() or timezone.utcnow() for duration s. `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: you can operate them while in real time, and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem rare, for running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just like in the example below, it is better to be safe than sorry). And the `utcnow()` style I have replaced will be much"], "original_ll": -3.4374382495880127, "sampled_ll": -2.517864465713501, "all_perturbed_sampled_ll": [-2.699857234954834], "all_perturbed_original_ll": [-3.3606412410736084], "perturbed_sampled_ll": -2.699857234954834, "perturbed_original_ll": -3.3606412410736084, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "sampled": "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "perturbed_sampled": ["[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In"], "perturbed_original": ["[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)"], "original_ll": -6.389296531677246, "sampled_ll": -6.7546000480651855, "all_perturbed_sampled_ll": [-6.7546000480651855], "all_perturbed_original_ll": [-6.389296531677246], "perturbed_sampled_ll": -6.7546000480651855, "perturbed_original_ll": -6.389296531677246, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "sampled": "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "perturbed_sampled": ["Convert properties with query to real methodsAs. * Convert properties with query to real methodsAs"], "perturbed_original": ["Convert properties with query to real methods (#7900) * Convert properties with query to real methods"], "original_ll": -4.225117206573486, "sampled_ll": -4.787213325500488, "all_perturbed_sampled_ll": [-4.707037925720215], "all_perturbed_original_ll": [-4.225117206573486], "perturbed_sampled_ll": -4.707037925720215, "perturbed_original_ll": -4.225117206573486, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse the order of links to reduce mouse distance for most popular", "sampled": "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "perturbed_sampled": ["Fix oversized width of table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding /hiding \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *"], "perturbed_original": ["Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Save some horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse position of links to reduce mouse distance for most popular"], "original_ll": -4.3492431640625, "sampled_ll": -3.6270110607147217, "all_perturbed_sampled_ll": [-3.839275360107422], "all_perturbed_original_ll": [-4.411411285400391], "perturbed_sampled_ll": -3.839275360107422, "perturbed_original_ll": -4.411411285400391, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "sampled": "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "perturbed_sampled": ["[AIRFLOW-5361] Add system tests for BigQuery (#5968)We"], "perturbed_original": ["[AIRFLOW-5361] Add system tests for BigQuery (#5968)"], "original_ll": -5.826155662536621, "sampled_ll": -6.287606716156006, "all_perturbed_sampled_ll": [-6.287606716156006], "all_perturbed_original_ll": [-5.826155662536621], "perturbed_sampled_ll": -6.287606716156006, "perturbed_original_ll": -5.826155662536621, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "sampled": "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "perturbed_sampled": ["AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add job run to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error"], "perturbed_original": ["AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run hook and operator tests * add run_kwargs to hook and operator tests"], "original_ll": -4.089051723480225, "sampled_ll": -3.424016237258911, "all_perturbed_sampled_ll": [-3.9286375045776367], "all_perturbed_original_ll": [-3.9035871028900146], "perturbed_sampled_ll": -3.9286375045776367, "perturbed_original_ll": -3.9035871028900146, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make Smart Sensors DB Migration idempotent (#13892)", "sampled": "Make Smart Sensors DB Migration idempotent (#13892)With", "perturbed_sampled": ["Make Smart Sensors DB Migration idempotent (#13892)With"], "perturbed_original": ["Make Smart Sensors DB Migration idempotent (#13892)"], "original_ll": -6.335483551025391, "sampled_ll": -6.85119104385376, "all_perturbed_sampled_ll": [-6.85119104385376], "all_perturbed_original_ll": [-6.335483551025391], "perturbed_sampled_ll": -6.85119104385376, "perturbed_original_ll": -6.335483551025391, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5702] Fix common docstring issues (#6372)", "sampled": "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "perturbed_sampled": ["[AIRFLOW-5702] Fix common docstring issues (#6372)The"], "perturbed_original": ["[AIRFLOW-5702] Fix common docstring issues (#6372)"], "original_ll": -5.809638023376465, "sampled_ll": -6.241412162780762, "all_perturbed_sampled_ll": [-6.241412162780762], "all_perturbed_original_ll": [-5.809638023376465], "perturbed_sampled_ll": -6.241412162780762, "perturbed_original_ll": -5.809638023376465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "sampled": "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "perturbed_sampled": ["[AIRFLOW-6146] Fix GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #544 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix"], "perturbed_original": ["[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * added a few warnings to GCP modules * fixed tests * updated UPDATING.md"], "original_ll": -4.861384868621826, "sampled_ll": -2.3684208393096924, "all_perturbed_sampled_ll": [-2.307835340499878], "all_perturbed_original_ll": [-4.688199043273926], "perturbed_sampled_ll": -2.307835340499878, "perturbed_original_ll": -4.688199043273926, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "sampled": "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "perturbed_sampled": ["DbApiHook: Get DBA hook in get_pandas_df (#9730) * DbApiHook: Handle pthread namespace in GetDbApi() (#9526) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check"], "perturbed_original": ["DbApiHook: Support kwargs in get_pandas_df (#9730) * BigQueryHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df"], "original_ll": -1.856416940689087, "sampled_ll": -2.613673448562622, "all_perturbed_sampled_ll": [-2.6772122383117676], "all_perturbed_original_ll": [-1.8441760540008545], "perturbed_sampled_ll": -2.6772122383117676, "perturbed_original_ll": -1.8441760540008545, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "sampled": "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "perturbed_sampled": ["Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579"], "perturbed_original": ["Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579"], "original_ll": -4.961422443389893, "sampled_ll": -4.961422443389893, "all_perturbed_sampled_ll": [-4.961422443389893], "all_perturbed_original_ll": [-4.961422443389893], "perturbed_sampled_ll": -4.961422443389893, "perturbed_original_ll": -4.961422443389893, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if an SlaMiss already exists in DB while inserting slas. If an SLA for a task is missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record into the SlaMiss table again, this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "sampled": "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of `OK` (that means that there was a proper removal of corruption). This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with a Windows machine when run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "perturbed_sampled": [") fixes in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files had a corruption status of `OK` (that means that there was a proper removal of corruption). This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file corruption associated with a Windows machine when run in a dapper scenario (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored as unordered sequence so it can be difficult to read, write,"], "perturbed_original": ["Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if an SlaMiss table exists in DB while inserting slas. This causes a problem: when SLA for a task is missed and recorded, on checking SLA again, this task comes again for the record, but if there's no recent run of the task and we try to insert the record in SlaMiss table again, we end up in Integrity error. This PR fixes that by avoiding the inserting of SlaMiss table if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>"], "original_ll": -3.6974518299102783, "sampled_ll": -3.0906713008880615, "all_perturbed_sampled_ll": [-3.2967758178710938], "all_perturbed_original_ll": [-3.616847515106201], "perturbed_sampled_ll": -3.2967758178710938, "perturbed_original_ll": -3.616847515106201, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "sampled": "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "perturbed_sampled": ["[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For"], "perturbed_original": ["[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)"], "original_ll": -6.993696212768555, "sampled_ll": -7.434621810913086, "all_perturbed_sampled_ll": [-7.434621810913086], "all_perturbed_original_ll": [-6.993696212768555], "perturbed_sampled_ll": -7.434621810913086, "perturbed_original_ll": -6.993696212768555, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improvements for transfer operators references (#12482)", "sampled": "Improvements for transfer operators references (#12482)The", "perturbed_sampled": ["Improvements for transfer operators references (#12482)The"], "perturbed_original": ["Improvements for transfer operators references (#12482)"], "original_ll": -6.506933212280273, "sampled_ll": -7.1466569900512695, "all_perturbed_sampled_ll": [-7.1466569900512695], "all_perturbed_original_ll": [-6.506933212280273], "perturbed_sampled_ll": -7.1466569900512695, "perturbed_original_ll": -6.506933212280273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "sampled": "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "perturbed_sampled": ["Handle DST better for Mac Instance s. (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now"], "perturbed_original": ["Handle DST better in Task Instance tool tips This dialog displayed the zone \"name\" based on the current time, which could lead to confusion when the date to be displayed was not in the daylight-savings state as \"now\"."], "original_ll": -4.448699474334717, "sampled_ll": -3.65997576713562, "all_perturbed_sampled_ll": [-3.640953302383423], "all_perturbed_original_ll": [-4.68629789352417], "perturbed_sampled_ll": -3.640953302383423, "perturbed_original_ll": -4.68629789352417, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add colors to airflow config command (#8404)", "sampled": "Add colors to airflow config command (#8404)The", "perturbed_sampled": ["Add colors to airflow config command (#8404)The"], "perturbed_original": ["Add colors to airflow config command (#8404)"], "original_ll": -6.870358943939209, "sampled_ll": -7.495344638824463, "all_perturbed_sampled_ll": [-7.495344638824463], "all_perturbed_original_ll": [-6.870358943939209], "perturbed_sampled_ll": -7.495344638824463, "perturbed_original_ll": -6.870358943939209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "sampled": "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "perturbed_sampled": ["[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured"], "perturbed_original": ["[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users \"We can see that Daniel Imberman is Bloomberg's Airflow PoC"], "original_ll": -4.35764741897583, "sampled_ll": -3.9237124919891357, "all_perturbed_sampled_ll": [-3.9237124919891357], "all_perturbed_original_ll": [-4.483520984649658], "perturbed_sampled_ll": -3.9237124919891357, "perturbed_original_ll": -4.483520984649658, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "sampled": "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "perturbed_sampled": ["Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If"], "perturbed_original": ["Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420"], "original_ll": -5.37913703918457, "sampled_ll": -5.801395893096924, "all_perturbed_sampled_ll": [-5.801395893096924], "all_perturbed_original_ll": [-5.37913703918457], "perturbed_sampled_ll": -5.801395893096924, "perturbed_original_ll": -5.37913703918457, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency", "sampled": "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it is an existing TI Task). The task to detect this is to read and return a task-related object. This object needs", "perturbed_sampled": ["Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it is an existing TI Task). The task to detect this is to detect and return a Tooltip instance instance. This object needs"], "perturbed_original": ["Add some support to TI BCO to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately 4.5% of our users experience some form of colour vision deficiency"], "original_ll": -3.9705076217651367, "sampled_ll": -3.9782652854919434, "all_perturbed_sampled_ll": [-4.0035481452941895], "all_perturbed_original_ll": [-4.081297397613525], "perturbed_sampled_ll": -4.0035481452941895, "perturbed_original_ll": -4.081297397613525, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating.", "sampled": "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to", "perturbed_sampled": ["Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL should be relative to base path and not relative to"], "perturbed_original": ["Chart: Allow ``webserver.base_url`` to be dynamically altered in <unk>config<unk>. As `config`'s documentation states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating."], "original_ll": -3.485475540161133, "sampled_ll": -4.162430763244629, "all_perturbed_sampled_ll": [-4.024364471435547], "all_perturbed_original_ll": [-3.828975200653076], "perturbed_sampled_ll": -4.024364471435547, "perturbed_original_ll": -3.828975200653076, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move setting of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "sampled": "Move setting of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "perturbed_sampled": ["Move setting of project service accounts to activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>"], "perturbed_original": ["Change of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>"], "original_ll": -4.423308849334717, "sampled_ll": -4.336629390716553, "all_perturbed_sampled_ll": [-4.348051071166992], "all_perturbed_original_ll": [-3.973822832107544], "perturbed_sampled_ll": -4.348051071166992, "perturbed_original_ll": -3.973822832107544, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "sampled": "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest aws versions and to be compatible with aws 1.7.0", "perturbed_sampled": ["Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest aws versions and to work with aws 1.7.0"], "perturbed_original": ["Add type hints to aws provider * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation -----Mik Bregu\u0142a <mik-laj@users.noreply.github.com>"], "original_ll": -4.058938980102539, "sampled_ll": -3.500737428665161, "all_perturbed_sampled_ll": [-3.619598388671875], "all_perturbed_original_ll": [-4.258445739746094], "perturbed_sampled_ll": -3.619598388671875, "perturbed_original_ll": -4.258445739746094, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add verify_ssl config for kubernetes (#13516)", "sampled": "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "perturbed_sampled": ["Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)"], "perturbed_original": ["Add verify_ssl config for kubernetes (#13516)"], "original_ll": -4.943172931671143, "sampled_ll": -2.856890916824341, "all_perturbed_sampled_ll": [-2.856890916824341], "all_perturbed_original_ll": [-4.943172931671143], "perturbed_sampled_ll": -2.856890916824341, "perturbed_original_ll": -4.943172931671143, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "sampled": "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "perturbed_sampled": ["Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker"], "perturbed_original": ["Updated documentation for the CI with mermaid sequence diagrams (#10380)"], "original_ll": -6.472273826599121, "sampled_ll": -6.581944465637207, "all_perturbed_sampled_ll": [-6.581944465637207], "all_perturbed_original_ll": [-6.472273826599121], "perturbed_sampled_ll": -6.581944465637207, "perturbed_original_ll": -6.472273826599121, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "sampled": "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "perturbed_sampled": ["[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The"], "perturbed_original": ["[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)"], "original_ll": -6.033203125, "sampled_ll": -6.346513748168945, "all_perturbed_sampled_ll": [-6.346513748168945], "all_perturbed_original_ll": [-6.033203125], "perturbed_sampled_ll": -6.346513748168945, "perturbed_original_ll": -6.033203125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "sampled": "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "perturbed_sampled": ["Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better way to refactoring: subclassing objects that are"], "perturbed_original": ["Use Python 3 style super classes (#11806) example: super(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```"], "original_ll": -3.0042622089385986, "sampled_ll": -3.6834604740142822, "all_perturbed_sampled_ll": [-3.7323453426361084], "all_perturbed_original_ll": [-3.386265277862549], "perturbed_sampled_ll": -3.7323453426361084, "perturbed_original_ll": -3.386265277862549, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "sampled": "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "perturbed_sampled": ["[AIRFLOW-XXX] Add history become ASF top level project (#4757)In"], "perturbed_original": ["[AIRFLOW-XXX] Add history become ASF top level project (#4757)"], "original_ll": -6.7266011238098145, "sampled_ll": -7.089080810546875, "all_perturbed_sampled_ll": [-7.089080810546875], "all_perturbed_original_ll": [-6.7266011238098145], "perturbed_sampled_ll": -7.089080810546875, "perturbed_original_ll": -6.7266011238098145, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "sampled": "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "perturbed_sampled": ["[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On"], "perturbed_original": ["[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)"], "original_ll": -6.535831928253174, "sampled_ll": -6.821267127990723, "all_perturbed_sampled_ll": [-6.821267127990723], "all_perturbed_original_ll": [-6.535831928253174], "perturbed_sampled_ll": -6.821267127990723, "perturbed_original_ll": -6.535831928253174, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "sampled": "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "perturbed_sampled": ["Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want this to add kwargs to `BaseSecrets`, for which"], "perturbed_original": ["Remove kwargs from Super secrets to AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`"], "original_ll": -4.820204257965088, "sampled_ll": -5.145382881164551, "all_perturbed_sampled_ll": [-4.804988861083984], "all_perturbed_original_ll": [-4.859484672546387], "perturbed_sampled_ll": -4.804988861083984, "perturbed_original_ll": -4.859484672546387, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "sampled": "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "perturbed_sampled": ["[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At"], "perturbed_original": ["[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)"], "original_ll": -5.729770183563232, "sampled_ll": -6.1820526123046875, "all_perturbed_sampled_ll": [-6.1820526123046875], "all_perturbed_original_ll": [-5.729770183563232], "perturbed_sampled_ll": -6.1820526123046875, "perturbed_original_ll": -5.729770183563232, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now", "sampled": "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "perturbed_sampled": ["[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4926) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Modify unit"], "perturbed_original": ["[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc . * [AIRFLOW-4092] Add gRPCOperator, unit test and added to Auto doc. * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now"], "original_ll": -3.095273733139038, "sampled_ll": -1.7969061136245728, "all_perturbed_sampled_ll": [-2.0795199871063232], "all_perturbed_original_ll": [-3.1129658222198486], "perturbed_sampled_ll": -2.0795199871063232, "perturbed_original_ll": -3.1129658222198486, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - Remove unnecessary specification of default conn ids", "sampled": "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to start_date. - Split the template into two - use @GoogleDags in", "perturbed_sampled": ["- google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to start_date. - Split the template into two - Use catch up=True in"], "perturbed_original": ["Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of dags; it is too long in some cases - Remove duplicates of default conn ids"], "original_ll": -4.918028831481934, "sampled_ll": -4.533754348754883, "all_perturbed_sampled_ll": [-4.748726844787598], "all_perturbed_original_ll": [-4.585586071014404], "perturbed_sampled_ll": -4.748726844787598, "perturbed_original_ll": -4.585586071014404, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter which allows users to get output in form of table, json or yaml.", "sampled": "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "perturbed_sampled": ["the airflow airflow plugins command output using this command. This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by"], "perturbed_original": ["Refactor plugins command output using 'output' parameter. This PR refactors the airflow plugins command to be compatible with 'output' parameter which allows users to get output in format of table, json or yaml."], "original_ll": -4.554455280303955, "sampled_ll": -4.462299346923828, "all_perturbed_sampled_ll": [-4.407708644866943], "all_perturbed_original_ll": [-4.070204257965088], "perturbed_sampled_ll": -4.407708644866943, "perturbed_original_ll": -4.070204257965088, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix task search function in Graph view (#15901)", "sampled": "Fix task search function in Graph view (#15901)One", "perturbed_sampled": ["Fix task search function in Graph view (#15901)One"], "perturbed_original": ["Fix task search function in Graph view (#15901)"], "original_ll": -6.499565124511719, "sampled_ll": -7.43597412109375, "all_perturbed_sampled_ll": [-7.43597412109375], "all_perturbed_original_ll": [-6.499565124511719], "perturbed_sampled_ll": -7.43597412109375, "perturbed_original_ll": -6.499565124511719, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "sampled": "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "perturbed_sampled": ["Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The"], "perturbed_original": ["Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)"], "original_ll": -6.875676155090332, "sampled_ll": -7.009141445159912, "all_perturbed_sampled_ll": [-7.009141445159912], "all_perturbed_original_ll": [-6.875676155090332], "perturbed_sampled_ll": -7.009141445159912, "perturbed_original_ll": -6.875676155090332, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in to get_conn - add salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "sampled": "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description to clarify some context (and not", "perturbed_sampled": ["[AIRFLOW-3993] Add tests for validation (#4829) - refactor code - update docs - change sign_in_form_name to (pass=@pass, secret=@sec, error=@error) - improve link link description to clarify some context (and not"], "perturbed_original": ["[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor and update docs - change sign_in to get_conn - add salesforce to devel_all packages - add note to UPDATING.md IMPORTANT Mike Laj <mik-laj@users.noreply.github.com>"], "original_ll": -3.955713987350464, "sampled_ll": -4.022988796234131, "all_perturbed_sampled_ll": [-4.10946798324585], "all_perturbed_original_ll": [-4.250702381134033], "perturbed_sampled_ll": -4.10946798324585, "perturbed_original_ll": -4.250702381134033, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "sampled": "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "perturbed_sampled": ["[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The"], "perturbed_original": ["[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)"], "original_ll": -5.189088344573975, "sampled_ll": -5.456293106079102, "all_perturbed_sampled_ll": [-5.456293106079102], "all_perturbed_original_ll": [-5.189088344573975], "perturbed_sampled_ll": -5.456293106079102, "perturbed_original_ll": -5.189088344573975, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "sampled": "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of extra options; this bug gives extra options to the URL to which I have", "perturbed_sampled": ["Extend HTTP redirects with LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of options; this bug gives extra options to the URL to which I have"], "perturbed_original": ["Extend HTTP extra_options to LivyHook and operator (#14816) HTTP service used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the Hook itself."], "original_ll": -4.13319206237793, "sampled_ll": -4.3155927658081055, "all_perturbed_sampled_ll": [-4.276033878326416], "all_perturbed_original_ll": [-4.585598468780518], "perturbed_sampled_ll": -4.276033878326416, "perturbed_original_ll": -4.585598468780518, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "sampled": "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "perturbed_sampled": ["[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:"], "perturbed_original": ["[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)"], "original_ll": -5.838682651519775, "sampled_ll": -5.734640598297119, "all_perturbed_sampled_ll": [-5.734640598297119], "all_perturbed_original_ll": [-5.838682651519775], "perturbed_sampled_ll": -5.734640598297119, "perturbed_original_ll": -5.838682651519775, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "sampled": "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "perturbed_sampled": ["[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This"], "perturbed_original": ["[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)"], "original_ll": -6.320858955383301, "sampled_ll": -6.596794605255127, "all_perturbed_sampled_ll": [-6.596794605255127], "all_perturbed_original_ll": [-6.320858955383301], "perturbed_sampled_ll": -6.596794605255127, "perturbed_original_ll": -6.320858955383301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use DAG_ACTIONS constant. (#16232)", "sampled": "Use DAG_ACTIONS constant. (#16232)About", "perturbed_sampled": ["Use DAG_ACTIONS constant. (#16232)About"], "perturbed_original": ["Use DAG_ACTIONS constant. (#16232)"], "original_ll": -5.436949253082275, "sampled_ll": -6.420441150665283, "all_perturbed_sampled_ll": [-6.420441150665283], "all_perturbed_original_ll": [-5.436949253082275], "perturbed_sampled_ll": -6.420441150665283, "perturbed_original_ll": -5.436949253082275, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clean up airflow.contrib in Kubernetes docs (#9551)", "sampled": "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "perturbed_sampled": ["Clean up airflow.contrib in Kubernetes docs (#9551)\""], "perturbed_original": ["Clean up airflow.contrib in Kubernetes docs (#9551)"], "original_ll": -5.611946105957031, "sampled_ll": -6.084292888641357, "all_perturbed_sampled_ll": [-6.084292888641357], "all_perturbed_original_ll": [-5.611946105957031], "perturbed_sampled_ll": -6.084292888641357, "perturbed_original_ll": -5.611946105957031, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is easy for us to maintain compatibility, so we should", "sampled": "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is there. It should be merged into a commit that", "perturbed_sampled": ["Add back-compat layer function. (#16582) It is unlikely that anyone would be using this function directly, but it is there. It should be merged into a commit that"], "perturbed_original": ["Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it needs an easy API layer to maintain compatibility, so we should"], "original_ll": -3.894094467163086, "sampled_ll": -3.898669719696045, "all_perturbed_sampled_ll": [-3.7296230792999268], "all_perturbed_original_ll": [-4.169802188873291], "perturbed_sampled_ll": -3.7296230792999268, "perturbed_original_ll": -4.169802188873291, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "sampled": "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "perturbed_sampled": ["[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's"], "perturbed_original": ["[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)"], "original_ll": -6.205388069152832, "sampled_ll": -6.484692573547363, "all_perturbed_sampled_ll": [-6.484692573547363], "all_perturbed_original_ll": [-6.205388069152832], "perturbed_sampled_ll": -6.484692573547363, "perturbed_original_ll": -6.205388069152832, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "detect incompatible docker server version in breeze (#9042)", "sampled": "detect incompatible docker server version in breeze (#9042)The", "perturbed_sampled": ["detect incompatible docker server version in breeze (#9042)The"], "perturbed_original": ["detect incompatible docker server version in breeze (#9042)"], "original_ll": -6.299882888793945, "sampled_ll": -6.861575603485107, "all_perturbed_sampled_ll": [-6.861575603485107], "all_perturbed_original_ll": [-6.299882888793945], "perturbed_sampled_ll": -6.861575603485107, "perturbed_original_ll": -6.299882888793945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make them available for python tools. Also /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. It also contains one small change for Linux users - the files created by docker gcloud are created with root user so in order to fix that the directories mounted from the host are fixed when you exit the tool - their ownership is changed to be owned by the host user", "sampled": "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app shows a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In general the tool will ask about the running OS during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "perturbed_sampled": ["* Cloud tool available in the trimmed down CI container (#9167) * New cloud tool available in the trimmed down CI container * All three CI tools now have sheers: the cloud app shows a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * * \"Automated Continuous Integration\" can be selected through the cloud tool general settings. * The tool will ask about the running OS during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration in the wiki page * Better support for"], "perturbed_original": ["Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make it easier to use the tools for python tools. Also /opt/airflow is now mounted from the Airflow sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. This release contains one small change for Linux users - the files created by docker gcloud are created with root user so in order to fix that the directories mounted from root are fixed when you run the tool - the /opt/airflow is changed to be owned by the host user"], "original_ll": -3.827514886856079, "sampled_ll": -3.1485157012939453, "all_perturbed_sampled_ll": [-3.346802234649658], "all_perturbed_original_ll": [-3.5926084518432617], "perturbed_sampled_ll": -3.346802234649658, "perturbed_original_ll": -3.5926084518432617, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "sampled": "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "perturbed_sampled": ["fix bug of SparkSql Operator log going to infinite loop. (#19449)For"], "perturbed_original": ["fix bug of SparkSql Operator log going to infinite loop. (#19449)"], "original_ll": -6.049223899841309, "sampled_ll": -6.560698509216309, "all_perturbed_sampled_ll": [-6.560698509216309], "all_perturbed_original_ll": [-6.049223899841309], "perturbed_sampled_ll": -6.560698509216309, "perturbed_original_ll": -6.049223899841309, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove unused 'context' variable in task_instance.py (#14049)", "sampled": "Remove unused 'context' variable in task_instance.py (#14049)I", "perturbed_sampled": ["Remove unused 'context' variable in task_instance.py (#14049)I"], "perturbed_original": ["Remove unused 'context' variable in task_instance.py (#14049)"], "original_ll": -4.48358154296875, "sampled_ll": -5.201879024505615, "all_perturbed_sampled_ll": [-5.201879024505615], "all_perturbed_original_ll": [-4.48358154296875], "perturbed_sampled_ll": -5.201879024505615, "perturbed_original_ll": -4.48358154296875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "UPDATING.md for changes included in 2.1.1 (#16615)", "sampled": "UPDATING.md for changes included in 2.1.1 (#16615)The", "perturbed_sampled": ["UPDATING.md for changes included in 2.1.1 (#16615)The"], "perturbed_original": ["UPDATING.md for changes included in 2.1.1 (#16615)"], "original_ll": -4.0340070724487305, "sampled_ll": -4.425573348999023, "all_perturbed_sampled_ll": [-4.425573348999023], "all_perturbed_original_ll": [-4.0340070724487305], "perturbed_sampled_ll": -4.425573348999023, "perturbed_original_ll": -4.0340070724487305, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Google Memcached hooks - improve protobuf messages handling (#11743)", "sampled": "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "perturbed_sampled": ["Google Memcached hooks - improve protobuf messages handling (#11743)\""], "perturbed_original": ["Google Memcached hooks - improve protobuf messages handling (#11743)"], "original_ll": -5.319725513458252, "sampled_ll": -5.819211959838867, "all_perturbed_sampled_ll": [-5.819211959838867], "all_perturbed_original_ll": [-5.319725513458252], "perturbed_sampled_ll": -5.819211959838867, "perturbed_original_ll": -5.319725513458252, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "sampled": "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second issue being that a task instance may have been closed.) Fixes a crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused", "perturbed_sampled": ["Fix crash when clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error in the error message: [ERROR] The application does not have a task instance, therefore not running in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second being that a task instance may not exist since the application may not have been closed.) Fixes a crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused"], "perturbed_original": ["Error when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error that occurs when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is not yet responding The previous TaskInstance was marked success without running. This was caused by #12910. It affects Airflow 2.0.0 and 2.0.1."], "original_ll": -3.191002130508423, "sampled_ll": -3.1482508182525635, "all_perturbed_sampled_ll": [-3.0915932655334473], "all_perturbed_original_ll": [-3.191354990005493], "perturbed_sampled_ll": -3.0915932655334473, "perturbed_original_ll": -3.191354990005493, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "sampled": "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "perturbed_sampled": ["Fix command line warnings related to build tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the"], "perturbed_original": ["Fix command to run tmux with breeze start-airflow (#11340) `breeze --start-airflow` -> `breeze start-airflow`"], "original_ll": -3.772520065307617, "sampled_ll": -4.41995096206665, "all_perturbed_sampled_ll": [-4.804483890533447], "all_perturbed_original_ll": [-3.6034741401672363], "perturbed_sampled_ll": -4.804483890533447, "perturbed_original_ll": -3.6034741401672363, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "sampled": "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "perturbed_sampled": ["[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've"], "perturbed_original": ["[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)"], "original_ll": -6.269988059997559, "sampled_ll": -6.512642860412598, "all_perturbed_sampled_ll": [-6.512642860412598], "all_perturbed_original_ll": [-6.269988059997559], "perturbed_sampled_ll": -6.512642860412598, "perturbed_original_ll": -6.269988059997559, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI modules via custom .pypirc file. This might allow to install dependencies from in-house, vetted registry of PyPI", "sampled": "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, pyPI_source, to look", "perturbed_sampled": ["Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds the ability of customising PyPI sources. When customising PyPI sources, the functionality is a bit different to regular sources. You use the module pyPI_source, to look"], "perturbed_original": ["Add capability of customising PyPI sources This change adds Add capability of customising PyPI sources This change adds capability of customising installation of PyPI modules via custom sources. This might allow to install dependencies from in-house, vetted , non-commercial sources on PyPI"], "original_ll": -3.6776766777038574, "sampled_ll": -3.3592031002044678, "all_perturbed_sampled_ll": [-3.0949506759643555], "all_perturbed_original_ll": [-3.6599016189575195], "perturbed_sampled_ll": -3.0949506759643555, "perturbed_original_ll": -3.6599016189575195, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "sampled": "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "perturbed_sampled": ["Update link to match what is in pre-commit (#16408) [The k8s schema repository that was used for chart pytest has now now been changed to the yml schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2014 David Hochhuber <dhochhuber@google.com> License: MIT Open Spaces License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1."], "perturbed_original": ["Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has gone stale with no new updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for this repo are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change [is for this updated fork](https://github.com/yannh/kubernetes-json-schema)"], "original_ll": -2.924253225326538, "sampled_ll": -2.0435242652893066, "all_perturbed_sampled_ll": [-2.293701648712158], "all_perturbed_original_ll": [-2.8991878032684326], "perturbed_sampled_ll": -2.293701648712158, "perturbed_original_ll": -2.8991878032684326, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update to latest pygrep pre-commit hook (#8489)", "sampled": "Update to latest pygrep pre-commit hook (#8489)A", "perturbed_sampled": ["Update to latest pygrep pre-commit hook (#8489)A"], "perturbed_original": ["Update to latest pygrep pre-commit hook (#8489)"], "original_ll": -4.832974433898926, "sampled_ll": -5.407581329345703, "all_perturbed_sampled_ll": [-5.407581329345703], "all_perturbed_original_ll": [-4.832974433898926], "perturbed_sampled_ll": -5.407581329345703, "perturbed_original_ll": -4.832974433898926, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "sampled": "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "perturbed_sampled": ["Replace deprecated operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated"], "perturbed_original": ["Replace deprecated dummy operator path in test_zip.zip (#13172) Replace directives in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```"], "original_ll": -3.2934682369232178, "sampled_ll": -2.4475862979888916, "all_perturbed_sampled_ll": [-2.322047710418701], "all_perturbed_original_ll": [-3.424074172973633], "perturbed_sampled_ll": -2.322047710418701, "perturbed_original_ll": -3.424074172973633, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "sampled": "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "perturbed_sampled": ["Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted"], "perturbed_original": ["Add example dag and system test for LocalFilesystemToGCSOperator (#9043)"], "original_ll": -6.64837121963501, "sampled_ll": -7.210554599761963, "all_perturbed_sampled_ll": [-7.210554599761963], "all_perturbed_original_ll": [-6.64837121963501], "perturbed_sampled_ll": -7.210554599761963, "perturbed_original_ll": -6.64837121963501, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "sampled": "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "perturbed_sampled": ["[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This"], "perturbed_original": ["[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)"], "original_ll": -5.231442928314209, "sampled_ll": -5.529385566711426, "all_perturbed_sampled_ll": [-5.529385566711426], "all_perturbed_original_ll": [-5.231442928314209], "perturbed_sampled_ll": -5.529385566711426, "perturbed_original_ll": -5.231442928314209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "sampled": "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "perturbed_sampled": ["Removed hardcoded connection types. This hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)"], "perturbed_original": ["Removed dependency on API object types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>"], "original_ll": -4.763741493225098, "sampled_ll": -4.207995891571045, "all_perturbed_sampled_ll": [-4.1557698249816895], "all_perturbed_original_ll": [-4.858769416809082], "perturbed_sampled_ll": -4.1557698249816895, "perturbed_original_ll": -4.858769416809082, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "sampled": "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "perturbed_sampled": ["Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed"], "perturbed_original": ["Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>"], "original_ll": -4.778177261352539, "sampled_ll": -4.294705390930176, "all_perturbed_sampled_ll": [-4.294705390930176], "all_perturbed_original_ll": [-4.778177261352539], "perturbed_sampled_ll": -4.294705390930176, "perturbed_original_ll": -4.778177261352539, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214", "sampled": "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for the Azure CLI", "perturbed_sampled": ["Log migrations info in consisten way (#13458) Resource based permissions are used inside the logging handlers so each next migration is differently formatted when doing these migrations (@gwern)\n\n: Added support for the Azure CLI"], "perturbed_original": ["Log migrations info in consisten way (#13458) Resource based permissions migration s is not mapped to specific handlers so each row is differently formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214"], "original_ll": -6.46252965927124, "sampled_ll": -5.869115352630615, "all_perturbed_sampled_ll": [-5.449915885925293], "all_perturbed_original_ll": [-6.330172538757324], "perturbed_sampled_ll": -5.449915885925293, "perturbed_original_ll": -6.330172538757324, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "JIRA and Github issues explanation (#8539)", "sampled": "JIRA and Github issues explanation (#8539)Still", "perturbed_sampled": ["JIRA and Github issues explanation (#8539)Still"], "perturbed_original": ["JIRA and Github issues explanation (#8539)"], "original_ll": -6.503997802734375, "sampled_ll": -7.256471157073975, "all_perturbed_sampled_ll": [-7.256471157073975], "all_perturbed_original_ll": [-6.503997802734375], "perturbed_sampled_ll": -7.256471157073975, "perturbed_original_ll": -6.503997802734375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "sampled": "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "perturbed_sampled": ["[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New"], "perturbed_original": ["[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)"], "original_ll": -5.6918487548828125, "sampled_ll": -6.017413139343262, "all_perturbed_sampled_ll": [-6.017413139343262], "all_perturbed_original_ll": [-5.6918487548828125], "perturbed_sampled_ll": -6.017413139343262, "perturbed_original_ll": -5.6918487548828125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "sampled": "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates", "perturbed_sampled": ["Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog for 1.10.15 and updates"], "perturbed_original": ["Add Changelog & Updating.md for 1.10.15. This commit adds Changelog & Updating.md for 1.10.15"], "original_ll": -2.8190317153930664, "sampled_ll": -3.319640636444092, "all_perturbed_sampled_ll": [-3.285676956176758], "all_perturbed_original_ll": [-2.5723845958709717], "perturbed_sampled_ll": -3.285676956176758, "perturbed_original_ll": -2.5723845958709717, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "sampled": "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "perturbed_sampled": ["Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders"], "perturbed_original": ["Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)"], "original_ll": -5.607777118682861, "sampled_ll": -4.800323009490967, "all_perturbed_sampled_ll": [-4.800323009490967], "all_perturbed_original_ll": [-5.607777118682861], "perturbed_sampled_ll": -4.800323009490967, "perturbed_original_ll": -5.607777118682861, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "sampled": "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "perturbed_sampled": ["[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This"], "perturbed_original": ["[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)"], "original_ll": -6.288146018981934, "sampled_ll": -6.573079586029053, "all_perturbed_sampled_ll": [-6.573079586029053], "all_perturbed_original_ll": [-6.288146018981934], "perturbed_sampled_ll": -6.573079586029053, "perturbed_original_ll": -6.288146018981934, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "sampled": "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "perturbed_sampled": ["[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP"], "perturbed_original": ["[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)"], "original_ll": -6.40598201751709, "sampled_ll": -6.659811496734619, "all_perturbed_sampled_ll": [-6.659811496734619], "all_perturbed_original_ll": [-6.40598201751709], "perturbed_sampled_ll": -6.659811496734619, "perturbed_original_ll": -6.40598201751709, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix backwards compatibility with k8s executor_config resources (#11796)", "sampled": "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "perturbed_sampled": ["Fix backwards compatibility with k8s executor_config resources (#11796)\""], "perturbed_original": ["Fix backwards compatibility with k8s executor_config resources (#11796)"], "original_ll": -5.910852432250977, "sampled_ll": -6.474430561065674, "all_perturbed_sampled_ll": [-6.474430561065674], "all_perturbed_original_ll": [-5.910852432250977], "perturbed_sampled_ll": -6.474430561065674, "perturbed_original_ll": -5.910852432250977, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "sampled": "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "perturbed_sampled": ["Docker context files should be available earlier (#12219) - in cases where we want to override constraints with local version, the docker-context-files should be accessible before any files"], "perturbed_original": ["Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be earlier than the Dockerfile"], "original_ll": -4.086678504943848, "sampled_ll": -4.257526397705078, "all_perturbed_sampled_ll": [-4.262121200561523], "all_perturbed_original_ll": [-4.1239118576049805], "perturbed_sampled_ll": -4.262121200561523, "perturbed_original_ll": -4.1239118576049805, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "sampled": "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "perturbed_sampled": ["fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>"], "perturbed_original": ["fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>"], "original_ll": -4.668304443359375, "sampled_ll": -4.217140197753906, "all_perturbed_sampled_ll": [-4.217140197753906], "all_perturbed_original_ll": [-4.668304443359375], "perturbed_sampled_ll": -4.217140197753906, "perturbed_original_ll": -4.668304443359375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Typo fix in TESTING.rst (#19216)", "sampled": "Typo fix in TESTING.rst (#19216)For", "perturbed_sampled": ["Typo fix in TESTING.rst (#19216)For"], "perturbed_original": ["Typo fix in TESTING.rst (#19216)"], "original_ll": -4.901196002960205, "sampled_ll": -5.675152778625488, "all_perturbed_sampled_ll": [-5.675152778625488], "all_perturbed_original_ll": [-4.901196002960205], "perturbed_sampled_ll": -5.675152778625488, "perturbed_original_ll": -4.901196002960205, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "sampled": "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "perturbed_sampled": ["[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The"], "perturbed_original": ["[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)"], "original_ll": -4.678685665130615, "sampled_ll": -5.020734786987305, "all_perturbed_sampled_ll": [-5.020734786987305], "all_perturbed_original_ll": [-4.678685665130615], "perturbed_sampled_ll": -5.020734786987305, "perturbed_original_ll": -4.678685665130615, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "sampled": "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "perturbed_sampled": ["[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\""], "perturbed_original": ["[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)"], "original_ll": -4.77077579498291, "sampled_ll": -5.062777996063232, "all_perturbed_sampled_ll": [-5.062777996063232], "all_perturbed_original_ll": [-4.77077579498291], "perturbed_sampled_ll": -5.062777996063232, "perturbed_original_ll": -4.77077579498291, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "sampled": "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "perturbed_sampled": ["Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The"], "perturbed_original": ["Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)"], "original_ll": -4.699961185455322, "sampled_ll": -5.13549280166626, "all_perturbed_sampled_ll": [-5.13549280166626], "all_perturbed_original_ll": [-4.699961185455322], "perturbed_sampled_ll": -5.13549280166626, "perturbed_original_ll": -4.699961185455322, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Simplified GCSTaskHandler configuration (#10365)", "sampled": "Simplified GCSTaskHandler configuration (#10365)The", "perturbed_sampled": ["Simplified GCSTaskHandler configuration (#10365)The"], "perturbed_original": ["Simplified GCSTaskHandler configuration (#10365)"], "original_ll": -6.538324356079102, "sampled_ll": -7.114240646362305, "all_perturbed_sampled_ll": [-7.114240646362305], "all_perturbed_original_ll": [-6.538324356079102], "perturbed_sampled_ll": -7.114240646362305, "perturbed_original_ll": -6.538324356079102, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently failing with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "sampled": "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory only for running reviews", "perturbed_sampled": ["Fix label_when_reviewed_workflow_run permissions * Fix label_when_reviewed_workflow_run permissions This commit was pushed by ionith on Tue Aug 14, 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews on workspace * Use working directory only for running reviews"], "perturbed_original": ["Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is failing with: ``` Run ./.github/actions/checks-action with: <unk> Action=<unk> name: Selective status status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by this client this is a workaround for this case, so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>"], "original_ll": -3.6983776092529297, "sampled_ll": -2.4744369983673096, "all_perturbed_sampled_ll": [-2.952528476715088], "all_perturbed_original_ll": [-3.699822425842285], "perturbed_sampled_ll": -2.952528476715088, "perturbed_original_ll": -3.699822425842285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "sampled": "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "perturbed_sampled": ["[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In"], "perturbed_original": ["[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)"], "original_ll": -5.498917102813721, "sampled_ll": -5.810244560241699, "all_perturbed_sampled_ll": [-5.810244560241699], "all_perturbed_original_ll": [-5.498917102813721], "perturbed_sampled_ll": -5.810244560241699, "perturbed_original_ll": -5.498917102813721, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Highlight code blocks (#6243)", "sampled": "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "perturbed_sampled": ["[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT"], "perturbed_original": ["[AIRFLOW-XXX] Highlight code blocks (#6243)"], "original_ll": -5.9480719566345215, "sampled_ll": -5.615355014801025, "all_perturbed_sampled_ll": [-5.615355014801025], "all_perturbed_original_ll": [-5.9480719566345215], "perturbed_sampled_ll": -5.615355014801025, "perturbed_original_ll": -5.9480719566345215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errors", "sampled": "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errorsThe", "perturbed_sampled": ["Adds new Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errorsThe"], "perturbed_original": ["Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name , release name and some minor errors"], "original_ll": -5.773518085479736, "sampled_ll": -6.035676956176758, "all_perturbed_sampled_ll": [-5.354254722595215], "all_perturbed_original_ll": [-5.813684940338135], "perturbed_sampled_ll": -5.354254722595215, "perturbed_original_ll": -5.813684940338135, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Small fixes in Google Cloud Secrets Manager guide (#12105)", "sampled": "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "perturbed_sampled": ["Small fixes in Google Cloud Secrets Manager guide (#12105)What's"], "perturbed_original": ["Small fixes in Google Cloud Secrets Manager guide (#12105)"], "original_ll": -6.858067512512207, "sampled_ll": -6.949158191680908, "all_perturbed_sampled_ll": [-6.949158191680908], "all_perturbed_original_ll": [-6.858067512512207], "perturbed_sampled_ll": -6.949158191680908, "perturbed_original_ll": -6.858067512512207, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "sampled": "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "perturbed_sampled": ["[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This"], "perturbed_original": ["[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)"], "original_ll": -5.081605434417725, "sampled_ll": -5.4234843254089355, "all_perturbed_sampled_ll": [-5.4234843254089355], "all_perturbed_original_ll": [-5.081605434417725], "perturbed_sampled_ll": -5.4234843254089355, "perturbed_original_ll": -5.081605434417725, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "sampled": "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "perturbed_sampled": ["Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What"], "perturbed_original": ["Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)"], "original_ll": -3.53136944770813, "sampled_ll": -4.0279436111450195, "all_perturbed_sampled_ll": [-4.0279436111450195], "all_perturbed_original_ll": [-3.53136944770813], "perturbed_sampled_ll": -4.0279436111450195, "perturbed_original_ll": -3.53136944770813, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "sampled": "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "perturbed_sampled": ["refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The"], "perturbed_original": ["refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)"], "original_ll": -4.94626522064209, "sampled_ll": -5.403241157531738, "all_perturbed_sampled_ll": [-5.403241157531738], "all_perturbed_original_ll": [-4.94626522064209], "perturbed_sampled_ll": -5.403241157531738, "perturbed_original_ll": -4.94626522064209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "sampled": "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "perturbed_sampled": ["[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST"], "perturbed_original": ["[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)"], "original_ll": -4.2683796882629395, "sampled_ll": -4.669395923614502, "all_perturbed_sampled_ll": [-4.669395923614502], "all_perturbed_original_ll": [-4.2683796882629395], "perturbed_sampled_ll": -4.669395923614502, "perturbed_original_ll": -4.2683796882629395, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix build error while running docker-compose #14868 #14906 * Add docker-compose command to run only files that can connect to docker (#15998) #14414 * Fix", "perturbed_sampled": ["Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix build error while build * Build dockerfile * fixup! #14868 #14906 * Add docker-compose command to run only files that can connect to the network (#15998) #14414 * Fix"], "perturbed_original": ["Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -2.048685073852539, "sampled_ll": -3.0158321857452393, "all_perturbed_sampled_ll": [-3.0641324520111084], "all_perturbed_original_ll": [-1.8413629531860352], "perturbed_sampled_ll": -3.0641324520111084, "perturbed_original_ll": -1.8413629531860352, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Bump Boto3 (#7851)", "sampled": "Bump Boto3 (#7851)Tunnel", "perturbed_sampled": ["Bump Boto3 (#7851)Tunnel"], "perturbed_original": ["Bump Boto3 (#7851)"], "original_ll": -7.283779621124268, "sampled_ll": -6.755589485168457, "all_perturbed_sampled_ll": [-6.755589485168457], "all_perturbed_original_ll": [-7.283779621124268], "perturbed_sampled_ll": -6.755589485168457, "perturbed_original_ll": -7.283779621124268, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "sampled": "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "perturbed_sampled": ["Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md"], "perturbed_original": ["Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst"], "original_ll": -3.119962453842163, "sampled_ll": -2.969393253326416, "all_perturbed_sampled_ll": [-2.969393253326416], "all_perturbed_original_ll": [-3.119962453842163], "perturbed_sampled_ll": -2.969393253326416, "perturbed_original_ll": -3.119962453842163, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "sampled": "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "perturbed_sampled": ["[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]"], "perturbed_original": ["[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)"], "original_ll": -6.357372283935547, "sampled_ll": -5.020843982696533, "all_perturbed_sampled_ll": [-5.020843982696533], "all_perturbed_original_ll": [-6.357372283935547], "perturbed_sampled_ll": -5.020843982696533, "perturbed_original_ll": -6.357372283935547, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix mypy for exasol and facebook hooks (#20291)", "sampled": "Fix mypy for exasol and facebook hooks (#20291)For", "perturbed_sampled": ["Fix mypy for exasol and facebook hooks (#20291)For"], "perturbed_original": ["Fix mypy for exasol and facebook hooks (#20291)"], "original_ll": -6.832136154174805, "sampled_ll": -7.407689571380615, "all_perturbed_sampled_ll": [-7.407689571380615], "all_perturbed_original_ll": [-6.832136154174805], "perturbed_sampled_ll": -7.407689571380615, "perturbed_original_ll": -6.832136154174805, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Standardize AWS Lambda naming (#20365)", "sampled": "Standardize AWS Lambda naming (#20365)With", "perturbed_sampled": ["Standardize AWS Lambda naming (#20365)With"], "perturbed_original": ["Standardize AWS Lambda naming (#20365)"], "original_ll": -6.150325298309326, "sampled_ll": -6.908303260803223, "all_perturbed_sampled_ll": [-6.908303260803223], "all_perturbed_original_ll": [-6.150325298309326], "perturbed_sampled_ll": -6.908303260803223, "perturbed_original_ll": -6.150325298309326, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "sampled": "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "perturbed_sampled": ["[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So"], "perturbed_original": ["[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]"], "original_ll": -6.083202362060547, "sampled_ll": -6.39528751373291, "all_perturbed_sampled_ll": [-6.39528751373291], "all_perturbed_original_ll": [-6.083202362060547], "perturbed_sampled_ll": -6.39528751373291, "perturbed_original_ll": -6.083202362060547, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix elasticsearch breaking the build (#7800)", "sampled": "Fix elasticsearch breaking the build (#7800)The", "perturbed_sampled": ["Fix elasticsearch breaking the build (#7800)The"], "perturbed_original": ["Fix elasticsearch breaking the build (#7800)"], "original_ll": -7.090944290161133, "sampled_ll": -7.705590724945068, "all_perturbed_sampled_ll": [-7.705590724945068], "all_perturbed_original_ll": [-7.090944290161133], "perturbed_sampled_ll": -7.705590724945068, "perturbed_original_ll": -7.090944290161133, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` process) of the task instance to the current process ID of the task_runner process when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "sampled": "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using \"dependencies\" as an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "perturbed_sampled": ["Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because the impersonation is not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix minor issues when using \"dependencies\" as an identifier in the dependency list. Workaround for dependency injection regression (#149625)\n\nImprove installation-time performance by using"], "perturbed_original": ["Fix impersonation issue with LocalTaskJob (#16852) Running task files in a folder with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` process) of the task_runner process to the current process ID of the task_runner process when we use impersonation . By Ashlee Berlin-Taylor <ash_github@firemirror.com>"], "original_ll": -4.265727519989014, "sampled_ll": -3.310912847518921, "all_perturbed_sampled_ll": [-3.37408185005188], "all_perturbed_original_ll": [-4.27426815032959], "perturbed_sampled_ll": -3.37408185005188, "perturbed_original_ll": -4.27426815032959, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Rename the main branch of the Airflow repo to be `main` (#16149)", "sampled": "Rename the main branch of the Airflow repo to be `main` (#16149)This", "perturbed_sampled": ["Rename the main branch of the Airflow repo to be `main` (#16149)This"], "perturbed_original": ["Rename the main branch of the Airflow repo to be `main` (#16149)"], "original_ll": -4.861001014709473, "sampled_ll": -5.263483047485352, "all_perturbed_sampled_ll": [-5.263483047485352], "all_perturbed_original_ll": [-4.861001014709473], "perturbed_sampled_ll": -5.263483047485352, "perturbed_original_ll": -4.861001014709473, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "sampled": "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "perturbed_sampled": ["[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From"], "perturbed_original": ["[AIRFLOW-6084] Add info endpoint to experimental api (#6651)"], "original_ll": -5.949138641357422, "sampled_ll": -6.390750408172607, "all_perturbed_sampled_ll": [-6.390750408172607], "all_perturbed_original_ll": [-5.949138641357422], "perturbed_sampled_ll": -6.390750408172607, "perturbed_original_ll": -5.949138641357422, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "sampled": "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "perturbed_sampled": ["[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There"], "perturbed_original": ["[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)"], "original_ll": -6.585800647735596, "sampled_ll": -6.959344387054443, "all_perturbed_sampled_ll": [-6.959344387054443], "all_perturbed_original_ll": [-6.585800647735596], "perturbed_sampled_ll": -6.959344387054443, "perturbed_original_ll": -6.585800647735596, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of time to create, and each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "sampled": "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,", "perturbed_sampled": ["Further speed connexion API tests with pytest session fixtures (#14746) Creating the Flask s testing sessions for Connexion take a significant amount of time. So it should be possible to reduce the complexity of those tests and make them quicker on the fly. Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support for models with default attributes explicit. Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,"], "perturbed_original": ["Further speed for API tests with pytest session variables. Creating the API and Connexion take a significant amount of time to create, and each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the configuration and test scope for api_connexion/endpoints down to 1 for me."], "original_ll": -3.846102476119995, "sampled_ll": -3.116626501083374, "all_perturbed_sampled_ll": [-3.3698692321777344], "all_perturbed_original_ll": [-3.942631721496582], "perturbed_sampled_ll": -3.3698692321777344, "perturbed_original_ll": -3.942631721496582, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update external docs URL for Segment (#13645)", "sampled": "Update external docs URL for Segment (#13645)The", "perturbed_sampled": ["Update external docs URL for Segment (#13645)The"], "perturbed_original": ["Update external docs URL for Segment (#13645)"], "original_ll": -6.923836708068848, "sampled_ll": -7.498605251312256, "all_perturbed_sampled_ll": [-7.498605251312256], "all_perturbed_original_ll": [-6.923836708068848], "perturbed_sampled_ll": -7.498605251312256, "perturbed_original_ll": -6.923836708068848, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy to worker deployments, in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the", "sampled": "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side worker templates more flexible when adding the use_rerun action or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to delay run start time or delay the start of a run by default.\n\nThis commit", "perturbed_sampled": ["Add support for modifying the deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more flexibility on the server side.\n\nSupport for \"run, after each run\" and \"run with new worker available at each run\" actions (#15100) This commit makes server side worker templates more flexible in adding the \"random\" or setting the status of a worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit updates server side templates to include a \"retry action\" to delay the start of another run by a fixed amount of time or delay the start of a run by default.\n\nThis commit"], "perturbed_original": ["Add support for modifying celery worker deployment update strategy (#15213) This includes settings to the worker template to allow passing a non-default deployment update strategy to worker deployments, that includes celery workers. The values have been set to allow 100% maxSurge along with 75% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. This allows new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time. `maxSurge` is the number of pods that can be scheduled beyond the replica count during a rolling deploy. Its possible to specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas scheduled when a rolling deployment started it would launch 4 new pods and then scale down the existing pods as the"], "original_ll": -3.409484624862671, "sampled_ll": -2.601733684539795, "all_perturbed_sampled_ll": [-2.611940622329712], "all_perturbed_original_ll": [-3.5326263904571533], "perturbed_sampled_ll": -2.611940622329712, "perturbed_original_ll": -3.5326263904571533, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add fudament for API based on connexion (#8149)", "sampled": "Add fudament for API based on connexion (#8149)So", "perturbed_sampled": ["Add fudament for API based on connexion (#8149)So"], "perturbed_original": ["Add fudament for API based on connexion (#8149)"], "original_ll": -6.420657157897949, "sampled_ll": -7.135887145996094, "all_perturbed_sampled_ll": [-7.135887145996094], "all_perturbed_original_ll": [-6.420657157897949], "perturbed_sampled_ll": -7.135887145996094, "perturbed_original_ll": -6.420657157897949, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Cleanup KubernetsPodOpertor tests (#15475)", "sampled": "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "perturbed_sampled": ["Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup"], "perturbed_original": ["Cleanup KubernetsPodOpertor tests (#15475)"], "original_ll": -7.8532633781433105, "sampled_ll": -7.018958568572998, "all_perturbed_sampled_ll": [-7.018958568572998], "all_perturbed_original_ll": [-7.8532633781433105], "perturbed_sampled_ll": -7.018958568572998, "perturbed_original_ll": -7.8532633781433105, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add example DAG and system test for MySQLToGCSOperator (#10990)", "sampled": "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "perturbed_sampled": ["Add example DAG and system test for MySQLToGCSOperator (#10990)A"], "perturbed_original": ["Add example DAG and system test for MySQLToGCSOperator (#10990)"], "original_ll": -6.152090072631836, "sampled_ll": -6.518339157104492, "all_perturbed_sampled_ll": [-6.518339157104492], "all_perturbed_original_ll": [-6.152090072631836], "perturbed_sampled_ll": -6.518339157104492, "perturbed_original_ll": -6.152090072631836, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix PyPI spelling (#13864)", "sampled": "Fix PyPI spelling (#13864)WASHINGTON,", "perturbed_sampled": ["Fix PyPI spelling (#13864)WASHINGTON,"], "perturbed_original": ["Fix PyPI spelling (#13864)"], "original_ll": -7.816772937774658, "sampled_ll": -8.67366886138916, "all_perturbed_sampled_ll": [-8.67366886138916], "all_perturbed_original_ll": [-7.816772937774658], "perturbed_sampled_ll": -8.67366886138916, "perturbed_original_ll": -7.816772937774658, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "sampled": "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "perturbed_sampled": ["[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's"], "perturbed_original": ["[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)"], "original_ll": -5.924832820892334, "sampled_ll": -6.113379001617432, "all_perturbed_sampled_ll": [-6.113379001617432], "all_perturbed_original_ll": [-5.924832820892334], "perturbed_sampled_ll": -6.113379001617432, "perturbed_original_ll": -5.924832820892334, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.", "sampled": "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.A", "perturbed_sampled": ["[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization , and show open slots and used slots.A"], "perturbed_original": ["[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots, used slots, and used slots."], "original_ll": -5.5927534103393555, "sampled_ll": -5.786152362823486, "all_perturbed_sampled_ll": [-5.950326442718506], "all_perturbed_original_ll": [-5.242125988006592], "perturbed_sampled_ll": -5.950326442718506, "perturbed_original_ll": -5.242125988006592, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 To be reviewed by : Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "sampled": "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a new command line argument parser to parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x releases which led to an unexpected error message when the script was executed from a Python3.x version that", "perturbed_sampled": ["Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Implement a new command line command prompt interface that attempts to parse a handwritten argument from a command line prompt. http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x releases that lead to an unexpected error message when the script was executed from a Python3.x version that"], "perturbed_original": ["Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Allow argument manipulation with argparse. - Replaced positional argument for PACKAGE with optional argument. # 13069 To be reviewed by : Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>"], "original_ll": -2.868077516555786, "sampled_ll": -3.188203811645508, "all_perturbed_sampled_ll": [-3.286492109298706], "all_perturbed_original_ll": [-3.168302297592163], "perturbed_sampled_ll": -3.286492109298706, "perturbed_original_ll": -3.168302297592163, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make models/taskinstance.py pylint compatible (#10499)", "sampled": "Make models/taskinstance.py pylint compatible (#10499)Still", "perturbed_sampled": ["Make models/taskinstance.py pylint compatible (#10499)Still"], "perturbed_original": ["Make models/taskinstance.py pylint compatible (#10499)"], "original_ll": -5.924649238586426, "sampled_ll": -6.753892421722412, "all_perturbed_sampled_ll": [-6.753892421722412], "all_perturbed_original_ll": [-5.924649238586426], "perturbed_sampled_ll": -6.753892421722412, "perturbed_original_ll": -5.924649238586426, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "sampled": "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "perturbed_sampled": ["[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\""], "perturbed_original": ["[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)"], "original_ll": -6.061471462249756, "sampled_ll": -6.3326826095581055, "all_perturbed_sampled_ll": [-6.3326826095581055], "all_perturbed_original_ll": [-6.061471462249756], "perturbed_sampled_ll": -6.3326826095581055, "perturbed_original_ll": -6.061471462249756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "sampled": "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "perturbed_sampled": ["[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About"], "perturbed_original": ["[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)"], "original_ll": -5.8410468101501465, "sampled_ll": -6.46656608581543, "all_perturbed_sampled_ll": [-6.46656608581543], "all_perturbed_original_ll": [-5.8410468101501465], "perturbed_sampled_ll": -6.46656608581543, "perturbed_original_ll": -5.8410468101501465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "sampled": "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "perturbed_sampled": ["[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration"], "perturbed_original": ["[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration"], "original_ll": -3.6449153423309326, "sampled_ll": -3.6449153423309326, "all_perturbed_sampled_ll": [-3.6449153423309326], "all_perturbed_original_ll": [-3.6449153423309326], "perturbed_sampled_ll": -3.6449153423309326, "perturbed_original_ll": -3.6449153423309326, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "sampled": "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "perturbed_sampled": ["[AIRFLOW-6326] Sort cli commands and arg (#6881)\""], "perturbed_original": ["[AIRFLOW-6326] Sort cli commands and arg (#6881)"], "original_ll": -6.4958271980285645, "sampled_ll": -6.957827568054199, "all_perturbed_sampled_ll": [-6.957827568054199], "all_perturbed_original_ll": [-6.4958271980285645], "perturbed_sampled_ll": -6.957827568054199, "perturbed_original_ll": -6.4958271980285645, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to use same version as the image but it should be unified (and in 1.10 when trying to build 2.7 image it would fail).", "sampled": "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the module will be added to a shared library it fails", "perturbed_sampled": ["Default python version is used when building image (#13285) For image build the python version is set to PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the path can be passed as a path parameter (#13478) it fails"], "perturbed_original": ["Default python version is used when building image (#13285) For image build the python version is passed to the build script, but there is one part of the build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to use different python version as the image but it should be unified (and in 1.10 when trying to build image from a file system it would fail)."], "original_ll": -3.6570262908935547, "sampled_ll": -3.1501235961914062, "all_perturbed_sampled_ll": [-3.1512627601623535], "all_perturbed_original_ll": [-3.9140713214874268], "perturbed_sampled_ll": -3.1512627601623535, "perturbed_original_ll": -3.9140713214874268, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Check python version before starting triggerer (#18926)", "sampled": "Check python version before starting triggerer (#18926)One", "perturbed_sampled": ["Check python version before starting triggerer (#18926)One"], "perturbed_original": ["Check python version before starting triggerer (#18926)"], "original_ll": -6.845289707183838, "sampled_ll": -7.593542098999023, "all_perturbed_sampled_ll": [-7.593542098999023], "all_perturbed_original_ll": [-6.845289707183838], "perturbed_sampled_ll": -7.593542098999023, "perturbed_original_ll": -6.845289707183838, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "sampled": "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "perturbed_sampled": ["Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>"], "perturbed_original": ["Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>"], "original_ll": -4.542260646820068, "sampled_ll": -4.120544910430908, "all_perturbed_sampled_ll": [-4.120544910430908], "all_perturbed_original_ll": [-4.542260646820068], "perturbed_sampled_ll": -4.120544910430908, "perturbed_original_ll": -4.542260646820068, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was not.", "sampled": "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was no", "perturbed_sampled": ["Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when there were no"], "perturbed_original": ["Remove Brent from Collaborators (#18182) ; he is already a committer so we don't this entry here. It was needed only when he was not."], "original_ll": -4.801669120788574, "sampled_ll": -4.851532936096191, "all_perturbed_sampled_ll": [-4.752305030822754], "all_perturbed_original_ll": [-4.8741044998168945], "perturbed_sampled_ll": -4.752305030822754, "perturbed_original_ll": -4.8741044998168945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "sampled": "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "perturbed_sampled": ["[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or"], "perturbed_original": ["[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)"], "original_ll": -4.827899932861328, "sampled_ll": -5.03603982925415, "all_perturbed_sampled_ll": [-5.03603982925415], "all_perturbed_original_ll": [-4.827899932861328], "perturbed_sampled_ll": -5.03603982925415, "perturbed_original_ll": -4.827899932861328, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "sampled": "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "perturbed_sampled": ["Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still"], "perturbed_original": ["Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)"], "original_ll": -4.170490741729736, "sampled_ll": -4.739984035491943, "all_perturbed_sampled_ll": [-4.739984035491943], "all_perturbed_original_ll": [-4.170490741729736], "perturbed_sampled_ll": -4.739984035491943, "perturbed_original_ll": -4.170490741729736, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well.", "sampled": "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the `exit` clause inside", "perturbed_sampled": ["Docs: Remind of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the `exit` clause inside"], "perturbed_original": ["Docs: Clarify behavior of new plugins. Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well."], "original_ll": -2.8407065868377686, "sampled_ll": -2.932061195373535, "all_perturbed_sampled_ll": [-3.204906702041626], "all_perturbed_original_ll": [-3.6201655864715576], "perturbed_sampled_ll": -3.204906702041626, "perturbed_original_ll": -3.6201655864715576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the worker pods that execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "sampled": "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels rather than running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "perturbed_sampled": ["[AIRFLOW-4739] Add ability to arbitrarily define user pod labels (#5376) Allow task definitions to specify labels rather than running a specific kubernetes user instead of running a definition in the entire cluster (#5743) Add new label for"], "perturbed_original": ["[AIRFLOW-4739] Add ability to specify kubernetes worker pod labels (#5376) Allow s a task to specify labels on the worker pods that execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`"], "original_ll": -3.903278350830078, "sampled_ll": -4.159265041351318, "all_perturbed_sampled_ll": [-4.346123218536377], "all_perturbed_original_ll": [-3.920492649078369], "perturbed_sampled_ll": -4.346123218536377, "perturbed_original_ll": -3.920492649078369, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "sampled": "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of DAG-API to #7627.", "perturbed_sampled": ["[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an experimental implementation of DAG-API to #7627."], "perturbed_original": ["[AIRFLOW-7080] Adds endpoint for experimental API to return a DAG's paused state (#7737) Adds an additional endpoint for experimental API to return the paused state of a DAG."], "original_ll": -4.299529075622559, "sampled_ll": -4.042131423950195, "all_perturbed_sampled_ll": [-3.956977128982544], "all_perturbed_original_ll": [-4.155221462249756], "perturbed_sampled_ll": -3.956977128982544, "perturbed_original_ll": -4.155221462249756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - Fix functionality last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG in DB - Change name last_scheduler_run to last_parsed_time, to better reflect what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for DB (from @makr) - add more debug output into DB options (#14572) #14579 - add a couple of new command lines. #14788 - add \"dbl\" and 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - new output of DB check is also output into a string, that matches", "perturbed_sampled": ["Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) : - add 'all' command (#14436) - add DB support (from @makr) (#14572) - set _start_time to null for DB (from @makr) - add more options into DB options (#14572) #14579 - add a couple of new command lines to DB build (#14589) - add \"dbl\" and 'all' command lines to DB build, to help debug ging #14728 - add \"all\" option in options file (#15008) - Add new DB command - DB check is also output into a string, that matches"], "perturbed_original": ["Rename last_scheduler_run into last_parsed_time, and ensure availability in DB (#14581) - The functionality last_scheduler_run was missed in the implementation when migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This makes the current schema fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of the data in queries. Check availability in DB - Change d the name to last_parsed_time, to better reflect what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Karal Naik <kaxilnaik@gmail.com>"], "original_ll": -3.4800262451171875, "sampled_ll": -2.965261220932007, "all_perturbed_sampled_ll": [-3.106248617172241], "all_perturbed_original_ll": [-3.7661616802215576], "perturbed_sampled_ll": -3.106248617172241, "perturbed_original_ll": -3.7661616802215576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. The --rm flag is added.", "sampled": "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "perturbed_sampled": ["[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed until after this change but now I have to remove them with `docker rm`. This should fix any bugs you"], "perturbed_original": ["[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery <unk>. The --rm flag is added."], "original_ll": -4.8016862869262695, "sampled_ll": -4.509748458862305, "all_perturbed_sampled_ll": [-4.5009284019470215], "all_perturbed_original_ll": [-5.0341315269470215], "perturbed_sampled_ll": -4.5009284019470215, "perturbed_original_ll": -5.0341315269470215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation mechanism this happens frequently enought and it is easy enough to fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "sampled": "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path to add a pip dependency (#13805) + Support for using \"py\" for path/filename name argument for cwd() function (#13979) + Add a -n option to pip install --help to get all available help files (#13952) + Added config.py to install pips from sources. * Added support for Windows + added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "perturbed_sampled": ["Add s python-core dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path to add python dependency (#13805) + Support for using \"py\" for path/filename name argument . function (#13979) + Add a -n or -y pip install --help to get all available help files (#13952) + Added config.py to install pips from sources. * Added support for Windows 10 + Supports Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always provides Windows 10 support Support"], "perturbed_original": ["Add open id dependency * Update initdb requirement Seems that python3-openid dependency is not properly installed by tools like poetry (it is properly installed by pip). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. While we don\u2019t use poetry as an official installation mechanism this happens frequently enought and it is easy enough to fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg"], "original_ll": -3.8997714519500732, "sampled_ll": -3.5295112133026123, "all_perturbed_sampled_ll": [-3.8681271076202393], "all_perturbed_original_ll": [-4.1727094650268555], "perturbed_sampled_ll": -3.8681271076202393, "perturbed_original_ll": -4.1727094650268555, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "sampled": "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also changes documentation output, in particular with respect to the default configuration", "perturbed_sampled": ["Doc: Use ``closer.lua`` for documentation on downloading sources (#18179) - Follows first version of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also changes documentation output, in particular with respect to the default configuration"], "perturbed_original": ["Doc: Use ``closer.lua`` script for downloading sources - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use hyperlinks with direct download from mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)"], "original_ll": -3.535367488861084, "sampled_ll": -3.2464778423309326, "all_perturbed_sampled_ll": [-3.2133734226226807], "all_perturbed_original_ll": [-3.8670969009399414], "perturbed_sampled_ll": -3.2133734226226807, "perturbed_original_ll": -3.8670969009399414, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in CI. The image was significantly smaller, but then for local development and testing you needed both full CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs in Travis, it increased time needed by developers to have a working environment and to keep it updated every time it was needed (by minutes) Also having two separately managed images made it rather complex to join some of the Travis CI jobs (there is a follow-up change with getting rid of Checklicence image). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "sampled": "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in the main task, and hence there was no effect on other tasks. This may have been because the main task had very limited execution time, but the results from the other tasks suggest that it had a more substantial effect. In the task with a small gain, you may need to adjust the speed of a small decrease of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for providing the details of this improvement, both in terms of how it affects execution time while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also received permission from my", "perturbed_sampled": ["[AIRFLOW-5830] Get a slim image (#6494) The slim image gave a small gain on executing the tests in the main task, and hence there was no effect in the other tasks. While we have expected that the slim image in the main task had very limited execution time, the results from the other tasks suggest that it had a more substantial time gain. However, since it is only passing the task with a small gain, you may need to adjust the speed to compensate for this small decrease of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank my colleague on the project for providing the testing guidelines for this improvement, both in terms of how it affects execution time while drawing tests and how he actually performed the task. The speed gain is in most cases about a 1:1 factor improvement over drawing the object. I also received permission from my"], "perturbed_original": ["[AIRFLOW-5830] Get rid of slim image (#6494) Slim-CI image gave only very small gain on executing the tests in CI. The image was significantly smaller, but then for local development and testing you needed both full CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of the CLI update to the image it turned to be premature optimisation really. While it sped-up (slightly on 10-20 seconds) some static check jobs , it increased time needed by developers to have a working environment and keep it updated every time the image changes or to run some tests as needed (by minutes) Also using two separately managed images made it rather complex to join some of them for new features in Travis CI (this is about to change with the coming of Checklicence image). With this change both static checks and tests are executed using single image per run. This also opens doors for further simplification of the scripts and easier implementation of production image."], "original_ll": -4.367262840270996, "sampled_ll": -3.17258358001709, "all_perturbed_sampled_ll": [-3.2742762565612793], "all_perturbed_original_ll": [-4.371788501739502], "perturbed_sampled_ll": -3.2742762565612793, "perturbed_original_ll": -4.371788501739502, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "sampled": "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "perturbed_sampled": ["[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When"], "perturbed_original": ["[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)"], "original_ll": -5.481460094451904, "sampled_ll": -5.807520389556885, "all_perturbed_sampled_ll": [-5.807520389556885], "all_perturbed_original_ll": [-5.481460094451904], "perturbed_sampled_ll": -5.807520389556885, "perturbed_original_ll": -5.481460094451904, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are defined on DAG. But I just found out that while we were storing the task-level callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we don't display them in the Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", "sampled": "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "perturbed_sampled": ["BugFix: Dag-level Callback will not run (#13651) In the airflow-test.json, Airflow now accepts /api/callback.json as a callback to receive new calls before the queue is cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -pv argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL query string using HTTP JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON"], "perturbed_original": ["BugFix: Dag-level Callback s are not run (#13651) In https://github.com/apache/airflow/pull/13163 - I was trying to only run Callback requests when they are defined on DAG. But I just found out that while we were storing the task-level callbacks as string in the JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store the callbacks as string, we don't display them in the Webserver and thus the actual contents are not used anywhere in the scheduler, this Scheduler just checks if the callbacks are defined and it runs them when the DAG is returned to"], "original_ll": -3.375293493270874, "sampled_ll": -2.7995567321777344, "all_perturbed_sampled_ll": [-3.010971784591675], "all_perturbed_original_ll": [-3.399210214614868], "perturbed_sampled_ll": -3.010971784591675, "perturbed_original_ll": -3.399210214614868, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed * Version suffix is only used to rename the binary packages not for the version itself * Release process description is updated with the release process * Package version is consistent - leading 0s are skipped in month and day", "sampled": "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to exist on the backports list already released by backports. This release candidate adds those changes and also updates all other packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "perturbed_sampled": ["Prepare release candidate to backport compiled packages (#8891) After preparing the release candidate and reviewing the packages, some changes turned out to exist on the backports list already released . This update adds those changes and also updates all other packages in the release so they don't need a build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * glpixmap: Fixed the comment \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is"], "perturbed_original": ["Prepare release candidate for backport packages (#8891) on the 2020.5.19 release candidate thread, after reviewing the packages, some changes turned out to be necessary. Therefore the date was changed to 2020.05.19 and this release candidate contains several of the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed * Version suffix is only used to rename the binary packages not for the version itself * Release process description is updated with the release process * Package s are not consistent - version numbers are skipped in month and day"], "original_ll": -4.2701568603515625, "sampled_ll": -3.0865206718444824, "all_perturbed_sampled_ll": [-3.317427158355713], "all_perturbed_original_ll": [-4.295200824737549], "perturbed_sampled_ll": -3.317427158355713, "perturbed_original_ll": -4.295200824737549, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'.", "sampled": "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';", "perturbed_sampled": ["Change render to render_template in plugins.rst (#13560) : Change render to render_template as BaseView object has no attribute 'render';"], "perturbed_original": ["Change render to render_template in plugins.rst (#13560) Changing render to render_template as the template has no attribute 'render'."], "original_ll": -4.506881237030029, "sampled_ll": -4.57221794128418, "all_perturbed_sampled_ll": [-4.4494099617004395], "all_perturbed_original_ll": [-4.215434551239014], "perturbed_sampled_ll": -4.4494099617004395, "perturbed_original_ll": -4.215434551239014, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "sampled": "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "perturbed_sampled": ["[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The"], "perturbed_original": ["[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)"], "original_ll": -5.850271224975586, "sampled_ll": -6.190381050109863, "all_perturbed_sampled_ll": [-6.190381050109863], "all_perturbed_original_ll": [-5.850271224975586], "perturbed_sampled_ll": -6.190381050109863, "perturbed_original_ll": -5.850271224975586, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure process.join is called with timeout", "sampled": "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "perturbed_sampled": ["Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where the test will fail in some cases if task files with tasks.\n\nFix quarantined/flaky tests in (#17378)"], "perturbed_original": ["Fix quarantined/flaky tests in test_local_task_job.py (#17385) This test tries to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure that they get called with timeout"], "original_ll": -3.2500123977661133, "sampled_ll": -2.8332574367523193, "all_perturbed_sampled_ll": [-3.192422389984131], "all_perturbed_original_ll": [-3.280881881713867], "perturbed_sampled_ll": -3.192422389984131, "perturbed_original_ll": -3.280881881713867, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from status", "sampled": "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from statusA.txt", "perturbed_sampled": ["Fixes * teething issues (#10145) * wrong issue id (from tests) * comment field was copied from statusA.txt"], "perturbed_original": ["Fixes : * teething issues (#10145) * wrong issue id (from tests) * comment field was copied from status"], "original_ll": -5.746716022491455, "sampled_ll": -5.737582683563232, "all_perturbed_sampled_ll": [-5.2583699226379395], "all_perturbed_original_ll": [-5.121947765350342], "perturbed_sampled_ll": -5.2583699226379395, "perturbed_original_ll": -5.121947765350342, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "sampled": "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "perturbed_sampled": ["Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe"], "perturbed_original": ["Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description"], "original_ll": -5.9300127029418945, "sampled_ll": -6.424129009246826, "all_perturbed_sampled_ll": [-6.424129009246826], "all_perturbed_original_ll": [-5.9300127029418945], "perturbed_sampled_ll": -6.424129009246826, "perturbed_original_ll": -5.9300127029418945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "sampled": "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "perturbed_sampled": ["[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By"], "perturbed_original": ["[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)"], "original_ll": -5.813697338104248, "sampled_ll": -6.207937717437744, "all_perturbed_sampled_ll": [-6.207937717437744], "all_perturbed_original_ll": [-5.813697338104248], "perturbed_sampled_ll": -6.207937717437744, "perturbed_original_ll": -5.813697338104248, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we recommend everyone to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group of user when starting the container, so the only real difference is that the \"airflow\" unmodifiable files such as python code belong to different group, which has no real value. You can still use whatever group you want for mounted files and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and we also have to remember that if the user belongs to other groups in the host, it will also", "sampled": "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to include port 80", "perturbed_sampled": ["Overwrite AirFlow from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue , and we don't want to fix it. (CVE-2018-634) #18760 : Out of range of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Out of range of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to include port 80"], "perturbed_original": ["Remove AIRFLOW_GID from images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons. However, for a long time we recommend everyone to use GID=0 in AIRFLOW images as it would make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group of the user after starting the container, so the only real difference is that the \"airflow\" unmodifiable files such as git will belong to different group, which has no effect in the performance. You can still use whatever group you want for mounted and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is active (and we also have to say that if the user belongs to other groups in the host, it will also"], "original_ll": -3.594271421432495, "sampled_ll": -2.159637212753296, "all_perturbed_sampled_ll": [-2.292301893234253], "all_perturbed_original_ll": [-3.6049535274505615], "perturbed_sampled_ll": -2.292301893234253, "perturbed_original_ll": -3.6049535274505615, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is repeated very often in discussions on Slack, so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "sampled": "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:", "perturbed_sampled": ["Clarify : Installation of new packages in docker-compose env \u2013 A problem with installing new packages in the Docker-compose environment \u2014 some packages may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:"], "perturbed_original": ["Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is repeated from time to time in discussions on Slack, so I decided to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200"], "original_ll": -3.7741124629974365, "sampled_ll": -2.6552650928497314, "all_perturbed_sampled_ll": [-2.96482515335083], "all_perturbed_original_ll": [-3.730803966522217], "perturbed_sampled_ll": -2.96482515335083, "perturbed_original_ll": -3.730803966522217, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "sampled": "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to other users from the same device while retaining data with existing credentials - now only changes the", "perturbed_sampled": ["Update persists-credentials (#13401) - to add persist-credentials #13389 wrongly added persists-credentials to other users from a root device while retaining data with existing credentials - now only changes the"], "perturbed_original": ["Update s: Previous change to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag."], "original_ll": -4.701555252075195, "sampled_ll": -4.391566276550293, "all_perturbed_sampled_ll": [-4.690366268157959], "all_perturbed_original_ll": [-4.951606750488281], "perturbed_sampled_ll": -4.690366268157959, "perturbed_original_ll": -4.951606750488281, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on if it should run. This changes that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open PRs from ashb/airflow, and this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this for was to find the \"parent\" commit, but there is any easier way we can do that: HEAD^1 with a fetch depth of 2 to the checkout option. So I've removed calculating that and where it is used. If we need to bring it back we should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "sampled": "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test is run in the context of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on lintingtest.py or a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "perturbed_sampled": ["Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing OpenAPI tests, but was abandoned due to the lack of testing support. This test now uses the selecttest module to do all the testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect code when the test is run in the context of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit test is back. Test-suite integration with linting and unit testing is back. When using the linting command against a build or a module in the module tree, the results of linting/tests uite are collected into a report, which is used for comparing the test with the"], "perturbed_original": ["Convert code generation tests to use selective checks (#12092) This ci test was bundled in with the existing code generation engine, but then performed it's _own_ checks for how it should run. This changes that to have selective_ci_checks.sh as a separate check. Additionally CI_SOURCE_REPO was often wrong -- at least for when I don't open PRs from ashb/airflow, and this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this was to find the \"parent\" commit, but there is any easier way we can do a \"repo\" with a fetch depth of 2 to the checkout option. So the only change I've seen is calculating that and where it is used. If we need to bring it back we should use the output from the `potiuk/get-workflow-origin` action -- that gets this value"], "original_ll": -4.373823642730713, "sampled_ll": -3.0044662952423096, "all_perturbed_sampled_ll": [-3.0615203380584717], "all_perturbed_original_ll": [-4.310787677764893], "perturbed_sampled_ll": -3.0615203380584717, "perturbed_original_ll": -4.310787677764893, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove redundant code to serialized k8s.V1Pod (#11602)", "sampled": "Remove redundant code to serialized k8s.V1Pod (#11602)The", "perturbed_sampled": ["Remove redundant code to serialized k8s.V1Pod (#11602)The"], "perturbed_original": ["Remove redundant code to serialized k8s.V1Pod (#11602)"], "original_ll": -6.118261337280273, "sampled_ll": -6.488145351409912, "all_perturbed_sampled_ll": [-6.488145351409912], "all_perturbed_original_ll": [-6.118261337280273], "perturbed_sampled_ll": -6.488145351409912, "perturbed_original_ll": -6.118261337280273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "sampled": "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "perturbed_sampled": ["Add the Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's"], "perturbed_original": ["Add Google Cloud Memorystore Memcached Operators : Create New Memcached Operator. Lead by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>"], "original_ll": -3.2125680446624756, "sampled_ll": -2.809814691543579, "all_perturbed_sampled_ll": [-2.848389148712158], "all_perturbed_original_ll": [-3.4539432525634766], "perturbed_sampled_ll": -2.848389148712158, "perturbed_original_ll": -3.4539432525634766, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our main \"CI\" workflow (the only one we have at the moment)", "sampled": "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our community and allow for an upcoming build to finish. This is", "perturbed_sampled": ["Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds in our community and allow for an upcoming push to a proper finish. This is"], "perturbed_original": ["Cancel queued/running builds on second push to PR - uses an action from PR to cancel any running builds for our main \"CI\" workflow (the only one we have at the moment)"], "original_ll": -4.701043128967285, "sampled_ll": -4.690144062042236, "all_perturbed_sampled_ll": [-4.827752590179443], "all_perturbed_original_ll": [-4.821591377258301], "perturbed_sampled_ll": -4.827752590179443, "perturbed_original_ll": -4.821591377258301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your cluster on by setting zone to None or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in short form.", "sampled": "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to Dataproc (#5171) When Auto-Zone is applied, all GDT rules", "perturbed_sampled": ["[AIRFLOW-3143] Support AutoZoneWith DataprocClusterCreateOperator (#5169) Allow you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules . Support AutoZone (#5171) When Auto-Zone is applied, all GDT rules"], "perturbed_original": ["[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your cluster on . This sets all of the values for auto zone to None or a blank string, making the parameter optional. Per the spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all cluster configuration files need to be in short form."], "original_ll": -4.201833724975586, "sampled_ll": -3.4625566005706787, "all_perturbed_sampled_ll": [-3.963139295578003], "all_perturbed_original_ll": [-3.952280282974243], "perturbed_sampled_ll": -3.963139295578003, "perturbed_original_ll": -3.952280282974243, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Resurrect python openapi client generator (#19155)", "sampled": "Resurrect python openapi client generator (#19155)When", "perturbed_sampled": ["Resurrect python openapi client generator (#19155)When"], "perturbed_original": ["Resurrect python openapi client generator (#19155)"], "original_ll": -6.4396209716796875, "sampled_ll": -7.058577060699463, "all_perturbed_sampled_ll": [-7.058577060699463], "all_perturbed_original_ll": [-6.4396209716796875], "perturbed_sampled_ll": -7.058577060699463, "perturbed_original_ll": -6.4396209716796875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out at 1200px", "sampled": "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out the topmost", "perturbed_sampled": ["set max tree width to 1200px (#16067) the totalwidth of the tree will still depend on the window size like before, but max out the topmost"], "perturbed_original": ["set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max tree width will be approximately 1200px"], "original_ll": -4.2503743171691895, "sampled_ll": -4.525360107421875, "all_perturbed_sampled_ll": [-4.355558395385742], "all_perturbed_original_ll": [-3.945439338684082], "perturbed_sampled_ll": -4.355558395385742, "perturbed_original_ll": -3.945439338684082, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix is_terminal_support_colors functtion (#9734)", "sampled": "Fix is_terminal_support_colors functtion (#9734)In", "perturbed_sampled": ["Fix is_terminal_support_colors functtion (#9734)In"], "perturbed_original": ["Fix is_terminal_support_colors functtion (#9734)"], "original_ll": -5.248140335083008, "sampled_ll": -5.800577163696289, "all_perturbed_sampled_ll": [-5.800577163696289], "all_perturbed_original_ll": [-5.248140335083008], "perturbed_sampled_ll": -5.800577163696289, "perturbed_original_ll": -5.248140335083008, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode by first (and only installs the GPL library by an optional extra, not by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "sampled": "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the library when a license doesn't apply. This should have been an exception, and it should have been handled with a warning. We now just get an error for this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "perturbed_sampled": ["Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the libraries when a license doesn't apply. This should have raised an exception, and it should have been followed by a warning. We shouldn't get an error for this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build failed if `--disable-test` was"], "perturbed_original": ["Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to follow the GPL'd `unidecode` module, but since 3.0[1] it has used text-unidecode by first installing the lib(it also installs the GPL library by an optional extra, not by default) so we can now use it. This lets us upgreade slugify libraries from the previous 1.2 to the latest 1.3, which is the version one of dbt's code is running. See update [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300"], "original_ll": -3.7174630165100098, "sampled_ll": -3.3693156242370605, "all_perturbed_sampled_ll": [-3.4607808589935303], "all_perturbed_original_ll": [-3.853532552719116], "perturbed_sampled_ll": -3.4607808589935303, "perturbed_original_ll": -3.853532552719116, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "sampled": "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "perturbed_sampled": ["[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It"], "perturbed_original": ["[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)"], "original_ll": -5.482000827789307, "sampled_ll": -5.832630634307861, "all_perturbed_sampled_ll": [-5.832630634307861], "all_perturbed_original_ll": [-5.482000827789307], "perturbed_sampled_ll": -5.832630634307861, "perturbed_original_ll": -5.482000827789307, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to send a success indicator when providing data files into", "sampled": "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "perturbed_sampled": ["[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSDrawSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google TV streaming as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug"], "perturbed_original": ["[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a feedback sensor to address the use case. You\u2019re accepting files from a third party vendor who refuses to send a success indicator when providing data files into"], "original_ll": -4.3078789710998535, "sampled_ll": -1.6591389179229736, "all_perturbed_sampled_ll": [-1.7331546545028687], "all_perturbed_original_ll": [-4.870422840118408], "perturbed_sampled_ll": -1.7331546545028687, "perturbed_original_ll": -4.870422840118408, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * updated doc", "sampled": "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * fix some", "perturbed_sampled": ["[AIRFLOW-4265] Lineage backend did not work normally * add debug log * change SendMessage to staticmethod * fix some"], "perturbed_original": ["[AIRFLOW-4265] Lineage backend did not work . * add debug log * change SendMessage to staticmethod * updated doc"], "original_ll": -5.632072448730469, "sampled_ll": -5.407741546630859, "all_perturbed_sampled_ll": [-5.584212303161621], "all_perturbed_original_ll": [-5.760003566741943], "perturbed_sampled_ll": -5.584212303161621, "perturbed_original_ll": -5.760003566741943, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Type-annotate SkipMixin and BaseXCom (#20011)", "sampled": "Type-annotate SkipMixin and BaseXCom (#20011)After", "perturbed_sampled": ["Type-annotate SkipMixin and BaseXCom (#20011)After"], "perturbed_original": ["Type-annotate SkipMixin and BaseXCom (#20011)"], "original_ll": -6.719922065734863, "sampled_ll": -7.235746383666992, "all_perturbed_sampled_ll": [-7.235746383666992], "all_perturbed_original_ll": [-6.719922065734863], "perturbed_sampled_ll": -7.235746383666992, "perturbed_original_ll": -6.719922065734863, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "sampled": "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "perturbed_sampled": ["[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]"], "perturbed_original": ["[AIRFLOW-XXX] Add autogenerated TOC (#6038)"], "original_ll": -5.2277350425720215, "sampled_ll": -4.811791896820068, "all_perturbed_sampled_ll": [-4.811791896820068], "all_perturbed_original_ll": [-5.2277350425720215], "perturbed_sampled_ll": -4.811791896820068, "perturbed_original_ll": -5.2277350425720215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926)", "sampled": "Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926)Branch:", "perturbed_sampled": ["Docs: Suggest airflow Env vars instead of Airflow Vars in best practises doc (#16926)Branch:"], "perturbed_original": ["Docs: Suggest use of Env XP and use of Airflow Vars in best practises doc (#16926)"], "original_ll": -4.6324944496154785, "sampled_ll": -4.696807384490967, "all_perturbed_sampled_ll": [-5.55005407333374], "all_perturbed_original_ll": [-5.3766961097717285], "perturbed_sampled_ll": -5.55005407333374, "perturbed_original_ll": -5.3766961097717285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being built in a separate workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs or PRs with failed jobs, the workflow will add a comment to the PR stating the reason why the PR", "sampled": "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "perturbed_sampled": ["Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running and cancelling. (#112322)\n\nRemoved obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably on the Windows platform. (This is possible due to the faster build functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target package and the new \"setopt...\". (#114876)\n\nWhen updating to an incompatible release release, do not re-generate install.sh or uninstall.sh .\n\nproperty"], "perturbed_original": ["Improve d canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run to the original PR with comment stating that the image is being built using separate workflow, including the link to that workflow. Thirdly - when canceling duplicate PRs that come with failed jobs, the user can add a comment to the PR with the reason why the PR"], "original_ll": -3.7442238330841064, "sampled_ll": -2.9868364334106445, "all_perturbed_sampled_ll": [-3.075493574142456], "all_perturbed_original_ll": [-4.05593204498291], "perturbed_sampled_ll": -3.075493574142456, "perturbed_original_ll": -4.05593204498291, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. And that is quite common case.", "sampled": "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. (Thanks to Matt J. H. for", "perturbed_sampled": ["Add bucket_name to template fileds in S3 operators (#13973) : it's impossible to create buckets using for example a bucket_name field. (Thanks to Matt J. H. for"], "perturbed_original": ["Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example name and date. And that is a very common case."], "original_ll": -5.528934955596924, "sampled_ll": -5.260386943817139, "all_perturbed_sampled_ll": [-4.727869510650635], "all_perturbed_original_ll": [-5.140732765197754], "perturbed_sampled_ll": -4.727869510650635, "perturbed_original_ll": -5.140732765197754, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Refactor create_app in airflow/www/app.py (#9291)", "sampled": "Refactor create_app in airflow/www/app.py (#9291)With", "perturbed_sampled": ["Refactor create_app in airflow/www/app.py (#9291)With"], "perturbed_original": ["Refactor create_app in airflow/www/app.py (#9291)"], "original_ll": -5.197209358215332, "sampled_ll": -5.8196868896484375, "all_perturbed_sampled_ll": [-5.8196868896484375], "all_perturbed_original_ll": [-5.197209358215332], "perturbed_sampled_ll": -5.8196868896484375, "perturbed_original_ll": -5.197209358215332, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "sampled": "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "perturbed_sampled": ["Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At"], "perturbed_original": ["Don't add User role perms to custom roles. (#13856) closes: #9245 #13511"], "original_ll": -5.268479824066162, "sampled_ll": -5.752065181732178, "all_perturbed_sampled_ll": [-5.752065181732178], "all_perturbed_original_ll": [-5.268479824066162], "perturbed_sampled_ll": -5.752065181732178, "perturbed_original_ll": -5.268479824066162, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter sidecar", "sampled": "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter and", "perturbed_sampled": [". ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter and"], "perturbed_original": ["Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the chart in the exporter sidecar"], "original_ll": -4.755273342132568, "sampled_ll": -5.021922588348389, "all_perturbed_sampled_ll": [-6.0949015617370605], "all_perturbed_original_ll": [-5.144566535949707], "perturbed_sampled_ll": -6.0949015617370605, "perturbed_original_ll": -5.144566535949707, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate between task instances even if it were functional.", "sampled": "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "perturbed_sampled": ["Remove datepicker for task instance detail view (#15284) Closes an issue by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced by the new OSX version where some image tags were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single"], "perturbed_original": ["Remove datepicker for task instance detail view . I solved the issue #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to move rather than fix. Also, I don't think it was an effective UX to navigate between task instances even if it were functional."], "original_ll": -3.6370608806610107, "sampled_ll": -3.17030668258667, "all_perturbed_sampled_ll": [-3.160888671875], "all_perturbed_original_ll": [-3.7847111225128174], "perturbed_sampled_ll": -3.160888671875, "perturbed_original_ll": -3.7847111225128174, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "sampled": "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "perturbed_sampled": ["[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W."], "perturbed_original": ["[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)"], "original_ll": -6.068178176879883, "sampled_ll": -6.045558452606201, "all_perturbed_sampled_ll": [-6.045558452606201], "all_perturbed_original_ll": [-6.068178176879883], "perturbed_sampled_ll": -6.045558452606201, "perturbed_original_ll": -6.068178176879883, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "sampled": "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "perturbed_sampled": ["[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics"], "perturbed_original": ["[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)"], "original_ll": -6.4276957511901855, "sampled_ll": -6.436801910400391, "all_perturbed_sampled_ll": [-6.436801910400391], "all_perturbed_original_ll": [-6.4276957511901855], "perturbed_sampled_ll": -6.436801910400391, "perturbed_original_ll": -6.4276957511901855, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "sampled": "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "perturbed_sampled": ["AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still"], "perturbed_original": ["AIRFLOW-6062 Watch worker pods from all namespaces (#8546)"], "original_ll": -6.724536418914795, "sampled_ll": -7.363710880279541, "all_perturbed_sampled_ll": [-7.363710880279541], "all_perturbed_original_ll": [-6.724536418914795], "perturbed_sampled_ll": -7.363710880279541, "perturbed_original_ll": -6.724536418914795, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "sampled": "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably want the newest", "perturbed_sampled": ["The last time they try to push the python build image when building on release branches (#15394) They use the same image image as master (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where it does not want the newest"], "perturbed_original": ["Don't try to push the python build image when building on release builds. They use the same python image as master (as already told in the script ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch."], "original_ll": -3.4654805660247803, "sampled_ll": -3.6694979667663574, "all_perturbed_sampled_ll": [-3.7931699752807617], "all_perturbed_original_ll": [-3.3451411724090576], "perturbed_sampled_ll": -3.7931699752807617, "perturbed_original_ll": -3.3451411724090576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "sampled": "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "perturbed_sampled": ["Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer"], "perturbed_original": ["Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`"], "original_ll": -4.177849769592285, "sampled_ll": -4.8453145027160645, "all_perturbed_sampled_ll": [-4.8453145027160645], "all_perturbed_original_ll": [-4.177849769592285], "perturbed_sampled_ll": -4.8453145027160645, "perturbed_original_ll": -4.177849769592285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "sampled": "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "perturbed_sampled": ["[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved"], "perturbed_original": ["[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)"], "original_ll": -5.315189361572266, "sampled_ll": -5.54821252822876, "all_perturbed_sampled_ll": [-5.54821252822876], "all_perturbed_original_ll": [-5.315189361572266], "perturbed_sampled_ll": -5.54821252822876, "perturbed_original_ll": -5.315189361572266, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "sampled": "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "perturbed_sampled": ["Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make the gconfig-editor configurable by logging"], "perturbed_original": ["Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format to display - Use two back-ticks for code-block instead of italics"], "original_ll": -4.726297855377197, "sampled_ll": -4.182541370391846, "all_perturbed_sampled_ll": [-4.2446489334106445], "all_perturbed_original_ll": [-4.528244972229004], "perturbed_sampled_ll": -4.2446489334106445, "perturbed_original_ll": -4.528244972229004, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Invalidate Vault cached prop when not authenticated (#17387)", "sampled": "Invalidate Vault cached prop when not authenticated (#17387)This", "perturbed_sampled": ["Invalidate Vault cached prop when not authenticated (#17387)This"], "perturbed_original": ["Invalidate Vault cached prop when not authenticated (#17387)"], "original_ll": -6.747575283050537, "sampled_ll": -7.304325580596924, "all_perturbed_sampled_ll": [-7.304325580596924], "all_perturbed_original_ll": [-6.747575283050537], "perturbed_sampled_ll": -7.304325580596924, "perturbed_original_ll": -6.747575283050537, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "sampled": "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "perturbed_sampled": ["[AIRFLOW-6517] make merge_dicts function recursive (#7111)For"], "perturbed_original": ["[AIRFLOW-6517] make merge_dicts function recursive (#7111)"], "original_ll": -6.1086835861206055, "sampled_ll": -6.581632137298584, "all_perturbed_sampled_ll": [-6.581632137298584], "all_perturbed_original_ll": [-6.1086835861206055], "perturbed_sampled_ll": -6.581632137298584, "perturbed_original_ll": -6.1086835861206055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "sampled": "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "perturbed_sampled": ["[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This"], "perturbed_original": ["[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)"], "original_ll": -6.3997931480407715, "sampled_ll": -6.662501811981201, "all_perturbed_sampled_ll": [-6.662501811981201], "all_perturbed_original_ll": [-6.3997931480407715], "perturbed_sampled_ll": -6.662501811981201, "perturbed_original_ll": -6.3997931480407715, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the meantime, importing the modules and then referring to the classes using modulename.Classname avoids the issue.", "sampled": "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add error flag to prevent auto-adding documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some class docs for some classes", "perturbed_sampled": ["stop rendering some class docs in wrong place \u2013 stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been switched to documentation/ providers.yandex.org will be treated as new docs and will be included in updates.* add error flag to prevent auto-adding documentation/ for docs.yandex.org for example with documentation/ must only be updated against documentation.yandex.org * fix some errors of auto-doc generation for example with docs.yandex.org when docstring is missing * add some class docs for some classes"], "perturbed_original": ["stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place : for example, include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses. I haven't tracked down the root cause. In the meantime, importing the modules and then relating to the classes using autoapi fixes the issue."], "original_ll": -3.5842390060424805, "sampled_ll": -2.805124044418335, "all_perturbed_sampled_ll": [-3.2150070667266846], "all_perturbed_original_ll": [-3.5180418491363525], "perturbed_sampled_ll": -3.2150070667266846, "perturbed_original_ll": -3.5180418491363525, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "sampled": "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "perturbed_sampled": ["[AIRFLOW-6262] add on_execute_callback to operators (#6831)I"], "perturbed_original": ["[AIRFLOW-6262] add on_execute_callback to operators (#6831)"], "original_ll": -5.792147636413574, "sampled_ll": -6.281447887420654, "all_perturbed_sampled_ll": [-6.281447887420654], "all_perturbed_original_ll": [-5.792147636413574], "perturbed_sampled_ll": -6.281447887420654, "perturbed_original_ll": -5.792147636413574, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "sampled": "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "perturbed_sampled": ["[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I"], "perturbed_original": ["[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)"], "original_ll": -5.860006809234619, "sampled_ll": -6.2007269859313965, "all_perturbed_sampled_ll": [-6.2007269859313965], "all_perturbed_original_ll": [-5.860006809234619], "perturbed_sampled_ll": -6.2007269859313965, "perturbed_original_ll": -5.860006809234619, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "sampled": "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "perturbed_sampled": ["Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to"], "perturbed_original": ["Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>"], "original_ll": -4.2777862548828125, "sampled_ll": -3.9892313480377197, "all_perturbed_sampled_ll": [-3.9892313480377197], "all_perturbed_original_ll": [-4.2777862548828125], "perturbed_sampled_ll": -3.9892313480377197, "perturbed_original_ll": -4.2777862548828125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run something with the write credentials. We should not override the in_container scripts, however because they become part of the image, so we should use those that came with the PR. That's why we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. We've made sure that those scripts in ci are self-contained and they do not need reach outside", "sampled": "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an unexpected conflict when we build the image from the command line. Thanks @joshmcdaniel for the report :) #10368 [master] add support for custom-generated image paths for imgTags (#10919) Fixed a possible crash when building Images with the wrong set of images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building images using the -c option without directories (#10925) #10856 [master] add -O option for Image::build_with_dir and", "perturbed_sampled": ["Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the image on Windows (from in-place to in-process!) It might help avoid an unexpected runtime problem when we build the image from the command line. Thanks @joshmcdaniel for the report :) #10368 [master] add support for custom-generated image paths . (#10919) Fixed a possible crash when building Images with the wrong set of images when doing an override of in_container in -g (#10924) [master] fix crash by not detecting images for some build paths #10914 Update #10910 Add support for custom-generated image paths (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building images using the -c option without an image path. #10856 [master] add -O option for Image::build_with_dir and"], "perturbed_original": ["Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on CI. We are overriding the ci scripts that we had before and build the image with the scripts taken from master to not give the authors the possibiility to run something with the write credentials. We should not override the in_container scripts, however because they become part of the image, so we should use those that we had before and build the image as well. That's why we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder scripts with the one from master. We've decided that those scripts in ci are self-contained and they no longer need reach outside"], "original_ll": -3.3929834365844727, "sampled_ll": -2.9961636066436768, "all_perturbed_sampled_ll": [-2.9339797496795654], "all_perturbed_original_ll": [-3.232356309890747], "perturbed_sampled_ll": -2.9339797496795654, "perturbed_original_ll": -3.232356309890747, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts against 200) So no need to check status code explicitly if either of these two methods are used to check the response.", "sampled": "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected with non-json queries, due to incorrect test assertions being marked as wrong on the test runner path. The test cases have also been", "perturbed_sampled": [") asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) will not pass as expected in all queries, due to incorrect test assertions being marked as wrong on the test runner path. The test cases have also been"], "perturbed_original": ["Remove redundant asserts in the response validation method call. Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts ). So no need to check status code explicitly if either one of two methods are used to check the response."], "original_ll": -3.6087636947631836, "sampled_ll": -2.2107129096984863, "all_perturbed_sampled_ll": [-2.25616717338562], "all_perturbed_original_ll": [-3.6946589946746826], "perturbed_sampled_ll": -2.25616717338562, "perturbed_original_ll": -3.6946589946746826, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "sampled": "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler will exit with a false exit code, as is normal (although I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the last exit code is false. In any case, it looks like we", "perturbed_sampled": ["Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler will exit with a false exit code, as is normal (although there is a note in the manual). That last check that comes out a false exit code is a feature introduced in Airflow 7.28, and thus the last exit code is false. In any case, it looks like we"], "perturbed_original": ["Pass ing SchedulerJob / SchedulerJob. Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be called, it would serialize all the DAGs in the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```"], "original_ll": -3.684197187423706, "sampled_ll": -3.6977012157440186, "all_perturbed_sampled_ll": [-3.686619281768799], "all_perturbed_original_ll": [-3.650411605834961], "perturbed_sampled_ll": -3.686619281768799, "perturbed_original_ll": -3.650411605834961, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "sampled": "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "perturbed_sampled": ["[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The"], "perturbed_original": ["[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)"], "original_ll": -5.665452480316162, "sampled_ll": -6.117630481719971, "all_perturbed_sampled_ll": [-6.117630481719971], "all_perturbed_original_ll": [-5.665452480316162], "perturbed_sampled_ll": -6.117630481719971, "perturbed_original_ll": -5.665452480316162, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "sampled": "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "perturbed_sampled": ["Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2"], "perturbed_original": ["Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155"], "original_ll": -4.144833564758301, "sampled_ll": -3.8231377601623535, "all_perturbed_sampled_ll": [-3.8231377601623535], "all_perturbed_original_ll": [-4.144833564758301], "perturbed_sampled_ll": -3.8231377601623535, "perturbed_original_ll": -4.144833564758301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Persist tags params in pagination (#15411)", "sampled": "Persist tags params in pagination (#15411)What's", "perturbed_sampled": ["Persist tags params in pagination (#15411)What's"], "perturbed_original": ["Persist tags params in pagination (#15411)"], "original_ll": -6.270488262176514, "sampled_ll": -6.770099639892578, "all_perturbed_sampled_ll": [-6.770099639892578], "all_perturbed_original_ll": [-6.270488262176514], "perturbed_sampled_ll": -6.770099639892578, "perturbed_original_ll": -6.270488262176514, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "sampled": "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "perturbed_sampled": ["[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There"], "perturbed_original": ["[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)"], "original_ll": -5.87980318069458, "sampled_ll": -6.262747287750244, "all_perturbed_sampled_ll": [-6.262747287750244], "all_perturbed_original_ll": [-5.87980318069458], "perturbed_sampled_ll": -6.262747287750244, "perturbed_original_ll": -5.87980318069458, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "sampled": "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "perturbed_sampled": ["Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug"], "perturbed_original": ["Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>"], "original_ll": -4.834514617919922, "sampled_ll": -4.32852029800415, "all_perturbed_sampled_ll": [-4.32852029800415], "all_perturbed_original_ll": [-4.834514617919922], "perturbed_sampled_ll": -4.32852029800415, "perturbed_original_ll": -4.834514617919922, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "sampled": "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a day or more. In that case this function will hang", "perturbed_sampled": ["Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit test timer will exceed the allocated time for the job for this function by a day or more. In that case this function will hang"], "perturbed_original": ["Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a large margin. (9X% tests successful). This change increases the limit."], "original_ll": -5.284729480743408, "sampled_ll": -4.908852577209473, "all_perturbed_sampled_ll": [-4.476420879364014], "all_perturbed_original_ll": [-5.272579193115234], "perturbed_sampled_ll": -4.476420879364014, "perturbed_original_ll": -5.272579193115234, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "sampled": "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "perturbed_sampled": ["Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file."], "perturbed_original": ["Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license"], "original_ll": -3.4906115531921387, "sampled_ll": -3.350512981414795, "all_perturbed_sampled_ll": [-3.350512981414795], "all_perturbed_original_ll": [-3.4906115531921387], "perturbed_sampled_ll": -3.350512981414795, "perturbed_original_ll": -3.4906115531921387, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "sampled": "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "perturbed_sampled": ["[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On"], "perturbed_original": ["[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)"], "original_ll": -6.128391742706299, "sampled_ll": -6.423715114593506, "all_perturbed_sampled_ll": [-6.423715114593506], "all_perturbed_original_ll": [-6.128391742706299], "perturbed_sampled_ll": -6.423715114593506, "perturbed_original_ll": -6.128391742706299, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "sampled": "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "perturbed_sampled": ["Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling"], "perturbed_original": ["Change Airflow version to 2.0.0a1 in Updating.md (#11508)"], "original_ll": -4.6624040603637695, "sampled_ll": -4.973670959472656, "all_perturbed_sampled_ll": [-4.973670959472656], "all_perturbed_original_ll": [-4.6624040603637695], "perturbed_sampled_ll": -4.973670959472656, "perturbed_original_ll": -4.6624040603637695, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fix broken link in experimental API deprecation headers (#13547)", "sampled": "fix broken link in experimental API deprecation headers (#13547)\"", "perturbed_sampled": ["fix broken link in experimental API deprecation headers (#13547)\""], "perturbed_original": ["fix broken link in experimental API deprecation headers (#13547)"], "original_ll": -5.491623878479004, "sampled_ll": -6.088904857635498, "all_perturbed_sampled_ll": [-6.088904857635498], "all_perturbed_original_ll": [-5.491623878479004], "perturbed_sampled_ll": -6.088904857635498, "perturbed_original_ll": -5.491623878479004, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error", "sampled": "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "perturbed_sampled": ["add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system test for Google"], "perturbed_original": ["add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag s and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error"], "original_ll": -4.384793758392334, "sampled_ll": -3.5069780349731445, "all_perturbed_sampled_ll": [-3.524186134338379], "all_perturbed_original_ll": [-4.417460918426514], "perturbed_sampled_ll": -3.524186134338379, "perturbed_original_ll": -4.417460918426514, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this method is inherently insecure in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Added explanation about it and explicit warning against this.", "sampled": "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works for us, but it will make the output of docker compose less human readable. We can find an easy", "perturbed_sampled": ["Adds warning about using multiple versions of packages (#16935) While we are supporting installing packages dynamically in our project, using docker build and docker compose while running docker has a few issues since we can add some dependencies: The current version works for us, but it will make the output of docker compose less human readable. We can find an easy"], "perturbed_original": ["Warning about using dynamic installation of packages. While we are supporting installing packages dynamically in our helm chart of compose while testing, this method is inherently insecure in production environments (it opens up for an attack that removing just one dependency of a dependency migh bring the Airflow deployment down). Added explanation about it and explicit warning against this."], "original_ll": -4.928926944732666, "sampled_ll": -4.152543067932129, "all_perturbed_sampled_ll": [-3.97829008102417], "all_perturbed_original_ll": [-4.878025054931641], "perturbed_sampled_ll": -3.97829008102417, "perturbed_original_ll": -4.878025054931641, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "sampled": "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "perturbed_sampled": ["[AIRFLOW-6362] Fix typehint for CommandType (#6906)In"], "perturbed_original": ["[AIRFLOW-6362] Fix typehint for CommandType (#6906)"], "original_ll": -5.7572245597839355, "sampled_ll": -6.185062885284424, "all_perturbed_sampled_ll": [-6.185062885284424], "all_perturbed_original_ll": [-5.7572245597839355], "perturbed_sampled_ll": -6.185062885284424, "perturbed_original_ll": -5.7572245597839355, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "sampled": "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the", "perturbed_sampled": ["Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the"], "perturbed_original": ["Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the references to the example folder structure instead of the later references to be consistent with the other examples in the documentation."], "original_ll": -3.8832805156707764, "sampled_ll": -4.034200191497803, "all_perturbed_sampled_ll": [-4.034200191497803], "all_perturbed_original_ll": [-4.069734573364258], "perturbed_sampled_ll": -4.034200191497803, "perturbed_original_ll": -4.069734573364258, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "sampled": "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "perturbed_sampled": ["Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]"], "perturbed_original": ["Change from Instance attribute to variable in JdbcOperator.execute (#7819)"], "original_ll": -4.893798351287842, "sampled_ll": -4.80837345123291, "all_perturbed_sampled_ll": [-4.80837345123291], "all_perturbed_original_ll": [-4.893798351287842], "perturbed_sampled_ll": -4.80837345123291, "perturbed_original_ll": -4.893798351287842, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectifies the mistake.", "sampled": "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectified a couple", "perturbed_sampled": ["Add issue form template for Helm Chat ! With so many people reviewing nobody else's project we forgot to add Helm Chart issue form :) This rectified a couple"], "perturbed_original": ["Add issue form template for Helm Chat (#17917) With so many folks reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectifies the mistake."], "original_ll": -5.648014545440674, "sampled_ll": -5.883324146270752, "all_perturbed_sampled_ll": [-6.2040934562683105], "all_perturbed_original_ll": [-5.74547004699707], "perturbed_sampled_ll": -6.2040934562683105, "perturbed_original_ll": -5.74547004699707, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Added Zalando to ``INTHEWILD.md`` (#18480)", "sampled": "Added Zalando to ``INTHEWILD.md`` (#18480)This", "perturbed_sampled": ["Added Zalando to ``INTHEWILD.md`` (#18480)This"], "perturbed_original": ["Added Zalando to ``INTHEWILD.md`` (#18480)"], "original_ll": -5.991697311401367, "sampled_ll": -6.383927822113037, "all_perturbed_sampled_ll": [-6.383927822113037], "all_perturbed_original_ll": [-5.991697311401367], "perturbed_sampled_ll": -6.383927822113037, "perturbed_original_ll": -5.991697311401367, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "sampled": "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more script options being combined", "perturbed_sampled": ["( instead of exit() ) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The <unk>ssh<unk> option should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more script options being combined"], "perturbed_original": ["Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start and exit, directly from site.py. However, if the interpreter is started with the `-S` flag, or if a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is loaded before start of the interpreter and is guaranteed to be present."], "original_ll": -2.9568071365356445, "sampled_ll": -2.93730092048645, "all_perturbed_sampled_ll": [-3.456916332244873], "all_perturbed_original_ll": [-2.964160203933716], "perturbed_sampled_ll": -3.456916332244873, "perturbed_original_ll": -2.964160203933716, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Correct typo (#20345)", "sampled": "Correct typo (#20345)There", "perturbed_sampled": ["Correct typo (#20345)There"], "perturbed_original": ["Correct typo (#20345)"], "original_ll": -7.567523956298828, "sampled_ll": -8.698271751403809, "all_perturbed_sampled_ll": [-8.698271751403809], "all_perturbed_original_ll": [-7.567523956298828], "perturbed_sampled_ll": -8.698271751403809, "perturbed_original_ll": -7.567523956298828, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "sampled": "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "perturbed_sampled": ["[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I"], "perturbed_original": ["[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)"], "original_ll": -6.203967094421387, "sampled_ll": -6.544014930725098, "all_perturbed_sampled_ll": [-6.544014930725098], "all_perturbed_original_ll": [-6.203967094421387], "perturbed_sampled_ll": -6.544014930725098, "perturbed_original_ll": -6.203967094421387, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Excludes .git-modules from rat-check (#14759)", "sampled": "Excludes .git-modules from rat-check (#14759)The", "perturbed_sampled": ["Excludes .git-modules from rat-check (#14759)The"], "perturbed_original": ["Excludes .git-modules from rat-check (#14759)"], "original_ll": -5.920202255249023, "sampled_ll": -6.4828782081604, "all_perturbed_sampled_ll": [-6.4828782081604], "all_perturbed_original_ll": [-5.920202255249023], "perturbed_sampled_ll": -6.4828782081604, "perturbed_original_ll": -5.920202255249023, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "sampled": "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "perturbed_sampled": ["[AIRFLOW-4681] Make sensors module for OpenCrypt easier and more concise (#7309) Remove all references to \"module\" from pylintt() , (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and"], "perturbed_original": ["[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local tests, such as the snakebite client in the sensors module. Ignore a huge number of checks for higher impact code."], "original_ll": -4.905141353607178, "sampled_ll": -2.940661907196045, "all_perturbed_sampled_ll": [-3.2096433639526367], "all_perturbed_original_ll": [-4.8335723876953125], "perturbed_sampled_ll": -3.2096433639526367, "perturbed_original_ll": -4.8335723876953125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameError: name 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "sampled": "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a migration fails it", "perturbed_sampled": ["Fix wait-for-migrations command in helm chart (#12522) If the migrations were not being applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: <unk>desc<unk> returned `name\" of \"Person::new\".\n\nIf a migration fails it"], "perturbed_original": ["Fix wait-for-migrations command in helm chart (#12522) If the migrations package is applied this would fail with `NameError: name 'log' is not defined`. (I guess no one would notice, as the container would restart, and try again.)"], "original_ll": -4.288155555725098, "sampled_ll": -3.775444269180298, "all_perturbed_sampled_ll": [-4.144096374511719], "all_perturbed_original_ll": [-4.319756507873535], "perturbed_sampled_ll": -4.144096374511719, "perturbed_original_ll": -4.319756507873535, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded with no return code because LocalTaskJob.handle_task_exit was not called after the task succeeded. Hence,", "sampled": "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "perturbed_sampled": ["Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the job is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a problem in Load job in http (http/http s) (#14883) Fixed one more crash bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10"], "perturbed_original": ["Run mini scheduler if stopped during task exit (#16289) Currently, the chances of tasks being killed by job heartbeat is high. This is because, after marking a task successful/failed in case the mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded and raises a return code because the command heartbeat is not called after the task succeeded. Hence,"], "original_ll": -4.049161434173584, "sampled_ll": -3.0231916904449463, "all_perturbed_sampled_ll": [-3.4519433975219727], "all_perturbed_original_ll": [-4.187267780303955], "perturbed_sampled_ll": -3.4519433975219727, "perturbed_original_ll": -4.187267780303955, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "sampled": "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "perturbed_sampled": ["Change prefix of AwsDynamoDB hook module (#11209) * align import path and configuration file in aws-dynamodb.yml * change the configuration file"], "perturbed_original": ["Change prefix of AwsDynamoDB hook module (#11209) Add hook to import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>"], "original_ll": -4.142271995544434, "sampled_ll": -3.656273126602173, "all_perturbed_sampled_ll": [-4.111469268798828], "all_perturbed_original_ll": [-4.076627254486084], "perturbed_sampled_ll": -4.111469268798828, "perturbed_original_ll": -4.076627254486084, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the buttons next to each DAG on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when trying to mark DAG run as failed - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "sampled": "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. You can switch between different screens or use both screens as shortcuts in a single Airflow session. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "perturbed_sampled": ["Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, which wasn't being used anymore with the new version of Airflow . Tap to continue and tap to dismiss). - Removed access to `Settings->More/Favourites/` (#19583) As you can see, changes were made to make Airflow behave better (especially for users with multiple Airflow screens). It's really only important if you use different screens from time to time. You can easily toggle between different screens or use both screens as shortcuts in a single Airflow session. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now"], "perturbed_original": ["Small improvements for DAG run. Similar to (#18715) I slightly improved some UI elements that were slightly thrown off. - Removed some weird behavior that made the buttons for each DAG on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks in view when trying to mark DAG run as failed - Changed font size inside DAG run circle from 8 to 4 characters. It makes it a lot more readable and it can still fit even 4-digit"], "original_ll": -3.972243547439575, "sampled_ll": -2.623509645462036, "all_perturbed_sampled_ll": [-2.7965095043182373], "all_perturbed_original_ll": [-3.846461296081543], "perturbed_sampled_ll": -2.7965095043182373, "perturbed_original_ll": -3.846461296081543, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add conn_type to Fix failing Livy Tests (#9258)", "sampled": "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "perturbed_sampled": ["Add conn_type to Fix failing Livy Tests (#9258)Dependencies:"], "perturbed_original": ["Add conn_type to Fix failing Livy Tests (#9258)"], "original_ll": -7.520173072814941, "sampled_ll": -6.660679817199707, "all_perturbed_sampled_ll": [-6.660679817199707], "all_perturbed_original_ll": [-7.520173072814941], "perturbed_sampled_ll": -6.660679817199707, "perturbed_original_ll": -7.520173072814941, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "sampled": "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "perturbed_sampled": ["Google Ads Hook: Support newer versions of the google-ads library (#17160)On"], "perturbed_original": ["Google Ads Hook: Support newer versions of the google-ads library (#17160)"], "original_ll": -5.2988739013671875, "sampled_ll": -5.799515724182129, "all_perturbed_sampled_ll": [-5.799515724182129], "all_perturbed_original_ll": [-5.2988739013671875], "perturbed_sampled_ll": -5.799515724182129, "perturbed_original_ll": -5.2988739013671875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "sampled": "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "perturbed_sampled": ["[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On"], "perturbed_original": ["[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)"], "original_ll": -6.4235148429870605, "sampled_ll": -6.8634257316589355, "all_perturbed_sampled_ll": [-6.8634257316589355], "all_perturbed_original_ll": [-6.4235148429870605], "perturbed_sampled_ll": -6.8634257316589355, "perturbed_original_ll": -6.4235148429870605, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "sampled": "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. [This image loading will", "perturbed_sampled": ["docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. [This image will will"], "perturbed_original": ["docs: TESTING.rst: fix not loading image (#14247) - [not just this loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not."], "original_ll": -2.693167209625244, "sampled_ll": -3.8226723670959473, "all_perturbed_sampled_ll": [-3.9104912281036377], "all_perturbed_original_ll": [-2.7435505390167236], "perturbed_sampled_ll": -3.9104912281036377, "perturbed_original_ll": -2.7435505390167236, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "sampled": "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main pagePosted", "perturbed_sampled": ["Add link to the main page to table of contents (#12594) Without this, it's not obvious how to get back to the main pagePosted"], "perturbed_original": ["Add link to docs index to table of contents . As I'm reading this, it's not obvious how to get back to the main page"], "original_ll": -4.041743755340576, "sampled_ll": -4.589877605438232, "all_perturbed_sampled_ll": [-4.054126262664795], "all_perturbed_original_ll": [-3.6734349727630615], "perturbed_sampled_ll": -4.054126262664795, "perturbed_original_ll": -3.6734349727630615, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "sampled": "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving Postgres table after database reload in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "perturbed_sampled": ["[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a duplicate user row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving or restoring a postgreshook file after database reload ing (#8640) Prevent crash when connecting to Postgres Hook with passwordless user (#8696) The method that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix"], "perturbed_original": ["[AIRFLOW-4734] Upsert functionality for MySQL: PostgresHook's subclass, for example DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's insert syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate the SQL. INSERT ON CONFLICT DO OR (\"new\" since Postgres 9.5)"], "original_ll": -3.757802724838257, "sampled_ll": -3.1991467475891113, "all_perturbed_sampled_ll": [-3.4352447986602783], "all_perturbed_original_ll": [-4.112466812133789], "perturbed_sampled_ll": -3.4352447986602783, "perturbed_original_ll": -4.112466812133789, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "sampled": "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for", "perturbed_sampled": ["Fix BaseSensorOperator soft_fail mode to respect downstream tasks (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for"], "perturbed_original": ["Adjust soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\"."], "original_ll": -4.939835071563721, "sampled_ll": -5.13771915435791, "all_perturbed_sampled_ll": [-4.985049247741699], "all_perturbed_original_ll": [-5.333724498748779], "perturbed_sampled_ll": -4.985049247741699, "perturbed_original_ll": -5.333724498748779, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add typing for grpc provider (#9884)", "sampled": "Add typing for grpc provider (#9884)Ripple", "perturbed_sampled": ["Add typing for grpc provider (#9884)Ripple"], "perturbed_original": ["Add typing for grpc provider (#9884)"], "original_ll": -6.876386642456055, "sampled_ll": -7.2989349365234375, "all_perturbed_sampled_ll": [-7.2989349365234375], "all_perturbed_original_ll": [-6.876386642456055], "perturbed_sampled_ll": -7.2989349365234375, "perturbed_original_ll": -6.876386642456055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecated classes lists", "sampled": "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "perturbed_sampled": ["Properly propagated warnings in operators (#9348) * Test operators in operators are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain"], "perturbed_original": ["Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Added refactored and deprecated classes lists"], "original_ll": -4.573043346405029, "sampled_ll": -4.3484954833984375, "all_perturbed_sampled_ll": [-4.363509654998779], "all_perturbed_original_ll": [-4.531724452972412], "perturbed_sampled_ll": -4.363509654998779, "perturbed_original_ll": -4.531724452972412, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "sampled": "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "perturbed_sampled": ["Dag bulk_sync_to_db dag_tag only remove not exists (#8189) * Dag bulk_sync_to_db dag_tag update hook updates (#8172) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db"], "perturbed_original": ["Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists (#8231) For now we remove all record in dag_tag, but actually we can't set to delete tag not exists in dag file anymore"], "original_ll": -3.9787375926971436, "sampled_ll": -2.1947596073150635, "all_perturbed_sampled_ll": [-2.139177083969116], "all_perturbed_original_ll": [-3.8730077743530273], "perturbed_sampled_ll": -2.139177083969116, "perturbed_original_ll": -3.8730077743530273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally * Kubernetes cluster is not deleted until environment is stopped * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "sampled": "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "perturbed_sampled": ["[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes management console * [AIRFLOW-5704] update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes local tests failing to pass benchmarks * [AIRFLOW-5704] fix tests failing when creating local test suite * [AIRFLOW-5704] fix tests failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have"], "perturbed_original": ["[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than the one from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally * Kubernetes tests do not crash when the local environment is stopped * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in Breeze to identify the right version * Kubernetes Version can be used to select the correct version * Editing Kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are"], "original_ll": -3.277486801147461, "sampled_ll": -1.931838870048523, "all_perturbed_sampled_ll": [-1.7844085693359375], "all_perturbed_original_ll": [-3.0787699222564697], "perturbed_sampled_ll": -1.7844085693359375, "perturbed_original_ll": -3.0787699222564697, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a dag object", "sampled": "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a dag objectThe", "perturbed_sampled": ["[AIRFLOW-3341] FAQ * object example (#4605) * added example of a function returning a dag objectThe"], "perturbed_original": ["[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a DAG object"], "original_ll": -5.816323757171631, "sampled_ll": -6.095394134521484, "all_perturbed_sampled_ll": [-6.103063583374023], "all_perturbed_original_ll": [-5.3167033195495605], "perturbed_sampled_ll": -6.103063583374023, "perturbed_original_ll": -5.3167033195495605, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Better message when Building Image fails or gets cancelled. (#11333)", "sampled": "Better message when Building Image fails or gets cancelled. (#11333)How", "perturbed_sampled": ["Better message when Building Image fails or gets cancelled. (#11333)How"], "perturbed_original": ["Better message when Building Image fails or gets cancelled. (#11333)"], "original_ll": -6.201083660125732, "sampled_ll": -6.958164215087891, "all_perturbed_sampled_ll": [-6.958164215087891], "all_perturbed_original_ll": [-6.201083660125732], "perturbed_sampled_ll": -6.958164215087891, "perturbed_original_ll": -6.201083660125732, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "sampled": "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "perturbed_sampled": ["[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This"], "perturbed_original": ["[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)"], "original_ll": -7.1574387550354, "sampled_ll": -7.375946998596191, "all_perturbed_sampled_ll": [-7.375946998596191], "all_perturbed_original_ll": [-7.1574387550354], "perturbed_sampled_ll": -7.375946998596191, "perturbed_original_ll": -7.1574387550354, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some code after a rebase. This adds the code and adds unit tests", "sampled": "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation needed to help the user understand the effect a particular file", "perturbed_sampled": ["[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation to help the user understand the effect a particular file"], "perturbed_original": ["[AIRFLOW-4883] Bug-fix for Kill hung file process managers . PR (#5605) was missing some code after I was making changes. This adds the code and adds unit tests"], "original_ll": -5.241806983947754, "sampled_ll": -5.1595635414123535, "all_perturbed_sampled_ll": [-5.164880752563477], "all_perturbed_original_ll": [-5.700996398925781], "perturbed_sampled_ll": -5.164880752563477, "perturbed_original_ll": -5.700996398925781, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove locks for upgrades in mssql (#17213) Closes: #17088", "sampled": "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "perturbed_sampled": ["Remove locks for upgrades in mssql (#17213) Closes: #17088The"], "perturbed_original": ["Remove locks for upgrades in mssql (#17213) Closes: #17088"], "original_ll": -4.8284101486206055, "sampled_ll": -5.301568984985352, "all_perturbed_sampled_ll": [-5.301568984985352], "all_perturbed_original_ll": [-4.8284101486206055], "perturbed_sampled_ll": -5.301568984985352, "perturbed_original_ll": -4.8284101486206055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "sampled": "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "perturbed_sampled": ["[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This"], "perturbed_original": ["[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)"], "original_ll": -5.327218055725098, "sampled_ll": -5.688559055328369, "all_perturbed_sampled_ll": [-5.688559055328369], "all_perturbed_original_ll": [-5.327218055725098], "perturbed_sampled_ll": -5.688559055328369, "perturbed_original_ll": -5.327218055725098, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not working on Windows. This fixes the problem by turning the error into warning, and directing the users to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also in our quick-start docker-compose.", "sampled": "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "perturbed_sampled": ["Improves quick-start docker-compose warnings . (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a nice hint about your input options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node , or set --extra-debug and --extra-info options in the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by"], "perturbed_original": ["Improves quick-start docker-compose warnings and documentation (#18164) The updated docker-compose had a bad behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not working on Windows. This fixes the problem , by changing the error into warning, and directing the users to the right instructions per operating system. Support for the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also in our quick-start docker-compose."], "original_ll": -3.794034957885742, "sampled_ll": -2.4158170223236084, "all_perturbed_sampled_ll": [-2.6227481365203857], "all_perturbed_original_ll": [-3.839282512664795], "perturbed_sampled_ll": -2.6227481365203857, "perturbed_original_ll": -3.839282512664795, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "sampled": "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "perturbed_sampled": ["Use the correct link for Apache Airflow Dockerhub repo . Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/"], "perturbed_original": ["Please provide correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not"], "original_ll": -3.3570971488952637, "sampled_ll": -2.6647961139678955, "all_perturbed_sampled_ll": [-3.7156076431274414], "all_perturbed_original_ll": [-3.4339585304260254], "perturbed_sampled_ll": -3.7156076431274414, "perturbed_original_ll": -3.4339585304260254, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and attach to it.", "sampled": "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "perturbed_sampled": ["Add an automatic restart to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers . We still expect the following, however. #10644 Update the documentation about \"Air"], "perturbed_original": ["Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, Airflow will not leave rogue ECS Tasks. At that point, ECSO operator will seek for any running instance and attach to it."], "original_ll": -5.026678085327148, "sampled_ll": -4.861417293548584, "all_perturbed_sampled_ll": [-4.697636604309082], "all_perturbed_original_ll": [-4.846114635467529], "perturbed_sampled_ll": -4.697636604309082, "perturbed_original_ll": -4.846114635467529, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "sampled": "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "perturbed_sampled": ["Add aws_conn_id to DynamoDBToS3Operator (#20363)New"], "perturbed_original": ["Add aws_conn_id to DynamoDBToS3Operator (#20363)"], "original_ll": -5.383778095245361, "sampled_ll": -5.835010051727295, "all_perturbed_sampled_ll": [-5.835010051727295], "all_perturbed_original_ll": [-5.383778095245361], "perturbed_sampled_ll": -5.835010051727295, "perturbed_original_ll": -5.383778095245361, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't need to compile Java code.", "sampled": "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using jdt-unstable the packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "perturbed_sampled": ["Use Debian's provided JRE from Buster (#8919) instead because the JDK (not even the JRE) from Sid is starting to break things. The new versions of the JRE/JRE-<version>-<br> can be installed with the following .\n\n. in jdt-unstable the packages that build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will add up with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when"], "perturbed_original": ["Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev (>= 8.4.0-4) and > gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > libmpx2 (>= 8.4.0-7) and only 8.3.0-6 is to be installed This changes our CI configuration to: 1. Not install something from Sid (only change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. No need to compile Java code."], "original_ll": -3.1274375915527344, "sampled_ll": -2.6779944896698, "all_perturbed_sampled_ll": [-3.0026626586914062], "all_perturbed_original_ll": [-3.091810941696167], "perturbed_sampled_ll": -3.0026626586914062, "perturbed_original_ll": -3.091810941696167, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "sampled": "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "perturbed_sampled": ["[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS"], "perturbed_original": ["[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)"], "original_ll": -6.312582492828369, "sampled_ll": -6.2323899269104, "all_perturbed_sampled_ll": [-6.2323899269104], "all_perturbed_original_ll": [-6.312582492828369], "perturbed_sampled_ll": -6.2323899269104, "perturbed_original_ll": -6.312582492828369, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve typing in airflow/models/pool.py (#9835)", "sampled": "Improve typing in airflow/models/pool.py (#9835)As", "perturbed_sampled": ["Improve typing in airflow/models/pool.py (#9835)As"], "perturbed_original": ["Improve typing in airflow/models/pool.py (#9835)"], "original_ll": -6.227518081665039, "sampled_ll": -6.864846229553223, "all_perturbed_sampled_ll": [-6.864846229553223], "all_perturbed_original_ll": [-6.227518081665039], "perturbed_sampled_ll": -6.864846229553223, "perturbed_original_ll": -6.227518081665039, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "perturbed_sampled": ["[AIRFLOW-6062] Executor would only delete workers in its own class * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Better error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set child parent of a worker"], "perturbed_original": ["[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * Executor would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -2.0847816467285156, "sampled_ll": -2.649420976638794, "all_perturbed_sampled_ll": [-2.5463671684265137], "all_perturbed_original_ll": [-2.334451675415039], "perturbed_sampled_ll": -2.5463671684265137, "perturbed_original_ll": -2.334451675415039, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "sampled": "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "perturbed_sampled": ["[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari"], "perturbed_original": ["[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)"], "original_ll": -5.82694673538208, "sampled_ll": -6.352202892303467, "all_perturbed_sampled_ll": [-6.352202892303467], "all_perturbed_original_ll": [-5.82694673538208], "perturbed_sampled_ll": -6.352202892303467, "perturbed_original_ll": -5.82694673538208, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster.", "sampled": "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume the redshift cluster.", "perturbed_sampled": ["Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These add the ability to pause and resume the redshift cluster."], "perturbed_original": ["(#19665) and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster."], "original_ll": -3.6216113567352295, "sampled_ll": -3.6062259674072266, "all_perturbed_sampled_ll": [-3.677670478820801], "all_perturbed_original_ll": [-3.966482639312744], "perturbed_sampled_ll": -3.677670478820801, "perturbed_original_ll": -3.966482639312744, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "sampled": "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "perturbed_sampled": ["Docker: Using task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection is suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The"], "perturbed_original": ["[AIRFLOW-5850] Capture task logs and log in (#6552) * [AIRFLOW-5850] Capture task logs and log in in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator"], "original_ll": -3.2144367694854736, "sampled_ll": -3.1164932250976562, "all_perturbed_sampled_ll": [-3.342484712600708], "all_perturbed_original_ll": [-3.5053999423980713], "perturbed_sampled_ll": -3.342484712600708, "perturbed_original_ll": -3.5053999423980713, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in docker-stack documentation (#16221)", "sampled": "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "perturbed_sampled": ["Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed"], "perturbed_original": ["Fix typo in docker-stack documentation (#16221)"], "original_ll": -5.430027961730957, "sampled_ll": -4.810083866119385, "all_perturbed_sampled_ll": [-4.810083866119385], "all_perturbed_original_ll": [-5.430027961730957], "perturbed_sampled_ll": -4.810083866119385, "perturbed_original_ll": -5.430027961730957, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "sampled": "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "perturbed_sampled": ["[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue Buffer\" [Z] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]"], "perturbed_original": ["[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue in case of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63."], "original_ll": -4.546453952789307, "sampled_ll": -3.7181692123413086, "all_perturbed_sampled_ll": [-3.7708852291107178], "all_perturbed_original_ll": [-4.559739589691162], "perturbed_sampled_ll": -3.7708852291107178, "perturbed_original_ll": -4.559739589691162, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "sampled": "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "perturbed_sampled": ["Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which the task was performed. A fix should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\""], "perturbed_original": ["Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in the clearing task results. The query did not have ordering and sometimes the tasks were returned in different order than expected."], "original_ll": -4.659791469573975, "sampled_ll": -4.204808235168457, "all_perturbed_sampled_ll": [-4.105716228485107], "all_perturbed_original_ll": [-4.750106334686279], "perturbed_sampled_ll": -4.105716228485107, "perturbed_original_ll": -4.750106334686279, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "sampled": "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "perturbed_sampled": ["Add error check for config_file parameter in GKEStartPodOperator (#17700)The"], "perturbed_original": ["Add error check for config_file parameter in GKEStartPodOperator (#17700)"], "original_ll": -6.057900905609131, "sampled_ll": -6.470308303833008, "all_perturbed_sampled_ll": [-6.470308303833008], "all_perturbed_original_ll": [-6.057900905609131], "perturbed_sampled_ll": -6.470308303833008, "perturbed_original_ll": -6.057900905609131, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "sampled": "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "perturbed_sampled": ["Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>"], "perturbed_original": ["Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>"], "original_ll": -5.106704235076904, "sampled_ll": -4.6911468505859375, "all_perturbed_sampled_ll": [-4.6911468505859375], "all_perturbed_original_ll": [-5.106704235076904], "perturbed_sampled_ll": -4.6911468505859375, "perturbed_original_ll": -5.106704235076904, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "sampled": "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "perturbed_sampled": ["Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The"], "perturbed_original": ["Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)"], "original_ll": -4.794984340667725, "sampled_ll": -5.170685768127441, "all_perturbed_sampled_ll": [-5.170685768127441], "all_perturbed_original_ll": [-4.794984340667725], "perturbed_sampled_ll": -5.170685768127441, "perturbed_original_ll": -4.794984340667725, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add some basic metrics to the Triggerer (#18214)", "sampled": "Add some basic metrics to the Triggerer (#18214)In", "perturbed_sampled": ["Add some basic metrics to the Triggerer (#18214)In"], "perturbed_original": ["Add some basic metrics to the Triggerer (#18214)"], "original_ll": -5.445986747741699, "sampled_ll": -5.9782538414001465, "all_perturbed_sampled_ll": [-5.9782538414001465], "all_perturbed_original_ll": [-5.445986747741699], "perturbed_sampled_ll": -5.9782538414001465, "perturbed_original_ll": -5.445986747741699, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the config, while it should be int like the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was forgotten. I think we can safely remove it because: * There", "sampled": "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used with XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "perturbed_sampled": ["[AIRFLOW-6740] Remove Undocumented, missing PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used with XFree86-EFI-32 (#6935), thus fixing many bugs\n\n(X11) Fix the errors caused by XFree86 when XFree86-CORE/QNX uses"], "perturbed_original": ["[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the config, while the value of <unk>num_proxies<unk> should be int like the other parameters. Those were fixed in #6901, but `num_proxies` was forgotten. Today, I think we can safely remove it . There"], "original_ll": -3.320007801055908, "sampled_ll": -2.675156831741333, "all_perturbed_sampled_ll": [-2.7071340084075928], "all_perturbed_original_ll": [-3.3172812461853027], "perturbed_sampled_ll": -2.7071340084075928, "perturbed_original_ll": -3.3172812461853027, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary rather than a NoneType since the argument should be a dictionary __Why__ * Building out type annotations is good for the code base * The query parameter is accessed with an index at one point, which means that it cannot be a None type, but should rather be defaulted to an empty dictionary if not provided * Remove useless return", "sampled": "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the code (Ezra), add support for a .getMethod that gets an instance and is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "perturbed_sampled": ["Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the code (Ezra), add support for a .getMethod that gets an instance, which is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, update unit tests for .getMethod() tests. * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . * Add type annotations to ZendeskHook; upgrade the test for Z"], "perturbed_original": ["Add type annotations to ZendeskHook, update unit test (#10888) * Add correct type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary rather than a NoneType since the argument should be a dictionary __Why__ * Introducing correct type annotations for ZendeskHook and each method * The type annotation is good for the code base * The instance is accessed with an index at one point, which means that it cannot be a None type, but should rather be defaulted to an empty dictionary if not provided * Remove useless return"], "original_ll": -3.707293748855591, "sampled_ll": -2.2390048503875732, "all_perturbed_sampled_ll": [-2.318131923675537], "all_perturbed_original_ll": [-3.4579644203186035], "perturbed_sampled_ll": -2.318131923675537, "perturbed_original_ll": -3.4579644203186035, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce amount of times the pod object", "sampled": "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "perturbed_sampled": ["[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was not working correctly on OpenVZ. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, which uses HTTP 1, to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` , which used HTTP 1, contains a typo in `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and avoid HTTP 1.5. (#3384) Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147"], "perturbed_original": ["[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs . - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults on <unk>args.py<unk> in comparison to `default_args.py`, also default `do_xcom_push` was overwritten , in contradiction to `BaseOperator` - kwarg `resources` is erroneously passed to `base_operator`, instead should be passed to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they can give arbitrary values in `setattr` - Reduce amount of times the pod object"], "original_ll": -3.4834775924682617, "sampled_ll": -3.0729053020477295, "all_perturbed_sampled_ll": [-3.254229784011841], "all_perturbed_original_ll": [-3.891055107116699], "perturbed_sampled_ll": -3.254229784011841, "perturbed_original_ll": -3.891055107116699, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "sampled": "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "perturbed_sampled": ["Add Parquet data type to BaseSQLToGCSOperator (#13359)In"], "perturbed_original": ["Add Parquet data type to BaseSQLToGCSOperator (#13359)"], "original_ll": -6.176843643188477, "sampled_ll": -6.630948066711426, "all_perturbed_sampled_ll": [-6.630948066711426], "all_perturbed_original_ll": [-6.176843643188477], "perturbed_sampled_ll": -6.630948066711426, "perturbed_original_ll": -6.176843643188477, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we reserve the right to delete it at any point (for instance once we add better methods of doing what the OP wanted with these hooks.)", "sampled": "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,", "perturbed_sampled": ["Mark passing pre/post control to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become easy to write without needing to care about the standard at all in order to make them useful. However the above concerns are addressed and addressed in the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,"], "perturbed_original": ["Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the complexity of both Airflow as well as the design of the DAG itself (for both API and code). By marking this features as experimental we reserve the right to delete it at any point (for instance once we add better methods of doing what the OP wanted with these hooks.)"], "original_ll": -4.022066593170166, "sampled_ll": -3.567488670349121, "all_perturbed_sampled_ll": [-3.6184139251708984], "all_perturbed_original_ll": [-4.0981764793396], "perturbed_sampled_ll": -3.6184139251708984, "perturbed_original_ll": -4.0981764793396, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "sampled": "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "perturbed_sampled": ["[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I"], "perturbed_original": ["[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)"], "original_ll": -5.51270866394043, "sampled_ll": -5.905052185058594, "all_perturbed_sampled_ll": [-5.905052185058594], "all_perturbed_original_ll": [-5.51270866394043], "perturbed_sampled_ll": -5.905052185058594, "perturbed_original_ll": -5.51270866394043, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "sampled": "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "perturbed_sampled": ["[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This"], "perturbed_original": ["[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)"], "original_ll": -6.514249324798584, "sampled_ll": -6.799709320068359, "all_perturbed_sampled_ll": [-6.799709320068359], "all_perturbed_original_ll": [-6.514249324798584], "perturbed_sampled_ll": -6.799709320068359, "perturbed_original_ll": -6.514249324798584, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix formatting in AWS Connections docs (#8223)", "sampled": "Fix formatting in AWS Connections docs (#8223)\"", "perturbed_sampled": ["Fix formatting in AWS Connections docs (#8223)\""], "perturbed_original": ["Fix formatting in AWS Connections docs (#8223)"], "original_ll": -6.179692268371582, "sampled_ll": -6.933568000793457, "all_perturbed_sampled_ll": [-6.933568000793457], "all_perturbed_original_ll": [-6.179692268371582], "perturbed_sampled_ll": -6.933568000793457, "perturbed_original_ll": -6.179692268371582, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "sampled": "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "perturbed_sampled": ["[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker"], "perturbed_original": ["[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker"], "original_ll": -3.1218481063842773, "sampled_ll": -3.1218481063842773, "all_perturbed_sampled_ll": [-3.1218481063842773], "all_perturbed_original_ll": [-3.1218481063842773], "perturbed_sampled_ll": -3.1218481063842773, "perturbed_original_ll": -3.1218481063842773, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "sampled": "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "perturbed_sampled": ["[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One"], "perturbed_original": ["[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)"], "original_ll": -6.870513439178467, "sampled_ll": -7.3190226554870605, "all_perturbed_sampled_ll": [-7.3190226554870605], "all_perturbed_original_ll": [-6.870513439178467], "perturbed_sampled_ll": -7.3190226554870605, "perturbed_original_ll": -6.870513439178467, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove unnecessary messages in CI (#7951)", "sampled": "Remove unnecessary messages in CI (#7951)Description:", "perturbed_sampled": ["Remove unnecessary messages in CI (#7951)Description:"], "perturbed_original": ["Remove unnecessary messages in CI (#7951)"], "original_ll": -6.142459869384766, "sampled_ll": -6.510763645172119, "all_perturbed_sampled_ll": [-6.510763645172119], "all_perturbed_original_ll": [-6.142459869384766], "perturbed_sampled_ll": -6.510763645172119, "perturbed_original_ll": -6.142459869384766, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "sampled": "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes", "perturbed_sampled": ["Clarifies argument for the AIRFLOW_VERSION argument in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes"], "perturbed_original": ["Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed before AIRFLOW_INSTALL_VERSION when the Docker image is installed (#12284). Fixes #8612"], "original_ll": -3.8051023483276367, "sampled_ll": -3.8938870429992676, "all_perturbed_sampled_ll": [-3.420625925064087], "all_perturbed_original_ll": [-3.702737808227539], "perturbed_sampled_ll": -3.420625925064087, "perturbed_original_ll": -3.702737808227539, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Submodules are needed to update constrains (#15242)", "sampled": "Submodules are needed to update constrains (#15242)I", "perturbed_sampled": ["Submodules are needed to update constrains (#15242)I"], "perturbed_original": ["Submodules are needed to update constrains (#15242)"], "original_ll": -5.360805511474609, "sampled_ll": -6.136443614959717, "all_perturbed_sampled_ll": [-6.136443614959717], "all_perturbed_original_ll": [-5.360805511474609], "perturbed_sampled_ll": -6.136443614959717, "perturbed_original_ll": -5.360805511474609, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixing MyPy issues in testa/jobs (#19998)", "sampled": "Fixing MyPy issues in testa/jobs (#19998)As", "perturbed_sampled": ["Fixing MyPy issues in testa/jobs (#19998)As"], "perturbed_original": ["Fixing MyPy issues in testa/jobs (#19998)"], "original_ll": -6.631772041320801, "sampled_ll": -7.173069953918457, "all_perturbed_sampled_ll": [-7.173069953918457], "all_perturbed_original_ll": [-6.631772041320801], "perturbed_sampled_ll": -7.173069953918457, "perturbed_original_ll": -6.631772041320801, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Use stable API versions where available (#17211)", "sampled": "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "perturbed_sampled": ["Chart: Use stable API versions where available (#17211)\"\n\nFix:"], "perturbed_original": ["Chart: Use stable API versions where available (#17211)"], "original_ll": -5.557555675506592, "sampled_ll": -5.0435004234313965, "all_perturbed_sampled_ll": [-5.0435004234313965], "all_perturbed_original_ll": [-5.557555675506592], "perturbed_sampled_ll": -5.0435004234313965, "perturbed_original_ll": -5.557555675506592, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "sampled": "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "perturbed_sampled": ["Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`"], "perturbed_original": ["Fix typo in docker-context-files/README.md (#12078) `par` -> `part`"], "original_ll": -4.603448390960693, "sampled_ll": -4.851093769073486, "all_perturbed_sampled_ll": [-4.851093769073486], "all_perturbed_original_ll": [-4.603448390960693], "perturbed_sampled_ll": -4.851093769073486, "perturbed_original_ll": -4.603448390960693, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add schema as DbApiHook instance attribute (#16521)", "sampled": "Add schema as DbApiHook instance attribute (#16521)One", "perturbed_sampled": ["Add schema as DbApiHook instance attribute (#16521)One"], "perturbed_original": ["Add schema as DbApiHook instance attribute (#16521)"], "original_ll": -5.966734409332275, "sampled_ll": -6.6604838371276855, "all_perturbed_sampled_ll": [-6.6604838371276855], "all_perturbed_original_ll": [-5.966734409332275], "perturbed_sampled_ll": -6.6604838371276855, "perturbed_original_ll": -5.966734409332275, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "sampled": "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "perturbed_sampled": ["Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If"], "perturbed_original": ["Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891"], "original_ll": -6.390114784240723, "sampled_ll": -6.725689888000488, "all_perturbed_sampled_ll": [-6.725689888000488], "all_perturbed_original_ll": [-6.390114784240723], "perturbed_sampled_ll": -6.725689888000488, "perturbed_original_ll": -6.390114784240723, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "sampled": "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation", "perturbed_sampled": ["Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of HTML files as described in the documentation"], "perturbed_original": ["Docs: Better description for \"other fields\" language. Fix for (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language."], "original_ll": -3.8986904621124268, "sampled_ll": -3.60378098487854, "all_perturbed_sampled_ll": [-3.5464799404144287], "all_perturbed_original_ll": [-4.265462875366211], "perturbed_sampled_ll": -3.5464799404144287, "perturbed_original_ll": -4.265462875366211, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "sampled": "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "perturbed_sampled": ["No message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe"], "perturbed_original": ["Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment , DB_URL with DB_URL"], "original_ll": -4.723080158233643, "sampled_ll": -4.782496452331543, "all_perturbed_sampled_ll": [-5.008337497711182], "all_perturbed_original_ll": [-4.955479145050049], "perturbed_sampled_ll": -5.008337497711182, "perturbed_original_ll": -4.955479145050049, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add ME-Br to who uses Airflow list (#9770)", "sampled": "Add ME-Br to who uses Airflow list (#9770)As", "perturbed_sampled": ["Add ME-Br to who uses Airflow list (#9770)As"], "perturbed_original": ["Add ME-Br to who uses Airflow list (#9770)"], "original_ll": -7.349530220031738, "sampled_ll": -7.802426338195801, "all_perturbed_sampled_ll": [-7.802426338195801], "all_perturbed_original_ll": [-7.349530220031738], "perturbed_sampled_ll": -7.802426338195801, "perturbed_original_ll": -7.349530220031738, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "sampled": "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "perturbed_sampled": ["[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless"], "perturbed_original": ["[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)"], "original_ll": -5.665342330932617, "sampled_ll": -5.958355903625488, "all_perturbed_sampled_ll": [-5.958355903625488], "all_perturbed_original_ll": [-5.665342330932617], "perturbed_sampled_ll": -5.958355903625488, "perturbed_original_ll": -5.665342330932617, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a version, so we don't have to edit a file and then remember to delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits or periods, but it's copy-pasteable this way.", "sampled": "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "perturbed_sampled": ["Simplify release process for PyPI . Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future, the build fails due to a bug (or the upstream maintainer decides to not add the suffix to a version), this suffix is used to add a new test to the build to fix it (which is typically"], "perturbed_original": ["Simplify release process for snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a version, so we don't have to edit a file or remember to delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits or words would be enough -- but it's copy-pasteable this way."], "original_ll": -3.695136547088623, "sampled_ll": -3.137489080429077, "all_perturbed_sampled_ll": [-3.029928684234619], "all_perturbed_original_ll": [-3.702331066131592], "perturbed_sampled_ll": -3.029928684234619, "perturbed_original_ll": -3.702331066131592, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "sampled": "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "perturbed_sampled": ["[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This"], "perturbed_original": ["[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)"], "original_ll": -5.537713050842285, "sampled_ll": -5.894845008850098, "all_perturbed_sampled_ll": [-5.894845008850098], "all_perturbed_original_ll": [-5.537713050842285], "perturbed_sampled_ll": -5.894845008850098, "perturbed_original_ll": -5.537713050842285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depenedncies of airflow, all the provider packages released at the time of the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies needed for core airflow installation. This allows to install/upgrade airflow without also forcing the provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the provider 'extras' for apache airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are now transitive only", "sampled": "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the core constraint generation, and the custom provider generation for generic services (#14151).\n\n* The generated providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class generator. It is based on \"create provider\", which has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "perturbed_sampled": ["Implements generation of constraints for core providers (#14227) There are two types of constraints now: * default constraints that contain all depots in the network (see docs) * basic constraints that are derived from provider objects , and * constraints for generic providers (similar to the core provider generation). * The new generators documentation (#14151) gives examples of the generation process. It covers the core constraint generation, and the constraint generation for generic services (#14151).\n\n* The generated providers docs are now the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class generator. It is based on \"create provider\", but has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator ** Support for \"create providers\","], "perturbed_original": ["Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all the core depenedncies of airflow, all the provider packages , at the time of the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is done correctly. * no-providers constraints - containing only the dependencies to core airflow installation. This allows to install/upgrade airflow without also forcing the package provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation for that feature has been added. Also the provider 'extras' for Airflow installations do not keep direct dependencies to the packages that are in the provider. Those dependencies are now transitive only"], "original_ll": -3.9686787128448486, "sampled_ll": -2.6518170833587646, "all_perturbed_sampled_ll": [-2.867246150970459], "all_perturbed_original_ll": [-3.8460564613342285], "perturbed_sampled_ll": -2.867246150970459, "perturbed_original_ll": -3.8460564613342285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - this have been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "sampled": "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - that is, if you use locks to do a transaction and then try to start a transaction later on. If you use mutexes, you would be stuck. (Issue #19857, #19867)\n\nIn some places, we added MSSQL replication support in MySQL 5, and there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance when running on", "perturbed_sampled": ["Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - that is, if you use mutexes to do a transaction and then try to write a transaction later on. If you use mutexes, you would be stuck. (Issue #19857, #19867)\n\nIn some cases, we have added parallelism support in MySQL 5, and there needs to be a way to tell it where to look for new data and replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance with parallelism.) See updates on"], "perturbed_original": ["Workaround s with MSSQL (#19856) We already have a mechanism to retry transactions that could encounter temporary deadlocks - this have been used for handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ) was deadlocked on lock resources with another transaction. The primary key has been chosen as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks."], "original_ll": -4.178400039672852, "sampled_ll": -3.0038838386535645, "all_perturbed_sampled_ll": [-3.0950891971588135], "all_perturbed_original_ll": [-4.22407865524292], "perturbed_sampled_ll": -3.0950891971588135, "perturbed_original_ll": -4.22407865524292, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures when executing long-running jobs", "sampled": "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures. You might want to", "perturbed_sampled": ["Databricks hook: fix expiration time check (#20036) : We detected a logical error in the check of expiration time that could lead to authentication problems, so you might want to"], "perturbed_original": ["Databricks hook: fix expiration time check (#20036) There was a logical error in a check of expiration time that led to authentication failures when executing long-running jobs"], "original_ll": -3.969269275665283, "sampled_ll": -3.750504732131958, "all_perturbed_sampled_ll": [-4.098731517791748], "all_perturbed_original_ll": [-4.1690473556518555], "perturbed_sampled_ll": -4.098731517791748, "perturbed_original_ll": -4.1690473556518555, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Pinterest to Airflow users list (#19117)", "sampled": "Add Pinterest to Airflow users list (#19117)If", "perturbed_sampled": ["Add Pinterest to Airflow users list (#19117)If"], "perturbed_original": ["Add Pinterest to Airflow users list (#19117)"], "original_ll": -8.04257583618164, "sampled_ll": -8.27450942993164, "all_perturbed_sampled_ll": [-8.27450942993164], "all_perturbed_original_ll": [-8.04257583618164], "perturbed_sampled_ll": -8.27450942993164, "perturbed_original_ll": -8.04257583618164, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in celery_executor error log * Add test for airflow/configuration.py::as_dict", "sampled": "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "perturbed_sampled": ["Prevent mixed case env vars from crashing the worker (#14380) * Handle misformed env vars without crashing * Include crash exception if not handled by tracing mechanism *"], "perturbed_original": ["Prevent mixed case env vars from crashing the container worker (#14380) * Handle misformed env vars without crashing * Include traceback in error log * Add test for airflow/configuration.py::as_dict"], "original_ll": -4.966541767120361, "sampled_ll": -4.844755172729492, "all_perturbed_sampled_ll": [-4.890107154846191], "all_perturbed_original_ll": [-4.794978618621826], "perturbed_sampled_ll": -4.890107154846191, "perturbed_original_ll": -4.794978618621826, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "sampled": "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "perturbed_sampled": ["[AIRFLOW-5049] Add validation before calling bigquery() in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries by using query_method_before() rather than query_config() * [AIRFLOW-5089] Fix wrong execution for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with a lot of fields (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column"], "perturbed_original": ["[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds validation method for the src_ftm_configs"], "original_ll": -2.717329978942871, "sampled_ll": -2.891718626022339, "all_perturbed_sampled_ll": [-2.8441758155822754], "all_perturbed_original_ll": [-2.7384767532348633], "perturbed_sampled_ll": -2.8441758155822754, "perturbed_original_ll": -2.7384767532348633, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, not just when the file itself changes.", "sampled": "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it much easier to keep up-to-date", "perturbed_sampled": ["Also check chart below). * the schema is actually changing itself! (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it much easier to keep up-to-date"], "perturbed_original": ["Also check chart schema when the schema itself changes (#15902) This changes the pre-commit hooks to also check schema when the schema itself changes, not just when the file itself changes."], "original_ll": -4.254692077636719, "sampled_ll": -3.8770689964294434, "all_perturbed_sampled_ll": [-4.2207841873168945], "all_perturbed_original_ll": [-4.001111030578613], "perturbed_sampled_ll": -4.2207841873168945, "perturbed_original_ll": -4.001111030578613, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "sampled": "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "perturbed_sampled": ["Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6"], "perturbed_original": ["to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059"], "original_ll": -4.427076816558838, "sampled_ll": -4.584950923919678, "all_perturbed_sampled_ll": [-4.584950923919678], "all_perturbed_original_ll": [-4.80714750289917], "perturbed_sampled_ll": -4.584950923919678, "perturbed_original_ll": -4.80714750289917, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "sampled": "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "perturbed_sampled": ["[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If"], "perturbed_original": ["[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)"], "original_ll": -5.876304626464844, "sampled_ll": -6.286564350128174, "all_perturbed_sampled_ll": [-6.286564350128174], "all_perturbed_original_ll": [-5.876304626464844], "perturbed_sampled_ll": -6.286564350128174, "perturbed_original_ll": -5.876304626464844, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "sampled": "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "perturbed_sampled": ["[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In"], "perturbed_original": ["[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)"], "original_ll": -5.826963901519775, "sampled_ll": -6.150223731994629, "all_perturbed_sampled_ll": [-6.150223731994629], "all_perturbed_original_ll": [-5.826963901519775], "perturbed_sampled_ll": -6.150223731994629, "perturbed_original_ll": -5.826963901519775, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a", "sampled": "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default behaviour) and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated after this branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "perturbed_sampled": ["fix for missing start_date and end_date by error! (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default ); and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (when using a libraries default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated after this branch was merged). Fixes #14358 (code in a hook might cause issues with objects"], "perturbed_original": ["Gracefully removing start_date and end_date for DagRun (#14452) closes: #14384 Refine w/#14452 This fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state to a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to recurrently transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun hadn't been in the failed or success state (e.g. , task instances weren't cleared), then this would lead to a"], "original_ll": -3.084679126739502, "sampled_ll": -2.28708553314209, "all_perturbed_sampled_ll": [-2.382878541946411], "all_perturbed_original_ll": [-3.2427666187286377], "perturbed_sampled_ll": -2.382878541946411, "perturbed_original_ll": -3.2427666187286377, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove unnecessary string concatenations in AirflowException messages (#18817)", "sampled": "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "perturbed_sampled": ["Remove unnecessary string concatenations in AirflowException messages (#18817)For"], "perturbed_original": ["Remove unnecessary string concatenations in AirflowException messages (#18817)"], "original_ll": -5.398134708404541, "sampled_ll": -6.124137878417969, "all_perturbed_sampled_ll": [-6.124137878417969], "all_perturbed_original_ll": [-5.398134708404541], "perturbed_sampled_ll": -6.124137878417969, "perturbed_original_ll": -5.398134708404541, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "sampled": "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "perturbed_sampled": ["Fixes failing test_views tests (#14599) This reverts commit Fixes failing tests with animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to"], "perturbed_original": ["Fixes some issues in test::Cache when running tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made this change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master."], "original_ll": -4.303369045257568, "sampled_ll": -3.03253436088562, "all_perturbed_sampled_ll": [-2.8723433017730713], "all_perturbed_original_ll": [-4.257969379425049], "perturbed_sampled_ll": -2.8723433017730713, "perturbed_original_ll": -4.257969379425049, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used an uppercase E. Also made the exception a bit clearer when running on kubernetes.", "sampled": "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they log the return code with a space before the space. This results in different exit", "perturbed_sampled": ["Update Spark submit operator for Spark 3 support . In spark 3 they log the exit code with a lowercase e, in spark 2 they log the return code with a space before the space. This results in an exit"], "perturbed_original": ["Update Spark submit operator for Spark 3 and Spark 2. In spark 3 they log the exit code with a lowercase E and in spark 2 they used an uppercase E. Also made the exception a bit clearer when on kubernetes."], "original_ll": -4.301545143127441, "sampled_ll": -4.08167028427124, "all_perturbed_sampled_ll": [-4.0897650718688965], "all_perturbed_original_ll": [-4.125467777252197], "perturbed_sampled_ll": -4.0897650718688965, "perturbed_original_ll": -4.125467777252197, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring exceptions here we let the failure be exposed where its broken, not in the next test that happens to run.", "sampled": "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "perturbed_sampled": ["The MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, it looks like the test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import db [#test #run] [ExceptionHandler]"], "perturbed_original": ["Fix broken MSSQL test (#17797) This fixed a bug which was causing the test to use the db . Also, by not ignoring exceptions here we let the failure be exposed where its broken, not in the next test that happens to run."], "original_ll": -4.434438705444336, "sampled_ll": -3.2608354091644287, "all_perturbed_sampled_ll": [-3.8214282989501953], "all_perturbed_original_ll": [-4.429766654968262], "perturbed_sampled_ll": -3.8214282989501953, "perturbed_original_ll": -4.429766654968262, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "sampled": "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "perturbed_sampled": ["Improve language of a BaseSensorOperator in UPDATING.md (#10332)For"], "perturbed_original": ["Improve language of a BaseSensorOperator in UPDATING.md (#10332)"], "original_ll": -6.469966411590576, "sampled_ll": -6.97290563583374, "all_perturbed_sampled_ll": [-6.97290563583374], "all_perturbed_original_ll": [-6.469966411590576], "perturbed_sampled_ll": -6.97290563583374, "perturbed_original_ll": -6.469966411590576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "sampled": "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "perturbed_sampled": ["[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard coded tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"myfuncode.cc\" , #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include"], "perturbed_original": ["[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, use `set_upstream` is change to shift in tutorial.py but still in manual mode. Perhaps sphinx is a better way"], "original_ll": -4.7604193687438965, "sampled_ll": -3.431135654449463, "all_perturbed_sampled_ll": [-3.425384044647217], "all_perturbed_original_ll": [-5.059401988983154], "perturbed_sampled_ll": -3.425384044647217, "perturbed_original_ll": -5.059401988983154, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "sampled": "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default role, though it's still better because it improves concurrency in the core. (This is really hard to evaluate, since they don't really know to", "perturbed_sampled": ["Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries for the default role, though it's still too early to say if it improves concurrency in the core. (This is really hard to evaluate, since they don't really know to"], "perturbed_original": ["Faster default queries during webserver start (#15017) This makes a handful of bigger queries instead of many small ones and speeds up the test by syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second in startup time (1 second on tests faster too)."], "original_ll": -4.243828296661377, "sampled_ll": -3.9926559925079346, "all_perturbed_sampled_ll": [-3.8792102336883545], "all_perturbed_original_ll": [-4.188701152801514], "perturbed_sampled_ll": -3.8792102336883545, "perturbed_original_ll": -4.188701152801514, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "sampled": "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "perturbed_sampled": ["Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a file with an `@` URL but still"], "perturbed_original": ["Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with sphinx in the requirements."], "original_ll": -5.419686317443848, "sampled_ll": -5.618743896484375, "all_perturbed_sampled_ll": [-5.497030258178711], "all_perturbed_original_ll": [-5.476748943328857], "perturbed_sampled_ll": -5.497030258178711, "perturbed_original_ll": -5.476748943328857, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fix tests (#11368)", "sampled": "fix tests (#11368)When", "perturbed_sampled": ["fix tests (#11368)When"], "perturbed_original": ["fix tests (#11368)"], "original_ll": -6.143937110900879, "sampled_ll": -7.5693182945251465, "all_perturbed_sampled_ll": [-7.5693182945251465], "all_perturbed_original_ll": [-6.143937110900879], "perturbed_sampled_ll": -7.5693182945251465, "perturbed_original_ll": -6.143937110900879, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update node installation cmd (#10744)", "sampled": "Update node installation cmd (#10744)When", "perturbed_sampled": ["Update node installation cmd (#10744)When"], "perturbed_original": ["Update node installation cmd (#10744)"], "original_ll": -7.4543538093566895, "sampled_ll": -8.300008773803711, "all_perturbed_sampled_ll": [-8.300008773803711], "all_perturbed_original_ll": [-7.4543538093566895], "perturbed_sampled_ll": -8.300008773803711, "perturbed_original_ll": -7.4543538093566895, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner", "sampled": "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "perturbed_sampled": ["- Fixed UI to include import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added"], "perturbed_original": ["Swap dag import /export icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner"], "original_ll": -5.944174766540527, "sampled_ll": -4.573419094085693, "all_perturbed_sampled_ll": [-4.442777633666992], "all_perturbed_original_ll": [-5.980999946594238], "perturbed_sampled_ll": -4.442777633666992, "perturbed_original_ll": -5.980999946594238, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "sampled": "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "perturbed_sampled": ["[AIRLFOW-XXX] Display other integrations in single table (#6133)If"], "perturbed_original": ["[AIRLFOW-XXX] Display other integrations in single table (#6133)"], "original_ll": -6.349506378173828, "sampled_ll": -6.7463698387146, "all_perturbed_sampled_ll": [-6.7463698387146], "all_perturbed_original_ll": [-6.349506378173828], "perturbed_sampled_ll": -6.7463698387146, "perturbed_original_ll": -6.349506378173828, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg in example DAG", "sampled": "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "perturbed_sampled": ["Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing"], "perturbed_original": ["Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg s (#18209) * DAG"], "original_ll": -3.9587743282318115, "sampled_ll": -3.110168695449829, "all_perturbed_sampled_ll": [-3.110168695449829], "all_perturbed_original_ll": [-3.92690372467041], "perturbed_sampled_ll": -3.110168695449829, "perturbed_original_ll": -3.92690372467041, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a DockerOperator that can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random return [random.random() for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as this would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash). To work with these requirements, we use base64 encoding to store a jinja generated python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "sampled": "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the code above, the output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "perturbed_sampled": ["Add a Docker Taskflow into my workflow to run with: Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the screenshot above, the output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead. Docker Project flow \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead. Fix my application \u2013 This has been a pain in"], "perturbed_original": ["Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a docker image that allows us to run that python function remotely. #15441 ( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import (e.g. using the [random.random() for i . ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container . The container would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their system (this implementation only requires python and bash). To work with these requirements, we use base64 encoding to store a jinja generated list of functions and inputs (which are actually the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these"], "original_ll": -3.4138376712799072, "sampled_ll": -2.61106014251709, "all_perturbed_sampled_ll": [-2.7591207027435303], "all_perturbed_original_ll": [-3.7413763999938965], "perturbed_sampled_ll": -2.7591207027435303, "perturbed_original_ll": -3.7413763999938965, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "sampled": "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "perturbed_sampled": ["Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew"], "perturbed_original": ["Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472"], "original_ll": -4.88201379776001, "sampled_ll": -4.845544338226318, "all_perturbed_sampled_ll": [-4.845544338226318], "all_perturbed_original_ll": [-4.88201379776001], "perturbed_sampled_ll": -4.845544338226318, "perturbed_original_ll": -4.88201379776001, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "sampled": "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "perturbed_sampled": ["Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA"], "perturbed_original": ["Unify command names in CLI * Unify command names in CLI * fixup! Unify command names in CLI"], "original_ll": -4.076107978820801, "sampled_ll": -4.379867076873779, "all_perturbed_sampled_ll": [-4.379867076873779], "all_perturbed_original_ll": [-3.775672197341919], "perturbed_sampled_ll": -4.379867076873779, "perturbed_original_ll": -3.775672197341919, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4000] Return response when no file (#4822)", "sampled": "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "perturbed_sampled": ["[AIRFLOW-4000] Return response when no file (#4822)Tekmara"], "perturbed_original": ["[AIRFLOW-4000] Return response when no file (#4822)"], "original_ll": -6.41796350479126, "sampled_ll": -6.605205059051514, "all_perturbed_sampled_ll": [-6.605205059051514], "all_perturbed_original_ll": [-6.41796350479126], "perturbed_sampled_ll": -6.605205059051514, "perturbed_original_ll": -6.41796350479126, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "sampled": "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "perturbed_sampled": ["Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't modify the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't modify the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,"], "perturbed_original": ["Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 , closes: #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: to define how some of the generic SQL operators are checked in a database for BigQuery replace the database hook with the .get_db_hook() method using database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize a hook method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class"], "original_ll": -4.0708088874816895, "sampled_ll": -1.9806418418884277, "all_perturbed_sampled_ll": [-1.987682819366455], "all_perturbed_original_ll": [-4.08172607421875], "perturbed_sampled_ll": -1.987682819366455, "perturbed_original_ll": -4.08172607421875, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "sampled": "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "perturbed_sampled": ["[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,"], "perturbed_original": ["[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)"], "original_ll": -6.674228668212891, "sampled_ll": -6.473708152770996, "all_perturbed_sampled_ll": [-6.473708152770996], "all_perturbed_original_ll": [-6.674228668212891], "perturbed_sampled_ll": -6.473708152770996, "perturbed_original_ll": -6.674228668212891, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "sampled": "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "perturbed_sampled": ["[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:"], "perturbed_original": ["[AIRFLOW-5690] Change log level local_task_job.py (#6422)"], "original_ll": -5.983975887298584, "sampled_ll": -5.925544261932373, "all_perturbed_sampled_ll": [-5.925544261932373], "all_perturbed_original_ll": [-5.983975887298584], "perturbed_sampled_ll": -5.925544261932373, "perturbed_original_ll": -5.983975887298584, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add test connection method to http hook (#16568)", "sampled": "Add test connection method to http hook (#16568)Widgets:", "perturbed_sampled": ["Add test connection method to http hook (#16568)Widgets:"], "perturbed_original": ["Add test connection method to http hook (#16568)"], "original_ll": -6.1205267906188965, "sampled_ll": -5.8179030418396, "all_perturbed_sampled_ll": [-5.8179030418396], "all_perturbed_original_ll": [-6.1205267906188965], "perturbed_sampled_ll": -5.8179030418396, "perturbed_original_ll": -6.1205267906188965, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "sampled": "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "perturbed_sampled": ["Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit 12e1a4f f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:"], "perturbed_original": ["Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Barros Obtain tree_data object endpoint from meta. closes: #16017"], "original_ll": -4.574052810668945, "sampled_ll": -3.151090383529663, "all_perturbed_sampled_ll": [-3.296224355697632], "all_perturbed_original_ll": [-4.98116397857666], "perturbed_sampled_ll": -3.296224355697632, "perturbed_original_ll": -4.98116397857666, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is a required argument). I looked for a test suite for the web UI, but it seems the existing tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "sampled": "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter parameter #4224 * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "perturbed_sampled": [") * * [AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Replace the \"customer_id\" parameter to the client_id parameter, update #4587 to #4224 * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/"], "perturbed_original": ["[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * Create new endpoint for each DAG in DagBag (`dag_id` is a string). I looked for a test suite for the web UI, but it seems the existing tests have all been dropped after the switch to FAB. I've created a new class for FAB tests and"], "original_ll": -3.6781342029571533, "sampled_ll": -1.7868448495864868, "all_perturbed_sampled_ll": [-1.9922912120819092], "all_perturbed_original_ll": [-3.4588096141815186], "perturbed_sampled_ll": -1.9922912120819092, "perturbed_original_ll": -3.4588096141815186, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "sampled": "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "perturbed_sampled": ["[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary"], "perturbed_original": ["[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)"], "original_ll": -5.493777751922607, "sampled_ll": -5.72956657409668, "all_perturbed_sampled_ll": [-5.72956657409668], "all_perturbed_original_ll": [-5.493777751922607], "perturbed_sampled_ll": -5.72956657409668, "perturbed_original_ll": -5.493777751922607, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "sampled": "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "perturbed_sampled": ["[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've"], "perturbed_original": ["[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)"], "original_ll": -6.727475166320801, "sampled_ll": -6.909038543701172, "all_perturbed_sampled_ll": [-6.909038543701172], "all_perturbed_original_ll": [-6.727475166320801], "perturbed_sampled_ll": -6.909038543701172, "perturbed_original_ll": -6.727475166320801, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix installation doc (#13462) The note should not be in the Bash code-block", "sampled": "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "perturbed_sampled": ["Fix installation doc (#13462) The note should not be in the Bash code-blockIt"], "perturbed_original": ["Fix installation doc (#13462) The note should not be in the Bash code-block"], "original_ll": -5.896306991577148, "sampled_ll": -6.292571067810059, "all_perturbed_sampled_ll": [-6.292571067810059], "all_perturbed_original_ll": [-5.896306991577148], "perturbed_sampled_ll": -6.292571067810059, "perturbed_original_ll": -5.896306991577148, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update INTHEWILD.md (#12060)", "sampled": "Update INTHEWILD.md (#12060)If", "perturbed_sampled": ["Update INTHEWILD.md (#12060)If"], "perturbed_original": ["Update INTHEWILD.md (#12060)"], "original_ll": -6.374277591705322, "sampled_ll": -6.978005886077881, "all_perturbed_sampled_ll": [-6.978005886077881], "all_perturbed_original_ll": [-6.374277591705322], "perturbed_sampled_ll": -6.978005886077881, "perturbed_original_ll": -6.374277591705322, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "GCP Secret Manager error handling for missing credentials (#17264)", "sampled": "GCP Secret Manager error handling for missing credentials (#17264)On", "perturbed_sampled": ["GCP Secret Manager error handling for missing credentials (#17264)On"], "perturbed_original": ["GCP Secret Manager error handling for missing credentials (#17264)"], "original_ll": -6.6242241859436035, "sampled_ll": -7.3111796379089355, "all_perturbed_sampled_ll": [-7.3111796379089355], "all_perturbed_original_ll": [-6.6242241859436035], "perturbed_sampled_ll": -7.3111796379089355, "perturbed_original_ll": -6.6242241859436035, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which needed to be downloaded. The new check uses already available airflow CI image and it performs all check in one docker command - thus is a lot faster and it also checks the image at the same time.", "sampled": "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This fix allows the image to be configured with the command specified without making you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "perturbed_sampled": ["Improve breeze resource check (#17492) The resource checker on breeze was working with two docker containers (2 sets instead of one) and it used an extra image which did not work properly. This fix allows the resource checker to be configured with the command specified without making you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't"], "perturbed_original": ["Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and used an extra image which needed to be downloaded. The new check uses already available airflow CI images so it does the resource check in one docker command - thus is a lot faster and it also checks one source image at the same time."], "original_ll": -4.124236583709717, "sampled_ll": -3.7408342361450195, "all_perturbed_sampled_ll": [-3.7930238246917725], "all_perturbed_original_ll": [-4.1629157066345215], "perturbed_sampled_ll": -3.7930238246917725, "perturbed_original_ll": -4.1629157066345215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds Github Oauth example with team based authorization (#17896)", "sampled": "Adds Github Oauth example with team based authorization (#17896)The", "perturbed_sampled": ["Adds Github Oauth example with team based authorization (#17896)The"], "perturbed_original": ["Adds Github Oauth example with team based authorization (#17896)"], "original_ll": -5.980979919433594, "sampled_ll": -6.504420757293701, "all_perturbed_sampled_ll": [-6.504420757293701], "all_perturbed_original_ll": [-5.980979919433594], "perturbed_sampled_ll": -6.504420757293701, "perturbed_original_ll": -5.980979919433594, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add taskflow to accepted words (#11902)", "sampled": "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "perturbed_sampled": ["Add taskflow to accepted words (#11902)(#11902)\n\nAdd"], "perturbed_original": ["Add taskflow to accepted words (#11902)"], "original_ll": -7.064770221710205, "sampled_ll": -5.021240234375, "all_perturbed_sampled_ll": [-5.021240234375], "all_perturbed_original_ll": [-7.064770221710205], "perturbed_sampled_ll": -5.021240234375, "perturbed_original_ll": -7.064770221710205, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 too", "sampled": "Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 tooBy", "perturbed_sampled": ["Remove reimported AirflowException class (#9525) It is at the top of the file and L1060 tooBy"], "perturbed_original": ["Remove reimported AirflowException class (#9525) It is from the top of the file and L1060 too"], "original_ll": -6.3342742919921875, "sampled_ll": -6.623619556427002, "all_perturbed_sampled_ll": [-6.64670991897583], "all_perturbed_original_ll": [-6.553539276123047], "perturbed_sampled_ll": -6.64670991897583, "perturbed_original_ll": -6.553539276123047, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move role guide to access control (#10755)", "sampled": "Move role guide to access control (#10755)In", "perturbed_sampled": ["Move role guide to access control (#10755)In"], "perturbed_original": ["Move role guide to access control (#10755)"], "original_ll": -7.302133560180664, "sampled_ll": -7.890768527984619, "all_perturbed_sampled_ll": [-7.890768527984619], "all_perturbed_original_ll": [-7.302133560180664], "perturbed_sampled_ll": -7.890768527984619, "perturbed_original_ll": -7.302133560180664, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "sampled": "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "perturbed_sampled": ["Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:"], "perturbed_original": ["Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`"], "original_ll": -4.365053176879883, "sampled_ll": -4.731324195861816, "all_perturbed_sampled_ll": [-4.731324195861816], "all_perturbed_original_ll": [-4.365053176879883], "perturbed_sampled_ll": -4.731324195861816, "perturbed_original_ll": -4.365053176879883, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "sampled": "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of operators and returned a new context. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a", "perturbed_sampled": ["Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack DAG removed some context of operators and returned a new context. This step restores additional context by removing /etc/migrations in the end point so users don't have to decide whether or not to do things with the file, or not even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a"], "perturbed_original": ["Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack syntax removed some context of using operators you may find useful. Some of the args in operators were moved to `default_args` to simplify authoring of operators but these args disappeared from the Slack operator how-to guide as a result. This PR should be an opportunity to restore the context of operators to the middle ground between example DAG enhancement and how-to guide showcase of the operators."], "original_ll": -3.8764941692352295, "sampled_ll": -3.503295660018921, "all_perturbed_sampled_ll": [-3.608091115951538], "all_perturbed_original_ll": [-3.9664206504821777], "perturbed_sampled_ll": -3.608091115951538, "perturbed_original_ll": -3.9664206504821777, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "sampled": "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "perturbed_sampled": ["[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The"], "perturbed_original": ["[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)"], "original_ll": -6.293060302734375, "sampled_ll": -6.640666961669922, "all_perturbed_sampled_ll": [-6.640666961669922], "all_perturbed_original_ll": [-6.293060302734375], "perturbed_sampled_ll": -6.640666961669922, "perturbed_original_ll": -6.293060302734375, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can print the exact command being executed for those. But when 'set +e' was set before the command was called - indicating that error in those functions should be ignored - this did not happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right after. This caused first time experience to be not good. The fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to the output file but they are", "sampled": "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with versions of the command before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is still a directory (#10552) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "perturbed_sampled": ["The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with versions of the docker toolchain later than 2.6.8, these functions do not return a response unless it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10568) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect () function returns an arbitrary URL (via curl) as hostname instead of just the docker service name (#10552) docker/docker() behaves incorrectly when using the command to create something that is still a directory (#10552) The docker/dockerlib.dirname function returns a single file path instead of an arbitrary number of ones (#10553)"], "perturbed_original": ["The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), and replace the real tools to get a clean command output (we can print the source) being executed for those. But when 'set +e' was set before the command was called - that error in those functions should be ignored - this did happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right after. This caused first time experience to be a lot of fun. The fix also fixes behaviour of stdout and output from those functions - previously they were joined and able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are print'd to the output file but they are"], "original_ll": -3.8325185775756836, "sampled_ll": -2.655191659927368, "all_perturbed_sampled_ll": [-2.7438464164733887], "all_perturbed_original_ll": [-4.177312850952148], "perturbed_sampled_ll": -2.7438464164733887, "perturbed_original_ll": -4.177312850952148, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use DAG context manager in examples (#13297)", "sampled": "Use DAG context manager in examples (#13297)Bump", "perturbed_sampled": ["Use DAG context manager in examples (#13297)Bump"], "perturbed_original": ["Use DAG context manager in examples (#13297)"], "original_ll": -6.229391574859619, "sampled_ll": -6.654886245727539, "all_perturbed_sampled_ll": [-6.654886245727539], "all_perturbed_original_ll": [-6.229391574859619], "perturbed_sampled_ll": -6.654886245727539, "perturbed_original_ll": -6.229391574859619, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "sampled": "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "perturbed_sampled": ["Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>"], "perturbed_original": ["Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>"], "original_ll": -4.493294715881348, "sampled_ll": -4.493294715881348, "all_perturbed_sampled_ll": [-4.493294715881348], "all_perturbed_original_ll": [-4.493294715881348], "perturbed_sampled_ll": -4.493294715881348, "perturbed_original_ll": -4.493294715881348, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "sampled": "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "perturbed_sampled": ["[AIRFLOW-5500] Fix the subsearch api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>"], "perturbed_original": ["[AIRFLOW-5500] Fix to UIAPI api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>"], "original_ll": -4.424356460571289, "sampled_ll": -3.934025764465332, "all_perturbed_sampled_ll": [-3.9452273845672607], "all_perturbed_original_ll": [-4.641602993011475], "perturbed_sampled_ll": -3.9452273845672607, "perturbed_original_ll": -4.641602993011475, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a call to it in initialize after prepare_syspath", "sampled": "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "perturbed_sampled": ["[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after <unk>-- of #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after"], "perturbed_original": ["[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a call to it in after prepare_syspath"], "original_ll": -4.638952732086182, "sampled_ll": -3.26456880569458, "all_perturbed_sampled_ll": [-3.8702619075775146], "all_perturbed_original_ll": [-4.614987850189209], "perturbed_sampled_ll": -3.8702619075775146, "perturbed_original_ll": -4.614987850189209, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "sampled": "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "perturbed_sampled": ["Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S."], "perturbed_original": ["Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880"], "original_ll": -5.728914737701416, "sampled_ll": -5.81010103225708, "all_perturbed_sampled_ll": [-5.81010103225708], "all_perturbed_original_ll": [-5.728914737701416], "perturbed_sampled_ll": -5.81010103225708, "perturbed_original_ll": -5.728914737701416, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Add custom labels for ingresses/PVCs (#20535)", "sampled": "Chart: Add custom labels for ingresses/PVCs (#20535)There", "perturbed_sampled": ["Chart: Add custom labels for ingresses/PVCs (#20535)There"], "perturbed_original": ["Chart: Add custom labels for ingresses/PVCs (#20535)"], "original_ll": -5.676176071166992, "sampled_ll": -6.228247165679932, "all_perturbed_sampled_ll": [-6.228247165679932], "all_perturbed_original_ll": [-5.676176071166992], "perturbed_sampled_ll": -6.228247165679932, "perturbed_original_ll": -5.676176071166992, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix breeze redirect on macOS (#14506)", "sampled": "Fix breeze redirect on macOS (#14506)The", "perturbed_sampled": ["Fix breeze redirect on macOS (#14506)The"], "perturbed_original": ["Fix breeze redirect on macOS (#14506)"], "original_ll": -7.647749900817871, "sampled_ll": -8.356447219848633, "all_perturbed_sampled_ll": [-8.356447219848633], "all_perturbed_original_ll": [-7.647749900817871], "perturbed_sampled_ll": -8.356447219848633, "perturbed_original_ll": -7.647749900817871, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "sampled": "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "perturbed_sampled": ["[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On"], "perturbed_original": ["[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)"], "original_ll": -5.8010735511779785, "sampled_ll": -6.152419567108154, "all_perturbed_sampled_ll": [-6.152419567108154], "all_perturbed_original_ll": [-5.8010735511779785], "perturbed_sampled_ll": -6.152419567108154, "perturbed_original_ll": -5.8010735511779785, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "sampled": "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "perturbed_sampled": ["Pin yin 2.0 <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437"], "perturbed_original": ["Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released . This is causing CI failures"], "original_ll": -4.01835298538208, "sampled_ll": -2.7895960807800293, "all_perturbed_sampled_ll": [-2.936955690383911], "all_perturbed_original_ll": [-4.1538238525390625], "perturbed_sampled_ll": -2.936955690383911, "perturbed_original_ll": -4.1538238525390625, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "sampled": "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "perturbed_sampled": ["Move out get_python_source from www, Move get_dag to www.utils (#7899)We"], "perturbed_original": ["Move out get_python_source from www, Move get_dag to www.utils (#7899)"], "original_ll": -5.583415508270264, "sampled_ll": -5.965904712677002, "all_perturbed_sampled_ll": [-5.965904712677002], "all_perturbed_original_ll": [-5.583415508270264], "perturbed_sampled_ll": -5.965904712677002, "perturbed_original_ll": -5.583415508270264, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "sampled": "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "perturbed_sampled": ["Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>"], "perturbed_original": ["Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>"], "original_ll": -5.223132133483887, "sampled_ll": -4.737282752990723, "all_perturbed_sampled_ll": [-4.737282752990723], "all_perturbed_original_ll": [-5.223132133483887], "perturbed_sampled_ll": -4.737282752990723, "perturbed_original_ll": -5.223132133483887, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache Airflow", "sampled": "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "perturbed_sampled": ["Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies . Kayzen AirflowThe"], "perturbed_original": ["Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache Airflow"], "original_ll": -5.671421051025391, "sampled_ll": -5.909276962280273, "all_perturbed_sampled_ll": [-5.992166042327881], "all_perturbed_original_ll": [-5.671421051025391], "perturbed_sampled_ll": -5.992166042327881, "perturbed_original_ll": -5.671421051025391, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clean up the pre-commit config file (#15681)", "sampled": "Clean up the pre-commit config file (#15681)Pushing", "perturbed_sampled": ["Clean up the pre-commit config file (#15681)Pushing"], "perturbed_original": ["Clean up the pre-commit config file (#15681)"], "original_ll": -4.721262454986572, "sampled_ll": -5.464875221252441, "all_perturbed_sampled_ll": [-5.464875221252441], "all_perturbed_original_ll": [-4.721262454986572], "perturbed_sampled_ll": -5.464875221252441, "perturbed_original_ll": -4.721262454986572, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _get_credentials method - which is a centerpiece of AWS provider and is likely to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 2.5.1 with this change included. Fixes: #20457", "sampled": "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the local storage (e.g. when a bucket is empty, the instance is still available)\n\nYou will end up with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "perturbed_sampled": ["Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The fix introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is set before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the AWS CLI if the credentials are not able to be stored in the local storage (e.g. when a cache folder is empty, the instance is frozen), you will end up with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2"], "perturbed_original": ["Fix backwards compatibility issue in API _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _get_credentials method - which is a centerpiece of AWS provider and is meant to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility in the derived classes. We already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we had to release 2.5.1 provider with this change included. Fixes: #20457"], "original_ll": -3.94303297996521, "sampled_ll": -2.9680914878845215, "all_perturbed_sampled_ll": [-2.9981484413146973], "all_perturbed_original_ll": [-4.012942790985107], "perturbed_sampled_ll": -2.9981484413146973, "perturbed_original_ll": -4.012942790985107, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "sampled": "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9505) * Support a docker-compose", "perturbed_sampled": ["Simplify APIs for JMeter, Airflow, and K8sPodOperator (#10393) for Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, however, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9505) * Support a docker-compose"], "perturbed_original": ["Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify the K8s Kubernetes Story Removes thousands of lines of code that essentially ammount to re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 Releases. * podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * #10298 Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>"], "original_ll": -3.4893150329589844, "sampled_ll": -2.7301337718963623, "all_perturbed_sampled_ll": [-2.843954086303711], "all_perturbed_original_ll": [-3.4581117630004883], "perturbed_sampled_ll": -2.843954086303711, "perturbed_original_ll": -3.4581117630004883, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python files, running the flake8 pre-commit hook would fail without obvious error (as in no error was printed, but exit code was 1). In debugging this I switch the pre-commit to `require_serial: true` and the problem went way - the fix for this is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker to create a random name and write the created container ID to a file", "sampled": "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues with getting a handle into the process that would get an image. However, if you just moved a python process, with no context or context with the existing processes, that would work - there might have been some process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the new method get_remote_image_info uses the context of a process if a context", "perturbed_sampled": ["Cope with multiple processes in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues with trying to get a process handle into the image process. We would get an image. However, if you just moved a python process, with no context or context with the existing processes, the image didn't work - there might have been some process called \"my_process_1\" with a handle from its parent, it didn't matter. In Python 3, the new Image API uses the context of a process if a context"], "perturbed_original": ["Cope thread processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python files, running the flake8 pre-commit hook would cause an obvious error (as in no error was printed, but exit code was 1). In debugging this I switch the pre-commit log message <unk>suppress -p to true` and the problem went way - the fix for this is: - Don't redirect stderr to docker (that silences the VERBOSE trace of the errors from docker) - Use `--cidfile` option to docker to create a random container ID and write the created container ID to a file"], "original_ll": -4.068980693817139, "sampled_ll": -3.1560921669006348, "all_perturbed_sampled_ll": [-3.3037519454956055], "all_perturbed_original_ll": [-4.250166893005371], "perturbed_sampled_ll": -3.3037519454956055, "perturbed_original_ll": -4.250166893005371, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixed button size in \"Actions\" group. (#17902)", "sampled": "Fixed button size in \"Actions\" group. (#17902)1)", "perturbed_sampled": ["Fixed button size in \"Actions\" group. (#17902)1)"], "perturbed_original": ["Fixed button size in \"Actions\" group. (#17902)"], "original_ll": -4.6174187660217285, "sampled_ll": -4.9131693840026855, "all_perturbed_sampled_ll": [-4.9131693840026855], "all_perturbed_original_ll": [-4.6174187660217285], "perturbed_sampled_ll": -4.9131693840026855, "perturbed_original_ll": -4.6174187660217285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add muldelete action to TaskInstanceModelView (#18438)", "sampled": "Add muldelete action to TaskInstanceModelView (#18438)From:", "perturbed_sampled": ["Add muldelete action to TaskInstanceModelView (#18438)From:"], "perturbed_original": ["Add muldelete action to TaskInstanceModelView (#18438)"], "original_ll": -6.475630283355713, "sampled_ll": -6.704144477844238, "all_perturbed_sampled_ll": [-6.704144477844238], "all_perturbed_original_ll": [-6.475630283355713], "perturbed_sampled_ll": -6.704144477844238, "perturbed_original_ll": -6.475630283355713, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "sampled": "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "perturbed_sampled": ["Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C"], "perturbed_original": ["Switch to released cancel-workflow-runs action (#10423) Follow up after #10368"], "original_ll": -6.2062530517578125, "sampled_ll": -6.371862411499023, "all_perturbed_sampled_ll": [-6.371862411499023], "all_perturbed_original_ll": [-6.2062530517578125], "perturbed_sampled_ll": -6.371862411499023, "perturbed_original_ll": -6.2062530517578125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples", "sampled": "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "perturbed_sampled": ["Separate Installing from sources section and also fixes link for binary download details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make"], "perturbed_original": ["Separate Installing from sources section and add more details (#18171) This PR separate Installing from sources section and also fixes links for binary source, it had `-bin` tag (which we don't use anymore. And I removed the section on verifying integrity. And add more details with examples"], "original_ll": -4.831807613372803, "sampled_ll": -3.2652435302734375, "all_perturbed_sampled_ll": [-3.2217392921447754], "all_perturbed_original_ll": [-4.614333152770996], "perturbed_sampled_ll": -3.2217392921447754, "perturbed_original_ll": -4.614333152770996, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "sampled": "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' no longer exists.", "perturbed_sampled": ["Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' no longer exists."], "perturbed_original": ["Synchronize function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways."], "original_ll": -4.979821681976318, "sampled_ll": -4.241580963134766, "all_perturbed_sampled_ll": [-4.241580963134766], "all_perturbed_original_ll": [-4.936769008636475], "perturbed_sampled_ll": -4.241580963134766, "perturbed_original_ll": -4.936769008636475, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm Chart tests in case default branch is different than main.", "sampled": "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (which should already match branch #1609), so we do not include the chart when testing for new", "perturbed_sampled": ["Disable Helm chart branch is in (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (which does not match branch #1609), so we do not include the chart when testing for new"], "perturbed_original": ["Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we want to run it from airflow version branches (similarly as providers) This change disables Helm Chart tests in case branch is different than main."], "original_ll": -5.295029163360596, "sampled_ll": -4.991035461425781, "all_perturbed_sampled_ll": [-4.908325672149658], "all_perturbed_original_ll": [-5.089760780334473], "perturbed_sampled_ll": -4.908325672149658, "perturbed_original_ll": -5.089760780334473, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix doc error (#5179)", "sampled": "[AIRFLOW-XXX] Fix doc error (#5179)Here", "perturbed_sampled": ["[AIRFLOW-XXX] Fix doc error (#5179)Here"], "perturbed_original": ["[AIRFLOW-XXX] Fix doc error (#5179)"], "original_ll": -6.116067886352539, "sampled_ll": -6.878108978271484, "all_perturbed_sampled_ll": [-6.878108978271484], "all_perturbed_original_ll": [-6.116067886352539], "perturbed_sampled_ll": -6.878108978271484, "perturbed_original_ll": -6.116067886352539, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "sampled": "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "perturbed_sampled": ["Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov"], "perturbed_original": ["Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>"], "original_ll": -3.375816822052002, "sampled_ll": -3.2358322143554688, "all_perturbed_sampled_ll": [-3.2358322143554688], "all_perturbed_original_ll": [-3.375816822052002], "perturbed_sampled_ll": -3.2358322143554688, "perturbed_original_ll": -3.375816822052002, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be in one place. Documentation for Celery does not describe the inspect ping command, but hopefully, this will be added soon.", "sampled": "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of these monitoring settings under the Advanced tab for the respective", "perturbed_sampled": ["Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring options provided for Airflow components, you also need to enable airflow: (default) Yes\n\nNote: Please ensure that you enabled some monitoring settings in package and app settings for the appropriate package and running app. If the device is not connected, you can see the status of these monitoring settings under the Advanced tab for the respective"], "perturbed_original": ["Documentation about Celery monitoring (#14533) Part of: Affock: Now that we have a full description of the monitoring of all Airflow components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be on one place. Documentation for Celery does not contain the inspect ping command, but hopefully, this will be added soon."], "original_ll": -3.8315184116363525, "sampled_ll": -3.775820016860962, "all_perturbed_sampled_ll": [-3.8465449810028076], "all_perturbed_original_ll": [-3.825690269470215], "perturbed_sampled_ll": -3.8465449810028076, "perturbed_original_ll": -3.825690269470215, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Added notification to solve \"docker-credential-service-error\" (#18524)", "sampled": "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "perturbed_sampled": ["Added notification to solve \"docker-credential-service-error\" (#18524)Tested"], "perturbed_original": ["Added notification to solve \"docker-credential-service-error\" (#18524)"], "original_ll": -4.5467000007629395, "sampled_ll": -4.871146202087402, "all_perturbed_sampled_ll": [-4.871146202087402], "all_perturbed_original_ll": [-4.5467000007629395], "perturbed_sampled_ll": -4.871146202087402, "perturbed_original_ll": -4.5467000007629395, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "sampled": "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "perturbed_sampled": ["[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What"], "perturbed_original": ["[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)"], "original_ll": -5.137643337249756, "sampled_ll": -5.546127796173096, "all_perturbed_sampled_ll": [-5.546127796173096], "all_perturbed_original_ll": [-5.137643337249756], "perturbed_sampled_ll": -5.546127796173096, "perturbed_original_ll": -5.137643337249756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Remove profiling link (#4602)", "sampled": "[AIRFLOW-XXX] Remove profiling link (#4602)One", "perturbed_sampled": ["[AIRFLOW-XXX] Remove profiling link (#4602)One"], "perturbed_original": ["[AIRFLOW-XXX] Remove profiling link (#4602)"], "original_ll": -6.615202903747559, "sampled_ll": -7.196170806884766, "all_perturbed_sampled_ll": [-7.196170806884766], "all_perturbed_original_ll": [-6.615202903747559], "perturbed_sampled_ll": -7.196170806884766, "perturbed_original_ll": -6.615202903747559, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but then it causes a lot of extra strain in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the subsequent merge cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but this happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "sampled": "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "perturbed_sampled": ["Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are"], "perturbed_original": ["Enables back ward cancellation on push. We disabled duplicate cancelling on push/schedule in #11397 but then it causes a bit of extra strain in case multiple PRs are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the subsequent merge cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but the PR is rarely enough to suffer the pain at expense of much more PRs getting to be moved into the queue in GitHub Actions."], "original_ll": -4.034751892089844, "sampled_ll": -1.8665119409561157, "all_perturbed_sampled_ll": [-1.8665119409561157], "all_perturbed_original_ll": [-4.215327739715576], "perturbed_sampled_ll": -1.8665119409561157, "perturbed_original_ll": -4.215327739715576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Disable experimental REST API by default (#12337)", "sampled": "Disable experimental REST API by default (#12337)In", "perturbed_sampled": ["Disable experimental REST API by default (#12337)In"], "perturbed_original": ["Disable experimental REST API by default (#12337)"], "original_ll": -5.420007705688477, "sampled_ll": -6.299905300140381, "all_perturbed_sampled_ll": [-6.299905300140381], "all_perturbed_original_ll": [-5.420007705688477], "perturbed_sampled_ll": -6.299905300140381, "perturbed_original_ll": -5.420007705688477, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds a forgotten word in a README.md (#12066)", "sampled": "Adds a forgotten word in a README.md (#12066)When", "perturbed_sampled": ["Adds a forgotten word in a README.md (#12066)When"], "perturbed_original": ["Adds a forgotten word in a README.md (#12066)"], "original_ll": -4.451131820678711, "sampled_ll": -5.095714092254639, "all_perturbed_sampled_ll": [-5.095714092254639], "all_perturbed_original_ll": [-4.451131820678711], "perturbed_sampled_ll": -5.095714092254639, "perturbed_original_ll": -4.451131820678711, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove flask-admin based Plugins (#11515)", "sampled": "Remove flask-admin based Plugins (#11515)It", "perturbed_sampled": ["Remove flask-admin based Plugins (#11515)It"], "perturbed_original": ["Remove flask-admin based Plugins (#11515)"], "original_ll": -6.669112205505371, "sampled_ll": -7.409968852996826, "all_perturbed_sampled_ll": [-7.409968852996826], "all_perturbed_original_ll": [-6.669112205505371], "perturbed_sampled_ll": -7.409968852996826, "perturbed_original_ll": -6.669112205505371, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix mypy apache kylin operators (#20595)", "sampled": "Fix mypy apache kylin operators (#20595)The", "perturbed_sampled": ["Fix mypy apache kylin operators (#20595)The"], "perturbed_original": ["Fix mypy apache kylin operators (#20595)"], "original_ll": -6.794467449188232, "sampled_ll": -7.321258068084717, "all_perturbed_sampled_ll": [-7.321258068084717], "all_perturbed_original_ll": [-6.794467449188232], "perturbed_sampled_ll": -7.321258068084717, "perturbed_original_ll": -6.794467449188232, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number it has the scheduler generates the following warning: `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be an action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific log notice for this case.", "sampled": "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number of slots defined, if the number of slots required exceeds the available pool you could find that, from our perspective, this indicates that something is wrong. This kind of event has to happen at least once every 15 seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be able to run much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", "perturbed_sampled": ["Add specific warning when Task requests for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number of slots defined, if the number of slots required exceeds the available pool you could find that, from a functional perspective, this indicates that something is wrong. This kind of event has to happen at least once in 15 seconds, based on our benchmarks. If you were using this code before, and not all tasks were configured to wait for this event, things should be able to run smoothly now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for properties of objects from the same object type (instead of \"Object \" if property does not match the object type of object specified in"], "perturbed_original": ["Add specific log notice! Task asks for more slots than pool defined with (#20178) In cases where task asks for more slots than the total number it has configured, the scheduler generates the following warning: <unk> {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the process run properly there must be an action from the user: 1. User must set the total number slots in the pool the task is using to request the slots. 2. User must change the task code to requests less slots. This PR add specific log notice for this case."], "original_ll": -3.83475661277771, "sampled_ll": -3.1840834617614746, "all_perturbed_sampled_ll": [-3.3448753356933594], "all_perturbed_original_ll": [-3.847167491912842], "perturbed_sampled_ll": -3.3448753356933594, "perturbed_original_ll": -3.847167491912842, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all the CLI command already turn the literal `DAGS_FOLDER` in to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR brings back this behaviour", "sampled": "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, this functionality is still missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "perturbed_sampled": ["Support DAGS folder to be at different location on scheduler and back after job completion. There has been some vestigial support for this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros if one wishes to use other features such as Mota etc. However, this functionality is still missing in newer versions of Docker and is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker"], "perturbed_original": ["Support DAGS folder being in different location on scheduler and runners (#16860) There has been good support for local DAGS folders in Airflow for a while (all the command already turn the internal `DAGS_FOLDER` in to the real location of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR brings back this behaviour"], "original_ll": -3.847554922103882, "sampled_ll": -3.453601837158203, "all_perturbed_sampled_ll": [-3.382504940032959], "all_perturbed_original_ll": [-3.84894061088562], "perturbed_sampled_ll": -3.382504940032959, "perturbed_original_ll": -3.84894061088562, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix Experimental API Client (#9849)", "sampled": "Fix Experimental API Client (#9849)In", "perturbed_sampled": ["Fix Experimental API Client (#9849)In"], "perturbed_original": ["Fix Experimental API Client (#9849)"], "original_ll": -6.725271701812744, "sampled_ll": -7.660573959350586, "all_perturbed_sampled_ll": [-7.660573959350586], "all_perturbed_original_ll": [-6.725271701812744], "perturbed_sampled_ll": -7.660573959350586, "perturbed_original_ll": -6.725271701812744, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this we throw an error, that is captured and showing using the existing import_error mechanism for DAGs. This almost certainly happens because a user has done \"something interesting\".", "sampled": "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, the number of rows was reduced. Now, we just add a line when creating. Also, the column name and ID used to include the query string, is now used to keep the query consistent if the query is a sequence -- i.e., there is no need to include the", "perturbed_sampled": ["Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the new query -- for example, the number of rows was reduced. Now, we just add a line when creating. Also, the column name and ID used to include the query string, is now used to keep the query consistent if it is a sequence -- i.e., there is no need for multiple column names in the"], "perturbed_original": ["Show DAG serialization errors in the DB. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this we throw an error to ensure this is captured and showing using the existing \"not for me\" behavior. This almost certainly happens because the user has done \"something interesting\"."], "original_ll": -3.8079771995544434, "sampled_ll": -3.432356834411621, "all_perturbed_sampled_ll": [-3.5223374366760254], "all_perturbed_original_ll": [-3.7511463165283203], "perturbed_sampled_ll": -3.5223374366760254, "perturbed_original_ll": -3.7511463165283203, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "sampled": "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "perturbed_sampled": ["Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection (#10261) * Various other tests *"], "perturbed_original": ["Update example on docs * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst"], "original_ll": -2.716885805130005, "sampled_ll": -2.9966976642608643, "all_perturbed_sampled_ll": [-3.5399181842803955], "all_perturbed_original_ll": [-3.3596596717834473], "perturbed_sampled_ll": -3.5399181842803955, "perturbed_original_ll": -3.3596596717834473, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "sampled": "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "perturbed_sampled": ["Add function to getFile() (#9685) added get() method to get current context method for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed"], "perturbed_original": ["Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute function. This functionality was not present on previous versions of AIP-31. Jonathan Shir <jonathan.shir@databand.ai>"], "original_ll": -4.226373195648193, "sampled_ll": -3.6902377605438232, "all_perturbed_sampled_ll": [-3.9781341552734375], "all_perturbed_original_ll": [-4.398109436035156], "perturbed_sampled_ll": -3.9781341552734375, "perturbed_original_ll": -4.398109436035156, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adding missing word to welcome message (#16726)", "sampled": "Adding missing word to welcome message (#16726)Dry", "perturbed_sampled": ["Adding missing word to welcome message (#16726)Dry"], "perturbed_original": ["Adding missing word to welcome message (#16726)"], "original_ll": -6.654922962188721, "sampled_ll": -7.028921127319336, "all_perturbed_sampled_ll": [-7.028921127319336], "all_perturbed_original_ll": [-6.654922962188721], "perturbed_sampled_ll": -7.028921127319336, "perturbed_original_ll": -6.654922962188721, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.", "sampled": "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.The", "perturbed_sampled": ["Fix broken master (isort fix) (#11954) Static checks breaks because of a Bad merge to Master.The"], "perturbed_original": ["Fix broken master (isort fix) (#11954) Static checks are failing by default resulting in a Bad merge to Master."], "original_ll": -6.013061046600342, "sampled_ll": -6.174251079559326, "all_perturbed_sampled_ll": [-6.647820472717285], "all_perturbed_original_ll": [-5.972385883331299], "perturbed_sampled_ll": -6.647820472717285, "perturbed_original_ll": -5.972385883331299, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has a set of provider packages that are needed by the extra and they will be installed automatically if this extra is specified. For now we do not add any version specificatiion, until we agree the process in #11425 and then we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "sampled": "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has to specify when it's installed. So you could now have two different versions at all times: On the one hand, airflow will install for the next day and install for the next day and night, then you can specify in which way. The other thing is for the next package install, I could not get the packages install", "perturbed_sampled": ["Adds automated installation of dependent packages (#11526) : whereas previous features are specifying when airflow is installed, this one triggers automatic installs of dependent packages. Each extra dependency package gets its own step, and you can specify when it's installed. So you could now have two options at all times: On the one hand, airflow will install for the day, download and install for the next day and then you can specify in which way. The other thing is for the next package install, I could not get the packages install"], "perturbed_original": ["Adds automated installation of dependent packages. When extras are installed, i.e. when airflow is installed, this one provides automatic installation of dependent packages. Each extra comes with a set of provider packages that are needed by the extra and they will be installed automatically if this extra is installed. For now we do not add any version specificatiion, until we agree the process in #11425 and #11426, we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464"], "original_ll": -4.125349998474121, "sampled_ll": -3.723193883895874, "all_perturbed_sampled_ll": [-3.7685439586639404], "all_perturbed_original_ll": [-3.707193374633789], "perturbed_sampled_ll": -3.7685439586639404, "perturbed_original_ll": -3.707193374633789, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Reorder middleware - ProxyFix and BaseUrl (#8157)", "sampled": "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "perturbed_sampled": ["Reorder middleware - ProxyFix and BaseUrl (#8157)The"], "perturbed_original": ["Reorder middleware - ProxyFix and BaseUrl (#8157)"], "original_ll": -6.123979568481445, "sampled_ll": -6.623643398284912, "all_perturbed_sampled_ll": [-6.623643398284912], "all_perturbed_original_ll": [-6.123979568481445], "perturbed_sampled_ll": -6.623643398284912, "perturbed_original_ll": -6.123979568481445, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however it has some unforeseen consequences. The 'provider_packages' script copy all the providers code for backports in order to refactor them to the empty \"airflow\" directory in provider_packages folder. The #10806 change turned that empty folder in 'airflow' package because it was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "sampled": "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which could only be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages were added or removed via explicit commands. If they were added the module-info of the module in question is now visible (the check of whether it was updated). In order to", "perturbed_sampled": ["Moves provider packages scripts to dev (#12082) The change #10806 made airflow and airflow2 packages implicit packages when \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which can only be called by the script \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" now makes sure package were removed by or removed via explicit commands. If they were added the module-info of the module in question is now visible (the check of whether it was updated). In order to"], "perturbed_original": ["Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages as they got imported. This is a good change, however result in some unforeseen consequences. The 'provider_packages' script copy all the providers code for backports in airflow and refactor them to the empty \"airflow\" directory in provider_packages folder. The #10806 change turned that empty folder into a package because it was in the same directory as the provider_packages scripts. Moving provider_packages scripts to dev solves this problem."], "original_ll": -3.9833812713623047, "sampled_ll": -3.8473503589630127, "all_perturbed_sampled_ll": [-4.009209632873535], "all_perturbed_original_ll": [-4.049409866333008], "perturbed_sampled_ll": -4.009209632873535, "perturbed_original_ll": -4.049409866333008, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Override project in dataprocSubmitJobOperator (#14981)", "sampled": "Override project in dataprocSubmitJobOperator (#14981)With", "perturbed_sampled": ["Override project in dataprocSubmitJobOperator (#14981)With"], "perturbed_original": ["Override project in dataprocSubmitJobOperator (#14981)"], "original_ll": -7.058787822723389, "sampled_ll": -7.659811496734619, "all_perturbed_sampled_ll": [-7.659811496734619], "all_perturbed_original_ll": [-7.058787822723389], "perturbed_sampled_ll": -7.659811496734619, "perturbed_original_ll": -7.058787822723389, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Upload provider distribution artifacts during CI (#19807)", "sampled": "Upload provider distribution artifacts during CI (#19807)We", "perturbed_sampled": ["Upload provider distribution artifacts during CI (#19807)We"], "perturbed_original": ["Upload provider distribution artifacts during CI (#19807)"], "original_ll": -7.708691120147705, "sampled_ll": -8.38458251953125, "all_perturbed_sampled_ll": [-8.38458251953125], "all_perturbed_original_ll": [-7.708691120147705], "perturbed_sampled_ll": -8.38458251953125, "perturbed_original_ll": -7.708691120147705, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "sampled": "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by name which is great for data consistency. Also on web browser it will search for the", "perturbed_sampled": ["Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g.: arbitrary text) in conn uri format. In that case airflow uses the conn uri format to look for the data by name which is great for data consistency. Also on web browser it works well looking for the"], "perturbed_original": ["Add support for arbitrary json in conn uri format (#15100) Currently in airflow web service and in the CLI , you can store arbitrary (e.g. nested) json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the conn URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Laxman <ash_github@firemirror.com>"], "original_ll": -3.4491465091705322, "sampled_ll": -3.6833443641662598, "all_perturbed_sampled_ll": [-3.9558017253875732], "all_perturbed_original_ll": [-3.3735852241516113], "perturbed_sampled_ll": -3.9558017253875732, "perturbed_original_ll": -3.3735852241516113, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "sampled": "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "perturbed_sampled": ["[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The"], "perturbed_original": ["[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)"], "original_ll": -5.797331809997559, "sampled_ll": -6.141626358032227, "all_perturbed_sampled_ll": [-6.141626358032227], "all_perturbed_original_ll": [-5.797331809997559], "perturbed_sampled_ll": -6.141626358032227, "perturbed_original_ll": -5.797331809997559, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6820] split breeze into functions (#7433)", "sampled": "[AIRFLOW-6820] split breeze into functions (#7433)The", "perturbed_sampled": ["[AIRFLOW-6820] split breeze into functions (#7433)The"], "perturbed_original": ["[AIRFLOW-6820] split breeze into functions (#7433)"], "original_ll": -6.755216598510742, "sampled_ll": -7.13143253326416, "all_perturbed_sampled_ll": [-7.13143253326416], "all_perturbed_original_ll": [-6.755216598510742], "perturbed_sampled_ll": -7.13143253326416, "perturbed_original_ll": -6.755216598510742, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "sampled": "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "perturbed_sampled": ["Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>"], "perturbed_original": ["Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>"], "original_ll": -4.839278221130371, "sampled_ll": -5.155980110168457, "all_perturbed_sampled_ll": [-5.155980110168457], "all_perturbed_original_ll": [-4.839278221130371], "perturbed_sampled_ll": -5.155980110168457, "perturbed_original_ll": -4.839278221130371, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "sampled": "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "perturbed_sampled": ["[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator"], "perturbed_original": ["[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator"], "original_ll": -3.294480562210083, "sampled_ll": -3.5100462436676025, "all_perturbed_sampled_ll": [-3.5100462436676025], "all_perturbed_original_ll": [-3.294480562210083], "perturbed_sampled_ll": -3.5100462436676025, "perturbed_original_ll": -3.294480562210083, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "sampled": "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "perturbed_sampled": ["CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 . By Martin Berlin-Leischner Fixed"], "perturbed_original": ["CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: ash_github <ash_github@firemirror.com>"], "original_ll": -4.909962177276611, "sampled_ll": -5.242663383483887, "all_perturbed_sampled_ll": [-5.75354528427124], "all_perturbed_original_ll": [-4.5256524085998535], "perturbed_sampled_ll": -5.75354528427124, "perturbed_original_ll": -4.5256524085998535, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "sampled": "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "perturbed_sampled": ["Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>"], "perturbed_original": ["Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>"], "original_ll": -4.182562828063965, "sampled_ll": -4.182562828063965, "all_perturbed_sampled_ll": [-4.182562828063965], "all_perturbed_original_ll": [-4.182562828063965], "perturbed_sampled_ll": -4.182562828063965, "perturbed_original_ll": -4.182562828063965, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is set. We are utilising aliases if possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work as expected so we have to run a few functions directly in other to show spinner. Extracted from #10368", "sampled": "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that often occurred after precommits were run during development. The change does not impact all build systems. For", "perturbed_sampled": ["When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI systems that often occurred after precommits were run during development. The change does not affect all build systems. For"], "perturbed_original": ["When precommits are run, output is limited to show errors only. (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is used. (#10628) We are utilising aliases if possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work as expected so we have to run a few functions directly in other to show spinner. ( #10368"], "original_ll": -4.0718560218811035, "sampled_ll": -3.147503137588501, "all_perturbed_sampled_ll": [-3.194725275039673], "all_perturbed_original_ll": [-3.73801589012146], "perturbed_sampled_ll": -3.194725275039673, "perturbed_original_ll": -3.73801589012146, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add note about using dag_run.conf in BashOperator (#9143)", "sampled": "Add note about using dag_run.conf in BashOperator (#9143)For", "perturbed_sampled": ["Add note about using dag_run.conf in BashOperator (#9143)For"], "perturbed_original": ["Add note about using dag_run.conf in BashOperator (#9143)"], "original_ll": -5.764639377593994, "sampled_ll": -6.227072238922119, "all_perturbed_sampled_ll": [-6.227072238922119], "all_perturbed_original_ll": [-5.764639377593994], "perturbed_sampled_ll": -6.227072238922119, "perturbed_original_ll": -5.764639377593994, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match", "sampled": "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "perturbed_sampled": ["Test exact match of the assertion (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead"], "perturbed_original": ["Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of exact match"], "original_ll": -3.916752576828003, "sampled_ll": -2.7977545261383057, "all_perturbed_sampled_ll": [-2.598262310028076], "all_perturbed_original_ll": [-3.8331568241119385], "perturbed_sampled_ll": -2.598262310028076, "perturbed_original_ll": -3.8331568241119385, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Added json_render method to separate filtering from view (#14024)", "sampled": "Added json_render method to separate filtering from view (#14024)I", "perturbed_sampled": ["Added json_render method to separate filtering from view (#14024)I"], "perturbed_original": ["Added json_render method to separate filtering from view (#14024)"], "original_ll": -5.7230143547058105, "sampled_ll": -6.441814422607422, "all_perturbed_sampled_ll": [-6.441814422607422], "all_perturbed_original_ll": [-5.7230143547058105], "perturbed_sampled_ll": -6.441814422607422, "perturbed_original_ll": -5.7230143547058105, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "sampled": "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "perturbed_sampled": ["[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The"], "perturbed_original": ["[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)"], "original_ll": -5.527222156524658, "sampled_ll": -5.849558353424072, "all_perturbed_sampled_ll": [-5.849558353424072], "all_perturbed_original_ll": [-5.527222156524658], "perturbed_sampled_ll": -5.849558353424072, "perturbed_original_ll": -5.527222156524658, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add placement_strategy option (#9444)", "sampled": "Add placement_strategy option (#9444)We", "perturbed_sampled": ["Add placement_strategy option (#9444)We"], "perturbed_original": ["Add placement_strategy option (#9444)"], "original_ll": -5.775016784667969, "sampled_ll": -6.8082594871521, "all_perturbed_sampled_ll": [-6.8082594871521], "all_perturbed_original_ll": [-5.775016784667969], "perturbed_sampled_ll": -6.8082594871521, "perturbed_original_ll": -5.775016784667969, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "sampled": "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You also get these two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and used to escape shell name\n\nThe `ciphers' field has been eliminated", "perturbed_sampled": ["Use sys.exit() instead . (#10414) The <unk>quit<unk> and `quit` functions are actually `site.Quitter` now. You also get these two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes The <unk>csh' command now return exit and status codes, instead of and error codes You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and used to escape shell name\n\nThe '<unk>commandname<unk> command has been eliminated"], "perturbed_original": ["Use sys.exit() instead of exit() (#10414) The `exit` and `quit` arguments are actually `site.Quitter` objects , which are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with a <unk>-S<unk> flag, or a custom `site.py` is used then `exit` and `quit` will not be present. It is recommended to use <unk>exit()<unk> which is built into the interpreter and is assumed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option."], "original_ll": -2.7601258754730225, "sampled_ll": -2.7118217945098877, "all_perturbed_sampled_ll": [-3.5006229877471924], "all_perturbed_original_ll": [-3.219055414199829], "perturbed_sampled_ll": -3.5006229877471924, "perturbed_original_ll": -3.219055414199829, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and ES, we discovered that airflow does now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "sampled": "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be returned to you if it is not found. All of this is available through the", "perturbed_sampled": ["[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will only work with OSX hosts . A default configuration will be returned to you if it is not found. All of the details are available through the"], "perturbed_original": ["[AIRFLOW-5139] Support for ES configs (#5760) * AIRFLOW-5139 Support for ES configs While attempting to create a self-signed TLS connection between airflow and ES, we discovered that airflow does now allow users to modify the SSL state of the elasticsearchtaskhandler. This should allow users to define ES settings in the airflow.cfg"], "original_ll": -4.223630428314209, "sampled_ll": -3.3788585662841797, "all_perturbed_sampled_ll": [-3.4481382369995117], "all_perturbed_original_ll": [-4.119253635406494], "perturbed_sampled_ll": -3.4481382369995117, "perturbed_original_ll": -4.119253635406494, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "sampled": "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "perturbed_sampled": ["[AIRFLOW-5435] Add fallback from java_id to project id in GKEPodOperator (#6051) * Implement a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect"], "perturbed_original": ["[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator"], "original_ll": -2.738751173019409, "sampled_ll": -3.9431238174438477, "all_perturbed_sampled_ll": [-4.537553310394287], "all_perturbed_original_ll": [-2.738751173019409], "perturbed_sampled_ll": -4.537553310394287, "perturbed_original_ll": -2.738751173019409, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "sampled": "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "perturbed_sampled": ["[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The"], "perturbed_original": ["[AIRFLOW-6959] Use NULL as dag.description default value (#7593)"], "original_ll": -6.613442897796631, "sampled_ll": -6.897268772125244, "all_perturbed_sampled_ll": [-6.897268772125244], "all_perturbed_original_ll": [-6.613442897796631], "perturbed_sampled_ll": -6.897268772125244, "perturbed_original_ll": -6.613442897796631, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "sampled": "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "perturbed_sampled": ["Fix break backports in packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport s. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio"], "perturbed_original": ["Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File <unk>id> 32, in <module> from airflow.models.baseoperator import chain to <unk>module> file using import name 'chain' ```"], "original_ll": -3.6533801555633545, "sampled_ll": -2.6417195796966553, "all_perturbed_sampled_ll": [-2.729328155517578], "all_perturbed_original_ll": [-5.444174289703369], "perturbed_sampled_ll": -2.729328155517578, "perturbed_original_ll": -5.444174289703369, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs", "sampled": "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "perturbed_sampled": ["Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst s files, bugfix: Do not store configuration of OpenVPN agent in"], "perturbed_original": ["Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete file, please add it to the docs"], "original_ll": -4.0744757652282715, "sampled_ll": -3.986768960952759, "all_perturbed_sampled_ll": [-4.52996826171875], "all_perturbed_original_ll": [-3.901812791824341], "perturbed_sampled_ll": -4.52996826171875, "perturbed_original_ll": -3.901812791824341, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "sampled": "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "perturbed_sampled": ["[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for new pod s being preempted by kuber (#6459) * [AIRFLOW-6012] - kubernetes-ip-config"], "perturbed_original": ["[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted"], "original_ll": -3.676008701324463, "sampled_ll": -3.5043020248413086, "all_perturbed_sampled_ll": [-3.539837598800659], "all_perturbed_original_ll": [-3.676008701324463], "perturbed_sampled_ll": -3.539837598800659, "perturbed_original_ll": -3.676008701324463, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "sampled": "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the kernel takes when", "perturbed_sampled": ["Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the test_process class takes when"], "perturbed_original": ["Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are retried sometimes and fail sometimes"], "original_ll": -4.105998516082764, "sampled_ll": -3.2886242866516113, "all_perturbed_sampled_ll": [-3.3140861988067627], "all_perturbed_original_ll": [-4.132963180541992], "perturbed_sampled_ll": -3.3140861988067627, "perturbed_original_ll": -4.132963180541992, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not strictly necessary because Recently GitHub recognized this as being a problem and introduced new rules for scheduled workflows. But for people who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public URL explaining it yet): > Scheduled workflows will be disabled by default in", "sampled": "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not showing up to all the active developers. Fixes #9077\n\nFixed issue with not accepting a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "perturbed_sampled": ["Limits CodeQL to run only in the Apache 2.4, 2.2 and 2.3. (#11264) It has been raised quite a few times that workflow added in the last patches can't be done in airflow.\n\nBug Fixes:\n\nFixed problem with new features not showing up to all the active developers. Fixes #9077\n\nFixed issue with triggering a log message without the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that could cause a stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have an active tag. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Adds #11227 Improved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes it easier to keep log messages separated. Adds #11229\n\nImproved"], "perturbed_original": ["Limits CodeQL workflow to code checked in the Apache Airflow repo (#11264) It has been raised quite a bit that workflow added in forked repositories might be invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not for them because Recently GitHub recognized this as being a problem and updated their rules for scheduled workflows. But for people who are already forked, it would be acceptable to not run CodeQL check with scheduled workflows. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public URL explaining it yet): > CodeQL workflow will be disabled by default in"], "original_ll": -4.258916854858398, "sampled_ll": -3.131984233856201, "all_perturbed_sampled_ll": [-3.0103399753570557], "all_perturbed_original_ll": [-4.320258617401123], "perturbed_sampled_ll": -3.0103399753570557, "perturbed_original_ll": -4.320258617401123, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "sampled": "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "perturbed_sampled": ["Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was removed in this Pull Request. Due to the change, we have to revert Master"], "perturbed_original": ["Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was set for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master"], "original_ll": -4.303107738494873, "sampled_ll": -3.8130943775177, "all_perturbed_sampled_ll": [-3.7751364707946777], "all_perturbed_original_ll": [-4.427061080932617], "perturbed_sampled_ll": -3.7751364707946777, "perturbed_original_ll": -4.427061080932617, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "sampled": "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example", "perturbed_sampled": ["Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use MPEG MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg Now you can connect to MySQL database from a web browser. For example"], "perturbed_original": ["Update install_mysql.sh (#12101) After Debian 9 and according to the official Debian manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of selecting \"keyring to add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or xml file extension. Also added better redirection on the apt-key list command."], "original_ll": -3.173780918121338, "sampled_ll": -2.1254050731658936, "all_perturbed_sampled_ll": [-2.479349136352539], "all_perturbed_original_ll": [-3.4599316120147705], "perturbed_sampled_ll": -2.479349136352539, "perturbed_original_ll": -3.4599316120147705, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "sampled": "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "perturbed_sampled": ["Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint"], "perturbed_original": ["Add Apache Airflow CODE_OF_CONDUCT.md (#9715)"], "original_ll": -4.547948837280273, "sampled_ll": -5.002066612243652, "all_perturbed_sampled_ll": [-5.002066612243652], "all_perturbed_original_ll": [-4.547948837280273], "perturbed_sampled_ll": -5.002066612243652, "perturbed_original_ll": -4.547948837280273, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "sampled": "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "perturbed_sampled": ["[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)"], "perturbed_original": ["[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)"], "original_ll": -4.913494110107422, "sampled_ll": -4.913494110107422, "all_perturbed_sampled_ll": [-4.913494110107422], "all_perturbed_original_ll": [-4.913494110107422], "perturbed_sampled_ll": -4.913494110107422, "perturbed_original_ll": -4.913494110107422, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "sampled": "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "perturbed_sampled": ["Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing"], "perturbed_original": ["Updating the InfluxDB example DAG to use the TaskFlow API (#18596)"], "original_ll": -4.973775863647461, "sampled_ll": -5.327398300170898, "all_perturbed_sampled_ll": [-5.327398300170898], "all_perturbed_original_ll": [-4.973775863647461], "perturbed_sampled_ll": -5.327398300170898, "perturbed_original_ll": -4.973775863647461, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of dependencies broke them: 1) SQS moto 2.2.6 broke SQS tests - the queue url in the 2.2.6+ version has to start with http:// or https:// 2) DataCatalog part of Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "sampled": "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of the database engines had changed the way the tests were created and they didn't have enough time to find the missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "perturbed_sampled": ["Fix providers tests in main branch with eager upgrades (#18040) : Refresh and fix failing tests in main branch because some recent release of the database engines had changed the way the tests were defined and because they didn't have enough time to identify and update the missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression"], "perturbed_original": ["Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog part require tests in main branch because some recent release of dependencies broke them: 1) SQS moto component failed SQS tests - the url in the 2.2.6+ version has to start with http:// or have https:// 2) DataCatalog part of Google Provider incorrectly imported types and broke tests (used beta instad rg path)"], "original_ll": -4.631836891174316, "sampled_ll": -3.396033525466919, "all_perturbed_sampled_ll": [-3.4822115898132324], "all_perturbed_original_ll": [-5.123173236846924], "perturbed_sampled_ll": -3.4822115898132324, "perturbed_original_ll": -5.123173236846924, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "sampled": "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "perturbed_sampled": ["[AIRFLOW-4686] Make dags Pylint compatible (#5753)The"], "perturbed_original": ["[AIRFLOW-4686] Make dags Pylint compatible (#5753)"], "original_ll": -6.411542892456055, "sampled_ll": -6.769205093383789, "all_perturbed_sampled_ll": [-6.769205093383789], "all_perturbed_original_ll": [-6.411542892456055], "perturbed_sampled_ll": -6.769205093383789, "perturbed_original_ll": -6.411542892456055, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "sampled": "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "perturbed_sampled": ["Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The"], "perturbed_original": ["Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112"], "original_ll": -3.9904048442840576, "sampled_ll": -4.237772464752197, "all_perturbed_sampled_ll": [-4.237772464752197], "all_perturbed_original_ll": [-3.9904048442840576], "perturbed_sampled_ll": -4.237772464752197, "perturbed_original_ll": -3.9904048442840576, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow ./run_tmux.sh script to run standalone (#13420)", "sampled": "Allow ./run_tmux.sh script to run standalone (#13420)I", "perturbed_sampled": ["Allow ./run_tmux.sh script to run standalone (#13420)I"], "perturbed_original": ["Allow ./run_tmux.sh script to run standalone (#13420)"], "original_ll": -4.795127868652344, "sampled_ll": -5.527097702026367, "all_perturbed_sampled_ll": [-5.527097702026367], "all_perturbed_original_ll": [-4.795127868652344], "perturbed_sampled_ll": -5.527097702026367, "perturbed_original_ll": -4.795127868652344, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "sampled": "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "perturbed_sampled": ["fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of"], "perturbed_original": ["fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id"], "original_ll": -3.8451452255249023, "sampled_ll": -3.963820457458496, "all_perturbed_sampled_ll": [-3.963820457458496], "all_perturbed_original_ll": [-3.8451452255249023], "perturbed_sampled_ll": -3.963820457458496, "perturbed_original_ll": -3.8451452255249023, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by `mypy --install-types` command. Part of #19891", "sampled": "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only warned about missing mypy types that it knew about.)", "perturbed_sampled": ["Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about. (It is only complaints about missing mypy types that MyPy complains about.)"], "perturbed_original": ["Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not add type definitions to indicate the lack of those. Added by `mypy --install-types` command. See also #19891"], "original_ll": -4.964541435241699, "sampled_ll": -4.3235249519348145, "all_perturbed_sampled_ll": [-4.254897594451904], "all_perturbed_original_ll": [-4.7246623039245605], "perturbed_sampled_ll": -4.254897594451904, "perturbed_original_ll": -4.7246623039245605, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "sampled": "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "perturbed_sampled": ["[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From"], "perturbed_original": ["[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)"], "original_ll": -4.9959516525268555, "sampled_ll": -5.436398029327393, "all_perturbed_sampled_ll": [-5.436398029327393], "all_perturbed_original_ll": [-4.9959516525268555], "perturbed_sampled_ll": -5.436398029327393, "perturbed_original_ll": -4.9959516525268555, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25th", "sampled": "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "perturbed_sampled": ["Doc: Update Helm ing Framework Release Date (#17244) We released it on 26th July 2021 instead of 25thThis"], "perturbed_original": ["Doc: Update Helm Chart 1.1.0 Release Date (#17244) and release it on 26th July 2021 instead of 25th"], "original_ll": -4.932730674743652, "sampled_ll": -5.283536434173584, "all_perturbed_sampled_ll": [-6.238496780395508], "all_perturbed_original_ll": [-4.8982110023498535], "perturbed_sampled_ll": -6.238496780395508, "perturbed_original_ll": -4.8982110023498535, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test with `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add new UI node files to rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "sampled": "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created (#14920) * make sure initial route is set to something we want before running the test suite (#15036)* Fixes a regression that was previously causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "perturbed_sampled": ["Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests for linting issues when router is set (#14920) * make sure initial route is set to something we want before running test suite (#15036)* Fixes a regression that was causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix"], "perturbed_original": ["Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion tests #4927 * fix test with `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest lib * Use UI node files instead of id libs * Add new UI node files to rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>"], "original_ll": -3.9776809215545654, "sampled_ll": -3.11045503616333, "all_perturbed_sampled_ll": [-3.2052290439605713], "all_perturbed_original_ll": [-3.926548719406128], "perturbed_sampled_ll": -3.2052290439605713, "perturbed_original_ll": -3.926548719406128, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "provide_session keep return type (#9787)", "sampled": "provide_session keep return type (#9787)What", "perturbed_sampled": ["provide_session keep return type (#9787)What"], "perturbed_original": ["provide_session keep return type (#9787)"], "original_ll": -6.97110652923584, "sampled_ll": -7.808758735656738, "all_perturbed_sampled_ll": [-7.808758735656738], "all_perturbed_original_ll": [-6.97110652923584], "perturbed_sampled_ll": -7.808758735656738, "perturbed_original_ll": -6.97110652923584, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows sidecars with webservers in them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress.", "sampled": "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "perturbed_sampled": ["Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status notifications (#16558) add DNS service status notifications (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS when generating DNS"], "perturbed_original": ["Chart: refactor ing new flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower servers. This allows sidecars with a rule defining them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress."], "original_ll": -4.04581880569458, "sampled_ll": -2.488743305206299, "all_perturbed_sampled_ll": [-2.723745822906494], "all_perturbed_original_ll": [-4.8367791175842285], "perturbed_sampled_ll": -2.723745822906494, "perturbed_original_ll": -4.8367791175842285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "sampled": "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "perturbed_sampled": ["[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What"], "perturbed_original": ["[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)"], "original_ll": -5.757751941680908, "sampled_ll": -6.2818827629089355, "all_perturbed_sampled_ll": [-6.2818827629089355], "all_perturbed_original_ll": [-5.757751941680908], "perturbed_sampled_ll": -6.2818827629089355, "perturbed_original_ll": -5.757751941680908, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "sampled": "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "perturbed_sampled": ["Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb"], "perturbed_original": ["Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>"], "original_ll": -4.6524786949157715, "sampled_ll": -4.803845405578613, "all_perturbed_sampled_ll": [-4.803845405578613], "all_perturbed_original_ll": [-4.6524786949157715], "perturbed_sampled_ll": -4.803845405578613, "perturbed_original_ll": -4.6524786949157715, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "sampled": "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "perturbed_sampled": ["Add on_kill support to KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the process will finish right away instead of waiting for the pod to destroy it."], "perturbed_original": ["Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a KPU in the UI, that the associated pod is also killed using the on_kill method."], "original_ll": -3.674380302429199, "sampled_ll": -3.393378496170044, "all_perturbed_sampled_ll": [-3.5103843212127686], "all_perturbed_original_ll": [-3.858513832092285], "perturbed_sampled_ll": -3.5103843212127686, "perturbed_original_ll": -3.858513832092285, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "sampled": "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "perturbed_sampled": ["Rename second pylint pre-commit hook to distinguish it from first (#13303)This"], "perturbed_original": ["Rename second pylint pre-commit hook to distinguish it from first (#13303)"], "original_ll": -5.093808650970459, "sampled_ll": -5.512474536895752, "all_perturbed_sampled_ll": [-5.512474536895752], "all_perturbed_original_ll": [-5.093808650970459], "perturbed_sampled_ll": -5.512474536895752, "perturbed_original_ll": -5.093808650970459, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixing bug which restricted the visibility of ImportErrors (#17924)", "sampled": "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "perturbed_sampled": ["Fixing bug which restricted the visibility of ImportErrors (#17924)The"], "perturbed_original": ["Fixing bug which restricted the visibility of ImportErrors (#17924)"], "original_ll": -5.1197614669799805, "sampled_ll": -5.636753559112549, "all_perturbed_sampled_ll": [-5.636753559112549], "all_perturbed_original_ll": [-5.1197614669799805], "perturbed_sampled_ll": -5.636753559112549, "perturbed_original_ll": -5.1197614669799805, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add missing values entries to Parameters in chart/README.md (#11477)", "sampled": "Add missing values entries to Parameters in chart/README.md (#11477)The", "perturbed_sampled": ["Add missing values entries to Parameters in chart/README.md (#11477)The"], "perturbed_original": ["Add missing values entries to Parameters in chart/README.md (#11477)"], "original_ll": -5.473202705383301, "sampled_ll": -5.898624897003174, "all_perturbed_sampled_ll": [-5.898624897003174], "all_perturbed_original_ll": [-5.473202705383301], "perturbed_sampled_ll": -5.898624897003174, "perturbed_original_ll": -5.473202705383301, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different than in tests and makes it difficult to run tests outside of the main directory.", "sampled": "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode", "perturbed_sampled": ["No PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When I changed user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode"], "perturbed_original": ["[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different in all Breeze tests and makes it difficult to run tests outside of the main directory."], "original_ll": -3.9104323387145996, "sampled_ll": -3.671786069869995, "all_perturbed_sampled_ll": [-3.556309938430786], "all_perturbed_original_ll": [-3.8635876178741455], "perturbed_sampled_ll": -3.556309938430786, "perturbed_original_ll": -3.8635876178741455, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixed failed pylint in master (#12938)", "sampled": "Fixed failed pylint in master (#12938)A", "perturbed_sampled": ["Fixed failed pylint in master (#12938)A"], "perturbed_original": ["Fixed failed pylint in master (#12938)"], "original_ll": -5.886841773986816, "sampled_ll": -6.618101596832275, "all_perturbed_sampled_ll": [-6.618101596832275], "all_perturbed_original_ll": [-5.886841773986816], "perturbed_sampled_ll": -6.618101596832275, "perturbed_original_ll": -5.886841773986816, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "sampled": "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "perturbed_sampled": ["Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook"], "perturbed_original": ["Remove duplicate docs for check-hooks-apply pre-commit (#12973)"], "original_ll": -5.150719165802002, "sampled_ll": -5.6031389236450195, "all_perturbed_sampled_ll": [-5.6031389236450195], "all_perturbed_original_ll": [-5.150719165802002], "perturbed_sampled_ll": -5.6031389236450195, "perturbed_original_ll": -5.150719165802002, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "sampled": "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "perturbed_sampled": ["[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd"], "perturbed_original": ["[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)"], "original_ll": -4.416003704071045, "sampled_ll": -3.90499210357666, "all_perturbed_sampled_ll": [-3.90499210357666], "all_perturbed_original_ll": [-4.416003704071045], "perturbed_sampled_ll": -3.90499210357666, "perturbed_original_ll": -4.416003704071045, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "sampled": "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "perturbed_sampled": ["[AIRFLOW-4750] Log identified zombie task instances (#5389)In"], "perturbed_original": ["[AIRFLOW-4750] Log identified zombie task instances (#5389)"], "original_ll": -6.6061553955078125, "sampled_ll": -6.9774041175842285, "all_perturbed_sampled_ll": [-6.9774041175842285], "all_perturbed_original_ll": [-6.6061553955078125], "perturbed_sampled_ll": -6.9774041175842285, "perturbed_original_ll": -6.6061553955078125, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) its task_1. if task_2 depends on task_1 this means task_2 is set downstream of tastk_1 - wondering if I am missing something here - or misreading it", "sampled": "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "perturbed_sampled": ["[AIRFLOW-XXXX] Fix typo from upstream to downstream. This is as it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will also consume inputs and outputs and receive output from the upstream channel. This allows for more separation. *"], "perturbed_original": ["[AIRFLOW-XXXX] Fix typo from \"DAG Run - downstream \" here is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task Instance Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) the other if task_2 depends on it. Which means task_2 is set downstream of tastk_1 - wondering if I am missing something here - or misreading it"], "original_ll": -3.6332876682281494, "sampled_ll": -2.750288486480713, "all_perturbed_sampled_ll": [-2.7110917568206787], "all_perturbed_original_ll": [-3.803412914276123], "perturbed_sampled_ll": -2.7110917568206787, "perturbed_original_ll": -3.803412914276123, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "sampled": "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "perturbed_sampled": ["Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer"], "perturbed_original": ["Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer."], "original_ll": -4.25317907333374, "sampled_ll": -4.343189239501953, "all_perturbed_sampled_ll": [-4.343189239501953], "all_perturbed_original_ll": [-4.25317907333374], "perturbed_sampled_ll": -4.343189239501953, "perturbed_original_ll": -4.25317907333374, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "sampled": "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "perturbed_sampled": ["[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks"], "perturbed_original": ["[AIRFLOW-5406] allow spark without kubernetes (#6921)"], "original_ll": -6.072336196899414, "sampled_ll": -6.523364067077637, "all_perturbed_sampled_ll": [-6.523364067077637], "all_perturbed_original_ll": [-6.072336196899414], "perturbed_sampled_ll": -6.523364067077637, "perturbed_original_ll": -6.072336196899414, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "sampled": "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we may not always get these errors anymore. The latest commit gives us a workaround", "perturbed_sampled": ["Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without more information. However now we may not always get these errors anymore. The latest commit gives no workaround"], "perturbed_original": ["Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has been improved by decreasing the number of checks for CI dockers and healthiness checks and is no longer needed."], "original_ll": -4.3843584060668945, "sampled_ll": -3.666752815246582, "all_perturbed_sampled_ll": [-3.9435970783233643], "all_perturbed_original_ll": [-4.1292338371276855], "perturbed_sampled_ll": -3.9435970783233643, "perturbed_original_ll": -4.1292338371276855, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Strict type check for google ads and cloud hooks (#11390)", "sampled": "Strict type check for google ads and cloud hooks (#11390)A", "perturbed_sampled": ["Strict type check for google ads and cloud hooks (#11390)A"], "perturbed_original": ["Strict type check for google ads and cloud hooks (#11390)"], "original_ll": -6.086731433868408, "sampled_ll": -6.577614784240723, "all_perturbed_sampled_ll": [-6.577614784240723], "all_perturbed_original_ll": [-6.086731433868408], "perturbed_sampled_ll": -6.577614784240723, "perturbed_original_ll": -6.086731433868408, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Dev: Clarify file naming in release verification doc (#19233)", "sampled": "Dev: Clarify file naming in release verification doc (#19233)If", "perturbed_sampled": ["Dev: Clarify file naming in release verification doc (#19233)If"], "perturbed_original": ["Dev: Clarify file naming in release verification doc (#19233)"], "original_ll": -6.128222465515137, "sampled_ll": -6.858707904815674, "all_perturbed_sampled_ll": [-6.858707904815674], "all_perturbed_original_ll": [-6.128222465515137], "perturbed_sampled_ll": -6.858707904815674, "perturbed_original_ll": -6.128222465515137, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve KubernetesPodOperator guide (#9079)", "sampled": "Improve KubernetesPodOperator guide (#9079)A", "perturbed_sampled": ["Improve KubernetesPodOperator guide (#9079)A"], "perturbed_original": ["Improve KubernetesPodOperator guide (#9079)"], "original_ll": -5.9975409507751465, "sampled_ll": -6.366732120513916, "all_perturbed_sampled_ll": [-6.366732120513916], "all_perturbed_original_ll": [-5.9975409507751465], "perturbed_sampled_ll": -6.366732120513916, "perturbed_original_ll": -5.9975409507751465, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Small typo in JdbcOperator (#18593)", "sampled": "Small typo in JdbcOperator (#18593)With", "perturbed_sampled": ["Small typo in JdbcOperator (#18593)With"], "perturbed_original": ["Small typo in JdbcOperator (#18593)"], "original_ll": -5.63859224319458, "sampled_ll": -6.444066524505615, "all_perturbed_sampled_ll": [-6.444066524505615], "all_perturbed_original_ll": [-5.63859224319458], "perturbed_sampled_ll": -6.444066524505615, "perturbed_original_ll": -5.63859224319458, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "sampled": "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all", "perturbed_sampled": ["Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeek (#18438) * Update all days to follow 'month' schedule * Update all"], "perturbed_original": ["Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 3. Add optional class that accepts any iterable as week_day."], "original_ll": -4.110437870025635, "sampled_ll": -3.4426755905151367, "all_perturbed_sampled_ll": [-4.116459846496582], "all_perturbed_original_ll": [-4.117380142211914], "perturbed_sampled_ll": -4.116459846496582, "perturbed_original_ll": -4.117380142211914, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "sampled": "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "perturbed_sampled": ["Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski"], "perturbed_original": ["Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>"], "original_ll": -3.785753011703491, "sampled_ll": -4.0247392654418945, "all_perturbed_sampled_ll": [-4.0247392654418945], "all_perturbed_original_ll": [-3.785753011703491], "perturbed_sampled_ll": -4.0247392654418945, "perturbed_original_ll": -3.785753011703491, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "sampled": "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "perturbed_sampled": ["[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse"], "perturbed_original": ["[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)"], "original_ll": -6.541927814483643, "sampled_ll": -6.67662239074707, "all_perturbed_sampled_ll": [-6.67662239074707], "all_perturbed_original_ll": [-6.541927814483643], "perturbed_sampled_ll": -6.67662239074707, "perturbed_original_ll": -6.541927814483643, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "sampled": "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "perturbed_sampled": ["Make models/pool.py pylint compatible * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for"], "perturbed_original": ["Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>"], "original_ll": -4.075239658355713, "sampled_ll": -3.68587064743042, "all_perturbed_sampled_ll": [-3.465177059173584], "all_perturbed_original_ll": [-4.075239658355713], "perturbed_sampled_ll": -3.465177059173584, "perturbed_original_ll": -4.075239658355713, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "sampled": "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "perturbed_sampled": ["[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The"], "perturbed_original": ["[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)"], "original_ll": -6.36091947555542, "sampled_ll": -6.634527206420898, "all_perturbed_sampled_ll": [-6.634527206420898], "all_perturbed_original_ll": [-6.36091947555542], "perturbed_sampled_ll": -6.634527206420898, "perturbed_original_ll": -6.36091947555542, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "sampled": "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "perturbed_sampled": ["Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For"], "perturbed_original": ["Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)"], "original_ll": -6.718223571777344, "sampled_ll": -7.345280170440674, "all_perturbed_sampled_ll": [-7.345280170440674], "all_perturbed_original_ll": [-6.718223571777344], "perturbed_sampled_ll": -7.345280170440674, "perturbed_original_ll": -6.718223571777344, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "sampled": "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "perturbed_sampled": ["Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A"], "perturbed_original": ["Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)"], "original_ll": -6.434218406677246, "sampled_ll": -6.828874588012695, "all_perturbed_sampled_ll": [-6.828874588012695], "all_perturbed_original_ll": [-6.434218406677246], "perturbed_sampled_ll": -6.828874588012695, "perturbed_original_ll": -6.434218406677246, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod 3. reconcile with any of the", "sampled": "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "perturbed_sampled": ["Fix full_pod for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support full_pod_spec on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, and complete_operator (#11925) * Fix"], "perturbed_original": ["Fix full_pod_spec for k8spodoperator * Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored in to kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod 3. reconcile with any of the"], "original_ll": -3.1907310485839844, "sampled_ll": -1.9140031337738037, "all_perturbed_sampled_ll": [-2.1788179874420166], "all_perturbed_original_ll": [-3.138169765472412], "perturbed_sampled_ll": -2.1788179874420166, "perturbed_original_ll": -3.138169765472412, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "sampled": "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "perturbed_sampled": ["[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon"], "perturbed_original": ["[AIRFLOW-6929] Add OpenAPI spec (#7549)"], "original_ll": -6.245003700256348, "sampled_ll": -6.41998291015625, "all_perturbed_sampled_ll": [-6.41998291015625], "all_perturbed_original_ll": [-6.245003700256348], "perturbed_sampled_ll": -6.41998291015625, "perturbed_original_ll": -6.245003700256348, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "sampled": "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "perturbed_sampled": ["[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A"], "perturbed_original": ["[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)"], "original_ll": -6.1467180252075195, "sampled_ll": -6.492127418518066, "all_perturbed_sampled_ll": [-6.492127418518066], "all_perturbed_original_ll": [-6.1467180252075195], "perturbed_sampled_ll": -6.492127418518066, "perturbed_original_ll": -6.1467180252075195, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "sampled": "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "perturbed_sampled": ["Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The"], "perturbed_original": ["Fix typo in the word 'instance' (#10902) `instnace` -> `instance`"], "original_ll": -5.098919868469238, "sampled_ll": -5.450592994689941, "all_perturbed_sampled_ll": [-5.450592994689941], "all_perturbed_original_ll": [-5.098919868469238], "perturbed_sampled_ll": -5.450592994689941, "perturbed_original_ll": -5.098919868469238, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "sampled": "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "perturbed_sampled": ["Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99"], "perturbed_original": ["Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8."], "original_ll": -4.402935028076172, "sampled_ll": -4.140133857727051, "all_perturbed_sampled_ll": [-4.140133857727051], "all_perturbed_original_ll": [-4.402935028076172], "perturbed_sampled_ll": -4.140133857727051, "perturbed_original_ll": -4.402935028076172, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "sampled": "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "perturbed_sampled": ["[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The"], "perturbed_original": ["[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)"], "original_ll": -6.262882232666016, "sampled_ll": -6.579794406890869, "all_perturbed_sampled_ll": [-6.579794406890869], "all_perturbed_original_ll": [-6.262882232666016], "perturbed_sampled_ll": -6.579794406890869, "perturbed_original_ll": -6.262882232666016, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape problem areas, this commit just fixes the last instances so that we can assert that `|safe` is never used.", "sampled": "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "perturbed_sampled": ["Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use :safe to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most"], "perturbed_original": ["Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to solve problem areas, this commit just fixes the last instances so that we can have a \"normal\" environment where `|safe` is never used."], "original_ll": -4.155300140380859, "sampled_ll": -2.714235305786133, "all_perturbed_sampled_ll": [-2.741666555404663], "all_perturbed_original_ll": [-3.9781899452209473], "perturbed_sampled_ll": -2.741666555404663, "perturbed_original_ll": -3.9781899452209473, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in processing environment variable file which prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "sampled": "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "perturbed_sampled": ["Errors in documentation of trying a buggy docker-compose v2 (#16989) Docker-Compose v1 has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is"], "perturbed_original": ["Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 has an error in processing environment variable file which prevents Breeze from running. Until it's fixed, we are going to print an error, explain how to fix it and exit - because the fix seems to be creating more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917"], "original_ll": -3.6711905002593994, "sampled_ll": -2.811732292175293, "all_perturbed_sampled_ll": [-2.701059103012085], "all_perturbed_original_ll": [-3.5054986476898193], "perturbed_sampled_ll": -2.701059103012085, "perturbed_original_ll": -3.5054986476898193, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "sampled": "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "perturbed_sampled": ["AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & let client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet"], "perturbed_original": ["AwsBaseHook make client_type & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type"], "original_ll": -2.6223320960998535, "sampled_ll": -2.1332950592041016, "all_perturbed_sampled_ll": [-2.4659714698791504], "all_perturbed_original_ll": [-2.811896562576294], "perturbed_sampled_ll": -2.4659714698791504, "perturbed_original_ll": -2.811896562576294, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "sampled": "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG entities (#17666)", "perturbed_sampled": ["Docs: Make ``DAG.is_active`` read-only in API with support for the readOnly=True property on DAG entities (#17666)"], "perturbed_original": ["Docs: Make ``DAG.is_active`` read-only in API (#17667) : Fix: - The property on DAG.is_active closes: #17639"], "original_ll": -4.130771636962891, "sampled_ll": -3.9657154083251953, "all_perturbed_sampled_ll": [-4.370526313781738], "all_perturbed_original_ll": [-4.441598415374756], "perturbed_sampled_ll": -4.370526313781738, "perturbed_original_ll": -4.441598415374756, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move the contribution workflow to the beginning of the file (#10092)", "sampled": "Move the contribution workflow to the beginning of the file (#10092)A", "perturbed_sampled": ["Move the contribution workflow to the beginning of the file (#10092)A"], "perturbed_original": ["Move the contribution workflow to the beginning of the file (#10092)"], "original_ll": -4.822419166564941, "sampled_ll": -5.323852062225342, "all_perturbed_sampled_ll": [-5.323852062225342], "all_perturbed_original_ll": [-4.822419166564941], "perturbed_sampled_ll": -5.323852062225342, "perturbed_original_ll": -4.822419166564941, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "shorten name of hook re imports of provide_session and create_session (#12936)", "sampled": "shorten name of hook re imports of provide_session and create_session (#12936)If", "perturbed_sampled": ["shorten name of hook re imports of provide_session and create_session (#12936)If"], "perturbed_original": ["shorten name of hook re imports of provide_session and create_session (#12936)"], "original_ll": -5.920376777648926, "sampled_ll": -6.436463832855225, "all_perturbed_sampled_ll": [-6.436463832855225], "all_perturbed_original_ll": [-5.920376777648926], "perturbed_sampled_ll": -6.436463832855225, "perturbed_original_ll": -5.920376777648926, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Update max_tis_per_query to better render on the webpage (#17971)", "sampled": "Update max_tis_per_query to better render on the webpage (#17971)A", "perturbed_sampled": ["Update max_tis_per_query to better render on the webpage (#17971)A"], "perturbed_original": ["Update max_tis_per_query to better render on the webpage (#17971)"], "original_ll": -5.869471549987793, "sampled_ll": -6.299273490905762, "all_perturbed_sampled_ll": [-6.299273490905762], "all_perturbed_original_ll": [-5.869471549987793], "perturbed_sampled_ll": -6.299273490905762, "perturbed_original_ll": -5.869471549987793, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the main shell is zsh on MacOS (which is the default) this PR changes it so that the output of `command -v bash` is used instead. Fixes #14754", "sampled": "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "perturbed_sampled": ["Fixes recent scripting breeze fix to work with zsh (#14787) The $script introduced in #14579 is not set when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This"], "perturbed_original": ["Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in breeze is not set when the main shell is zsh on MacOS (which is why this PR changes it so that the output of `command -v bash` is used instead of the usual BASH variable). #14754"], "original_ll": -3.9511241912841797, "sampled_ll": -3.859037399291992, "all_perturbed_sampled_ll": [-3.940670967102051], "all_perturbed_original_ll": [-4.14277458190918], "perturbed_sampled_ll": -3.940670967102051, "perturbed_original_ll": -4.14277458190918, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix capitalisation of boolean in config (#13569)", "sampled": "Fix capitalisation of boolean in config (#13569)The", "perturbed_sampled": ["Fix capitalisation of boolean in config (#13569)The"], "perturbed_original": ["Fix capitalisation of boolean in config (#13569)"], "original_ll": -6.264561653137207, "sampled_ll": -7.038703918457031, "all_perturbed_sampled_ll": [-7.038703918457031], "all_perturbed_original_ll": [-6.264561653137207], "perturbed_sampled_ll": -7.038703918457031, "perturbed_original_ll": -6.264561653137207, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add missing type of tests to breeze. (#18504)", "sampled": "Add missing type of tests to breeze. (#18504)From", "perturbed_sampled": ["Add missing type of tests to breeze. (#18504)From"], "perturbed_original": ["Add missing type of tests to breeze. (#18504)"], "original_ll": -6.598736763000488, "sampled_ll": -7.274570941925049, "all_perturbed_sampled_ll": [-7.274570941925049], "all_perturbed_original_ll": [-6.598736763000488], "perturbed_sampled_ll": -7.274570941925049, "perturbed_original_ll": -6.598736763000488, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Move backport packages to GA (#8391)", "sampled": "Move backport packages to GA (#8391)The", "perturbed_sampled": ["Move backport packages to GA (#8391)The"], "perturbed_original": ["Move backport packages to GA (#8391)"], "original_ll": -6.16018009185791, "sampled_ll": -6.8451128005981445, "all_perturbed_sampled_ll": [-6.8451128005981445], "all_perturbed_original_ll": [-6.16018009185791], "perturbed_sampled_ll": -6.8451128005981445, "perturbed_original_ll": -6.16018009185791, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "sampled": "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "perturbed_sampled": ["[AIRFLOW-6118] [AIP-21] [AIRFLOW-6118] API operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR fixes bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains"], "perturbed_original": ["[AIRFLOW-6118] Renaming Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation notices to the config files for the contrib modules * fixed tests * updated UPDATING.md"], "original_ll": -4.617997169494629, "sampled_ll": -4.130293369293213, "all_perturbed_sampled_ll": [-3.901190996170044], "all_perturbed_original_ll": [-4.776862621307373], "perturbed_sampled_ll": -3.901190996170044, "perturbed_original_ll": -4.776862621307373, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "sampled": "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "perturbed_sampled": ["[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update README, docs, test docs -"], "perturbed_original": ["- use get_autocommit to JdbcHook (#6232) - add tests - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>"], "original_ll": -4.231499671936035, "sampled_ll": -4.002114772796631, "all_perturbed_sampled_ll": [-4.6221842765808105], "all_perturbed_original_ll": [-4.304318428039551], "perturbed_sampled_ll": -4.6221842765808105, "perturbed_original_ll": -4.304318428039551, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "sampled": "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation interface. These have some new and interesting commands. The new commands are:", "perturbed_sampled": ["[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek has been added as a task automation interface. These have some new features like more commands. The new commands are:"], "perturbed_original": ["[AIRFLOW-XXXX] Add Gojek as an Airflow tool (#8070) Gojek uses Airflow as a pipeline scheduler and ETL tool on our data warehouses and machine learning pipelines."], "original_ll": -4.719353199005127, "sampled_ll": -4.406858444213867, "all_perturbed_sampled_ll": [-4.394442558288574], "all_perturbed_original_ll": [-4.7729644775390625], "perturbed_sampled_ll": -4.394442558288574, "perturbed_original_ll": -4.7729644775390625, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "sampled": "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "perturbed_sampled": ["Updating Amazon-AWS example DAGs to use XComArgs (#16868)It"], "perturbed_original": ["Updating Amazon-AWS example DAGs to use XComArgs (#16868)"], "original_ll": -6.1924638748168945, "sampled_ll": -6.631679534912109, "all_perturbed_sampled_ll": [-6.631679534912109], "all_perturbed_original_ll": [-6.1924638748168945], "perturbed_sampled_ll": -6.631679534912109, "perturbed_original_ll": -6.1924638748168945, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "sampled": "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "perturbed_sampled": ["Restore base lineage backend (#14146) Restore base lineage back end This is the base lineage backend which can be extended to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This"], "perturbed_original": ["Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send data to any custom backend. closes: Contact: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>"], "original_ll": -3.789785385131836, "sampled_ll": -3.7949421405792236, "all_perturbed_sampled_ll": [-3.889660596847534], "all_perturbed_original_ll": [-4.172298908233643], "perturbed_sampled_ll": -3.889660596847534, "perturbed_original_ll": -4.172298908233643, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Remove redundant code from breeze initialization (#9375)", "sampled": "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "perturbed_sampled": ["Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure"], "perturbed_original": ["Remove redundant code from breeze initialization (#9375)"], "original_ll": -6.790793418884277, "sampled_ll": -4.725438594818115, "all_perturbed_sampled_ll": [-4.725438594818115], "all_perturbed_original_ll": [-6.790793418884277], "perturbed_sampled_ll": -4.725438594818115, "perturbed_original_ll": -6.790793418884277, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the UI and are helpful for observability of Ariflow", "sampled": "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "perturbed_sampled": ["[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still in the API but are not part of the code.\n\n[AIRFLOW-4421]"], "perturbed_original": ["[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the UI and are a major feature for observability of Ariflow"], "original_ll": -5.324529647827148, "sampled_ll": -3.6911561489105225, "all_perturbed_sampled_ll": [-3.833681344985962], "all_perturbed_original_ll": [-5.288050174713135], "perturbed_sampled_ll": -3.833681344985962, "perturbed_original_ll": -5.288050174713135, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "sampled": "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "perturbed_sampled": ["Allow hvac pakage installation using 'hashicorp' extra (#7915)In"], "perturbed_original": ["Allow hvac pakage installation using 'hashicorp' extra (#7915)"], "original_ll": -5.687289237976074, "sampled_ll": -6.11829137802124, "all_perturbed_sampled_ll": [-6.11829137802124], "all_perturbed_original_ll": [-5.687289237976074], "perturbed_sampled_ll": -6.11829137802124, "perturbed_original_ll": -5.687289237976074, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic.", "sampled": "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be better at", "perturbed_sampled": ["Use context-assignment in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be better at"], "perturbed_original": ["Use generic information in UpdateMask component (#13146) The UpdateMask is used in other features like variables and dag. So the docs should be more generic."], "original_ll": -5.623473167419434, "sampled_ll": -5.845587730407715, "all_perturbed_sampled_ll": [-5.493014335632324], "all_perturbed_original_ll": [-5.333133220672607], "perturbed_sampled_ll": -5.493014335632324, "perturbed_original_ll": -5.333133220672607, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "sampled": "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "perturbed_sampled": ["Add Migration guide from the experimental API to the REST API . #5: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC"], "perturbed_original": ["Add Migration guide from the REST API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>"], "original_ll": -3.3390846252441406, "sampled_ll": -3.9371025562286377, "all_perturbed_sampled_ll": [-4.18824577331543], "all_perturbed_original_ll": [-3.1500418186187744], "perturbed_sampled_ll": -4.18824577331543, "perturbed_original_ll": -3.1500418186187744, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "sampled": "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "perturbed_sampled": ["Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors > has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME"], "perturbed_original": ["Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used from <unk>NodeSelectors<unk> to a `DeprecationWarning` and also simplifies that code path."], "original_ll": -4.508952617645264, "sampled_ll": -4.120049953460693, "all_perturbed_sampled_ll": [-4.3319993019104], "all_perturbed_original_ll": [-4.602807521820068], "perturbed_sampled_ll": -4.3319993019104, "perturbed_original_ll": -4.602807521820068, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.", "sampled": "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.\n\nAdded", "perturbed_sampled": ["Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect ly introduced the expression `extraSecrets`.\n\nAdded"], "perturbed_original": ["Chart docs: Fix . (#16305) Found a couple places where we have incorrect examples of `extraSecrets`."], "original_ll": -5.29566764831543, "sampled_ll": -5.006393909454346, "all_perturbed_sampled_ll": [-5.546966552734375], "all_perturbed_original_ll": [-5.5568928718566895], "perturbed_sampled_ll": -5.546966552734375, "perturbed_original_ll": -5.5568928718566895, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow passing backend_kwargs to AWS SSM client (#8802)", "sampled": "Allow passing backend_kwargs to AWS SSM client (#8802)With", "perturbed_sampled": ["Allow passing backend_kwargs to AWS SSM client (#8802)With"], "perturbed_original": ["Allow passing backend_kwargs to AWS SSM client (#8802)"], "original_ll": -6.160957336425781, "sampled_ll": -6.720598220825195, "all_perturbed_sampled_ll": [-6.720598220825195], "all_perturbed_original_ll": [-6.160957336425781], "perturbed_sampled_ll": -6.720598220825195, "perturbed_original_ll": -6.160957336425781, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix kinesis test (#18337)", "sampled": "Fix kinesis test (#18337)On", "perturbed_sampled": ["Fix kinesis test (#18337)On"], "perturbed_original": ["Fix kinesis test (#18337)"], "original_ll": -6.094775199890137, "sampled_ll": -7.206113338470459, "all_perturbed_sampled_ll": [-7.206113338470459], "all_perturbed_original_ll": [-6.094775199890137], "perturbed_sampled_ll": -7.206113338470459, "perturbed_original_ll": -6.094775199890137, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Clean up incorrect class names of Google system tests (#19956)", "sampled": "Clean up incorrect class names of Google system tests (#19956)A", "perturbed_sampled": ["Clean up incorrect class names of Google system tests (#19956)A"], "perturbed_original": ["Clean up incorrect class names of Google system tests (#19956)"], "original_ll": -6.384636402130127, "sampled_ll": -6.8947014808654785, "all_perturbed_sampled_ll": [-6.8947014808654785], "all_perturbed_original_ll": [-6.384636402130127], "perturbed_sampled_ll": -6.8947014808654785, "perturbed_original_ll": -6.384636402130127, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and above) to see all of the links.", "sampled": "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage.", "perturbed_sampled": ["Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage."], "perturbed_original": ["Allow viewers to see all docs links (#14197) Currently Viewers can only see the links in the docs menu. This PR allows Viewers (and above) to see all of the links."], "original_ll": -3.4734418392181396, "sampled_ll": -3.4790704250335693, "all_perturbed_sampled_ll": [-3.4790704250335693], "all_perturbed_original_ll": [-3.837973117828369], "perturbed_sampled_ll": -3.4790704250335693, "perturbed_original_ll": -3.837973117828369, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Improve Google PubSub hook publish method (#7831)", "sampled": "Improve Google PubSub hook publish method (#7831)By", "perturbed_sampled": ["Improve Google PubSub hook publish method (#7831)By"], "perturbed_original": ["Improve Google PubSub hook publish method (#7831)"], "original_ll": -7.006582736968994, "sampled_ll": -7.602819919586182, "all_perturbed_sampled_ll": [-7.602819919586182], "all_perturbed_original_ll": [-7.006582736968994], "perturbed_sampled_ll": -7.602819919586182, "perturbed_original_ll": -7.006582736968994, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "sampled": "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "perturbed_sampled": ["Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where"], "perturbed_original": ["Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to version 2.1.2 when it has been released."], "original_ll": -3.213484764099121, "sampled_ll": -2.743013381958008, "all_perturbed_sampled_ll": [-2.743013381958008], "all_perturbed_original_ll": [-3.295393705368042], "perturbed_sampled_ll": -2.743013381958008, "perturbed_original_ll": -3.295393705368042, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "sampled": "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - do not use", "perturbed_sampled": ["Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in bash.exe (#13484) - fix the use"], "perturbed_original": ["Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.objects. BashOperator` to `from airflow.operators.bash import chain<unk> - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`"], "original_ll": -3.35652494430542, "sampled_ll": -3.733330488204956, "all_perturbed_sampled_ll": [-3.707841157913208], "all_perturbed_original_ll": [-4.284509658813477], "perturbed_sampled_ll": -3.707841157913208, "perturbed_original_ll": -4.284509658813477, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installation for quite some time at least verbally but also in the Helm Chart documentation. However we missed such recommendation in the general Postgres area of 'Setting Up the database` doc. This PR adds a note that we can refer to when explaining problems with connections and stability to the users who use Postgres without PGBouncer proxy (which is known to help in such cases)", "sampled": "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installations, for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in OpenWrt-1.9.0 (#18465) When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not", "perturbed_sampled": ["Add PGBouncer recommendation in \"setup-database' doc. Prior to this update, we were recommending using PGBouncer for PGIDB database installations, for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses a different database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in separate files. When specifying OpenSSH keys in PGBouncer , we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not"], "perturbed_original": ["Add PGBouncer recommendation in Postgres documentation (#18399) We were recommending using PGBouncer for Postgres installation for quite some time at least verbally but also in the product documentation. However we missed showing PGBouncer recommendation in the general Postgres area of 'Setting Up the database` doc. This issue is a note that we can refer to when explaining problems with connections and stability to the users who use Postgres without PGBouncer proxy (which is known to help in such cases)"], "original_ll": -4.387980937957764, "sampled_ll": -2.8348398208618164, "all_perturbed_sampled_ll": [-3.2039284706115723], "all_perturbed_original_ll": [-3.9779231548309326], "perturbed_sampled_ll": -3.2039284706115723, "perturbed_original_ll": -3.9779231548309326, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add template fields to neo4j operator (#20043)", "sampled": "Add template fields to neo4j operator (#20043)In", "perturbed_sampled": ["Add template fields to neo4j operator (#20043)In"], "perturbed_original": ["Add template fields to neo4j operator (#20043)"], "original_ll": -6.545008659362793, "sampled_ll": -7.165198802947998, "all_perturbed_sampled_ll": [-7.165198802947998], "all_perturbed_original_ll": [-6.545008659362793], "perturbed_sampled_ll": -7.165198802947998, "perturbed_original_ll": -6.545008659362793, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Fix typo in example (#13321) False should not be passed as a string", "sampled": "Fix typo in example (#13321) False should not be passed as a stringDescription", "perturbed_sampled": ["Fix typo in example (#13321) False should not be passed as a stringDescription"], "perturbed_original": ["Fix typo in example (#13321) False should not be passed as a string"], "original_ll": -5.053643226623535, "sampled_ll": -5.650160312652588, "all_perturbed_sampled_ll": [-5.650160312652588], "all_perturbed_original_ll": [-5.053643226623535], "perturbed_sampled_ll": -5.650160312652588, "perturbed_original_ll": -5.053643226623535, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "sampled": "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "perturbed_sampled": ["Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded"], "perturbed_original": ["Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E"], "original_ll": -4.646146774291992, "sampled_ll": -4.444993019104004, "all_perturbed_sampled_ll": [-4.444993019104004], "all_perturbed_original_ll": [-4.646146774291992], "perturbed_sampled_ll": -4.444993019104004, "perturbed_original_ll": -4.646146774291992, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "sampled": "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "perturbed_sampled": ["[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The"], "perturbed_original": ["[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)"], "original_ll": -6.095162391662598, "sampled_ll": -6.4268951416015625, "all_perturbed_sampled_ll": [-6.4268951416015625], "all_perturbed_original_ll": [-6.095162391662598], "perturbed_sampled_ll": -6.4268951416015625, "perturbed_original_ll": -6.095162391662598, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard names for new users endpoints. * Document user access. * Remove unused tests. * Make roles tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "sampled": "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "perturbed_sampled": ["Standardize default fab perms (#14946) and roll back changes. * Add custom view class tests. * fix missing clear permission. * Add custom view classes tests. * Add a custom views test. * -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test."], "perturbed_original": ["Standardize default permissions (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add the rest of the mappings. * Add the rest of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard names for users endpoints. * Document user access. * Remove unused tests. * Make tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Update role definitions and user access. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge."], "original_ll": -3.7460806369781494, "sampled_ll": -2.2837164402008057, "all_perturbed_sampled_ll": [-2.2318766117095947], "all_perturbed_original_ll": [-3.3124592304229736], "perturbed_sampled_ll": -2.2318766117095947, "perturbed_original_ll": -3.3124592304229736, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}, {"original": "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged", "sampled": "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging", "perturbed_sampled": ["[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values if user has multiple requests Fix action_logging"], "perturbed_original": ["Log action.form and action_logging so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged"], "original_ll": -5.176823616027832, "sampled_ll": -4.477470874786377, "all_perturbed_sampled_ll": [-5.141716957092285], "all_perturbed_original_ll": [-4.792178153991699], "perturbed_sampled_ll": -5.141716957092285, "perturbed_original_ll": -4.792178153991699, "perturbed_sampled_ll_std": 1, "perturbed_original_ll_std": 1}], "metrics": {"roc_auc": 0.5372739999999999, "fpr": [0.0, 0.002, 0.002, 0.004, 0.004, 0.006, 0.006, 0.012, 0.012, 0.014, 0.014, 0.016, 0.016, 0.018, 0.018, 0.022, 0.022, 0.024, 0.024, 0.028, 0.028, 0.032, 0.032, 0.038, 0.038, 0.042, 0.042, 0.044, 0.044, 0.048, 0.048, 0.05, 0.05, 0.054, 0.054, 0.058, 0.058, 0.062, 0.062, 0.066, 0.066, 0.07, 0.07, 0.074, 0.074, 0.08, 0.08, 0.088, 0.088, 0.092, 0.092, 0.096, 0.096, 0.102, 0.102, 0.108, 0.108, 0.112, 0.112, 0.116, 0.116, 0.12, 0.12, 0.126, 0.126, 0.128, 0.128, 0.13, 0.13, 0.134, 0.134, 0.142, 0.142, 0.146, 0.146, 0.148, 0.148, 0.152, 0.152, 0.154, 0.154, 0.156, 0.156, 0.158, 0.158, 0.162, 0.162, 0.164, 0.164, 0.166, 0.166, 0.17, 0.17, 0.174, 0.174, 0.176, 0.176, 0.18, 0.18, 0.182, 0.182, 0.186, 0.186, 0.188, 0.188, 0.19, 0.19, 0.192, 0.192, 0.202, 0.202, 0.206, 0.206, 0.208, 0.208, 0.214, 0.214, 0.216, 0.216, 0.22, 0.22, 0.226, 0.226, 0.23, 0.23, 0.234, 0.234, 0.236, 0.236, 0.24, 0.24, 0.242, 0.242, 0.244, 0.244, 0.248, 0.248, 0.25, 0.824, 0.828, 0.828, 0.83, 0.83, 0.834, 0.834, 0.838, 0.838, 0.84, 0.84, 0.844, 0.844, 0.848, 0.848, 0.85, 0.85, 0.856, 0.856, 0.864, 0.864, 0.868, 0.868, 0.872, 0.872, 0.874, 0.874, 0.876, 0.876, 0.878, 0.878, 0.884, 0.884, 0.89, 0.89, 0.892, 0.892, 0.894, 0.894, 0.896, 0.896, 0.9, 0.9, 0.902, 0.902, 0.904, 0.904, 0.906, 0.906, 0.908, 0.908, 0.91, 0.91, 0.912, 0.912, 0.918, 0.918, 0.924, 0.924, 0.938, 0.938, 0.946, 0.946, 0.964, 0.964, 0.976, 0.976, 0.978, 0.978, 0.99, 0.99, 0.994, 0.994, 0.998, 0.998, 1.0, 1.0], "tpr": [0.0, 0.0, 0.006, 0.006, 0.008, 0.008, 0.01, 0.01, 0.014, 0.014, 0.02, 0.02, 0.038, 0.038, 0.042, 0.042, 0.044, 0.044, 0.048, 0.048, 0.05, 0.05, 0.052, 0.052, 0.056, 0.056, 0.058, 0.058, 0.062, 0.062, 0.064, 0.064, 0.068, 0.068, 0.072, 0.072, 0.074, 0.074, 0.076, 0.076, 0.078, 0.078, 0.082, 0.082, 0.09, 0.09, 0.098, 0.098, 0.102, 0.102, 0.108, 0.108, 0.124, 0.124, 0.128, 0.128, 0.13, 0.13, 0.134, 0.134, 0.14, 0.14, 0.148, 0.148, 0.152, 0.152, 0.162, 0.162, 0.164, 0.164, 0.166, 0.166, 0.168, 0.168, 0.174, 0.174, 0.18, 0.18, 0.182, 0.182, 0.186, 0.186, 0.19, 0.19, 0.192, 0.192, 0.204, 0.204, 0.206, 0.206, 0.21, 0.21, 0.212, 0.212, 0.216, 0.216, 0.218, 0.218, 0.222, 0.222, 0.224, 0.224, 0.226, 0.226, 0.232, 0.232, 0.238, 0.238, 0.244, 0.244, 0.246, 0.246, 0.248, 0.248, 0.25, 0.25, 0.256, 0.256, 0.26, 0.26, 0.262, 0.262, 0.266, 0.266, 0.27, 0.27, 0.274, 0.274, 0.278, 0.278, 0.28, 0.28, 0.286, 0.286, 0.288, 0.288, 0.29, 0.29, 0.872, 0.872, 0.874, 0.874, 0.88, 0.88, 0.882, 0.882, 0.884, 0.884, 0.886, 0.886, 0.888, 0.888, 0.89, 0.89, 0.892, 0.892, 0.898, 0.898, 0.9, 0.9, 0.902, 0.902, 0.906, 0.906, 0.908, 0.908, 0.912, 0.912, 0.918, 0.918, 0.922, 0.922, 0.924, 0.924, 0.926, 0.926, 0.932, 0.932, 0.934, 0.934, 0.942, 0.942, 0.946, 0.946, 0.95, 0.95, 0.954, 0.954, 0.956, 0.956, 0.962, 0.962, 0.964, 0.964, 0.968, 0.968, 0.97, 0.97, 0.972, 0.972, 0.978, 0.978, 0.98, 0.98, 0.982, 0.982, 0.984, 0.984, 0.99, 0.99, 0.994, 0.994, 0.998, 0.998, 1.0]}, "pr_metrics": {"pr_auc": 0.5328733277988558, "precision": [0.5, 0.4994994994994995, 0.5, 0.49949849548645936, 0.49899598393574296, 0.4994974874371859, 0.5, 0.499496475327291, 0.49899193548387094, 0.49949545913218973, 0.5, 0.4994944388270981, 0.49898785425101216, 0.49848024316109424, 0.49898580121703856, 0.49949238578680205, 0.5, 0.5005086469989827, 0.5010183299389002, 0.5015290519877675, 0.5010204081632653, 0.501532175689479, 0.5010224948875256, 0.5015353121801432, 0.5020491803278688, 0.5025641025641026, 0.5030800821355236, 0.5035971223021583, 0.5041152263374485, 0.5036045314109165, 0.5041237113402062, 0.5046439628482973, 0.5051652892561983, 0.5056876938986556, 0.5062111801242236, 0.5067357512953368, 0.5072614107883817, 0.5077881619937694, 0.5083160083160083, 0.5078043704474505, 0.5072916666666667, 0.5067778936392076, 0.5073068893528184, 0.5078369905956113, 0.5083682008368201, 0.5089005235602094, 0.5083857442348009, 0.5089192025183631, 0.509453781512605, 0.5099894847528917, 0.5105263157894737, 0.5110642781875658, 0.5116033755274262, 0.5121436114044351, 0.5116279069767442, 0.5121693121693122, 0.5127118644067796, 0.513255567338282, 0.5127388535031847, 0.512221041445271, 0.5127659574468085, 0.5133120340788072, 0.5138592750533049, 0.5133404482390609, 0.5138888888888888, 0.5133689839572193, 0.512847965738758, 0.5123258306538049, 0.5128755364806867, 0.5123523093447906, 0.5129032258064516, 0.5123789020452099, 0.5118534482758621, 0.5124056094929881, 0.5118790496760259, 0.5113513513513513, 0.5119047619047619, 0.5113759479956663, 0.5108459869848156, 0.511400651465798, 0.5108695652173914, 0.5103373231773667, 0.5098039215686274, 0.5092693565976009, 0.509825327510917, 0.5103825136612021, 0.5098468271334792, 0.5104052573932092, 0.5098684210526315, 0.5093304061470911, 0.5087912087912088, 0.5093509350935094, 0.5088105726872246, 0.5093715545755237, 0.5088300220750552, 0.5093922651933702, 0.5099557522123894, 0.5105204872646734, 0.5099778270509978, 0.5094339622641509, 0.51, 0.5105672969966629, 0.5111358574610245, 0.5105908584169454, 0.5100446428571429, 0.5094972067039106, 0.5100671140939598, 0.509518477043673, 0.5089686098654709, 0.5095398428731762, 0.5089887640449439, 0.5095613048368954, 0.509009009009009, 0.5084554678692221, 0.5090293453724605, 0.5096045197740113, 0.5090497737556561, 0.5096262740656852, 0.5102040816326531, 0.5096481271282634, 0.5102272727272728, 0.5108077360637088, 0.5113895216400911, 0.5119726339794755, 0.5114155251141552, 0.5108571428571429, 0.5102974828375286, 0.5108820160366552, 0.5114678899082569, 0.5120551090700345, 0.5114942528735632, 0.5120828538550057, 0.511520737327189, 0.5121107266435986, 0.5127020785219399, 0.5121387283236994, 0.5127314814814815, 0.5133256083429896, 0.5127610208816705, 0.5133565621370499, 0.5127906976744186, 0.5133876600698487, 0.513986013986014, 0.5134189031505251, 0.514018691588785, 0.5146198830409356, 0.5140515222482436, 0.5134818288393904, 0.5129107981220657, 0.5135135135135135, 0.5129411764705882, 0.5135453474676089, 0.5141509433962265, 0.5370370370370371, 0.5390334572490706, 0.5373134328358209, 0.5393258426966292, 0.5413533834586466, 0.539622641509434, 0.5416666666666666, 0.5399239543726235, 0.5381679389312977, 0.5363984674329502, 0.5384615384615384, 0.5366795366795367, 0.5387596899224806, 0.5408560311284046, 0.5390625, 0.5372549019607843, 0.5393700787401575, 0.5375494071146245, 0.5357142857142857, 0.5378486055776892, 0.54, 0.5381526104417671, 0.5362903225806451, 0.5384615384615384, 0.540650406504065, 0.5387755102040817, 0.5368852459016393, 0.5390946502057613, 0.5413223140495868, 0.5435684647302904, 0.5416666666666666, 0.5439330543933054, 0.5462184873949579, 0.5443037974683544, 0.5423728813559322, 0.5446808510638298, 0.5427350427350427, 0.5407725321888412, 0.5387931034482759, 0.5411255411255411, 0.5434782608695652, 0.5458515283842795, 0.543859649122807, 0.5462555066079295, 0.5442477876106194, 0.5466666666666666, 0.5491071428571429, 0.547085201793722, 0.5495495495495496, 0.5520361990950227, 0.5545454545454546, 0.5570776255707762, 0.5596330275229358, 0.5576036866359447, 0.5555555555555556, 0.5534883720930233, 0.5560747663551402, 0.5539906103286385, 0.5518867924528302, 0.5497630331753555, 0.5523809523809524, 0.5502392344497608, 0.5480769230769231, 0.5458937198067633, 0.5485436893203883, 0.5463414634146342, 0.5490196078431373, 0.5517241379310345, 0.5495049504950495, 0.5522388059701493, 0.55, 0.5477386934673367, 0.5505050505050505, 0.5532994923857868, 0.5510204081632653, 0.5538461538461539, 0.5515463917525774, 0.5492227979274611, 0.5520833333333334, 0.5549738219895288, 0.5526315789473685, 0.5555555555555556, 0.5585106382978723, 0.5561497326203209, 0.553763440860215, 0.5567567567567567, 0.5543478260869565, 0.5573770491803278, 0.554945054945055, 0.5524861878453039, 0.55, 0.547486033519553, 0.5449438202247191, 0.5423728813559322, 0.5454545454545454, 0.5485714285714286, 0.5459770114942529, 0.5491329479768786, 0.5465116279069767, 0.543859649122807, 0.5470588235294118, 0.5443786982248521, 0.5416666666666666, 0.5449101796407185, 0.5421686746987951, 0.5454545454545454, 0.5487804878048781, 0.5460122699386503, 0.5432098765432098, 0.5403726708074534, 0.54375, 0.5408805031446541, 0.5379746835443038, 0.535031847133758, 0.5384615384615384, 0.5419354838709678, 0.538961038961039, 0.5424836601307189, 0.5460526315789473, 0.5496688741721855, 0.5533333333333333, 0.5503355704697986, 0.5540540540540541, 0.5578231292517006, 0.5547945205479452, 0.5586206896551724, 0.5555555555555556, 0.5524475524475524, 0.5492957746478874, 0.5460992907801419, 0.5428571428571428, 0.5467625899280576, 0.5434782608695652, 0.5401459854014599, 0.5441176470588235, 0.5481481481481482, 0.5522388059701493, 0.5488721804511278, 0.5454545454545454, 0.5419847328244275, 0.5384615384615384, 0.5426356589147286, 0.546875, 0.5433070866141733, 0.5396825396825397, 0.536, 0.5403225806451613, 0.5447154471544715, 0.5409836065573771, 0.5371900826446281, 0.5416666666666666, 0.5462184873949579, 0.5423728813559322, 0.5470085470085471, 0.5517241379310345, 0.5565217391304348, 0.5526315789473685, 0.5486725663716814, 0.5535714285714286, 0.5585585585585585, 0.5636363636363636, 0.5596330275229358, 0.5555555555555556, 0.5514018691588785, 0.5471698113207547, 0.5428571428571428, 0.5384615384615384, 0.5339805825242718, 0.5294117647058824, 0.5346534653465347, 0.54, 0.5353535353535354, 0.5306122448979592, 0.5257731958762887, 0.53125, 0.5368421052631579, 0.5319148936170213, 0.5268817204301075, 0.532608695652174, 0.5384615384615384, 0.5444444444444444, 0.550561797752809, 0.5454545454545454, 0.5402298850574713, 0.5348837209302325, 0.5294117647058824, 0.5357142857142857, 0.5421686746987951, 0.5487804878048781, 0.5432098765432098, 0.5375, 0.5316455696202531, 0.5256410256410257, 0.5324675324675324, 0.5394736842105263, 0.5333333333333333, 0.527027027027027, 0.5342465753424658, 0.5416666666666666, 0.5352112676056338, 0.5428571428571428, 0.5507246376811594, 0.5441176470588235, 0.5522388059701493, 0.5606060606060606, 0.5538461538461539, 0.5625, 0.5714285714285714, 0.5645161290322581, 0.5573770491803278, 0.5666666666666667, 0.576271186440678, 0.5689655172413793, 0.5614035087719298, 0.5714285714285714, 0.5636363636363636, 0.5740740740740741, 0.5849056603773585, 0.5769230769230769, 0.5686274509803921, 0.58, 0.5714285714285714, 0.5833333333333334, 0.5957446808510638, 0.5869565217391305, 0.5777777777777777, 0.5909090909090909, 0.6046511627906976, 0.6190476190476191, 0.6097560975609756, 0.625, 0.6410256410256411, 0.631578947368421, 0.6486486486486487, 0.6666666666666666, 0.6571428571428571, 0.6470588235294118, 0.6666666666666666, 0.65625, 0.6774193548387096, 0.7, 0.6896551724137931, 0.6785714285714286, 0.7037037037037037, 0.6923076923076923, 0.68, 0.6666666666666666, 0.6521739130434783, 0.6363636363636364, 0.6190476190476191, 0.6, 0.5789473684210527, 0.5555555555555556, 0.5882352941176471, 0.5625, 0.5333333333333333, 0.5, 0.5384615384615384, 0.5, 0.45454545454545453, 0.5, 0.5555555555555556, 0.625, 0.5714285714285714, 0.6666666666666666, 0.6, 0.75, 0.6666666666666666, 0.5, 0.0, 1.0], "recall": [1.0, 0.998, 0.998, 0.996, 0.994, 0.994, 0.994, 0.992, 0.99, 0.99, 0.99, 0.988, 0.986, 0.984, 0.984, 0.984, 0.984, 0.984, 0.984, 0.984, 0.982, 0.982, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.978, 0.978, 0.978, 0.978, 0.978, 0.978, 0.978, 0.978, 0.978, 0.978, 0.976, 0.974, 0.972, 0.972, 0.972, 0.972, 0.972, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.968, 0.968, 0.968, 0.968, 0.966, 0.964, 0.964, 0.964, 0.964, 0.962, 0.962, 0.96, 0.958, 0.956, 0.956, 0.954, 0.954, 0.952, 0.95, 0.95, 0.948, 0.946, 0.946, 0.944, 0.942, 0.942, 0.94, 0.938, 0.936, 0.934, 0.934, 0.934, 0.932, 0.932, 0.93, 0.928, 0.926, 0.926, 0.924, 0.924, 0.922, 0.922, 0.922, 0.922, 0.92, 0.918, 0.918, 0.918, 0.918, 0.916, 0.914, 0.912, 0.912, 0.91, 0.908, 0.908, 0.906, 0.906, 0.904, 0.902, 0.902, 0.902, 0.9, 0.9, 0.9, 0.898, 0.898, 0.898, 0.898, 0.898, 0.896, 0.894, 0.892, 0.892, 0.892, 0.892, 0.89, 0.89, 0.888, 0.888, 0.888, 0.886, 0.886, 0.886, 0.884, 0.884, 0.882, 0.882, 0.882, 0.88, 0.88, 0.88, 0.878, 0.876, 0.874, 0.874, 0.872, 0.872, 0.872, 0.29, 0.29, 0.288, 0.288, 0.288, 0.286, 0.286, 0.284, 0.282, 0.28, 0.28, 0.278, 0.278, 0.278, 0.276, 0.274, 0.274, 0.272, 0.27, 0.27, 0.27, 0.268, 0.266, 0.266, 0.266, 0.264, 0.262, 0.262, 0.262, 0.262, 0.26, 0.26, 0.26, 0.258, 0.256, 0.256, 0.254, 0.252, 0.25, 0.25, 0.25, 0.25, 0.248, 0.248, 0.246, 0.246, 0.246, 0.244, 0.244, 0.244, 0.244, 0.244, 0.244, 0.242, 0.24, 0.238, 0.238, 0.236, 0.234, 0.232, 0.232, 0.23, 0.228, 0.226, 0.226, 0.224, 0.224, 0.224, 0.222, 0.222, 0.22, 0.218, 0.218, 0.218, 0.216, 0.216, 0.214, 0.212, 0.212, 0.212, 0.21, 0.21, 0.21, 0.208, 0.206, 0.206, 0.204, 0.204, 0.202, 0.2, 0.198, 0.196, 0.194, 0.192, 0.192, 0.192, 0.19, 0.19, 0.188, 0.186, 0.186, 0.184, 0.182, 0.182, 0.18, 0.18, 0.18, 0.178, 0.176, 0.174, 0.174, 0.172, 0.17, 0.168, 0.168, 0.168, 0.166, 0.166, 0.166, 0.166, 0.166, 0.164, 0.164, 0.164, 0.162, 0.162, 0.16, 0.158, 0.156, 0.154, 0.152, 0.152, 0.15, 0.148, 0.148, 0.148, 0.148, 0.146, 0.144, 0.142, 0.14, 0.14, 0.14, 0.138, 0.136, 0.134, 0.134, 0.134, 0.132, 0.13, 0.13, 0.13, 0.128, 0.128, 0.128, 0.128, 0.126, 0.124, 0.124, 0.124, 0.124, 0.122, 0.12, 0.118, 0.116, 0.114, 0.112, 0.11, 0.108, 0.108, 0.108, 0.106, 0.104, 0.102, 0.102, 0.102, 0.1, 0.098, 0.098, 0.098, 0.098, 0.098, 0.096, 0.094, 0.092, 0.09, 0.09, 0.09, 0.09, 0.088, 0.086, 0.084, 0.082, 0.082, 0.082, 0.08, 0.078, 0.078, 0.078, 0.076, 0.076, 0.076, 0.074, 0.074, 0.074, 0.072, 0.072, 0.072, 0.07, 0.068, 0.068, 0.068, 0.066, 0.064, 0.064, 0.062, 0.062, 0.062, 0.06, 0.058, 0.058, 0.056, 0.056, 0.056, 0.054, 0.052, 0.052, 0.052, 0.052, 0.05, 0.05, 0.05, 0.048, 0.048, 0.048, 0.046, 0.044, 0.044, 0.042, 0.042, 0.042, 0.04, 0.038, 0.038, 0.036, 0.034, 0.032, 0.03, 0.028, 0.026, 0.024, 0.022, 0.02, 0.02, 0.018, 0.016, 0.014, 0.014, 0.012, 0.01, 0.01, 0.01, 0.01, 0.008, 0.008, 0.006, 0.006, 0.004, 0.002, 0.0, 0.0]}, "loss": 0.46712667220114423}