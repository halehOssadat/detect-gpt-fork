{"original": ["Correct command for starting Celery Flower (#9483)", "Fixing mypy issues inside tests model (#20026)", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the pain of upgrading to Airflow 2.0 for users of the KubernetesPodOperator. Users will be allowed to continue using the airflow.kubernetes custom classes * spellcheck * spelling * clean up unecessary files in 1.10 * clean up unecessary files in 1.10 * clean up unecessary files in 1.10", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)", "Fix spelling (#11404)", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)", "[AIRFLOW-5614] Enable Fernet by default (#6282)", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was rised during git add.", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining it for now. This is captured in #18777", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the company", "Mark trigger-controller-dag test as xfail (#8015)", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes tests, we found out that I need to fix handling of parameters - we change Kubernetes version used via Kind and the old versions are no longer valid, however it was not properly removed/saved. We use the opportunity to add automated tests for that feature. (cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3. We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only used for development, not production.", "Enable Black on Connexion API folders (#10545)", "Fix documentation for provider's release (#14654)", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodules to pull in the third party actions we want to use - with recent(ish) changes in review for submodules on GitHub we still get the same \"review/audit\" visibility for changes, but this way we don't have to either \"pollute\" our repo with the actions code, nor do we have to maintain a fork of the third party action.", "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, plus while `pandas` now supports many pre-compiled packages it still can take forever where it needs to be compiled. So for first-time users this can be a turn off. If pandas is already installed this will work fine, but if not users have an option to run `pip install apache-airflow[pandas]` closes #12500", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * add test to all executors * fix test", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as in a custom operator link) then the value would _always_ be subject to the `orm_deserialize_value` which would likely give the wrong result on custom XCom backends. This wasn't a problem for anyone using `ti.xcom_pull` as it handled this directly.", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)", "[AIRFLOW-7041] make bowler dependency local (#7691)", "Fix Python Docstring parameters (#12513)", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking When running webserver as daemon it can happend that it will not start due to existing pidfile. This PR improves whole webserver command and adds pidfile checking. * fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking", "UX Enhancement: Add button to clear search query from DAG search (#11583)", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)", "[AIRFLOW-6635] Speed up static checks (#7256)", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)", "Move out sendgrid emailer from airflow.contrib (#9355)", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/11704", "Pin pandas-gbq to <0.15.0 (#15114)", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek Potiuk was accepted and merged by Lucas-C in his pre-commit hooks implementation (released today ver. 1.1.7) so we can switch back to it.", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.", "Added Viscovery to the list of companies using Apache Airflow (#18683)", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`, or `timezone.utcnow() - start_dttm` will work fine in 99% of cases, but it has one fatal flaw: They both operate on system time, and that can go backwards. While this might be surprising, it can happen -- usually due to clocks being adjusted. And while it is might seem rare, for long running processes it is more common than we might expect. Most of these durations are harmless to get wrong (just being logs) it is better to be safe than sorry. Also the `utcnow()` style I have replaced will be much", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)", "Convert properties with query to real methods (#7900) * Convert properties with query to real methods", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal \"links\" in DAGs table * Reverse the order of links to reduce mouse distance for most popular", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to glue job run * add run_kwargs to hook and operator tests", "Make Smart Sensors DB Migration idempotent (#13892)", "[AIRFLOW-5702] Fix common docstring issues (#6372)", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Support kwargs in get_pandas_df * BigQueryHook: Support kwargs in get_pandas_df * ExasolHook: Support kwargs in get_pandas_df * PrestoHook: Support kwargs in get_pandas_df * HiveServer2Hook: Support kwargs in get_pandas_df", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas does not consider if an SlaMiss already exists in DB while inserting slas. If an SLA for a task is missed and recorded, on checking SLA again, this task comes up again if there's no recent run of the task and we try to insert the record into the SlaMiss table again, this results in Integrity error. This PR fixes that by avoiding insert if the record already exists Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)", "Improvements for transfer operators references (#12482)", "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to confusion when the date to be displayed was not in the same daylight-savings state as \"now\".", "Add colors to airflow config command (#8404)", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicate that Daniel Imberman is Bloomberg's Airflow PoC", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindness Approximately 4.5% of people experience some form of colour vision deficiency", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values are passed through `tpl`, one would expect `config.webserver.base_url` to also support templating.", "Move setting of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@google.com>", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/log/s3_task_handler.py * Fix expectation for submit_job * Fix documentation Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Add verify_ssl config for kubernetes (#13516)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` instead of ``` super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs) ```", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc * [AIRFLOW-4092] fix documentation errors * [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tidy up the chaining of tasks in some cases - Remove unnecessary specification of default conn ids", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter which allows users to get output in form of table, json or yaml.", "Fix task search function in Graph view (#15901)", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in to get_conn - add salesforce to devel_all packages - add note to UPDATING.md Co-Authored-By: mik-laj <mik-laj@users.noreply.github.com>", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)", "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)", "Use DAG_ACTIONS constant. (#16232)", "Clean up airflow.contrib in Kubernetes docs (#9551)", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is easy for us to maintain compatibility, so we should", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)", "detect incompatible docker server version in breeze (#9042)", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have shebangs which make them available for python tools. Also /opt/airflow is now mounted from the host Airflow sources which makes it possible for the tools to copy files directly to/from the sources of Airflow. It also contains one small change for Linux users - the files created by docker gcloud are created with root user so in order to fix that the directories mounted from the host are fixed when you exit the tool - their ownership is changed to be owned by the host user", "fix bug of SparkSql Operator log going to infinite loop. (#19449)", "Remove unused 'context' variable in task_instance.py (#14049)", "UPDATING.md for changes included in 2.1.1 (#16615)", "Google Memcached hooks - improve protobuf messages handling (#11743)", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g. The previous TaskInstance has not yet run because its dependencies are not yet met The previous TaskInstance has not yet run because scheduler is busy, the previous TaskInstance was marked success without running. This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.", "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` -> `breeze start-airflow`", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI modules via custom .pypirc file. This might allow to install dependencies from in-house, vetted registry of PyPI", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema)", "Update to latest pygrep pre-commit hook (#8489)", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`: ``` from airflow.operators.dummy_operator import DummyOperator ``` with ``` from airflow.operators.dummy import DummyOperator ```", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)", "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db reset. This commit fixes this behavior. closes: #13214", "JIRA and Github issues explanation (#8539)", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md for 1.10.15", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)", "Fix backwards compatibility with k8s executor_config resources (#11796)", "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be earlier in the Dockerfile", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>", "Typo fix in TESTING.rst (#19216)", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)", "Simplified GCSTaskHandler configuration (#10365)", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This workflow run is currently failing with: ``` Run ./.github/actions/checks-action with: token: *** name: Selective build check status: in_progress sha: 2cf8c7f268c1db73d840f029aa5180941519c492 details_url: https://github.com/apache/airflow/actions/runs/960898933 output: {\"summary\": \"Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) \"} Error: Resource not accessible by integration ``` so I _think_ this will help * Update .github/workflows/label_when_reviewed_workflow_run.yml Co-authored-by: Jarek Potiuk <jarek@potiuk.com>", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)", "[AIRFLOW-XXX] Highlight code blocks (#6243)", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errors", "Small fixes in Google Cloud Secrets Manager guide (#12105)", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * fixup! fixup! Create a documentation package for Docker image * fixup! fixup! fixup! Create a documentation package for Docker image * Apply suggestions from code review Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Bump Boto3 (#7851)", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.rst", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)", "Fix mypy for exasol and facebook hooks (#20291)", "Standardize AWS Lambda naming (#20365)", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]", "Fix elasticsearch breaking the build (#7800)", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly. This change fixes it by matching the parent process ID (the `sudo` process) of the task instance to the current process ID of the task_runner process when we use impersonation Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Rename the main branch of the Airflow repo to be `main` (#16149)", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of time to create, and each of these test modules creates the Flask app with the same set up \"initializers\". To make this work we had to switch away from `unittest.TestCase`, as pytest fixtures won't work with TestCase subclasses. The `configured_app` fixture is defined at the module level, otherwise each _subclass_ would have it's \"own\" module scope fixture. This takes the test time for api_connexion/endpoints down to sub-1 minute for me.", "Update external docs URL for Segment (#13645)", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy to worker deployments, in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time. `maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the", "Add fudament for API based on connexion (#8149)", "Cleanup KubernetsPodOpertor tests (#15475)", "Add example DAG and system test for MySQLToGCSOperator (#10990)", "Fix PyPI spelling (#13864)", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages.py - Replaced sys.argv manipulation with argparse. - Replaced positional argument for PACKAGE with optional argument. Issue : 13069 To be reviewed by : Kamil, Jarek. * Modified help text for prepare_provider_packages as suggested by Kamil. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com> * Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup. Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>", "Make models/taskinstance.py pylint compatible (#10499)", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-6326] Sort cli commands and arg (#6881)", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_MINOR_VERSION but there is a part of the build (preparing airflow package) that uses python installed on host. This is fine for Master/2.0 to use same version as the image but it should be unified (and in 1.10 when trying to build 2.7 image it would fail).", "Check python version before starting triggerer (#18926)", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin>", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was not.", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086)", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well.", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels on the worker pods that execute that task by specifying an extra field in executor_config like so `executor_config={\"KubernetesExecutor\": {\"labels\": {\"foo\":\"bar\"}}}`", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental API to return the paused state of a DAG.", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - Fix functionality last_scheduler_run was missed in the process of migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db. This issue will fail DAG.deactivate_stale_dags() method, and blocks users from checking the last schedule time of each DAG in DB - Change name last_scheduler_run to last_parsed_time, to better reflect what it does now. Migration script is added, and codebase is updated - To ensure the migration scripts can work, we have to limit the columns needed in create_dag_specific_permissions(), so migration 2c6edca13270 can work with the renamed column. Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `dockery system prune`. The --rm flag is added.", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools like poetry (it is properly resolved by pip). The result is that old version of python3-openid is installed when poetry is used and errors when initdb is run. While we do not use poetry as an official installation mechanism this happens frequently enought and it is easy enough to fix that we can add this dependency to make it easier for poetry users. Related to #13711 #13558 #13149 * Update setup.cfg", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors - Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178)", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in CI. The image was significantly smaller, but then for local development and testing you needed both full CI and SLIM-CI image. This made the scripts and docker image needlessly complex - especially in the wake of coming Production image it turned to be premature optimisation really. While it sped-up (slightly - by 10-20 seconds) some static check jobs in Travis, it increased time needed by developers to have a working environment and to keep it updated every time it was needed (by minutes) Also having two separately managed images made it rather complex to join some of the Travis CI jobs (there is a follow-up change with getting rid of Checklicence image). With this change both static checks and tests are executed using single image. That also opens doors for further simplification of the scripts and easier implementation of production image.", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)", "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/13163 - I attempted to only run Callback requests when they are defined on DAG. But I just found out that while we were storing the task-level callbacks as string in Serialized JSON, we were not storing DAG level callbacks and hence it default to None when the Serialized DAG was deserialized which meant that the DAG callbacks were not run. This PR fixes it, we don't need to store DAG level callbacks as string, as we don't display them in the Webserver and the actual contents are not used anywhere in the Scheduler itself. Scheduler just checks if the callbacks are defined and sends it to", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to be necessary. Therefore the date was changed to 2020.5.20 with the folowing fixes: * cncf.kubernetes.example_dags were hard-coded and added for all packagesa and they were removed * Version suffix is only used to rename the binary packages not for the version itself * Release process description is updated with the release process * Package version is consistent - leading 0s are skipped in month and day", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render'.", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky/quarantined tests in test_local_task_job.py by removing assert not process.is_alive() in the tests and making sure process.join is called with timeout", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from status", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type description", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, however for a long time we recommend everyone to use GID=0 in order to make it possible to run the image with Arbitrary UID. Setting different group than 0 has NO VALUE actually. You can still override the group of user when starting the container, so the only real difference is that the \"airflow\" unmodifiable files such as python code belong to different group, which has no real value. You can still use whatever group you want for mounted files and modifiable resources. Airflow Docker image will work perfectly fine when the main group of the user is 0 (and we also have to remember that if the user belongs to other groups in the host, it will also", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is repeated very often in discussions on Slack, so I would like to update this tutorial to make this task easier. See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to python-setup rather than checkout action. Also one of the checkout actions used master rather than v2 tag.", "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but then performed it's _own_ checks on if it should run. This changes that to have selective_ci_checks.sh do this check. Additionally CI_SOURCE_REPO was often wrong -- at least for me as I don't open PRs from ashb/airflow, and this lead to a confusing message: > https://github.com/ashb/airflow.git Branch my_branch does not exist But all we were using this for was to find the \"parent\" commit, but there is any easier way we can do that: HEAD^1 with a fetch depth of 2 to the checkout option. So I've removed calculating that and where it is used. If we need to bring it back we should use the output from the `potiuk/get-workflow-origin` action -- that gets the correct value", "Remove redundant code to serialized k8s.V1Pod (#11602)", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.kedzierski@polidea.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our main \"CI\" workflow (the only one we have at the moment)", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GCP decide what zone to put your cluster on by setting zone to None or a blank string, making the parameter optional. Per the API spec at https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig, this means that all machineTypeUris have to be in short form.", "Resurrect python openapi client generator (#19155)", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out at 1200px", "Fix is_terminal_support_colors functtion (#9734)", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd `unidecode` module, but since Slugify 3.0[1] it has used text-unidecode by first (and only installs the GPL library by an optional extra, not by default) so we can now use it. This lets us upgreade text-unidecode from 1.2 to the latest 1.3, which is the version one of dbt's dependencies needs. [1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionCompleteSensor This commit add a GoogleCloudStorageUploadSessionCompleteSensor to address the use case of accepting files from a third party vendor who refuses to send a success indicator when providing data files into", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * updated doc", "Type-annotate SkipMixin and BaseXCom (#20011)", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)", "Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926)", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running (using merge-request) and canceling (with cancel notifications). First of all we are using merged commit from the PR, not the original commit from the PR. Secondly - the workflow run notifies the original PR with comment stating that the image is being built in a separate workflow - including the link to that workflow. Thirdly - when canceling duplicate PRs or PRs with failed jobs, the workflow will add a comment to the PR stating the reason why the PR", "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. And that is quite common case.", "Refactor create_app in airflow/www/app.py (#9291)", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter sidecar", "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a static heading. The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate between task instances even if it were functional.", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)", "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in the comments in ci_prepare_prod_image_on_ci.sh) so we don't want to try and push the python image when we aren't building the main branch.", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` -> `start_date_lte`", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format it using two back-ticks for code-block instead of italics", "Invalidate Vault cached prop when not authenticated (#17387)", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.hooks incorrectly include docs for airflow.exceptions.AirflowException and airflow.hooks.base_hook.BaseHook in the yandex module, as if those classes had been defined in that module. This is almost certainly a bug in autoapi or one of the libraries it uses, but I haven't tracked down the root cause. In the meantime, importing the modules and then referring to the classes using modulename.Classname avoids the issue.", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>", "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on CI. We are overriding the ci scripts that we use to build the image with the scripts taken from master to not give roque PR authors the possibiility to run something with the write credentials. We should not override the in_container scripts, however because they become part of the image, so we should use those that came with the PR. That's why we have to move the \"in_container\" scripts out of the \"ci\" folder and only override the \"ci\" folder with the one from master. We've made sure that those scripts in ci are self-contained and they do not need reach outside", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'check_content_in_response' already take care of status code check (by default asserts against 200) So no need to check status code explicitly if either of these two methods are used to check the response.", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER. ``` root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0 Before: 9 passed, 120 deselected, 2 warnings in 22.11s ======================================================================================================= After: 9 passed, 120 deselected, 2 warnings in 10.56s ======================================================================================================= ```", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> Closes #18155", "Persist tags params in pagination (#15411)", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astronomer.io>", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a small margin. (9X% tests successful). This change increases the limit.", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` was missing Apache license", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)", "fix broken link in experimental API deprecation headers (#13547)", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator * remove gcs_to_sheets from missing example dags * fix doc error", "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this method is inherently insecure in production environments (it opens up for an attack where removing dependency of a dependency migh bring the Airflow deployment down). Added explanation about it and explicit warning against this.", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references to be consistent with the other examples in the documentation.", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)", "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectifies the mistake.", "Added Zalando to ``INTHEWILD.md`` (#18480)", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.", "Correct typo (#20345)", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)", "Excludes .git-modules from rat-check (#14759)", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylinttodo.txt and begin making changes, including using local variables where class ones are not needed. Where possible, clarify some local variables names, such as the snakebite client in the HDFSSensor module. Ignore corresponding pylint checks for higher impact code.", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameError: name 'log' is not defined`. (I guess no one really noticed as the container would restart, and try again.)", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high. This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled, we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat, the heartbeat detects that this task has succeeded with no return code because LocalTaskJob.handle_task_exit was not called after the task succeeded. Hence,", "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws providers Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-sm` class that made the buttons next to each DAG on DAGs list not centered perfectly. - Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word) - Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change) - Fixed strange use of punctuation marks inside a view when trying to mark DAG run as failed - Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit", "Add conn_type to Fix failing Livy Tests (#9258)", "Google Ads Hook: Support newer versions of the google-ads library (#17160)", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main page", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's \"REPLACE INTO\" syntax and is not applicable to PostgreSQL. This pulls out the sql generation code for insert/upsert out in to a method that is then overridden in the PostgreSQL subclass to generate the \"INSERT ... ON CONFLICT DO UPDATE\" syntax (\"new\" since Postgres 9.5)", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=\"True\".", "Add typing for grpc provider (#9884)", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and deprecated classes lists", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag only remove not exists For now we remove all record in dag_tag, but actually we only need to delete tag not exists in dag file anymore", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing * Fixed problem that Kubernetes tests were testing latest master rather than what came from the local sources. * Moved Kubernetes scripts to 'in_container' dir where they belong now * Kubernetes tests are now better suited for running locally * Kubernetes cluster is not deleted until environment is stopped * Kubernetes image is built outside of the container and passed as .tar * Kubectl version name is corrected in the Dockerfile * Kubernetes Version can be used to select Kubernetes versio * Running kubernetes scripts is now easy in Breeze * Instructions on how to run Kubernetes tests are", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a dag object", "Better message when Building Image fails or gets cancelled. (#11333)", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some code after a rebase. This adds the code and adds unit tests", "Remove locks for upgrades in mssql (#17213) Closes: #17088", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-Linux users. It expected the .env file to be created always, but the instructions to create them were not working on Windows. This fixes the problem by turning the error into warning, and directing the users to the right instructions per operating system. Also the recent ``DUMB_INIT_SESS_ID`` was added for worker to allow to handle signals properly also in our quick-start docker-compose.", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache/airflow requires auth while https://hub.docker.com/r/apache/airflow does not", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and attach to it.", "Add aws_conn_id to DynamoDBToS3Operator (#20363)", "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break on Buster as the versions of packages conflict: > The following packages have unmet dependencies: > libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed > Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installed This changes our CI docker images to: 1. Not install something from Sid (unstable, packages change/get updated) when we are using Buster (stable, only security fixes). 2. Installed the JRE, not the JDK. We don't need to compile Java code.", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)", "Improve typing in airflow/models/pool.py (#9835)", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Executor would only delete pods in its own namespace * add tests * clean up PR * static tests * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com> * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume a redshift cluster.", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] Capture task logs in DockerSwarmOperator * [AIRFLOW-5850] Fix the mock in the docker swarm tests * [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator", "Fix typo in docker-stack documentation (#16221)", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" (#5191) This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The query did not have ordering and sometimes the tasks were returned in different order than expected.", "Add error check for config_file parameter in GKEStartPodOperator (#17700)", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)", "Add some basic metrics to the Triggerer (#18214)", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This parameter is deprecated by werkzeug, see: https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120 However, it is also broken. The value is acquired as string from the config, while it should be int like the other `x_*` attributes. Those were fixed in #6901, but `num_proxies` was forgotten. I think we can safely remove it because: * There", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __What__ * Add correct type annotations to ZendeskHook and each method * Update one unit test to call an empty dictionary rather than a NoneType since the argument should be a dictionary __Why__ * Building out type annotations is good for the code base * The query parameter is accessed with an index at one point, which means that it cannot be a None type, but should rather be defaulted to an empty dictionary if not provided * Remove useless return", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing from docs of `KubernetesPodOperator` - `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults to False in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator` - `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches) - `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr` - Reduce amount of times the pod object", "Add Parquet data type to BaseSQLToGCSOperator (#13359)", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" what an operator does will greatly increase the \"accidental complexity\" of both Airflow (for us as developers) and of the DAG itself (for our users). By marking this features as experimental we reserve the right to delete it at any point (for instance once we add better methods of doing what the OP wanted with these hooks.)", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)", "Fix formatting in AWS Connections docs (#8223)", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)", "Remove unnecessary messages in CI (#7951)", "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with AIRFLOW_INSTALL_VERSION when the Docker image is build. Fixes #8612", "Submodules are needed to update constrains (#15242)", "Fixing MyPy issues in testa/jobs (#19998)", "Chart: Use stable API versions where available (#17211)", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`", "Add schema as DbApiHook instance attribute (#16521)", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` is the only way to configure workers, so we can remove the \"other fields\" language.", "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URL", "Add ME-Br to who uses Airflow list (#9770)", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a version, so we don't have to edit a file and then remember to delete it! It's a little bit messy to get the suffix like this -- using sed to strip out any leading digits or periods, but it's copy-pasteable this way.", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depenedncies of airflow, all the provider packages released at the time of the relese of that version, as well as all transitive dependencies. Following those constraints, you can be sure Airflow's installation is repeatable * no-providers constraints - containing only the dependencies needed for core airflow installation. This allows to install/upgrade airflow without also forcing the provider's to be installed at specific version of Airflow. This allows for flexible management of Airflow and Provider packages separately. Documentation about it has been added. Also the provider 'extras' for apache airflow do not keep direct dependencies to the packages needed by the provider. Those dependencies are now transitive only", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - this have been helpful with handling MySQL deadlocks - however similar problems occur occasionally in MSSQL and there we get a DBAPIError rather than OperationalError: `sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction (Process ID 55) was deadlocked on lock resources with another process and has been chosen as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)'); This PR adds DBAPIError to the list of errors that are handled by `run_with_db_retries` to mitigate such occasional deadlocks.", "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures when executing long-running jobs", "Add Pinterest to Airflow users list (#19117)", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in celery_executor error log * Add test for airflow/configuration.py::as_dict", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hook Adds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable. * [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hook Adds a common method for validating the src_ftm_configs", "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, not just when the file itself changes.", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: 1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set. 2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a", "Remove unnecessary string concatenations in AirflowException messages (#18817)", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883e591b467994. Not sure what happended. We made that change because of cPython vulnerability (https://github.com/python/cpython/pull/24297/files) in #14341 I am not sure what happened here but this should fix the Master.", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they used an uppercase E. Also made the exception a bit clearer when running on kubernetes.", "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring exceptions here we let the failure be exposed where its broken, not in the next test that happens to run.", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst which is hard to maintain, such as `set_upstream` is change to shift in tutorial.py but still in tutorial.rst. Use sphinx is a better way", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default Airflow roles. On my machine with 5k DAGs, this led to a reduction of 1 second in startup time (bonus, makes tests faster too).", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` in the requirements.", "fix tests (#11368)", "Update node installation cmd (#10744)", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner", "[AIRLFOW-XXX] Display other integrations in single table (#6133)", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BigQueryToMsSqlOperator * Updating source_project_dataset_table arg in example DAG", "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a DockerOperator that can run that python function remotely. ``` @task.docker( image=\"quay.io/bitnami/python:3.8.8\", force_pull=True, docker_url=\"unix://var/run/docker.sock\", network_mode=\"bridge\", api_version='auto', ) def f(): import random return [random.random() for i in range(10000000)] ``` One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as this would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash). To work with these requirements, we use base64 encoding to store a jinja generated python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLI", "[AIRFLOW-4000] Return response when no file (#4822)", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844 #14184 This PR refactor SQL/BigQuery Check operators to reduce duplicated code: create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() method Add a database kwarg *CheckOperators for a consistent interface create _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuery create _QuboleCheckOperatorMixin to remove duplicate code replace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)", "Add test connection method to http hook (#16568)", "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl> Obtain tree_data object endpoint from meta. closes: #16017", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint Call `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is a required argument). I looked for a test suite for the web UI, but it seems the existing tests have all been disabled since the switch to FAB. I've created a new class for FAB tests and", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)", "Fix installation doc (#13462) The note should not be in the Bash code-block", "Update INTHEWILD.md (#12060)", "GCP Secret Manager error handling for missing credentials (#17264)", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which needed to be downloaded. The new check uses already available airflow CI image and it performs all check in one docker command - thus is a lot faster and it also checks the image at the same time.", "Adds Github Oauth example with team based authorization (#17896)", "Add taskflow to accepted words (#11902)", "Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 too", "Move role guide to access control (#10755)", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of using operators that users may find useful. Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result. This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)", "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real tools to get verbose behaviour (we can print the exact command being executed for those. But when 'set +e' was set before the command was called - indicating that error in those functions should be ignored - this did not happen. The functions set 'set -e' just before returning the non-zero value, effectively exiting the script right after. This caused first time experience to be not good. The fix also fixes behaviour of stdout and stderr for those functions - previously they were joined to be able to be printed to OUTPUT_FILE but this lost the stderr/stdout distinction. Now both stdout and stderr are printed to the output file but they are", "Use DAG context manager in examples (#13297)", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings import code into a dedicated function in settings.py and adds a call to it in initialize after prepare_syspath", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880", "Chart: Add custom labels for ingresses/PVCs (#20535)", "Fix breeze redirect on macOS (#14506)", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures", "Move out get_python_source from www, Move get_dag to www.utils (#7899)", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache Airflow", "Clean up the pre-commit config file (#15681)", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _get_credentials method - which is a centerpiece of AWS provider and is likely to be overwritten by the user who want for example inject auditing or other credentials-related custom beheviours when interfacing with AWS even if the method is protected. The change added default for region, which caused signature incompatibility with such derived classes. Unfortunately, we already released 2.5.0 provider with this change. We had to yank it and in order to avoid adding backwards-incompatible 3.0.0 release we are going to release 2.5.1 with this change included. Fixes: #20457", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes Story Removes thousands of lines of code that essentially ammount to us re-creating the Kubernetes API. Will offer a faster, simpler KubernetesExecutor for 2.0 * Fix podgen tests * fix documentation * simplify validate function * @mik-laj comments * spellcheck * spellcheck * Update airflow/executors/kubernetes_executor.py Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python files, running the flake8 pre-commit hook would fail without obvious error (as in no error was printed, but exit code was 1). In debugging this I switch the pre-commit to `require_serial: true` and the problem went way - the fix for this is: - Don't redirect stderr to /dev/null (that silences both our VERBOSE trace output, and the errors from docker) - Use `--cidfile` option to docker to create a random name and write the created container ID to a file", "Fixed button size in \"Actions\" group. (#17902)", "Add muldelete action to TaskInstanceModelView (#18438)", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions` short circuits anyways.", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (similarly as providers) This change disables Helm Chart tests in case default branch is different than main.", "[AIRFLOW-XXX] Fix doc error (#5179)", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be in one place. Documentation for Celery does not describe the inspect ping command, but hopefully, this will be added soon.", "Added notification to solve \"docker-credential-service-error\" (#18524)", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)", "[AIRFLOW-XXX] Remove profiling link (#4602)", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but then it causes a lot of extra strain in case several commits are merged in quick succession. The master merges are always full builds and take a lot of time, but if we merge PRs quickly, the subsequent merge cancels the previous ones. This has the negative consequence that we might not know who broke the master build, but this happens rarely enough to suffer the pain at expense of much less strained queue in GitHub Actions.", "Disable experimental REST API by default (#12337)", "Adds a forgotten word in a README.md (#12066)", "Remove flask-admin based Plugins (#11515)", "Fix mypy apache kylin operators (#20595)", "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number it has the scheduler generates the following warning: `[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0\u2502172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408 However this message is very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be an action from the user: 1. User to increase the total number of slots in the Pool 2. User need to change the task code to requests less slots. This PR add specific log notice for this case.", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in Airflow for a while (all the CLI command already turn the literal `DAGS_FOLDER` in to the real value of the DAGS folder when loading dags), but sometime around 1.10.1-1.10.3 it got fully broken and the scheduler only ever passed full paths to DAG files. This PR brings back this behaviour", "Fix Experimental API Client (#9849)", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example: ```json \"dag\": { \"tasks\": [ \"serialization_failed\" ], ``` (`tasks` should be a list of dictionaries. It clearly isn't.) Instead of doing this we throw an error, that is captured and showing using the existing import_error mechanism for DAGs. This almost certainly happens because a user has done \"something interesting\".", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst * fixup! Upddate example on docs/howto/connection/index.rst", "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute function. This functionality is part of AIP-31. Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>", "Adding missing word to welcome message (#16726)", "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has a set of provider packages that are needed by the extra and they will be installed automatically if this extra is specified. For now we do not add any version specificatiion, until we agree the process in #11425 and then we should be able to implement an automated way of getting information about cross-package version dependencies. Fixes: #11464", "Reorder middleware - ProxyFix and BaseUrl (#8157)", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. This is a good change, however it has some unforeseen consequences. The 'provider_packages' script copy all the providers code for backports in order to refactor them to the empty \"airflow\" directory in provider_packages folder. The #10806 change turned that empty folder in 'airflow' package because it was in the same directory as the provider_packages scripts. Moving the scripts to dev solves this problem.", "Override project in dataprocSubmitJobOperator (#14981)", "Upload provider distribution artifacts during CI (#19807)", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field. But the URI format can only handle primitive key-value pairs. This PR provides support for arbitrary json in the URI format. Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com> Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)", "[AIRFLOW-6820] split breeze into functions (#7433)", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to only show errors, unless verbose variable is set. We are utilising aliases if possible but in case of pre-commits they are run in non-interactive shell which means that aliases do not work as expected so we have to run a few functions directly in other to show spinner. Extracted from #10368", "Add note about using dag_run.conf in BashOperator (#9143)", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match", "Added json_render method to separate filtering from view (#14024)", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)", "Add placement_strategy option (#9444)", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present. Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While attempting to create a self-signed TLS connection between airflow and ES, we discovered that airflow does now allow users to modify the SSL state of the elasticsearchtaskhandler. This commit will allow users to define ES settings in the airflow.cfg", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator * fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ``` File \"/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py\", line 32, in <module> from airflow.models.baseoperator import chain ImportError: cannot import name 'chain' ```", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 was missing from the breeze-complete list and the docs", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441) These tests are flaky and fail sometimes", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in forked repositories might be pretty invasive for the forks - especially when it comes to scheduled workflows as they might eat quota or at least jobs for those organisations/people who fork repositories. This is not strictly necessary because Recently GitHub recognized this as being a problem and introduced new rules for scheduled workflows. But for people who are already forked, it would be nice to not run those actions. It is enough that the CodeQL check is done when PR is opened to the \"apache/airflow\" repository. Quote from the emails received by Github (no public URL explaining it yet): > Scheduled workflows will be disabled by default in", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101) which caused failing Master", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9 instead of using \"apt-key add\" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either \"gpg\" or \"asc\" as file extension. Also added better redirection on the apt-key list command.", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of dependencies broke them: 1) SQS moto 2.2.6 broke SQS tests - the queue url in the 2.2.6+ version has to start with http:// or https:// 2) DataCatalog part of Google Provider incorrectly imported types and broke tests (used beta instad of datacatalog path)", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112", "Allow ./run_tmux.sh script to run standalone (#13420)", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id from self.hook.project_id", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about lack of those. Added by `mypy --install-types` command. Part of #19891", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25th", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting with `skipLibCheck` - fix motion warning on test with `resolutions` in `package.json` * remove resolutions in package.json * Upgrade to latest Chakra release * Use username instead of id in route * Add new UI node files to rebuild check * Add linting dependencies Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>", "provide_session keep return type (#9787)", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies. This allows sidecars with webservers in them to function when networkpolicy is enabled. This also renamed the existing parameter used to define `from` in the networkpolicies ingress.", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>", "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a KubernetesPodOperator task in the airflow UI, that the associated pod is also killed using the on_kill method.", "Rename second pylint pre-commit hook to distinguish it from first (#13303)", "Fixing bug which restricted the visibility of ImportErrors (#17924)", "Add missing values entries to Parameters in chart/README.md (#11477)", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set in the interactive Breeze entry - which is different than in tests and makes it difficult to run tests outside of the main directory.", "Fixed failed pylint in master (#12938)", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a)", "[AIRFLOW-4750] Log identified zombie task instances (#5389)", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances will have ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of (depends on) its task_1. if task_2 depends on task_1 this means task_2 is set downstream of tastk_1 - wondering if I am missing something here - or misreading it", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer.", "[AIRFLOW-5406] allow spark without kubernetes (#6921)", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code. This has long been improved by decreasing memory pressure for CI dockers and healthiness checks and is no longer needed.", "Strict type check for google ads and cloud hooks (#11390)", "Dev: Clarify file naming in release verification doc (#19233)", "Improve KubernetesPodOperator guide (#9079)", "Small typo in JdbcOperator (#18593)", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor. 1. Extract shared code to utils. 2. Allow any iterable as week_day.", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isort Co-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes a bug where the `full_pod_spec` argument is never factored into the kubernetespodoperator. The new order of operations is as follows: 1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod 2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod 3. reconcile with any of the", "[AIRFLOW-6929] Add OpenAPI spec (#7549)", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to correctly escape problem areas, this commit just fixes the last instances so that we can assert that `|safe` is never used.", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in processing environment variable file which prevents Breeze from running. Until it is fixed, we are going to print an error, explain how to disable it and exit - because the workaround introduces more problems than it solves (passing environment variables to container is broken partially) Also see https://github.com/docker/compose-cli/issues/1917", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987) * AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG.is_active closes: #17639", "Move the contribution workflow to the beginning of the file (#10092)", "shorten name of hook re imports of provide_session and create_session (#12936)", "Update max_tis_per_query to better render on the webpage (#17971)", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the main shell is zsh on MacOS (which is the default) this PR changes it so that the output of `command -v bash` is used instead. Fixes #14754", "Fix capitalisation of boolean in config (#13569)", "Add missing type of tests to breeze. (#18504)", "Move backport packages to GA (#8391)", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 (renaming GCP operators and hooks): * renamed GCP modules * adde deprecation warnings to the contrib modules * fixed tests * updated UPDATING.md", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update docs Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes: #14106 Co-authored-by: Joao Ponte <jpe@plista.com> Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>", "Remove redundant code from breeze initialization (#9375)", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the UI and are helpful for observability of Ariflow", "Allow hvac pakage installation using 'hashicorp' extra (#7915)", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be more generic.", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com> Co-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.", "Allow passing backend_kwargs to AWS SSM client (#8802)", "Fix kinesis test (#18337)", "Clean up incorrect class names of Google system tests (#19956)", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This PR allows Viewers (and above) to see all of the links.", "Improve Google PubSub hook publish method (#7831)", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 as it has been released.", "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator` - `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installation for quite some time at least verbally but also in the Helm Chart documentation. However we missed such recommendation in the general Postgres area of 'Setting Up the database` doc. This PR adds a note that we can refer to when explaining problems with connections and stability to the users who use Postgres without PGBouncer proxy (which is known to help in such cases)", "Add template fields to neo4j operator (#20043)", "Fix typo in example (#13321) False should not be passed as a string", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)", "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add some of the mappings. * Add the rest of the mappings. * Fix permission names. * Fix permission names. * Use standard names for new users endpoints. * Document user access. * Remove unused tests. * Make roles tests pass by cleaning test roles from test_views.py. * Remove old permission names. * Update role tests. * Reorder permissions. * Remove RESOURCE_ROLE_MODEL_VIEW. * Remove db merge.", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values so both GET and POST are properly logged"], "sampled": ["Correct command for starting Celery Flower (#9483)This", "Fixing mypy issues inside tests model (#20026)Coding", "Make K8sPodOperator backwards compatible (#12384) * Make the KubernetesPodOperator backwards compatible This PR significantly reduces the amount of time required for developers to properly implement this library, as it should now be backward compatible quickly. Please be aware that this PR is only working on Fedora 25 and newer. This is a separate PR, so your version of Kubernetes will not be affected", "[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010)One", "Fix spelling (#11404)This", "[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)Wash", "[AIRFLOW-5614] Enable Fernet by default (#6282)One", "Make Cloud Build system tests setup runnable (#10692) This change fixes error: open(quickstart.sh): Permission denied that was created. (#10676)\n\nBugfix: Fix crash", "Quarantine iest_no_orphan_process_will_be_left (#18778) This test fails too often. Quarantining more than one adult animal at a time", "Update Thumbtack points of contact in Airflow Users list (#9701) The previously-listed person is no longer at the companyWhen", "Mark trigger-controller-dag test as xfail (#8015)The", "[AIRFLOW-6491] Improve handling of Breeze parameters (#7084) While working on improving the way we run Kubernetes, we decided to try to fix some of those nasty Breeze parameters that aren't quite right. In other words, it's been around for a while. (Brief explanation, please.) If you're looking for a list of Breeze parameters, you'll see them in the Breeze_Kube.conf file. For example: type: \"options\" { \"verbose_commands\"", "Chart: Update postgres subchart to 10.5.3 (#17041) We were on 6.3.12 and the current latest version is 10.5.3.\n\nSubplotting: Plotting your data\u00b6 Use these instructions to plot your sub-data: Note You can create subplanets when adding a sub-column to existing charts or sub-colors in", "Enable Black on Connexion API folders (#10545)The", "Fix documentation for provider's release (#14654)We", "Run \"third party\" github actions from submodules instead (#13514) Rather than having to mirror all the repos we can instead use git submodule update commands to update only those we need... which doesn't require extra dependencies at all. This requires minimal amount of code changes (a single dependency) from core to be passed down to each new repo, as opposed to manually re-compiling changes we made before for each repo, so can be implemented more quickly in many cases. (#13515) Add", "Make `pandas` an optional core dependency (#17575) We only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526)\n\nWe only use `pandas` in `DbApiHook.get` if our `query` has the `coreData` key returned by the API (#17526) db_id is needed to query DB keys (#17558)\n\nis needed to query DB keys (#17558) db_get_field() throws an exception if the field is not defined (#17529)\n\ndiscards invalid fields if", "[AIRFLOW-4565] instrument celery executor (#5321) * instrument celery executor * remove unused import * nit * remove unused import * fix missing plugin *", "Add Playsimple Games to \"Who uses Apache Airflow?\" (#10253)As", "Make `XCom.get_one` return full, not abbreviated values (#18274) If you used this class method directly (such as while iterating over a property list or a list of nested objects), it is possible to have `XCom.get_one` return no values (such as zero or a number less than the number of selected properties).\n\nImprovement: the correct code for retrieving", "Upgrade FAB to 3.1.1 (#11884) We can also remove the FAB Actions fix with composite PKs (https://github.com/docker/docker/pull/11884). We did this to enable support of more types", "[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304)For", "[AIRFLOW-7041] make bowler dependency local (#7691)How", "Fix Python Docstring parameters (#12513)The", "Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168)The", "[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245) * [AIRFLOW-6624] Improve webserver command with pidfile checking (#7249) * [AIRFLOW-665] Update webserver to 10.13 for build. * [AIRFLOW-665] Handle exceptions. * [AIRFLOW-666] Add a debug message when failing with an error when webserver fails to start. * [AIRFLOW-666] Fix webkit crash that may", "UX Enhancement: Add button to clear search query from DAG search (#11583)I", "[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154)Get", "[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164)Lint", "[AIRFLOW-6635] Speed up static checks (#7256)By", "[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511)The", "[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037)The", "Move out sendgrid emailer from airflow.contrib (#9355)So", "Chart: Allow disabling `git-sync` for Webserver (#15314) closes https://github.com/apache/airflow/issues/15314", "Pin pandas-gbq to <0.15.0 (#15114)Description:", "[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooks The fuzzy licence matching implemented by Jarek will not happen in Firefox 45 and beyond. However, a similar feature is likely to be introduced sometime in Firefox 51.\n\nSee also the", "Chart: Allow setting an existing secret for PgBouncer config (#15296) Previously, if a user wanted to supply the username and password of a private PgBouncer user, they had to specify an existing secret for their PgBouncer instance. This is no longer necessary and is now enabled by default.\n\nBug Fixes\n\n(5926dd98) The test class for the (un)formatted HTML-formatted-HTML attribute on", "Added Viscovery to the list of companies using Apache Airflow (#18683)The", "Don't use time.time() or timezone.utcnow() for duration calculations (#12353) `time.time() - start`,`time.time() - end', and `time.time()` are all now defined with `type` or `time.time()` 's options\n\nFixed bug: fix bug that made non-time object types default as argument of `format` method on Time object (thanks @DrewYea )\n\nAdded option `'type-argument' to Time.format, allowing the user to specify the type arguments to be output (thanks @dana )\n\nAdded option `'size' to Time.format('date', 'time', 'datetime') to change the size of the datetime object to a longer Time datetime object\n\nAdded option `'timeout' to Time.format, indicating how many seconds of time to wait before continuing parsing a given date\n\nAdd option `'format", "[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911)In", "Convert properties with query to real methods (#7900) * Convert properties with query to real methodsAs", "Fix oversized width of DAGs table with hide/reveal of \"links\" (#11866) * Conserve horizontal space by adding hide/reveal of \"links\" (#11836) * Fix overflow of table (e.g. large rows or lots of numbers) (#10752) *", "[AIRFLOW-5361] Add system tests for BigQuery (#5968)We", "AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796) * add run_job_kwargs to Glue job\n\n* fix failure to update Glue object #16794\n\n* fix error", "Make Smart Sensors DB Migration idempotent (#13892)With", "[AIRFLOW-5702] Fix common docstring issues (#6372)The", "[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#724)\n\n- Fix EAP #723\n\n- Fix EAP #722 (#723)\n\n- Fix EAP #727 (#727)\n\n- Fix EAP #626 (-1450)\n\n- Fix EAP #539 (#539)\n\n- Fix EAP #573 (#574)\n\n- Fix EAP #572 (#573)\n\n- Fix", "DbApiHook: Support kwargs in get_pandas_df (#9730) * DbApiHook: Handle pthread namespace in dbase_get_pd() (#9569) * DbApiHook: Don't return in GetDbApi when using PDBA(True) (#9526) * DbApiHook: Call dbase_set_pd() for DbApi hooks (#8852) * DbApiHook: Don't check", "Quarantine test_mark_success_no_kill test (#17580) This test is flaky. Logging it in: #17579", "Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553) The DagFileProcessor.manage_slas callback should be called once if any files have a corruption status of `OK` (that means that there was a proper removal of corruption). This fix breaks backwards compat with DagEdit; see https://github.com/LukasZibella/DagEdit/issues/19554 for details. (bug 87829) Fixed file manager issue with a Windows machine when run in a GUI mode (#19540)\n\nFeatures\n\nFix missing DSDT entries for several user input files (#18981) DSDT is usually stored in an unordered sequence so it can be used to read, write,", "[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158)For", "Improvements for transfer operators references (#12482)The", "Handle DST better in Task Instance tool tips (#8104) We displayed the zone \"name\" based on the current time, which could lead to incorrect location of items in the Zone. (#8105)\n\nBug Fixes\n\nGeneral\n\nCortana: Improve Cortana feedback experience (#8104) Cortana now", "Add colors to airflow config command (#8404)The", "[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462) * Add Bloomberg to list of Airflow users * Indicates if Airflow has been setup and configured", "Add missing tests for snowflake changes (#16463) Fixes problem introduced in #16420If", "Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910) Currently there is no way to determine the state of a Tooltip object (it is an existing TI Task). The task to detect this is to read and return a task-related object. This object needs", "Chart: Allow ``webserver.base_url`` to be templated (#16126) As `config`'s documentation states values of base URL is relative to base path and not relative to", "Move setting of project ID after activating service account (#17866) Co-authored-by: Dmytro Khimich <khimich@hc-de.org>", "Add type hints to aws provider (#11531) * Added type hints to aws provider * Update airflow/providers/amazon/aws/compute-power-policy to reflect latest aws versions and to be compatible with aws 1.7.0", "Add verify_ssl config for kubernetes (#13516)(https://github.com/github/kubernetes/budrus/pull/13516)", "Updated documentation for the CI with mermaid sequence diagrams (#10380)Docker", "[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445)The", "Use Python 3 style super classes (#11806) example: ``` super().__init__(label, validators, **kwargs) ``` #11806\n\nA better syntax for subclassing objects that are", "[AIRFLOW-XXX] Add history become ASF top level project (#4757)In", "[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535)On", "Remove kwargs from Super calls in AWS Secrets Backends (#9523) We don't want pass to kwargs to `BaseSecrets`, for which", "[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038)At", "[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4922) * [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4921) * [AIRFLOW-4092] Add gRPCOperator, unit", "Clean-up of google cloud example dags - batch 2 (#19527) - Use static start_date - Use catchup=False - Tear down all code related to start_date. - Split the template into two - use @GoogleDags in", "Refactor plugins command output using AirflowConsole (#13036) This PR refactors the airflow plugins command to be compatible with 'output' parameter, which means that new functions are automatically added. This allows code generated by", "Fix task search function in Graph view (#15901)One", "Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)The", "[AIRFLOW-3993] Add tests for salesforce hook (#4829) - refactor code - update docs - change sign_in_form_name to sign_in_form(data=user=@user and secret=@sec, error=@error) - improve link link description to clarify some context (and not", "[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384)The", "Extend HTTP extra_options to LivyHook and operator (#14816) The LivyHook used by the LivyOperator has a lot of extra options; this bug gives extra options to the URL to which I have", "[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951)TWEAK:", "[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)This", "Use DAG_ACTIONS constant. (#16232)About", "Clean up airflow.contrib in Kubernetes docs (#9551)\"", "Add back-compat layer to clear_task_instances (#16582) It is unlikely that anyone is using this function directly, but it is there. It should be merged into a commit that", "[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453)Atlas's", "detect incompatible docker server version in breeze (#9042)The", "Improved cloud tool available in the trimmed down CI container (#9167) * Improved cloud tool available in the trimmed down CI container The tools now have sheers: the cloud app shows a list of open CI jobs and the tool will highlight them automatically. * More info about the app on DevOps wiki page * More info about the app on DevOps wiki page * \"Automated Continuous Integration\" can be selected through the cloud tool. In general the tool will ask about the running OS during installation. We also support the CI workflow using Jenkins and Git-Flow. * More info about automated continuous integration at DevOps wiki page * Better support for", "fix bug of SparkSql Operator log going to infinite loop. (#19449)For", "Remove unused 'context' variable in task_instance.py (#14049)I", "UPDATING.md for changes included in 2.1.1 (#16615)The", "Google Memcached hooks - improve protobuf messages handling (#11743)\"", "Fix crash when user clicks on \"Task Instance Details\" caused by start_date being None (#14416) This is to fix the following error in the log message: [ERROR] The application does not have a task instance, therefore not starting in a supported way\n\n(this has two causes, one being that the application is not running in a supported way and the second issue being that a task instance may have been closed.) Fixes a crash caused by closing task instances when the application is not visible\n\nFix bug which sometimes caused", "Fix command to run tmux with breeze in BREEZE.rst (#11340) `breeze --start-airflow` now builds the", "[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)I've", "Add capability of customising PyPI sources (#11385) * Add capability of customising PyPI sources This change adds capability of customising installation of PyPI sources, the functionality is a bit different to regular sources. You use the command, pyPI_source, to look", "Update link to match what is in pre-commit (#16408) [The k8s schema repository that has been used for chart pytest has now now been changed to the stable k8s schema repository]\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com>\n\n(c) 2014 Jonathon Schmiedl <jonschmiedl@gmail.com> Code of Conduct: https://medium.com/@jonachaschewd\n\n(c) 2015 David Hochhuber <dhochhuber@google.com> License: MIT Open source: http://opensource.org/licenses/MIT License https://www.flickr.com/photos/davidhochhuber\n\n(c) 2015 David Hochhuber http://www.github.com/davidhochhuber\n\n[New to c)\n\n1.", "Update to latest pygrep pre-commit hook (#8489)A", "Replace deprecated dummy operator path in test_zip.zip (#13172) Replace deprecated path in `tests/dags/test_zip/test_zip.inc` with `tests/dags/test_zip/TestDir.inc` (#13160) Replace deprecated alias in test_zip.inc with `tests/dags/test_zip/TestApi.inc` (#12917) Replace deprecated", "Add example dag and system test for LocalFilesystemToGCSOperator (#9043)Posted", "[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039)This", "Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639) Co-authored-by: Domenic Sapere (#19644)", "Fix typo in Google Display & Video 360 guide Co-authored-by: michalslowikowski00 <michal.slowikowski@gmail.com>\n\n-fixed", "Log migrations info in consisten way (#13458) Resource based permissions migration changes logging handlers so each next migration is differently formatted when doing airflow db migrations (@gwern)\n\n: Added support for the Azure CLI", "JIRA and Github issues explanation (#8539)Still", "[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139)New", "Add Changelog & Updating.md for 1.10.15 (#14870) This commit adds Changelog & Updating.md and updates", "Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949)Groups/DagRun.ID/Folders", "[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434)This", "[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251)LWP", "Fix backwards compatibility with k8s executor_config resources (#11796)\"", "Docker context files should be available earlier (#12219) If you want to override constraints with local version, the docker-context-files should be accessible before any files", "fixed typo in confirm script (#8419) Co-authored-by: michalslowikowski00 <michal.slowikowski00@gmail.com>", "Typo fix in TESTING.rst (#19216)For", "[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747)The", "[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702)\"", "Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224)The", "Simplified GCSTaskHandler configuration (#10365)The", "Fix label_when_reviewed_workflow_run permissions (#16596) * Fix label_when_reviewed_workflow_run permissions This commit was merged on 2017-10-29. * Label labels for run reviews are now triggered in workflows * Fix labels labels for run reviews are now triggered in workflows * Fix label labels for run reviews are now triggered in workflows * Enable running run reviews (#16555) * Use working directory only for running reviews", "[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877)In", "[AIRFLOW-XXX] Highlight code blocks (#6243)HIGHLIGHT", "Update Helm Chart docs for 1.0.0 release (#15957) Updates repo name and chart name and some minor errorsThe", "Small fixes in Google Cloud Secrets Manager guide (#12105)What's", "[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949)This", "Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)What", "refactor: fixed type annotation for 'sql' in MySqlOperator (#17388)The", "[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626)MUST", "Create a documentation package for Docker image (#14765) * Create a documentation package for Docker image * fixup! Create a documentation package for Docker image * Build dockerfile * Check for docker build errors * Fix build error while running docker-compose #14868 #14906 * Add docker-compose command to run only files that can connect to docker (#15998) #14414 * Fix", "Bump Boto3 (#7851)Tunnel", "Add reference to the ASF Code of Conduct (#9453) * Add reference to the ASF Code of Conduct * Update CONTRIBUTING.md", "[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)[AIRFLOW-XXX]", "Fix mypy for exasol and facebook hooks (#20291)For", "Standardize AWS Lambda naming (#20365)With", "[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976) [ci skip]So", "Fix elasticsearch breaking the build (#7800)The", "Fix impersonation issue with LocalTaskJob (#16852) Running a task with run_as_user fails because PIDs are not matched correctly.\n\nFix locale handling (#16852) Language bindings should never run as root. (#149625)\n\nDependency Injection: Fix dependency injection issues when using \"dependencies\" as an identifier in the dependencies definition\n\nFix dependency injection regression (#149625)\n\nImprove installation-time performance by using", "Rename the main branch of the Airflow repo to be `main` (#16149)This", "[AIRFLOW-6084] Add info endpoint to experimental api (#6651)From", "[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)There", "Further speed up Connexion API tests with pytest session fixtures (#14746) Creating the Flask API and Connexion take a significant amount of development time. That means it should be possible to reduce the complexity of those tests and make them quicker on the machine. #14672 Implement django model-specific query methods for model attributes (#14686) Prevent model attributes from being automatically generated by Django when a model instance has been created.\n\nMake Connexion support for models with default names (#14662) Improve Connexion behavior to allow custom attributes to be added to multiple Connexion instances,", "Update external docs URL for Segment (#13645)The", "Add support for modifying celery worker deployment strategy (#15213) This commit modifies the worker template to allow passing a non-default deployment update strategy. This version is only useful if the worker template supports an override mode.\n\nSupport for multiple types of workers (#15066) This commit updates the server side worker template to support multiple worker types and worker architectures. This provides more customization on the server side.\n\nSupport for adding \"re-run after each run\" and \"re-start with new worker available at each run\" actions (#15100) This commit makes server side worker templates more flexible when adding the use_rerun action or setting the status of the worker to \"upcoming\" (#15050).\n\nSupport for adding \"retry\" actions to server-side worker templates (#15044) This commit allows server side templates to include a \"retry action\" to delay run start time or delay the start of a run by default.\n\nThis commit", "Add fudament for API based on connexion (#8149)So", "Cleanup KubernetsPodOpertor tests (#15475)Presto_Cleanup", "Add example DAG and system test for MySQLToGCSOperator (#10990)A", "Fix PyPI spelling (#13864)WASHINGTON,", "[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759)It's", "[AIRFLOW-4422] Pool utilization stats (#5453) Add stats to record pool utilization such as open slots and used slots.A", "Rewrite handwritten argument parser in prepare_provider_packages.py (#13234) * Rewrite handwritten argument parser in prepare_provider_packages(lib). [@sjoerd]. ** Documentation on a new command line argument parser to parse the argument from a command line, see http://dev.python.org/package/prompt.html ** Update documentation on an example illustrating the new parser on a subpkg * Fix a rare regression in Python 3.x releases which led to an unexpected error message when the script was executed from a Python3.x version that", "Make models/taskinstance.py pylint compatible (#10499)Still", "[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637)\"", "[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)About", "[AIRFLOW-4970] Add Google Campaign Manager integration (#6169) * [AIRFLOW-4970] Add Google Campaign Manager integration", "[AIRFLOW-6326] Sort cli commands and arg (#6881)\"", "Default python version is used when building image (#13285) For image build the python version is passed via PYTHON_MAJOR_VERSION, which is different than the version in .py files (#13515) When image build fails, the output of test_path is returned instead of the build path (#13627) (#12945) for using shared libraries (#14059) When checking if the module will be added to a shared library it fails", "Check python version before starting triggerer (#18926)One", "Add proper link for wheel packages in docs. (#15999) Co-authored-by: jarek <jarek@penguin.org>", "Remove Brent from Collaborators (#18182) Brent is already a committer so we don't this entry here. It was needed only when he was no", "[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_job or", "Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404)Still", "Docs: Clarify behavior of delete_worker_pods_on_failure (#14958) Clarify that the `delete_worker_pods_on_failure` variable is available only during execution of the `exit` clause inside", "[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376) Allow task definitions to specify labels rather than running a specific kubernetes user instead of running a definition in the entire pod. A new label for", "[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737) Adds an additional endpoint to the experimental DAG-API (#7757)\n\nStructure:\n\nMoved an entire group of DAG-API to #7627.", "Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581) - add \"dbl\" to db path (#14589) - add 'all' command (#14436) - add debug support (from @makr) (#14572) - set _start_time to null for DB (from @makr) - add more debug output into DB options (#14572) #14579 - add a couple of new command lines. #14788 - add \"dbl\" and 'all' command lines to DB build, to help debug DBConfig (#14563) #14728 - add \"all\" option in options file (#15008) #14537 - new output of DB check is also output into a string, that matches", "[AIRFLOW-6511] Remove BATS docker containers (#7103) The containers were not removed and you have to remove them with `docker rm`. This should fix any bugs you", "Add open id dependency (#13714) * Adds python3-openid requirement Seems that python3-openid dependency is not properly solved by tools/packages. * New code path to add a pip dependency (#13805) + Support for using \"py\" for path/filename name argument for cwd() function (#13979) + Add a -n option to pip install --help to get all available help files (#13952) + Added config.py to install pips from sources. * Added support for Windows + added Python 2.7.x * Added a pip2 dependencies option. (#13898) + Pip install now always fails + Support", "Doc: Use ``closer.lua`` script for downloading sources (#18179) - Follows first point of https://infra.apache.org/commons/commons-lang/3.5.0/closer.lua (#17078) - Correct a bug related to `download`. This also changes documentation output, in particular with respect to the default configuration", "[AIRFLOW-5830] Get rid of slim image (#6494) The slim image gave only very small gain on executing the tests in the main task, and hence there was no effect on other tasks. This may have been because the main task had very limited execution time, but the results from the other tasks suggest that it had a more substantial effect. In the task with a small gain, you may need to adjust the speed of a small decrease of image size to increase the accuracy in the results. This may help with speed increases during work times.\n\n\n[AIRFLOW-5740] Increase the speed of the draw (#6644) I would like to thank Richard on the project for providing the details of this improvement, both in terms of how it affects execution time while in progress and how he actually performed the task. The speed gain is in both cases about a 1:1 factor improvement over drawing the object. I also received permission from my", "[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059)When", "BugFix: Dag-level Callback Requests were not run (#13651) In https://github.com/apache/airflow/pull/2535 (TavisJ) Airflow now accepts /api/callback.json as a callback to receive new calls before the callstack has cleared (#13668) in https://github.com/apache/airflow/pull/14053 (MattZ) Fixed -pv argument to add to the end of a scriptline (#13654) Improved performance when parsing URL query string, #13702 Fixed -v argument to add to the end of a scriptline (#13684) Improved performance when parsing URL query string, #13826 Improved performance when parsing URL in the JSON API (#13861) Fixed an issue with JSON payload type that caused script outputs to always include trailing '+' (#13862) Fixed a potential hang if the server received the wrong value for a type parameter when sending JSON", "Prepare release candidate for backport packages (#8891) After preparing the 2020.5.19 release candidate and reviewing the packages, some changes turned out to exist on the backports list already released by backports. This release candidate adds those changes and also updates all other packages in the release so they don't need backports to build. * libdrm: Fix the backport for linux kernel 3.1 (and possibly other kernels), see #1466 (thanks Jan Hegerl <jah@jahnemann.de> for reporting this); * libdrm/drm/bluetooth: Ignore \"gl_pixmap\" in glibc_pixmap.h (by Jan Hegerl) when the driver is", "Change render to render_template in plugins.rst (#13560) Changing render to render_template as BaseView object has no attribute 'render';", "[AIRFLOW-6872] Fix: Show Git Version in UI (#7493)The", "Fix quarantined/flaky tests in test_local_task_job.py (#17385) This PR attempts to fix some flaky tests in test_worker_as_job_test.py, where the test will fail on empty task files with tasks.\n\nFix quarantined/flaky tests in test_utils.py (#17378)", "Fixes quarantine parsing teething issues (#10145) * wrong issue id (from tests) * comment field was copied from statusA.txt", "Fix documentation for PythonVirtualenvOperator (#11700) Fixed the op_args type descriptionThe", "[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701)By", "Remove AIRFLOW_GID from Docker images (#18747) The AIRFLOW_GID parameter was in the images for historical reasons, and now AirFlow needs to be set to NULL so it isn't overwritten. This is a non-security issue (#18832), so we need to fix it. (CVE-2018-634) #18760 : Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764).\n\n: Improve detection of docker images with a few hundred files (#18760) Previously we expected docker images with around 50,000 files, whereas this number is actually the case now. This doesn't affect security at all (#18764). #18768 : Docker and docker-machine should run differently depending on the container host (#18768) We also need to include port 80", "Clarify installation of new packages in docker-compose env (#15433) The problem with installing new packages in the Docker-compose environment is that some packages may not be installed correctly. This behavior has been fixed in the latest Docker version. Newer versions of docker-compose:", "Update persists-credentials (#13401) Previous change to add persist-credentials #13389 wrongly added persists-credentials to other users from the same device while retaining data with existing credentials - now only changes the", "Convert OpenAPI client generation tests to use selective checks (#12092) This test was bundled in with the existing needs-api tests, but was abandoned due to the API's lack of testing support. The test uses the selecttest library for the testing. It includes both a selection test, and a test for the selecttest callback. This tests the client being compiled as intended, including checking that the compiler doesn't emit incorrect headers when the test is run in the context of a specified -h flag. #8109 #8109 Test-suite integration with linting and unit testing (#8105) Test-suite integration with linting and unit testing is back. When using the linting command on lintingtest.py or a module in the module tree, the results of linting/tests and assert.equal() are collected into a report, which is used for unit tests.\n\nAdded the test with the", "Remove redundant code to serialized k8s.V1Pod (#11602)The", "Add Google Cloud Memorystore Memcached Operators (#10121) Co-authored-by: Tobiasz K\u0119dzierski <tobiasz.k.k.dzierki@gmail.com> Status: Rejected\n\n1/12/2015 Resolved: It's", "Cancel queued/running builds on second push to PR (#9050) This uses an action from the marketplace to cancel any running builds for our community and allow for an upcoming build to finish. This is", "[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169) Allows you to let GDT automatically apply the automatic zone.\n\nSupport Autotargeting (#5175) Allows you to auto-target. Previously this was on by default.\n\nAllow GDT to apply an extra time before applying any additional rules to Dataproc (#5171) When Auto-Zone is applied, all GDT rules", "Resurrect python openapi client generator (#19155)When", "set max tree width to 1200px (#16067) the totalwidth of the tree view will depend on the window size like before, but max out the topmost", "Fix is_terminal_support_colors functtion (#9734)In", "Remove vendored nvd3 and slugify libraries (#9136) We pulled in them because slugify _used_ to default to the GPL'd version of the library when a license doesn't apply. This should have been an exception, and it should have been handled with a warning. We now just get an error for this since the exception is not raised: -0.747\n\nFix a build when `--disable-test` is non-1-argument (-0.766) The build would fail if `--disable-test` was", "[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074)It", "[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166) * [AIRFLOW-4397] Add GCSUploadSessionStartSensor ( #4362) * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=2): * Add support for Google Home as well as Chromecast * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Add support for OpenEXR * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug/9458089?rss=0): * Enable the GCSListenerInterface implementation * [SIGTERM](https://bugs.launchpad.net/ubuntu/+source/go-sig/bug", "[AIRFLOW-4265] Lineage backend did not work normally (#5067) * add debug log * change SendMessage to staticmethod * fix some", "Type-annotate SkipMixin and BaseXCom (#20011)After", "[AIRFLOW-XXX] Add autogenerated TOC (#6038)Greetings!\n\n[COUNTY]", "Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926)Branch:", "Improve running and canceliling of the PR-triggered builds. (#11268) The PR builds are now better handled with regards to both running and canceling. (#112322)\n\nRemoved obsolete function \"build_build\" to improve performance and memory usage. (#112744)\n\nThe new builds should be faster on non-Linux systems, notably Linux x86. (This is due to the new build_clean() functions added in the latest version.) (#112510)\n\nFeatures\n\nFixed bug #112831, #112842 when creating packages to use the default \"build_build\" command line argument (which ignored the option \"-m\" or \"--min\".) (#114317)\n\nSupport the new build.target property of \"setopt...\". (#114876)\n\nWhen updating to an incompatible target, do not re-generate install.sh , only uninstall.sh .\n\nproperty", "Add bucket_name to template fileds in S3 operators (#13973) Without that it's impossible to create buckets using for example execution date. (Thanks to Matt J. H. for", "Refactor create_app in airflow/www/app.py (#9291)With", "Don't add User role perms to custom roles. (#13856) closes: #9245 #13511At", "Chart: Fix ``PgBouncer`` exporter sidecar (#16099) An extra colon crept in and was breaking the pgbouncer exporter and", "Remove datepicker for task instance detail view (#15284) Closes #15261 by removing the datetimepicker and replacing it with a datepicker. #17043 Fixes (#15270) Fixes a crash introduced in OSX where task items were duplicated due to the new OSX version. #17018 Fixes (#15256) Fixes a crash with multiple images showing from a single", "[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262)R.W.", "[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)Dynamics", "AIRFLOW-6062 Watch worker pods from all namespaces (#8546)Still", "Don't try to push the python build image when building on release branches (#15394) They use the same python image as master (as already mentioned in previous section) instead of the latest one, this causes an issue in the .py build script where you probably want the newest", "Fix typo in api_connexion/openapi/v1.yaml (#9986) `startd_ate_lte` no longer", "[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914)Moved", "Fix session_lifetime_minutes config docs (#12628) - Update `version_added` to 1.10.13 - Better format of docs (#12513) - Make gconfig-editor configurable by logging", "Invalidate Vault cached prop when not authenticated (#17387)This", "[AIRFLOW-6517] make merge_dicts function recursive (#7111)For", "[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677)This", "stop rendering some class docs in wrong place (#8095) * stop rendering some class docs in wrong place Docs generated for providers.yandex.org have been updated into documentation/ providers.yandex.org will be treated as new docs and will be updated in updates.* add error flag to prevent auto-adding documentation/ for docs.yandex.org for example with documentation/ providers.yandex.org will only be updated against documentation.yandex.org * fix some errors of docs/documents.yandex.org for example with docs.yandex.org in docs/ docstring is missing * add some class docs for some classes", "[AIRFLOW-6262] add on_execute_callback to operators (#6831)I", "[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)I", "Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354) Co-authored-by: ShakaMukumar\n\nRefactored ``LoggingCommandExecutor`` to", "Do not override in_container scripts when building the image (#10442) After #10368, we've changed the way we build the images on Windows (from in-place to in-process!) It might help avoid an unexpected conflict when we build the image from the command line. Thanks @joshmcdaniel for the report :) #10368 [master] add support for custom-generated image paths for imgTags (#10919) Fixed a possible crash when building Images with the wrong set of images when doing an empty build in -g (#10924) [master] fix crash by not detecting images for #10943 (#10958) #10914 Update #10901 fix for Image::link() failing (#10927) #10860 Update #11065 Fix issue in Image::build_with_dir when building images using the -c option without directories (#10925) #10856 [master] add -O option for Image::build_with_dir and", "Remove redundant asserts in tests/www/test_views.py (#12176) Methods 'check_content_not_in_response'/'content_not_found' (https://github.com/libjunit/junit/issues/12176) and 'check_content_not_in_response/content_not_found' (https://github.com/libjunit/junit/issues/12176) still don't pass as expected with non-json queries, due to incorrect test assertions being marked as wrong on the test runner path. The test cases have also been", "Pass SchedulerJob.subdir to Dagbag (#13291) Because `SchedulerJob.subdir` was not used in Airflow, the scheduler will exit with a false exit code, as is normal (although I should note that the last check that comes for this feature was done in Airflow 7.28, and thus the last exit code is false. In any case, it looks like we", "[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700)The", "Upgrade ``importlib-resources`` version (#18209) Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com> --- 2.0.2b2", "Persist tags params in pagination (#15411)What's", "[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696)There", "Add TargetQueryValue to KEDA Autoscaler (#9748) Co-authored-by: Daniel Imberman <daniel@astro.ac.uk>\n\nBug", "Increase time limit for Helm chart unit tests (#20525) Sometimes the helm chart unit tests exceed the allocated time for the job for Public Runners by a day or more. In that case this function will hang", "Add Apache License to .github/workflows/repo-sync.yml (#10229) `.github/workflows/repo-sync.yml` is a configuration file.", "[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772)On", "Change Airflow version to 2.0.0a1 in Updating.md (#11508)Bundling", "fix broken link in experimental API deprecation headers (#13547)\"", "add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066) * add separate example dag and system test for GCSToGoogleSheetsOperator (#9067) * add separate example dags and system tests for Google", "Adds warning about using dynamic installation of packages (#16935) While we are supporting installing packages dynamically in our helm chart and docker compose while testing, this has a few issues since we can add some dependencies: The current docker compose works for us, but it will make the output of docker compose less human readable. We can find an easy", "[AIRFLOW-6362] Fix typehint for CommandType (#6906)In", "Doc: Fix incorrect filename references (#20277) Minor typo corrections. I changed the filenames in the example folder structure instead of the later references, because it gave me a lot of trouble fixing the", "Change from Instance attribute to variable in JdbcOperator.execute (#7819)[b][#8212]", "Add issue form template for Helm Chat (#17917) With so many people reviewing nobody noticed that we forgot to add Helm Chart issue form :) This rectified a couple", "Added Zalando to ``INTHEWILD.md`` (#18480)This", "Use sys.exit() instead of exit() (#12084) The `exit` and `quit` functions are actually `site.Quitter` and `site.Quit` (#12083) The `next` function should handle newlines and tab characters as intended (#12084) The `--formatting` option was intentionally omitted #12089\n\nPerformance Improvements\n\nThere are some significant performance improvements in this release.\n\nThe `nested` feature has become the default in some places like some scripts containing `--script-directory|` to allow more script options being combined", "Correct typo (#20345)There", "[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)I", "Excludes .git-modules from rat-check (#14759)The", "[AIRFLOW-4681] Make sensors module pylint compatible (#7309) Remove all references to sensor modules from pylintt() and module() (#7626)\n\nBugfixes in pylintt()\n\nMake pylintt() install script less prone to breakage\n\nImprove bug support for the C99 and POSIX standards modules\n\nBUGFIXES in pylintt() (with patch from Senthil Venkatesh)\n\n(with patch from Senthil Venkatesh) Fix typo in \"dont_read_print\" clause and", "Fix wait-for-migrations command in helm chart (#12522) If the migrations weren't yet applied this would fail with `NameNotFoundErrors` if there were errors in migrate commands such as: \"Error: Cannot set `name\" of \"Person::new\".\n\nIf a migration fails it", "Run mini scheduler in LocalTaskJob during task exit (#16289) Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is ~50%. Fixed issue for job running on local task during task exit (#16290) Fixed bug in LocalTaskManager where task not waiting was shown in detail (#15995) Fixed a crash bug (http/http request), and more (#15765)\n\nV0.3.1 (#16285)\n\nv0.3.8\n\nFixed bug in LocalTaskManager when job fails (#14656)\n\nFixed Bug in LocalTaskManager when executing a task (#14215) Fixed a crash bug (#14652)\n\n\nv0.3.10", "Change prefix of AwsDynamoDB hook module (#11209) * align import path of AwsDynamoDBHook in aws-dynamodb.yml * change the configuration file", "Small improvements for Airflow UI (#18715) I slightly improved some small UI elements that were a bit off. Changes: - Removed `btn-airflow-menu-icon` and `btn-airflow-menu-type` (both had their functionality removed. The `btn-airflow-menu-icon` was an icon that was always displayed when a single button was clicked, which wasn't being used anymore since the new version of Airflow supports double tap to dismiss). - Removed `Airflow-button` from `Settings->More/Favourites/` (#19583) As you can see, changes were made that will make Airflow behave better (especially for users with multiple Airflow screens). It's really only relevant if you use different screens from time to time. You can switch between different screens or use both screens as shortcuts in a single Airflow session. You can also switch between screens using a combination of the two controls mentioned above. AirFlow now", "Add conn_type to Fix failing Livy Tests (#9258)Dependencies:", "Google Ads Hook: Support newer versions of the google-ads library (#17160)On", "[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962)On", "docs: TESTING.rst: fix not loading image (#14247) [This image loads](https://github.com/apache/airflow/issues/130) . This problem is fixed. [This image loading will", "Add link to docs index to table of contents (#12594) Without this, it's not obvious how to get back to the main pagePosted", "[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625) PostgresHook's postgres_hook_add_user() method cannot be invoked with a new row. (#8626) No crash when a PostgresHook is closed. Fix crash when connecting with user account not created yet (#8632) Fix crash when saving Postgres table after database reload in PostgresHook#reset (#8640) Prevent crash when reloading Postgres Hook with passwordless user (#8696) The function that creates new user with Postgres Hook.prepare() is called twice more than usual (#8618) Fix", "Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867) Fixes the BaseSensorOperator to make respect the upstream tasks when a task fails for", "Add typing for grpc provider (#9884)Ripple", "Properly propagated warnings in operators (#9348) * Test warnings are properly propagated * Adjust deprecation warnings * Separate tests and extensions to maintain", "Dag bulk_sync_to_db dag_tag only remove not exists (#8231) * Dag bulk_sync_to_db dag_tag update (#8227) * Dag bulk_sync_to_db commit hook updates (#8052) * Dag bulk_sync_to_db remove not exists (#8192) * Dag bulk_sync_to_db update hook updates (#8173) * Dag bulk_sync_to_db", "[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496) * [AIRFLOW-5704] fix error when adding items to the queue in the Kubernetes cluster and update log for cluster_info * [AIRFLOW-5704] fix local tests failing to build * [AIRFLOW-5704] fix local tests failing to create local test suite * [AIRFLOW-5704] fix tests failing to run against machines outside of their local cluster * [AIRFLOW-5704] fix local tests failing to pass benchmarks * [AIRFLOW-5704] fixes tests failing to pass benchmarks * [AIRFLOW-5704] fix test failing when creating a local test suite * [AIRFLOW-5704] fix test failing when creating multiple cluster configurations (with multiple machines) * [AIRFLOW-5704] fix local tests failing to run when creating multiple cluster configurations which have", "[AIRFLOW-3341] FAQ return DAG object example (#4605) * added example of a function returning a dag objectThe", "Better message when Building Image fails or gets cancelled. (#11333)How", "[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)This", "[AIRFLOW-4883] Bug-fix for Kill hung file process managers (#5639) Previous PR (#5605) was missing some documentation needed to help the user understand the effect a particular file", "Remove locks for upgrades in mssql (#17213) Closes: #17088The", "[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396)This", "Improves quick-start docker-compose warnings and documentation (#18164) The recently updated docker-compose had a bit broken behaviour for non-standard inputs. This update adds a couple of options to warn you about that: [x86] Use --extra-debug instead of --extra-info to get additional information about running containers. For example: docker image --extra-debug --extra-info --type=command=node <docker-compose.yml> will set --extra-debug and --extra-info options for the docker-compose.yml . The --extra-debug option is useful if you want to see how your containers are running (e.g. by", "Use the correct link for Apache Airflow Dockerhub repo (#13752) https://hub.docker.com/repository/docker/apache-airflow/releases/current/0.5/ Apache Airflow Dockerflow repository (#14016) https://upstack.apache.org/projects/docker/apairflow/releases/current/0,0/", "Add reattach flag to ECSOperator (#10643) ..so that whenever the Airflow server restarts, it does not leave rogue ECS drivers exposed. We still expect the following, however. #10644 Update the documentation about \"Air", "Add aws_conn_id to DynamoDBToS3Operator (#20363)New", "Use Debian's provided JRE from Buster (#8919) Installing the JDK (not even the JRE) from Sid is starting to break things. The new version of JRE/JRE-<version>-<br> can be found here .\n\n. When using jdt-unstable the packages used to build the binary are renamed.\n\nThe /boot partition is now called .\n\nversion.\n\nWhen running the JIT compiler, new version flags will be passed with each JIT command. The default is 0 , but a value of 1 is better. See the -f option when compiling.\n\noption when compiling. The default is , but a value of 1 is better. See the -f option when", "[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106)GCS", "Improve typing in airflow/models/pool.py (#9835)As", "[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123) * [AIRFLOW-6062] Fixed a bug where it wasn't possible to remove sub-classes (class and object) with get_class_with_name() * [AIRFLOW-6086] Fixed crash for worker that wasn't being terminated with stop_worker() * [AIRFLOW-6086] Fix error handling with set_child_parent_with_name() * [AIRFLOW-6086] Set parent of worker", "[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434)Atari", "Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665) These operators provide the ability to pause and resume the redshift cluster.", "[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552) * [AIRFLOW-5850] The client connects to the Server using docker swarm. The connection can be suspended. * [AIRFLOW-5850] The client connects to the Server using docker swarm. The", "Fix typo in docker-stack documentation (#16221)\"\n\n\"Fixed", "[AIRFLOW-4416] Revert \"Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)\" [HDR] [BZ] [SEL]\n\n2017-01-21 20:13:54.593 [0x8000fec000]", "Fix occasional cleartask failures (#18859) The cleartask tests occasionally failed due to not consistent sequence in which task clearing was performed. The default should prevent repeated failures. (b9fb68c)\n\nWhen the task name is something other than \"Task #3, Clearing\"", "Add error check for config_file parameter in GKEStartPodOperator (#17700)The", "Add API Endpoint - DagRuns Batch (#9556) Co-authored-by: Ephraim Anierobi <splendourbabes@gmail.com>", "Improvements for `SnowflakeHook.get_sqlalchemy_engine` (#20509)The", "Add some basic metrics to the Triggerer (#18214)In", "[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359) This was previously an error because it would create a duplicate PROXY_ADD_DUPLICATIONS_OR_WEAKS_THAT_DOESNOT_GET_A_NEXT_FIX_NUM.\n\n(SAP) Add missing default values for PROXY_FOREIGN_LANGUAGE_VALUE (PROD_FUNC_FUNCTION)\n\n(SAP) Remove unused global variables from the FUNC_STARTUP, FUNCTIONS and FUNCTIONS_STARTUP (PROD_FUNCTION);\n\n(X11) Don't allow the -E option to be used with XFree86-EFI-32 (#6935), which creates many bugs\n\n(X11) Fix the case when XFree86-CORE/QNX uses", "Add type annotations to ZendeskHook, update unit test (#10888) * Add type annotations to ZendeskHook __getMethod(). (#10889) (Ezra) - Support for .getMethod() on Zendesk class: update the code (Ezra), add support for a .getMethod that gets an instance and is then passed to methods like methods/getAllMethods() , method/getAllMethodsExpr() . (#11003) * Support for .getMethod() on Zendesk class::getInstance() and getResult() * Add type annotations to ZendeskHook, upgrade the tests for .getMethod() tests. (#10888) * Support for ZendeskHook __getMethod() using ZendeskExpr types, instead of .getMethod() . (#10889) * Add type annotations to ZendeskHook; upgrade the test for Z", "[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524) - `security_context` was missing `proxy`. (#3369) Fixed a bug in `io.net/\u200bnetworkio`. (#3367) Updated `nshttp.h`, `net/\u200binet6.h` implementation to use HTTP 2.0 by default (#3066) - `net/\u200binet6.h` implementation removed `HttpClient` (#3070) - Fixed `net/inet6.h#1217.1` to use HTTP 2.0 and use `ProxyPass` (#3384) Fixed crash on Windows (#5228) - Fixed issue with netcat connection. - Fixed missing `http://` before `http_get(). (#2789) - Fixed crash for older version of netcat (#4365, not supported on Linux, FreeBSD, Mac OSX, OpenBSD etc..) - Fixed bug #3147", "Add Parquet data type to BaseSQLToGCSOperator (#13359)In", "Mark passing pre/post execute callbacks to operators as experimental. (#18140) My primary concern here is that by being able to arbitrarily \"change\" if an Operator can be written, that operator and its \"superclass\" will become incredibly easy to write without needing to care about the actual implementation at all in order to make them useful. However the above concerns are mitigated for the now available standard (see below).\n\nPipe\n\npipes\n\nPipes are all that matter when building applications with async/await in Elm,", "[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690)I", "[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)This", "Fix formatting in AWS Connections docs (#8223)\"", "[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003) [AIRFLOW-XXX] 1-setup-env.sh should only run in docker", "[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431)One", "Remove unnecessary messages in CI (#7951)Description:", "Clarifies version args for installing 1.10 in Docker (#12875) This change clarifies that AIRFLOW_VERSION should be passed together with the version of FLOW_VERSION as an argument. This removes", "Submodules are needed to update constrains (#15242)I", "Fixing MyPy issues in testa/jobs (#19998)As", "Chart: Use stable API versions where available (#17211)\"\n\nFix:", "Fix typo in docker-context-files/README.md (#12078) `par` -> `part`Gekko`", "Add schema as DbApiHook instance attribute (#16521)One", "Fix cached_property MyPy declaration and related MyPy errors (#20226) Part of #19891If", "Docs: Better description for `pod_template_file` (#16861) In Airflow 2+, `pod_template_file` now uses plain text for metadata instead of nested files as described in the documentation", "Fix error message in production entrypoint.sh (#8396) Fix non-existent BACKEND environment variable, replace with DB_URLThis\n\nThe", "Add ME-Br to who uses Airflow list (#9770)As", "[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717)Bless", "Simplify release process for PyPI snapshots (#13020) Setuptools has a built in mechanism for adding a `suffix` to a release build. If in the future a build fails due to a bug (or the upstream maintainer forgot to add support for a version), this suffix is used to add a new test to the build to fix it (which is typically", "[AIRFLOW-5680] Fixes Kubernetes hangs (#6347)This", "Implements generation of separate constraints for core and providers (#14227) There are two types of constraints now: * default constraints that contain all depots in the network (see docs) * custom constraints from provider objects #14152\n\n* support for generic providers (similar to providers.generate) #14137\n\nImprovements:\n\n* The new generators documentation (#14151) gives examples of the generation process. It covers the core constraint generation, and the custom provider generation for generic services (#14151).\n\n* The generated providers docs now include the docs for all provider classes, including generic providers (#14152).\n\n* Improved handling for the new provider class generator. It is based on \"create provider\", which has many of the features of \"run provider\" (#14147).\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create provider\", a generic provider generator\n\n* Support for \"create providers\",", "Workaround occasional deadlocks with MSSQL (#19856) We already have a mechanism to retry operations that could result in temporary deadlocks - that is, if you use locks to do a transaction and then try to start a transaction later on. If you use mutexes, you would be stuck. (Issue #19857, #19867)\n\nIn some places, we added MSSQL replication support in MySQL 5, and there needs to be a way to tell it where to look for transactions to replicate it to. (If this is the case, I think it'll help us when looking for ways to improve performance when running on", "Databricks hook: fix expiration time check (#20036) There was a logical error in the check of expiration time that could lead to authentication failures. You might want to", "Add Pinterest to Airflow users list (#19117)If", "Prevent mixed case env vars from crashing processes like worker (#14380) * Handle misformed env vars without crashing * Include traceback in exception if not handled by tracing mechanism *", "[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671) * AIRFLOW-5477 * [AIRFLOW-5068] Fix incorrect warning when using some queries by using invalid_query_parameter() in query_config() * [AIRFLOW-5089] Fix wrong column name for HQL query in multi_field_by_name table (#5681) * [AIRFLOW-5494] Fix crash when accessing nested tables with multiple queries (#5433) * [AIRFLOW-5097] Fix crash when accessing data_model_by_column", "Also check chart schema when the schema itself changes (#15902) This changes our schema pre-commit hooks to also run when the schema itself changes, making it much easier to keep up-to-date", "Allow to define custom XCom class (#8560) * Allow to define custom XCom class closes: #8059A6", "[AIRFLOW-3601] Update operators to BigQuery to support location (#6020)If", "[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036)In", "Gracefully handle missing start_date and end_date for DagRun (#14452) closes: #14384 This PR fixes two issues: (1) the fix for missing start_date and end_date is not applied to running projects (the default behaviour) and (2) it leads to incorrect code when using (2). Fixes #14393 (partial).\n\n(partial). Fixed bug #14355 (when using a library's default constructor), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial).\n\n(partial), caused by using the wrong method name and/or argument types for the \"new\" operator. Fixes #14356 (partial). Fixed bug #14501 (code in a get/set/delete hook might cause issues with objects created or updated after this branch was merged). Fixes #14358 (partial).\n\n(code in a hook might cause issues with objects", "Remove unnecessary string concatenations in AirflowException messages (#18817)For", "Fixes failing test_views tests (#14599) This reverts commit 49952e79b04da932242ebf3981883dfd2ebab6fba2.\n\nFeatures\n\nSkeuomorphic animations (#15121) This reverts commit 9054e2ed99f6d0b7525f07cfdbc6e0ea2ba2d081, which caused a few test failures with this animation feature. See #15104. Changes in 2.1.4-rc2-1\n\nUpgrade to V8 2.8.23.\n\nUpgrade to V8 2.8.22.\n\nUpgrade to", "Update Spark submit operator for Spark 3 support (#8730) In spark 3 they log the exit code with a lowercase e, in spark 2 they log the return code with a space before the space. This results in different exit", "Fix broken MSSQL test (#17797) This broken test was causing the next test to use the db to fail. Also, by not ignoring the MSSQL test, my test wasn't failing; in fact it was failing as expected: [TestDriverTest] #import <sqlite3.h> [TestDriverTest #run] [ExceptionHandler]", "Improve language of a BaseSensorOperator in UPDATING.md (#10332)For", "[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868) Recently we hard code in tutorial.rst where we explicitly include some optional parts by adding @include :\n\n{ \"main.c\" : #include \"myfuncode.cc\" , #include \"prog.c\" @include : [ \"myfuncode.cc\" ], #include", "Faster default role syncing during webserver start (#15017) This makes a handful of bigger queries instead of many queries when syncing the default role, though it's still better because it improves concurrency in the core. (This is really hard to evaluate, since they don't really know to", "Don't reference sphinx airflow theme via `@` URL in requirements. (#12957) PyPI rejects uploading a dist with an `@` URL but still", "fix tests (#11368)When", "Update node installation cmd (#10744)When", "Swap dag import error dropdown icons (#18207) - Open/Close icons were backwards. This swaps them to be consistent with the expand/close buttons. (#18209) [fixed] - Added", "[AIRLFOW-XXX] Display other integrations in single table (#6133)If", "Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052) * Replacing non-attribute template_fields for BSP_CAS (replacing non-attribute template_fields for BigQueryToMsSqlOperator); * Replacing", "Add a Docker Taskflow decorator (#15330) Add the ability to run @task.docker on a python function and turn it into a Docker event in a process builder\n\non a python function and turn it into a docker event in a process builder Replace task.docker.run('my_task'), which I previously added, with 'file.sh'. This solves a few problems here\u2014as you can see in the code above, the output file gets added and processed directly into a directory before a task can launch an event.\n\nTask flow and image configs \u2013 In the case of my app not being able to start if a task cannot connect to an image, the workflow now allows changing our image to use our task instead.\n\n\u2013 In the case of my app not being able to start if a task cannot connect to an an image, the workflow now allows changing our image to use our task instead. Fix dockerfile.sh \u2013 This has been a pain in", "Fixes uploading of doc artifacts. (#10441) Requires #10470 and #10472F\n\nNew", "Unify command names in CLI (#10720) * Unify command names in CLI * fixup! Unify command names in CLIA", "[AIRFLOW-4000] Return response when no file (#4822)Tekmara", "Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024\n\ncheck operators (#12677) closes: #10271 related: #9844, #15018, #16101, #17024 Test: don't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569\n\ndon't leak the object reference when changing property (#15192) closes: #10148, #13855, #14569 Test: remove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283\n\nremove old tests of getNextItem() (#15194) closes: #9722, #10852, #11714, #11283 Test: avoid double call(Object#value,", "[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214)Ajax,", "[AIRFLOW-5690] Change log level local_task_job.py (#6422)Branch:", "Add test connection method to http hook (#16568)Widgets:", "Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018) Co-authored-by: Felipe Sosa\n\ncommit db0a4fb4bc0b8a8cec2a2c8928b6f6ebc2b3feb Merge: f2ddfb5 6ca4f3f Author: R\u00e9mi Verschelde <remi@verschelde.fr> Date:", "[AIRFLOW-3773] Fix /refresh_all endpoint (#4597) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4587) * [AIRFLOW-3773] Fix /refresh_all endpoint (#4818) Thanks! https://github.com/airflow/airflow/issues/4597 * [AIRFLOW-3773] Don't drop HTTP status code (#4482) Thanks! https://github.com/airflow/airflow/issues/4587 * [AIRFLOW-3773] Revert \"change the client's default transport\" (#4466) * [AIRFLOW-3773] Add the \"customer_id\" parameter to the client_id parameter parameter #4224 * [REMOTE-3766] Fix /close endpoint (#4433) http://api.airflow.com/airflow/api/airflow.de/", "[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)Binary", "[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560)I've", "Fix installation doc (#13462) The note should not be in the Bash code-blockIt", "Update INTHEWILD.md (#12060)If", "GCP Secret Manager error handling for missing credentials (#17264)On", "Improve breeze resource check (#17492) The resource check in breeze was slow (3 docker commands instead of one) and it used an extra image which did not work properly. This fix allows the image to be configured with the command specified without making you use the wrong command. Added: Option to skip a given Dockerfile (#17497) If Dockerfiles do not exist they won't", "Adds Github Oauth example with team based authorization (#17896)The", "Add taskflow to accepted words (#11902)(#11902)\n\nAdd", "Remove reimported AirflowException class (#9525) It is imported at the top of the file and L1060 tooBy", "Move role guide to access control (#10755)In", "Fix file name to verify release packages (#16605) typo: `check.files.py` -> `check_files.py`Image:", "Doc: Restoring additional context in Slack operators how-to guide (#18985) A recent update to the Slack example DAG removed some context of operators and returned a new context. This step removes context by removing /etc/migrations in the end point so that operators don't have to worry about whether or not to do things with the file, or whether to even try to write operations. (#17545)\n\nLogger\n\nMake it easier to log to the Slack context as a", "[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099)The", "The verbose functions will not exit immediately if not asked to (#10731) The docker(), helm(), kubectl() functions replace the real command names with the output, for compatibility with versions of the command before 2.6.8, these functions do not return a response except if it was successful (#10734) The docker.connect() function does not accept arbitrary parameters by default (#10565) The docker.exec() function does not accept arbitrary parameters by default (#10564) The dnfcdd.docker() function does not accept arbitrary parameters by default (#10544) The dnfcdd.docker_connect function accepts an arbitrary URL (via curl) as hostname instead of just the docker directory name (#10552) docker/docker() behaves incorrectly when using the command name of something that is still a directory (#10552) The docker/dockerlib.dirname function should return a single file path instead of an arbitrary number of ones (#10553)", "Use DAG context manager in examples (#13297)Bump", "Docs for multiple pool slots (#20257) Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>", "[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdags Co-authored-by: Charles-Coulomb Csomanski <charles.csomanski@gmail.com>", "[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330) Moves the airflow_local_settings to the correct folder after import. Fixes #5328 and #5426\n\n(#5330) Moves the airflow_local_settings to the correct folder after", "Adds provider package documentation in installation.rst (#12203) Addresses initial version of #11880S.", "Chart: Add custom labels for ingresses/PVCs (#20535)There", "Fix breeze redirect on macOS (#14506)The", "[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886)On", "Pin moto to <2 (#14433) https://pypi.org/project/moto/#history -- moto 2.0 <2 <2 https://pypi.org/project/moto/#history * moto 2.0: Fix #14437", "Move out get_python_source from www, Move get_dag to www.utils (#7899)We", "Set default logger in logging Mixin (#20355) Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandware.com>", "Added Kayzen to INTHEWILD.md (#16154) Added Kayzen to the list of companies using Apache AirflowThe", "Clean up the pre-commit config file (#15681)Pushing", "Fix backwards compatibility issue in AWS provider's _get_credentials (#20463) The #19815 change introduced backwards incompatibility for the _validate_tls option in AWS CloudTrail. (Hint: Make sure that _get_credentials is used from before).\n\nWhen using the Amazon Web Services CLI tool to configure an EC2 instance, you may encounter a \"proxmox failure\" with the command: Amazon::AWS::Configuration::getInstance().registerWithFile('myconfig', ...)\n\nBug #19710 when adding credentials via the CLI if the credentials are not yet in the local storage (e.g. when a bucket is empty, the instance is still available)\n\nYou will end up with an unsecured AWS object. You may need to delete or reformat the bucket before adding that AWS object to EC2", "Simplify the K8sExecutor and K8sPodOperator (#10393) * Simplify Airflow on Kubernetes. (#9882) * Add API support for the pod-spec module. This module is not yet finished, but I have begun testing it. * Implement support for Docker Compose. (#9730) * Allow k8s-api-test to test the pod operation, see http://kb.docker.com/Docker/en/latest#K8sAPI Test for k8s-jenkins. (#9740) * Implement support for Docker Compose. (#9505) * Support a docker-compose", "Cope with multiple processes get_remote_image_info in parallel (#9105) When I'd made a change to a large number of python processes, there was usually no process named in Python and we were having issues with getting a handle into the process that would get an image. However, if you just moved a python process, with no context or context with the existing processes, that would work - there might have been some process called \"my_process_1\" with a handle from its parent, but that didn't matter. In Python 3, the new method get_remote_image_info uses the context of a process if a context", "Fixed button size in \"Actions\" group. (#17902)1)", "Add muldelete action to TaskInstanceModelView (#18438)From:", "Switch to released cancel-workflow-runs action (#10423) Follow up after #10368C", "Separate Installing from sources section and add more details (#18171) This PR separate installing Airflow from sources section and also fixes links for binary download. Also, make app launcher more helpful (#14761)\n\nThis PR separate installing Airflow from sources section and also fixes links for binary download. Also, make", "Removes unnecessary function call (#15956) No need to make this call, as if no perms are passed `sync_resource_permissions' no longer exists.", "Disable Helm tests when branch is not main (#17457) We are preparing Helm chart from main branch only and we never run it from airflow version branches (which should already match branch #1609), so we do not include the chart when testing for new", "[AIRFLOW-XXX] Fix doc error (#5179)Here", "Update hashicorp-vault.rst (#20348) * Update hashicorp-vault.rst Co-authored-by: Alexey Kuznetsov", "Add docs about Celery monitoring (#14533) Part of: #11161 To have a full description of the monitoring of all Airflow components, you will also need to enable airflow: (default) Yes\n\nNote: Please ensure you have enabled some monitoring settings in the app settings for the appropriate package and running app. If the device is not working, check the status of these monitoring settings under the Advanced tab for the respective", "Added notification to solve \"docker-credential-service-error\" (#18524)Tested", "[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504)What", "[AIRFLOW-XXX] Remove profiling link (#4602)One", "Enables back duplicate cancelling on push/schedule (#11471) We disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are the same (#11407).\n\nWe disabled duplicate cancelling on push/schedule in #11397 but we now have some more options for it. The code is still quite verbose at this moment but it is now possible to disable for an entire push/schedule if all the parameters are", "Disable experimental REST API by default (#12337)In", "Adds a forgotten word in a README.md (#12066)When", "Remove flask-admin based Plugins (#11515)It", "Fix mypy apache kylin operators (#20595)The", "Add specific warning when Task asks for more slots than pool defined with (#20178) In cases where task asks for more pool slots than the total number of slots defined, if the number of slots required exceeds the available pool you could find that, from our perspective, this indicates that something is wrong. This kind of event has to happen at least once every 15 seconds, based on our benchmarks. If you were using tasks.getPoolLists() and not all tasks were waiting for this event, things should be able to run much faster now and a single instance of this code could be enough to improve the behavior of your task.\n\n\nTask.addProperties() for instance of tasks.getPoolLists() now returns proper object for objects of the same object type (instead of \"Object reference object,\" if you had object type of Object instance in", "Support DAGS folder being in different location on scheduler and runners (#16860) There has been some vestigial support for this concept in older versions of Docker, using libcsh and it is also required to be implemented in FreeBSD or FreeBSD-based distros if you want to use other dependencies such as Mota etc. However, this functionality is still missing in newer releases. This is now resolved by adding this hook: \"libcsh_runhooks_exec.csh\" In Docker", "Fix Experimental API Client (#9849)In", "Show DAG serialization errors in the UI. (#12866) The previous behaviour led to \"bad\" data being written in the DB -- for example, the number of rows was reduced. Now, we just add a line when creating. Also, the column name and ID used to include the query string, is now used to keep the query consistent if the query is a sequence -- i.e., there is no need to include the", "Update example on docs/howto/connection/index.rst (#10236) * Upddate example on docs/howto/connection/index.rst (#10261) * Various other tests *", "Add function to get current context (#9631) Support for getting current context at any code location that runs under the scope of BaseOperator.execute() (#9517) Added a setter attribute for the get() method (#9510) Fixed", "Adding missing word to welcome message (#16726)Dry", "Fix broken master (isort fix) (#11954) Static checks are failing because of a Bad merge to Master.The", "Adds automated installation of dependent packages (#11526) When extras are specifying when airflow is installed, this one triggers installation of dependent packages. Each extra has to specify when it's installed. So you could now have two different versions at all times: On the one hand, airflow will install for the next day and install for the next day and night, then you can specify in which way. The other thing is for the next package install, I could not get the packages install", "Reorder middleware - ProxyFix and BaseUrl (#8157)The", "Moves provider packages scripts to dev (#12082) The change #10806 made airflow works with implicit packages when \"airflow\" got imported. So the \"expose\" option got removed but the scripts, which could only be done using \"pkgutil\" did not work yet. This was changed (#12101)\n\nThe script \"pkgutil\" no longer makes sure if packages were added or removed via explicit commands. If they were added the module-info of the module in question is now visible (the check of whether it was updated). In order to", "Override project in dataprocSubmitJobOperator (#14981)With", "Upload provider distribution artifacts during CI (#19807)We", "Add support for arbitrary json in conn uri format (#15100) Currently in airflow web UI and the CLI you can store arbitrary (e.g.: json) data in conn uri format. In that case conn uri format will also look for the data by name which is great for data consistency. Also on web browser it will search for the", "[AIRFLOW-3217] Button to toggle line wrapping in log and code views (#4277)The", "[AIRFLOW-6820] split breeze into functions (#7433)The", "Restore airflow.www.app.csrf to avoid breaking change (#9402) Co-authored-by: Tomek Urbaszek <tolek@wip.io>", "[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488) * [AIRFLOW-XXX] Add How-To-Guide to GoogleEncryptsToSFTPOperator", "CI: Propogate Exit Code Correctly (#9247) This was unfortunately broken since #9138 Co-authored-by: Ash Berlin-Leischner Fixed", "Add type annotations to providers/vertica (#9936) Co-authored-by: Johan Eklund <jeklund@zynga.com>", "When precommits are run, output is silenced (#10390) The output of pre-commit builds on both CI and locally is now limited to the first commit. For this reason, it is discouraged to commit from the main tree right after precommits are run. This change takes care of some of the issues with certain CI builds that often occurred after precommits were run during development. The change does not impact all build systems. For", "Add note about using dag_run.conf in BashOperator (#9143)For", "Test exact match of Executor name (#10465) Use `self.assertEqual` instead of `self.assertIn` to do an actual EOF test (#10463)\n\nUse `self.assertForked` instead of `self.assertFork' (#10031)\n\ninstead", "Added json_render method to separate filtering from view (#14024)I", "[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863)The", "Add placement_strategy option (#9444)We", "Use sys.exit() instead of exit() (#10414) The `exit` and `quit` functions are actually `site.Quitter` now. You also get these two other changes: The `csh' and `bash' commands now return exit and status codes, instead of exit and error codes (#11495)\n\nand commands now return exit and status codes, instead of and errors (#11495) You use '`file' instead of `(file' .. '/etc/passwd ...')' (#11634)\n\ninstead of (#11634) The `commandname(name)' command is now handled and used to escape shell name\n\nThe `ciphers' field has been eliminated", "[AIRFLOW-5139] Allow custom ES configs (#5760) * AIRFLOW-5139 Allow custom ES configs While running on an OSX host, this will auto-discover the configuration. This feature will work against OSX hosts running VirtualBox. A default configuration will be returned to you if it is not found. All of this is available through the", "[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051) * [AIRFLOW-5506] Add a new option for controlling whether the client device is always offline * [AIRFLOW-5423] Add GKEConnectItemPropertyCallback to detect", "[AIRFLOW-6959] Use NULL as dag.description default value (#7593)The", "Fix failing backport packages test (#13497) In #13473 - I updated the deprecated packages but looks like it broke backport packages: ```. c2gfx.c2gfx (9ff00eb2), #13507. libs.aio-util.c2gfx.aio (3bb4efe0), #14322. d2gfx.c2gfx (94bb834e), #14016. db2gfx.c2gfx (c7dd8ca4), #14317. d2gfx.c2gfx-dev.c2gfx.d2gfx (69dabb05), #14054. utils.aio.aio", "Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876) 2.0.1 bugfix: Do not store configuration of OpenVPN agent in", "[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber\u2026 (#6606) * [AIRFLOW-6013] - Support for a new pod registry system\u2026 (#6459) * [AIRFLOW-6012] - kubernetes-ip-config", "Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries. This is the actual syscall that the kernel takes when", "Limits CodeQL workflow to run only in the Apache Airflow repo (#11264) It has been raised quite a few times that workflow added in the last patches can't run code in airflow.\n\nBug Fixes:\n\nFixed problem with new features not showing up to all the active developers. Fixes #9077\n\nFixed issue with not accepting a log message as the latest command line option. Fixes #11296\n\nFixed a regression in \"set all\" method that caused stack trace to be incorrect. Adds #11321\n\nAdded option on \"list-tags\". No longer prompts users if they don't have tags specified. Adds #11276\n\nFixed issue where \"list-items\" would not trigger a log message and would silently truncate all messages. Fixes #11290\n\nImproved ability to add / remove tags on the fly from the \"List Tags\" tab. Makes sure to keep log messages separated. Adds #11229\n\nImproved", "Fix failing spelling check on Master (#15998) For some reason https://github.com/apache/airflow/pull/15972 was green-lit for this Pull Request. Due to the change, we have to revert Master", "Update install_mysql.sh (#12101) After Debian 9 and according to the manual https://manpages.debian.org/stretch/manual/install_mysql.conf you will need to install:\n\n$ apt-get install libmysqlclient-dev -y\n\nOr if you want to use the MySQL drivers you need to install:\n\n$ apt-get install libmysqlclient-dev libmpg123_dbg -y\n\nNow you can connect to the MySQL database from a web browser. For example", "Add Apache Airflow CODE_OF_CONDUCT.md (#9715)Mint", "[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611)", "Updating the InfluxDB example DAG to use the TaskFlow API (#18596)Pushing", "Fix providers tests in main branch with eager upgrades (#18040) The SQS and DataCatalog were failing tests in main branch because some recent release of the database engines had changed the way the tests were created and they didn't have enough time to find the missing dependencies in each test suite, causing the tests to crash when attempting to run them. This is fixed now.\n\nBug Fixes:\n\nFix some regression", "[AIRFLOW-4686] Make dags Pylint compatible (#5753)The", "Add permissions for stable API (#10594) Related Github Issue: https://github.com/apache/airflow/issues/8112The", "Allow ./run_tmux.sh script to run standalone (#13420)I", "fix: dataprocpysparkjob project_id as self.project_id (#17075) set project_id as self.project_id instead of", "Adds missing mypy types (#20324) This PR adds a few missing type stub packages that we have but so far MyPy did not complain about. (It only warned about missing mypy types that it knew about.)", "[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)From", "Doc: Update Helm Chart 1.1.0 Release Date (#17244) We released it on 26th July 2021 instead of 25thThis", "Adds initial router, routes, and placeholder views (#14927) * Adds initial router, routes, and placeholder views * fix router tests - fix linting issues when router is created (#14920) * make sure initial route is set to something we want before running the test suite (#15036)* Fixes a regression that was previously causing linting issues with the test suite. Please see the new test suite test-suite for more information. * fix", "provide_session keep return type (#9787)What", "Chart: refactor webserver and flower networkpolicy (#16619) This adds support for overriding ports on the webserver and flower networkpolicies.\n\nBug Fixes\n\ndnsmasq: add DNS service status code (#16558)\n\nadd DNS service status code (#16558) gcr: improve querystring.is_invalid_domain() (#16558)\n\nimprove querystring.is_invalid_domain() (#16558) gcr, gcr8: check for CRLF/NFS files when generating DNS", "[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980)What", "Enhanced configure_environment.sh declared readonly varaible (#17619) Co-authored-by: Shraman Basyal <shramanb", "Add on_kill support for the KubernetesPodOperator (#10666) This PR ensures that when a user kills a Kubernetes pod, the operation will finish right away instead of waiting for the pod to destroy it.", "Rename second pylint pre-commit hook to distinguish it from first (#13303)This", "Fixing bug which restricted the visibility of ImportErrors (#17924)The", "Add missing values entries to Parameters in chart/README.md (#11477)The", "[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254) Breeze did not have PYTHONPATH set up for interactive Breeze (#7277) When setting the user environment to a shell, Emacs used global options which didn't work properly (#6915) Text mode", "Fixed failed pylint in master (#12938)A", "Remove duplicate docs for check-hooks-apply pre-commit (#12973)Hook", "[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625) (cherry picked from commit 87150f33e58fe5a8918c4a3f5e5f3dbd0f7c5)\n\nAdd", "[AIRFLOW-4750] Log identified zombie task instances (#5389)In", "[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595) This is how it used to be: Each DAG Run will consume a separate channel. That means: One DAG will read inputs and outputs and send output.\n\nOne DAG will read input and outputs and send output. One DAG will also consume inputs and outputs and receive output from the upstream channel.\n\nOne DAG will consume inputs and outputs and receive output from the upstream channel. This allows you to: *", "Avoid confusion in doc for CeleryKubernetesExecutor (#13116) Make the doc around CeleryKubernetesExecutor clearer", "[AIRFLOW-5406] allow spark without kubernetes (#6921)Hooks", "Remove workaround for docker-compose-failures (#18539) Long time ago we had unknown docker-compose failures that returned 254 exit code without a traceback. However now we may not always get these errors anymore. The latest commit gives us a workaround", "Strict type check for google ads and cloud hooks (#11390)A", "Dev: Clarify file naming in release verification doc (#19233)If", "Improve KubernetesPodOperator guide (#9079)A", "Small typo in JdbcOperator (#18593)With", "Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940) * Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#18438) * Update all days to follow 'month' schedule * Update all", "Fix missing dash in flag for statsd container (#10691) Co-authored-by: Kamil Olszewski <kamil.olszewski", "[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)Alfonse", "Make models/pool.py pylint compatible (#8068) * Make models/pool.py pylint compatible * Fixed for isortable/pump_tables.py * Fixed for", "[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852)The", "Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)For", "Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003)A", "Fix full_pod_spec for k8spodoperator (#12354) * Fix full_pod_spec for k8spodoperator Fixes #10894 * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support k8spodoperator on platforms requiring all of the required APIs (#12475) * Support full_parse_pod for ipa_parse_spokes (#10458) * Fix test failures for ipa_parse_spokes, ipa_skip_spokes, full_parse_spokes, full_pod, ipa_parse, ipa_jump_spokes (#11890) * Support full_pod on ipa_parse and ipa_parse-parse (#12040) * Fix full_parsepod for ipa_skip_spokes, ipa_skip_spokes, full_parse, ipa_jump_spokes (#11925) * Fix", "[AIRFLOW-6929] Add OpenAPI spec (#7549)Shen_Dragon", "[AIRFLOW-3745] Fix viewer not able to view dag details (#4569)A", "Fix typo in the word 'instance' (#10902) `instnace` -> `instance`The", "Revert \"KubernetesJobWatcher no longer inherits from Process (#11017)\" (#11065) This reverts commit 15dc5ca8e8c28cf2e5baf88b2c7ba7814e5c8f99", "[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627)The", "Don't use the `|safe` filter in code, it's risky (#9180) Most things already use the `Markup` class to encode text, you can use it to skip that step (#10228)\n\nDon't use the `|safe` filter in code, it's risky (#9180) Most", "Errors out instead of trying to workaround buggy docker-compose v2 (#16989) Docker-Compose v2 Beta has an error in the documentation that is expected to be present. #17078\n\ndocker's --continue argument has no effect when run in daemon mode (#1599)\n\nargument has no effect when run in daemon mode (#1599) When using docker-call-user with docker daemon, \"custodeserver\" is required in order to run this. (#16013)\n\nwith docker daemon, \"custodeserver\" is", "AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type`\n\nlet client_type = get_client_type(resource_type);\n\nlet resource_type = get_resource_type(client_type);\n\nlet socket_socket = io::io::from_string( & request).to_socketed( & response);\n\nlet", "Docs: Make ``DAG.is_active`` read-only in API (#17667) Add readOnly=True property on DAG entities (#17666)", "Move the contribution workflow to the beginning of the file (#10092)A", "shorten name of hook re imports of provide_session and create_session (#12936)If", "Update max_tis_per_query to better render on the webpage (#17971)A", "Fixes recent scripting breeze fix to work also with zsh (#14787) The BASH variable introduced in #14579 is not set when the scriptsignatures are being checked again or updated, which means the shell doesn't know if the scriptsignatures that are passed in to the script are still valid. This", "Fix capitalisation of boolean in config (#13569)The", "Add missing type of tests to breeze. (#18504)From", "Move backport packages to GA (#8391)The", "[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046) PR contains changes regarding AIP-21 and related parts.\n\nPR is now in place.\n\nFIXED: PR contains many bugs, notably missing /r/myapp.js module as a hook to trigger the API.\n\nPR contains", "[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232) - add tests - update README, add \"get_autocommit\" docs -", "[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070) Gojek uses Airflow as a task automation interface. These have some new and interesting commands. The new commands are:", "Updating Amazon-AWS example DAGs to use XComArgs (#16868)It", "Restore base lineage backend (#14146) This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend. closes #14141\n\ntls2tls: fix authentication check for TLS certificate (#11387) This", "Remove redundant code from breeze initialization (#9375)\"\n\n\"@bzr/@r/@lww-devel/configure", "[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210) These are still used in the API but are not part of the code.\n\n[AIRFLOW-4421]", "Allow hvac pakage installation using 'hashicorp' extra (#7915)In", "Use generic information in UpdateMask component (#13146) The UpdateMask is used in connection, pools, variables and dag. So the docs should be better at", "Add Migration guide from the experimental API to the REST API (#9771) Co-authored-by: Kaxil Naik <kaxil.natik@gmail.com> Added a new RPC", "Change KPO node_selectors warning to proper deprecationwarning (#15507) Changes the warning KPO raises when `node_selectors ` has been removed. Fixes #14759.\n\nkpoe-add-kpool-key: remove duplicate dependencies from KPOE_PREFIX_NAME", "Chart docs: Fix ``extrasecrets`` example (#16305) Found a couple places where we have incorrect examples of `extraSecrets`.\n\nAdded", "Allow passing backend_kwargs to AWS SSM client (#8802)With", "Fix kinesis test (#18337)On", "Clean up incorrect class names of Google system tests (#19956)A", "Allow viewers to see all docs links (#14197) Currently, Viewers can only see the \"Documentation\" link in the docs menu. This means that the users can't skip the docs of a webpage.", "Improve Google PubSub hook publish method (#7831)By", "Chart: Update the default Airflow version to ``2.1.2`` (#17013) Updates default Airflow version to 2.1.2 (#10151)\n\nBug Fixes\n\nFixed an issue where", "Replace deprecated module and operator in example_tasks.py (#13473) - `from airflow.operators.bash_operator import BashOperators` (#13476) - fix the use of `.shcmd()` in some commands (#13484) - do not use", "Add PGBouncer recommendation in \"setup-database' doc. (#18399) We were recommending using PGBouncer for all Postgres installations, for clarity. Now we simply recommend using PGBouncer. (#18399) Our PGCouncer recommendation now uses PGCouncer's database engine. (#18402) Our PGCouncer and PGBouncer recommendation are still used in OpenWrt-1.9.0 (#18465) When specifying OpenSSH keys in PGBouncer and /etc/pki/php-fopen.conf, we are still limited by OpenSSH keys configured in /etc/pki/. (#18485) The \"systemd -W pkg:openSSH-client\" has been disabled for the Pkgfile in v2.19. If this is not", "Add template fields to neo4j operator (#20043)In", "Fix typo in example (#13321) False should not be passed as a stringDescription", "Add new Committers to docs (#15235) Announcement Link: https://lists.apache.org/thread.html/rcc95b0e080dc6e\n\nAdded", "[AIRFLOW-5426] Adjust import path in Dataproc example (#6033)The", "Standardize default fab perms (#14946) * Add back changes. * Add custom view class tests. * Cover missing clear permission. * Add custom view classes tests. * Add a custom views test. -------------- * add a custom views test. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * add custom views tests. * add custom views tests. * fix missing clear permission. * remove obsolete test. * remove obsolete test.", "[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064) Log request.values if user has multiple forms. (#6057) Fix action_logging"]}